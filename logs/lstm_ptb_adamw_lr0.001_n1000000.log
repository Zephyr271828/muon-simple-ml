2025-12-09 12:03:41.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 9.205100059509277
2025-12-09 12:03:41.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 9.20567512512207
2025-12-09 12:03:41.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 9.20506477355957
2025-12-09 12:03:41.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 9.203628540039062
2025-12-09 12:03:41.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 9.203341484069824
2025-12-09 12:03:41.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 9.202483177185059
2025-12-09 12:03:41.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 9.201665878295898
2025-12-09 12:03:41.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 9.198931694030762
2025-12-09 12:03:41.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 9.196157455444336
2025-12-09 12:03:41.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 9.191699981689453
2025-12-09 12:03:41.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 9.18872356414795
2025-12-09 12:03:41.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 9.185067176818848
2025-12-09 12:03:41.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 9.181365013122559
2025-12-09 12:03:41.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 9.17813491821289
2025-12-09 12:03:41.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 9.165499687194824
2025-12-09 12:03:41.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 9.160609245300293
2025-12-09 12:03:41.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 9.152652740478516
2025-12-09 12:03:41.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 9.1402587890625
2025-12-09 12:03:41.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 9.119734764099121
2025-12-09 12:03:41.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 9.09855842590332
2025-12-09 12:03:41.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 9.063288688659668
2025-12-09 12:03:41.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 9.015883445739746
2025-12-09 12:03:41.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 8.971270561218262
2025-12-09 12:03:41.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 8.867826461791992
2025-12-09 12:03:41.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 8.712007522583008
2025-12-09 12:03:41.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 8.528656005859375
2025-12-09 12:03:41.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 8.327872276306152
2025-12-09 12:03:41.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 8.055398941040039
2025-12-09 12:03:41.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 7.866264820098877
2025-12-09 12:03:41.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 7.707476615905762
2025-12-09 12:03:41.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 7.461443901062012
2025-12-09 12:03:41.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 7.315182685852051
2025-12-09 12:03:41.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 7.194985866546631
2025-12-09 12:03:41.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 7.093490123748779
2025-12-09 12:03:41.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 6.897833347320557
2025-12-09 12:03:41.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 6.985199451446533
2025-12-09 12:03:41.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 6.9049482345581055
2025-12-09 12:03:41.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 6.755046844482422
2025-12-09 12:03:41.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 6.7959208488464355
2025-12-09 12:03:41.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 6.772089958190918
2025-12-09 12:03:41.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 6.8633341789245605
2025-12-09 12:03:41.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 6.763053894042969
2025-12-09 12:03:41.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 6.76893949508667
2025-12-09 12:03:41.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 6.800558567047119
2025-12-09 12:03:42.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 6.8572258949279785
2025-12-09 12:03:42.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 6.771770000457764
2025-12-09 12:03:42.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 6.872374534606934
2025-12-09 12:03:42.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 6.839547157287598
2025-12-09 12:03:42.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 6.823349475860596
2025-12-09 12:03:42.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 6.775278568267822
2025-12-09 12:03:42.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 6.756989002227783
2025-12-09 12:03:42.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 6.819493770599365
2025-12-09 12:03:42.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 6.779209613800049
2025-12-09 12:03:42.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 6.736379623413086
2025-12-09 12:03:42.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 6.837657451629639
2025-12-09 12:03:42.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 6.764193058013916
2025-12-09 12:03:42.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 6.770057201385498
2025-12-09 12:03:42.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 6.755313396453857
2025-12-09 12:03:42.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 6.792622089385986
2025-12-09 12:03:42.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 6.706943988800049
2025-12-09 12:03:42.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 6.757645130157471
2025-12-09 12:03:42.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 6.765883445739746
2025-12-09 12:03:42.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 6.758917331695557
2025-12-09 12:03:42.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 6.820470333099365
2025-12-09 12:03:42.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 6.766073226928711
2025-12-09 12:03:42.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 6.827020168304443
2025-12-09 12:03:42.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 6.78648042678833
2025-12-09 12:03:42.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 6.692943096160889
2025-12-09 12:03:42.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 6.781229496002197
2025-12-09 12:03:42.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 6.779755592346191
2025-12-09 12:03:42.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 6.80112886428833
2025-12-09 12:03:42.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 6.779358386993408
2025-12-09 12:03:42.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 6.8008904457092285
2025-12-09 12:03:42.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 6.70363712310791
2025-12-09 12:03:42.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 6.716165542602539
2025-12-09 12:03:42.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 6.757878303527832
2025-12-09 12:03:42.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 6.778686046600342
2025-12-09 12:03:42.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 6.710202693939209
2025-12-09 12:03:42.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 6.748625755310059
2025-12-09 12:03:42.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 6.75856351852417
2025-12-09 12:03:42.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 6.774251937866211
2025-12-09 12:03:42.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 6.732509613037109
2025-12-09 12:03:42.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 6.752132415771484
2025-12-09 12:03:42.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 6.773159503936768
2025-12-09 12:03:42.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 6.740392208099365
2025-12-09 12:03:42.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 6.756288528442383
2025-12-09 12:03:42.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 6.733081817626953
2025-12-09 12:03:42.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 6.736330509185791
2025-12-09 12:03:42.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 6.716757774353027
2025-12-09 12:03:42.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 6.700275897979736
2025-12-09 12:03:42.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 6.764686107635498
2025-12-09 12:03:42.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 6.800228595733643
2025-12-09 12:03:42.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 6.713230609893799
2025-12-09 12:03:42.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 6.681634902954102
2025-12-09 12:03:42.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 6.7403883934021
2025-12-09 12:03:42.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 6.774528503417969
2025-12-09 12:03:42.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 6.753606796264648
2025-12-09 12:03:42.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 6.685797691345215
2025-12-09 12:03:42.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 6.717543125152588
2025-12-09 12:03:42.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 6.723428249359131
2025-12-09 12:03:42.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009698463103929542 Training loss: 6.695141315460205
2025-12-09 12:03:42.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.000883022221559489 Training loss: 6.733541488647461
2025-12-09 12:03:42.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.00075 Training loss: 6.722756862640381
2025-12-09 12:03:42.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0005868240888334653 Training loss: 6.765988826751709
2025-12-09 12:03:42.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00041317591116653486 Training loss: 6.722109794616699
2025-12-09 12:03:42.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002500000000000001 Training loss: 6.722066879272461
2025-12-09 12:03:42.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.00011697777844051105 Training loss: 6.691293239593506
2025-12-09 12:03:42.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 3.0153689607045842e-05 Training loss: 6.661661148071289
2025-12-09 12:03:42.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 6.720914363861084
