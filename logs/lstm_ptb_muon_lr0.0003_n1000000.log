2025-12-09 12:04:40.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 9.214483261108398
2025-12-09 12:04:40.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 9.215339660644531
2025-12-09 12:04:40.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 9.215664863586426
2025-12-09 12:04:40.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 9.215073585510254
2025-12-09 12:04:40.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 9.214529991149902
2025-12-09 12:04:40.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 9.214927673339844
2025-12-09 12:04:41.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 9.21426010131836
2025-12-09 12:04:41.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 9.215067863464355
2025-12-09 12:04:41.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 9.214740753173828
2025-12-09 12:04:41.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 9.21423053741455
2025-12-09 12:04:41.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 9.214510917663574
2025-12-09 12:04:41.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 9.21479320526123
2025-12-09 12:04:41.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 9.21377182006836
2025-12-09 12:04:41.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 9.213876724243164
2025-12-09 12:04:41.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 9.2136812210083
2025-12-09 12:04:41.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 9.2135009765625
2025-12-09 12:04:41.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 9.213239669799805
2025-12-09 12:04:41.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 9.212871551513672
2025-12-09 12:04:41.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 9.212533950805664
2025-12-09 12:04:41.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 9.212224960327148
2025-12-09 12:04:41.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 9.212156295776367
2025-12-09 12:04:41.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 9.211153030395508
2025-12-09 12:04:41.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 9.210943222045898
2025-12-09 12:04:41.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 9.210622787475586
2025-12-09 12:04:41.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 9.210515022277832
2025-12-09 12:04:41.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 9.209932327270508
2025-12-09 12:04:41.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 9.20986557006836
2025-12-09 12:04:41.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 9.208999633789062
2025-12-09 12:04:41.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 9.208955764770508
2025-12-09 12:04:41.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 9.207841873168945
2025-12-09 12:04:41.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 9.207893371582031
2025-12-09 12:04:41.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 9.207262992858887
2025-12-09 12:04:41.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 9.206971168518066
2025-12-09 12:04:41.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 9.206489562988281
2025-12-09 12:04:41.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 9.205711364746094
2025-12-09 12:04:41.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 9.205302238464355
2025-12-09 12:04:41.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 9.204537391662598
2025-12-09 12:04:41.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 9.204504013061523
2025-12-09 12:04:41.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 9.203428268432617
2025-12-09 12:04:41.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 9.2028226852417
2025-12-09 12:04:41.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 9.202515602111816
2025-12-09 12:04:41.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 9.20123291015625
2025-12-09 12:04:41.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 9.200417518615723
2025-12-09 12:04:41.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 9.199799537658691
2025-12-09 12:04:41.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 9.199917793273926
2025-12-09 12:04:41.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 9.198537826538086
2025-12-09 12:04:41.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 9.197699546813965
2025-12-09 12:04:41.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 9.196616172790527
2025-12-09 12:04:41.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 9.195481300354004
2025-12-09 12:04:41.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 9.194993019104004
2025-12-09 12:04:41.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 9.194281578063965
2025-12-09 12:04:41.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 9.193095207214355
2025-12-09 12:04:41.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 9.193645477294922
2025-12-09 12:04:41.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 9.191280364990234
2025-12-09 12:04:41.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 9.19084358215332
2025-12-09 12:04:41.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 9.190627098083496
2025-12-09 12:04:41.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 9.188633918762207
2025-12-09 12:04:41.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 9.18891429901123
2025-12-09 12:04:41.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 9.187501907348633
2025-12-09 12:04:41.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 9.18478775024414
2025-12-09 12:04:42.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 9.186189651489258
2025-12-09 12:04:42.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 9.183385848999023
2025-12-09 12:04:42.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 9.183296203613281
2025-12-09 12:04:42.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 9.18134880065918
2025-12-09 12:04:42.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 9.181385040283203
2025-12-09 12:04:42.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 9.18113899230957
2025-12-09 12:04:42.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 9.179003715515137
2025-12-09 12:04:42.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 9.176730155944824
2025-12-09 12:04:42.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 9.17731761932373
2025-12-09 12:04:42.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 9.175856590270996
2025-12-09 12:04:42.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 9.173502922058105
2025-12-09 12:04:42.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 9.173240661621094
2025-12-09 12:04:42.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 9.170836448669434
2025-12-09 12:04:42.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 9.169829368591309
2025-12-09 12:04:42.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 9.16817855834961
2025-12-09 12:04:42.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 9.165240287780762
2025-12-09 12:04:42.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 9.165356636047363
2025-12-09 12:04:42.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 9.165140151977539
2025-12-09 12:04:42.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 9.164169311523438
2025-12-09 12:04:42.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 9.15903091430664
2025-12-09 12:04:42.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 9.15799331665039
2025-12-09 12:04:42.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 9.157147407531738
2025-12-09 12:04:42.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 9.156033515930176
2025-12-09 12:04:42.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 9.152589797973633
2025-12-09 12:04:42.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 9.152776718139648
2025-12-09 12:04:42.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 9.149876594543457
2025-12-09 12:04:42.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 9.146342277526855
2025-12-09 12:04:42.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 9.149504661560059
2025-12-09 12:04:42.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 9.145686149597168
2025-12-09 12:04:42.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 9.142090797424316
2025-12-09 12:04:42.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 9.141707420349121
2025-12-09 12:04:42.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 9.138299942016602
2025-12-09 12:04:42.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 9.135589599609375
2025-12-09 12:04:42.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 9.138262748718262
2025-12-09 12:04:42.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 9.132377624511719
2025-12-09 12:04:42.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 9.133572578430176
2025-12-09 12:04:42.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 9.129109382629395
2025-12-09 12:04:42.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 9.12651538848877
2025-12-09 12:04:42.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 9.12702751159668
2025-12-09 12:04:42.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 9.118846893310547
2025-12-09 12:04:42.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0002909538931178862 Training loss: 9.120482444763184
2025-12-09 12:04:42.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00026490666646784665 Training loss: 9.123991012573242
2025-12-09 12:04:42.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000225 Training loss: 9.120641708374023
2025-12-09 12:04:42.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00017604722665003956 Training loss: 9.10910415649414
2025-12-09 12:04:42.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00012395277334996044 Training loss: 9.115778923034668
2025-12-09 12:04:42.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 7.500000000000002e-05 Training loss: 9.111594200134277
2025-12-09 12:04:42.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 3.509333353215331e-05 Training loss: 9.11138916015625
2025-12-09 12:04:42.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113751e-06 Training loss: 9.11166763305664
2025-12-09 12:04:42.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 9.113990783691406
