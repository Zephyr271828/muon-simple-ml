Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:03:10.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 9.209024429321289
2025-12-09 12:03:10.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 9.209566116333008
2025-12-09 12:03:10.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 9.209246635437012
2025-12-09 12:03:10.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 9.20930004119873
2025-12-09 12:03:10.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 9.210488319396973
2025-12-09 12:03:10.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 9.209168434143066
2025-12-09 12:03:10.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 9.209073066711426
2025-12-09 12:03:10.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 9.208855628967285
2025-12-09 12:03:10.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 9.208351135253906
2025-12-09 12:03:10.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 9.20811939239502
2025-12-09 12:03:10.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 9.20893383026123
2025-12-09 12:03:10.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 9.207524299621582
2025-12-09 12:03:11.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 9.207283020019531
2025-12-09 12:03:11.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 9.207274436950684
2025-12-09 12:03:11.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 9.206236839294434
2025-12-09 12:03:11.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 9.206640243530273
2025-12-09 12:03:11.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 9.205229759216309
2025-12-09 12:03:11.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 9.205496788024902
2025-12-09 12:03:11.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 9.204668998718262
2025-12-09 12:03:11.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 9.20315170288086
2025-12-09 12:03:11.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 9.202880859375
2025-12-09 12:03:11.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 9.201762199401855
2025-12-09 12:03:11.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 9.20173168182373
2025-12-09 12:03:11.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 9.200715065002441
2025-12-09 12:03:11.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 9.199572563171387
2025-12-09 12:03:11.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 9.199329376220703
2025-12-09 12:03:11.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 9.198172569274902
2025-12-09 12:03:11.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 9.196495056152344
2025-12-09 12:03:11.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 9.195527076721191
2025-12-09 12:03:11.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 9.195600509643555
2025-12-09 12:03:11.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 9.195064544677734
2025-12-09 12:03:11.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 9.19383430480957
2025-12-09 12:03:11.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 9.191217422485352
2025-12-09 12:03:11.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 9.186532020568848
2025-12-09 12:03:11.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 9.187793731689453
2025-12-09 12:03:11.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 9.188499450683594
2025-12-09 12:03:11.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 9.187067031860352
2025-12-09 12:03:11.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 9.183317184448242
2025-12-09 12:03:11.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 9.1834135055542
2025-12-09 12:03:11.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 9.180062294006348
2025-12-09 12:03:11.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 9.177169799804688
2025-12-09 12:03:11.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 9.176996231079102
2025-12-09 12:03:11.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 9.171857833862305
2025-12-09 12:03:11.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 9.172595977783203
2025-12-09 12:03:11.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 9.16873836517334
2025-12-09 12:03:11.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 9.160918235778809
2025-12-09 12:03:11.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 9.160917282104492
2025-12-09 12:03:11.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 9.158665657043457
2025-12-09 12:03:11.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 9.154533386230469
2025-12-09 12:03:11.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 9.152827262878418
2025-12-09 12:03:11.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 9.14614486694336
2025-12-09 12:03:11.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 9.14345932006836
2025-12-09 12:03:11.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 9.139439582824707
2025-12-09 12:03:11.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 9.135503768920898
2025-12-09 12:03:11.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 9.1261568069458
2025-12-09 12:03:11.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 9.118470191955566
2025-12-09 12:03:11.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 9.109370231628418
2025-12-09 12:03:11.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 9.096487045288086
2025-12-09 12:03:11.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 9.089252471923828
2025-12-09 12:03:11.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 9.055608749389648
2025-12-09 12:03:11.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 9.048818588256836
2025-12-09 12:03:11.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 9.042821884155273
2025-12-09 12:03:11.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 9.013386726379395
2025-12-09 12:03:11.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 8.993734359741211
2025-12-09 12:03:11.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 8.981754302978516
2025-12-09 12:03:11.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 8.9314603805542
2025-12-09 12:03:11.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 8.889989852905273
2025-12-09 12:03:11.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 8.833755493164062
2025-12-09 12:03:11.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 8.782258987426758
2025-12-09 12:03:11.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 8.69974422454834
2025-12-09 12:03:11.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 8.676621437072754
2025-12-09 12:03:11.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 8.597393989562988
2025-12-09 12:03:11.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 8.433536529541016
2025-12-09 12:03:11.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 8.402475357055664
2025-12-09 12:03:11.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 8.294014930725098
2025-12-09 12:03:12.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 8.235062599182129
2025-12-09 12:03:12.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 8.12389087677002
2025-12-09 12:03:12.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 8.055445671081543
2025-12-09 12:03:12.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 7.967375755310059
2025-12-09 12:03:12.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 7.882314682006836
2025-12-09 12:03:12.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 7.803153991699219
2025-12-09 12:03:12.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 7.746076583862305
2025-12-09 12:03:12.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 7.633280277252197
2025-12-09 12:03:12.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 7.593998908996582
2025-12-09 12:03:12.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 7.538854122161865
2025-12-09 12:03:12.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 7.461554527282715
2025-12-09 12:03:12.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 7.364379405975342
2025-12-09 12:03:12.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 7.399495601654053
2025-12-09 12:03:12.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 7.31929349899292
2025-12-09 12:03:12.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 7.257993221282959
2025-12-09 12:03:12.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 7.280298709869385
2025-12-09 12:03:12.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 7.212914943695068
2025-12-09 12:03:12.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 7.143744945526123
2025-12-09 12:03:12.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 7.169068336486816
2025-12-09 12:03:12.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 7.079456806182861
2025-12-09 12:03:12.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 7.08006477355957
2025-12-09 12:03:12.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 7.021658420562744
2025-12-09 12:03:12.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 7.015956401824951
2025-12-09 12:03:12.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 6.952155113220215
2025-12-09 12:03:12.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 6.956890106201172
2025-12-09 12:03:12.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.698463103929542e-05 Training loss: 6.95953369140625
2025-12-09 12:03:12.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 8.83022221559489e-05 Training loss: 6.933084011077881
2025-12-09 12:03:12.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 7.500000000000001e-05 Training loss: 6.908402919769287
2025-12-09 12:03:12.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 5.868240888334653e-05 Training loss: 6.955794334411621
2025-12-09 12:03:12.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 4.131759111665349e-05 Training loss: 6.827764511108398
2025-12-09 12:03:12.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 2.500000000000001e-05 Training loss: 6.91732120513916
2025-12-09 12:03:12.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 1.1697777844051105e-05 Training loss: 6.860717296600342
2025-12-09 12:03:12.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 3.0153689607045845e-06 Training loss: 6.893388748168945
2025-12-09 12:03:12.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 6.77175760269165
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:03:26.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 9.210470199584961
2025-12-09 12:03:26.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 9.208928108215332
2025-12-09 12:03:26.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 9.208910942077637
2025-12-09 12:03:26.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 9.209657669067383
2025-12-09 12:03:26.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 9.20874309539795
2025-12-09 12:03:26.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 9.207987785339355
2025-12-09 12:03:26.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 9.207764625549316
2025-12-09 12:03:26.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 9.20699691772461
2025-12-09 12:03:26.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 9.206282615661621
2025-12-09 12:03:26.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 9.20443058013916
2025-12-09 12:03:26.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 9.20409870147705
2025-12-09 12:03:26.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 9.203482627868652
2025-12-09 12:03:26.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 9.202958106994629
2025-12-09 12:03:26.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 9.201371192932129
2025-12-09 12:03:26.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 9.199092864990234
2025-12-09 12:03:26.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 9.197945594787598
2025-12-09 12:03:26.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 9.197216987609863
2025-12-09 12:03:26.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 9.193087577819824
2025-12-09 12:03:26.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 9.19147777557373
2025-12-09 12:03:26.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 9.19011402130127
2025-12-09 12:03:26.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 9.186257362365723
2025-12-09 12:03:26.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 9.183893203735352
2025-12-09 12:03:26.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 9.181035995483398
2025-12-09 12:03:26.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 9.174363136291504
2025-12-09 12:03:26.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 9.171165466308594
2025-12-09 12:03:26.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 9.172521591186523
2025-12-09 12:03:26.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 9.166775703430176
2025-12-09 12:03:26.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 9.160114288330078
2025-12-09 12:03:26.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 9.148968696594238
2025-12-09 12:03:26.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 9.15009880065918
2025-12-09 12:03:26.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 9.137142181396484
2025-12-09 12:03:26.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 9.127141952514648
2025-12-09 12:03:26.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 9.114133834838867
2025-12-09 12:03:26.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 9.105562210083008
2025-12-09 12:03:26.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 9.09489631652832
2025-12-09 12:03:26.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 9.073454856872559
2025-12-09 12:03:26.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 9.032629013061523
2025-12-09 12:03:26.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 9.019078254699707
2025-12-09 12:03:26.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 8.975287437438965
2025-12-09 12:03:26.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 8.92182731628418
2025-12-09 12:03:26.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 8.876191139221191
2025-12-09 12:03:26.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 8.774328231811523
2025-12-09 12:03:26.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 8.689545631408691
2025-12-09 12:03:26.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 8.600035667419434
2025-12-09 12:03:26.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 8.386239051818848
2025-12-09 12:03:26.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 8.310145378112793
2025-12-09 12:03:26.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 8.169756889343262
2025-12-09 12:03:26.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 8.013375282287598
2025-12-09 12:03:26.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 7.903594017028809
2025-12-09 12:03:26.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 7.7548933029174805
2025-12-09 12:03:26.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 7.615947246551514
2025-12-09 12:03:27.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 7.54857063293457
2025-12-09 12:03:27.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 7.4529218673706055
2025-12-09 12:03:27.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 7.324344158172607
2025-12-09 12:03:27.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 7.182264804840088
2025-12-09 12:03:27.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 7.146724224090576
2025-12-09 12:03:27.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 7.122191905975342
2025-12-09 12:03:27.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 7.097362518310547
2025-12-09 12:03:27.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 7.024107933044434
2025-12-09 12:03:27.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 7.0494794845581055
2025-12-09 12:03:27.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 6.930126190185547
2025-12-09 12:03:27.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 6.96366548538208
2025-12-09 12:03:27.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 6.939218521118164
2025-12-09 12:03:27.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 6.836644172668457
2025-12-09 12:03:27.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 6.829689025878906
2025-12-09 12:03:27.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 6.85658597946167
2025-12-09 12:03:27.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 6.848033905029297
2025-12-09 12:03:27.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 6.7638115882873535
2025-12-09 12:03:27.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 6.802125930786133
2025-12-09 12:03:27.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 6.7989912033081055
2025-12-09 12:03:27.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 6.731551170349121
2025-12-09 12:03:27.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 6.764561176300049
2025-12-09 12:03:27.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 6.73665714263916
2025-12-09 12:03:27.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 6.734529972076416
2025-12-09 12:03:27.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 6.733067989349365
2025-12-09 12:03:27.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 6.747648239135742
2025-12-09 12:03:27.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 6.638643741607666
2025-12-09 12:03:27.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 6.7759881019592285
2025-12-09 12:03:27.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 6.662108898162842
2025-12-09 12:03:27.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 6.73314905166626
2025-12-09 12:03:27.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 6.766542911529541
2025-12-09 12:03:27.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 6.757148265838623
2025-12-09 12:03:27.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 6.713791370391846
2025-12-09 12:03:27.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 6.674701690673828
2025-12-09 12:03:27.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 6.756387233734131
2025-12-09 12:03:27.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 6.719552516937256
2025-12-09 12:03:27.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 6.729499816894531
2025-12-09 12:03:27.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 6.731727600097656
2025-12-09 12:03:27.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 6.671203136444092
2025-12-09 12:03:27.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 6.730409145355225
2025-12-09 12:03:27.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 6.7323808670043945
2025-12-09 12:03:27.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 6.789694309234619
2025-12-09 12:03:27.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 6.720399379730225
2025-12-09 12:03:27.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 6.737796783447266
2025-12-09 12:03:27.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 6.717953205108643
2025-12-09 12:03:27.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 6.679248332977295
2025-12-09 12:03:27.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 6.749728202819824
2025-12-09 12:03:27.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 6.771157741546631
2025-12-09 12:03:27.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 6.806411266326904
2025-12-09 12:03:27.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 6.672481536865234
2025-12-09 12:03:27.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0002909538931178862 Training loss: 6.727003574371338
2025-12-09 12:03:27.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00026490666646784665 Training loss: 6.758341312408447
2025-12-09 12:03:27.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000225 Training loss: 6.681698799133301
2025-12-09 12:03:27.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00017604722665003956 Training loss: 6.7508544921875
2025-12-09 12:03:27.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00012395277334996044 Training loss: 6.7509965896606445
2025-12-09 12:03:27.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 7.500000000000002e-05 Training loss: 6.7605509757995605
2025-12-09 12:03:27.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 3.509333353215331e-05 Training loss: 6.71946907043457
2025-12-09 12:03:27.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113751e-06 Training loss: 6.731075286865234
2025-12-09 12:03:27.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 6.7166900634765625
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:03:41.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 9.205100059509277
2025-12-09 12:03:41.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 9.20567512512207
2025-12-09 12:03:41.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 9.20506477355957
2025-12-09 12:03:41.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 9.203628540039062
2025-12-09 12:03:41.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 9.203341484069824
2025-12-09 12:03:41.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 9.202483177185059
2025-12-09 12:03:41.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 9.201665878295898
2025-12-09 12:03:41.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 9.198931694030762
2025-12-09 12:03:41.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 9.196157455444336
2025-12-09 12:03:41.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 9.191699981689453
2025-12-09 12:03:41.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 9.18872356414795
2025-12-09 12:03:41.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 9.185067176818848
2025-12-09 12:03:41.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 9.181365013122559
2025-12-09 12:03:41.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 9.17813491821289
2025-12-09 12:03:41.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 9.165499687194824
2025-12-09 12:03:41.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 9.160609245300293
2025-12-09 12:03:41.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 9.152652740478516
2025-12-09 12:03:41.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 9.1402587890625
2025-12-09 12:03:41.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 9.119734764099121
2025-12-09 12:03:41.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 9.09855842590332
2025-12-09 12:03:41.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 9.063288688659668
2025-12-09 12:03:41.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 9.015883445739746
2025-12-09 12:03:41.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 8.971270561218262
2025-12-09 12:03:41.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 8.867826461791992
2025-12-09 12:03:41.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 8.712007522583008
2025-12-09 12:03:41.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 8.528656005859375
2025-12-09 12:03:41.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 8.327872276306152
2025-12-09 12:03:41.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 8.055398941040039
2025-12-09 12:03:41.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 7.866264820098877
2025-12-09 12:03:41.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 7.707476615905762
2025-12-09 12:03:41.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 7.461443901062012
2025-12-09 12:03:41.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 7.315182685852051
2025-12-09 12:03:41.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 7.194985866546631
2025-12-09 12:03:41.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 7.093490123748779
2025-12-09 12:03:41.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 6.897833347320557
2025-12-09 12:03:41.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 6.985199451446533
2025-12-09 12:03:41.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 6.9049482345581055
2025-12-09 12:03:41.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 6.755046844482422
2025-12-09 12:03:41.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 6.7959208488464355
2025-12-09 12:03:41.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 6.772089958190918
2025-12-09 12:03:41.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 6.8633341789245605
2025-12-09 12:03:41.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 6.763053894042969
2025-12-09 12:03:41.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 6.76893949508667
2025-12-09 12:03:41.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 6.800558567047119
2025-12-09 12:03:42.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 6.8572258949279785
2025-12-09 12:03:42.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 6.771770000457764
2025-12-09 12:03:42.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 6.872374534606934
2025-12-09 12:03:42.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 6.839547157287598
2025-12-09 12:03:42.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 6.823349475860596
2025-12-09 12:03:42.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 6.775278568267822
2025-12-09 12:03:42.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 6.756989002227783
2025-12-09 12:03:42.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 6.819493770599365
2025-12-09 12:03:42.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 6.779209613800049
2025-12-09 12:03:42.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 6.736379623413086
2025-12-09 12:03:42.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 6.837657451629639
2025-12-09 12:03:42.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 6.764193058013916
2025-12-09 12:03:42.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 6.770057201385498
2025-12-09 12:03:42.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 6.755313396453857
2025-12-09 12:03:42.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 6.792622089385986
2025-12-09 12:03:42.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 6.706943988800049
2025-12-09 12:03:42.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 6.757645130157471
2025-12-09 12:03:42.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 6.765883445739746
2025-12-09 12:03:42.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 6.758917331695557
2025-12-09 12:03:42.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 6.820470333099365
2025-12-09 12:03:42.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 6.766073226928711
2025-12-09 12:03:42.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 6.827020168304443
2025-12-09 12:03:42.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 6.78648042678833
2025-12-09 12:03:42.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 6.692943096160889
2025-12-09 12:03:42.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 6.781229496002197
2025-12-09 12:03:42.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 6.779755592346191
2025-12-09 12:03:42.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 6.80112886428833
2025-12-09 12:03:42.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 6.779358386993408
2025-12-09 12:03:42.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 6.8008904457092285
2025-12-09 12:03:42.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 6.70363712310791
2025-12-09 12:03:42.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 6.716165542602539
2025-12-09 12:03:42.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 6.757878303527832
2025-12-09 12:03:42.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 6.778686046600342
2025-12-09 12:03:42.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 6.710202693939209
2025-12-09 12:03:42.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 6.748625755310059
2025-12-09 12:03:42.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 6.75856351852417
2025-12-09 12:03:42.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 6.774251937866211
2025-12-09 12:03:42.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 6.732509613037109
2025-12-09 12:03:42.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 6.752132415771484
2025-12-09 12:03:42.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 6.773159503936768
2025-12-09 12:03:42.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 6.740392208099365
2025-12-09 12:03:42.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 6.756288528442383
2025-12-09 12:03:42.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 6.733081817626953
2025-12-09 12:03:42.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 6.736330509185791
2025-12-09 12:03:42.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 6.716757774353027
2025-12-09 12:03:42.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 6.700275897979736
2025-12-09 12:03:42.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 6.764686107635498
2025-12-09 12:03:42.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 6.800228595733643
2025-12-09 12:03:42.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 6.713230609893799
2025-12-09 12:03:42.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 6.681634902954102
2025-12-09 12:03:42.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 6.7403883934021
2025-12-09 12:03:42.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 6.774528503417969
2025-12-09 12:03:42.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 6.753606796264648
2025-12-09 12:03:42.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 6.685797691345215
2025-12-09 12:03:42.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 6.717543125152588
2025-12-09 12:03:42.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 6.723428249359131
2025-12-09 12:03:42.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009698463103929542 Training loss: 6.695141315460205
2025-12-09 12:03:42.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.000883022221559489 Training loss: 6.733541488647461
2025-12-09 12:03:42.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.00075 Training loss: 6.722756862640381
2025-12-09 12:03:42.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0005868240888334653 Training loss: 6.765988826751709
2025-12-09 12:03:42.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00041317591116653486 Training loss: 6.722109794616699
2025-12-09 12:03:42.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002500000000000001 Training loss: 6.722066879272461
2025-12-09 12:03:42.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.00011697777844051105 Training loss: 6.691293239593506
2025-12-09 12:03:42.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 3.0153689607045842e-05 Training loss: 6.661661148071289
2025-12-09 12:03:42.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 6.720914363861084
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:03:55.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 9.209250450134277
2025-12-09 12:03:55.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 9.20848274230957
2025-12-09 12:03:55.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 9.207980155944824
2025-12-09 12:03:55.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 9.20474624633789
2025-12-09 12:03:55.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 9.203522682189941
2025-12-09 12:03:55.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 9.1973237991333
2025-12-09 12:03:55.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 9.193097114562988
2025-12-09 12:03:55.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 9.186467170715332
2025-12-09 12:03:56.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 9.171623229980469
2025-12-09 12:03:56.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 9.165823936462402
2025-12-09 12:03:56.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 9.147723197937012
2025-12-09 12:03:56.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 9.12002182006836
2025-12-09 12:03:56.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 9.07490348815918
2025-12-09 12:03:56.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 9.018562316894531
2025-12-09 12:03:56.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 8.900736808776855
2025-12-09 12:03:56.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 8.71423053741455
2025-12-09 12:03:56.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 8.349832534790039
2025-12-09 12:03:56.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 8.009297370910645
2025-12-09 12:03:56.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 7.682666301727295
2025-12-09 12:03:56.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 7.441252708435059
2025-12-09 12:03:56.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 7.2086310386657715
2025-12-09 12:03:56.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 7.047186374664307
2025-12-09 12:03:56.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 6.9445977210998535
2025-12-09 12:03:56.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 6.835400581359863
2025-12-09 12:03:56.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 6.8048481941223145
2025-12-09 12:03:56.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 6.724974155426025
2025-12-09 12:03:56.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 6.831225395202637
2025-12-09 12:03:56.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 6.839715003967285
2025-12-09 12:03:56.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 6.90850830078125
2025-12-09 12:03:56.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 6.938464164733887
2025-12-09 12:03:56.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 6.915770053863525
2025-12-09 12:03:56.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 6.964052677154541
2025-12-09 12:03:56.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 6.912509441375732
2025-12-09 12:03:56.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 6.889625072479248
2025-12-09 12:03:56.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 6.869534492492676
2025-12-09 12:03:56.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 6.830481052398682
2025-12-09 12:03:56.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 6.824329376220703
2025-12-09 12:03:56.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 6.865135669708252
2025-12-09 12:03:56.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 6.821558475494385
2025-12-09 12:03:56.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 6.868709564208984
2025-12-09 12:03:56.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 6.758964538574219
2025-12-09 12:03:56.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 6.848674774169922
2025-12-09 12:03:56.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 6.837765693664551
2025-12-09 12:03:56.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 6.7941575050354
2025-12-09 12:03:56.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 6.781761646270752
2025-12-09 12:03:56.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 6.87875509262085
2025-12-09 12:03:56.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 6.84808349609375
2025-12-09 12:03:56.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 6.725821495056152
2025-12-09 12:03:56.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 6.849806308746338
2025-12-09 12:03:56.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 6.812334060668945
2025-12-09 12:03:56.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 6.855916976928711
2025-12-09 12:03:56.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 6.839410781860352
2025-12-09 12:03:56.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 6.864078998565674
2025-12-09 12:03:56.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 6.817417144775391
2025-12-09 12:03:56.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 6.856346607208252
2025-12-09 12:03:56.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 6.741355895996094
2025-12-09 12:03:56.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 6.775213718414307
2025-12-09 12:03:56.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 6.804570198059082
2025-12-09 12:03:56.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 6.855913162231445
2025-12-09 12:03:56.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 6.832452297210693
2025-12-09 12:03:56.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 6.7736711502075195
2025-12-09 12:03:56.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 6.809383392333984
2025-12-09 12:03:56.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 6.820450305938721
2025-12-09 12:03:56.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 6.725386142730713
2025-12-09 12:03:56.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 6.781227111816406
2025-12-09 12:03:56.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 6.833776473999023
2025-12-09 12:03:56.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 6.7936601638793945
2025-12-09 12:03:56.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 6.870271682739258
2025-12-09 12:03:56.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 6.842087268829346
2025-12-09 12:03:56.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 6.870007038116455
2025-12-09 12:03:56.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 6.744718074798584
2025-12-09 12:03:56.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 6.820641994476318
2025-12-09 12:03:57.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 6.795774459838867
2025-12-09 12:03:57.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 6.801806449890137
2025-12-09 12:03:57.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 6.840578556060791
2025-12-09 12:03:57.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 6.816405296325684
2025-12-09 12:03:57.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 6.734302043914795
2025-12-09 12:03:57.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 6.85374641418457
2025-12-09 12:03:57.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 6.865849018096924
2025-12-09 12:03:57.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 6.731993198394775
2025-12-09 12:03:57.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 6.861001014709473
2025-12-09 12:03:57.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 6.857686996459961
2025-12-09 12:03:57.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 6.808369159698486
2025-12-09 12:03:57.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 6.776035308837891
2025-12-09 12:03:57.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 6.829299449920654
2025-12-09 12:03:57.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 6.852065086364746
2025-12-09 12:03:57.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 6.776773452758789
2025-12-09 12:03:57.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 6.8312883377075195
2025-12-09 12:03:57.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 6.8360795974731445
2025-12-09 12:03:57.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 6.83233118057251
2025-12-09 12:03:57.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 6.855138778686523
2025-12-09 12:03:57.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 6.865377426147461
2025-12-09 12:03:57.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 6.857497692108154
2025-12-09 12:03:57.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 6.861233234405518
2025-12-09 12:03:57.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 6.8348612785339355
2025-12-09 12:03:57.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 6.909553050994873
2025-12-09 12:03:57.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 6.843113422393799
2025-12-09 12:03:57.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 6.909501075744629
2025-12-09 12:03:57.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 6.957088947296143
2025-12-09 12:03:57.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 6.965234756469727
2025-12-09 12:03:57.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.002909538931178863 Training loss: 6.870306968688965
2025-12-09 12:03:57.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002649066664678467 Training loss: 6.826803684234619
2025-12-09 12:03:57.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0022500000000000003 Training loss: 6.840785026550293
2025-12-09 12:03:57.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0017604722665003959 Training loss: 6.921166896820068
2025-12-09 12:03:57.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0012395277334996046 Training loss: 6.814672470092773
2025-12-09 12:03:57.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0007500000000000003 Training loss: 6.756309986114502
2025-12-09 12:03:57.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0003509333353215332 Training loss: 6.897526741027832
2025-12-09 12:03:57.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113752e-05 Training loss: 6.835457801818848
2025-12-09 12:03:57.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 6.666847229003906
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:04:10.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 9.20436954498291
2025-12-09 12:04:10.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 9.203845024108887
2025-12-09 12:04:10.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 9.20042896270752
2025-12-09 12:04:10.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 9.194469451904297
2025-12-09 12:04:10.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 9.180693626403809
2025-12-09 12:04:10.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 9.162062644958496
2025-12-09 12:04:10.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 9.132651329040527
2025-12-09 12:04:10.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 9.07766056060791
2025-12-09 12:04:11.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 8.891289710998535
2025-12-09 12:04:11.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 8.499991416931152
2025-12-09 12:04:11.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 7.954373359680176
2025-12-09 12:04:11.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 7.4146528244018555
2025-12-09 12:04:11.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 7.085751533508301
2025-12-09 12:04:11.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 6.868657112121582
2025-12-09 12:04:11.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 6.723479270935059
2025-12-09 12:04:11.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 6.896402835845947
2025-12-09 12:04:11.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 6.888359069824219
2025-12-09 12:04:11.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 6.947624206542969
2025-12-09 12:04:11.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 7.029740333557129
2025-12-09 12:04:11.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 6.945425987243652
2025-12-09 12:04:11.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 7.117844104766846
2025-12-09 12:04:11.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 7.098738670349121
2025-12-09 12:04:11.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 6.995744705200195
2025-12-09 12:04:11.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 6.912787914276123
2025-12-09 12:04:11.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 7.010916709899902
2025-12-09 12:04:11.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 7.068184852600098
2025-12-09 12:04:11.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 6.990671157836914
2025-12-09 12:04:11.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 7.010786533355713
2025-12-09 12:04:11.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 7.026779651641846
2025-12-09 12:04:11.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 7.101102352142334
2025-12-09 12:04:11.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 6.967899799346924
2025-12-09 12:04:11.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 6.976363182067871
2025-12-09 12:04:11.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 6.982227325439453
2025-12-09 12:04:11.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 7.084316730499268
2025-12-09 12:04:11.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 6.991450786590576
2025-12-09 12:04:11.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 7.084311008453369
2025-12-09 12:04:11.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 7.11817741394043
2025-12-09 12:04:11.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 7.11536979675293
2025-12-09 12:04:11.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 7.074993133544922
2025-12-09 12:04:11.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 7.002956390380859
2025-12-09 12:04:11.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 7.003382205963135
2025-12-09 12:04:11.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 7.089687824249268
2025-12-09 12:04:11.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 6.993752479553223
2025-12-09 12:04:11.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 7.113613128662109
2025-12-09 12:04:11.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 7.114232540130615
2025-12-09 12:04:11.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 7.07330846786499
2025-12-09 12:04:11.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 7.033651351928711
2025-12-09 12:04:11.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 7.100823879241943
2025-12-09 12:04:11.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 7.105159282684326
2025-12-09 12:04:11.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 7.109808921813965
2025-12-09 12:04:11.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 6.962384223937988
2025-12-09 12:04:11.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 7.110578536987305
2025-12-09 12:04:11.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 7.009326457977295
2025-12-09 12:04:11.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 7.04449462890625
2025-12-09 12:04:11.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 7.035348892211914
2025-12-09 12:04:11.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 7.035664081573486
2025-12-09 12:04:11.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 7.099505424499512
2025-12-09 12:04:11.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 7.099474906921387
2025-12-09 12:04:11.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 7.080123424530029
2025-12-09 12:04:11.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 7.000962257385254
2025-12-09 12:04:11.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 7.079909801483154
2025-12-09 12:04:11.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 7.1234822273254395
2025-12-09 12:04:11.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 7.1272292137146
2025-12-09 12:04:11.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 7.108482360839844
2025-12-09 12:04:11.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 7.038355350494385
2025-12-09 12:04:11.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 7.2524518966674805
2025-12-09 12:04:11.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 7.086324691772461
2025-12-09 12:04:11.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 7.112554550170898
2025-12-09 12:04:11.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 7.168956279754639
2025-12-09 12:04:11.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 7.162772178649902
2025-12-09 12:04:11.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 7.146013259887695
2025-12-09 12:04:11.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 7.034452438354492
2025-12-09 12:04:12.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 7.187313556671143
2025-12-09 12:04:12.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 7.225828170776367
2025-12-09 12:04:12.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 7.155353546142578
2025-12-09 12:04:12.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 7.204015254974365
2025-12-09 12:04:12.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 7.180032730102539
2025-12-09 12:04:12.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 7.123750686645508
2025-12-09 12:04:12.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 7.299188137054443
2025-12-09 12:04:12.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 7.265347480773926
2025-12-09 12:04:12.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 7.275278568267822
2025-12-09 12:04:12.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 7.20993709564209
2025-12-09 12:04:12.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 7.249151229858398
2025-12-09 12:04:12.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 7.316743850708008
2025-12-09 12:04:12.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 7.258840560913086
2025-12-09 12:04:12.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 7.203760147094727
2025-12-09 12:04:12.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 7.189537525177002
2025-12-09 12:04:12.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 7.353734016418457
2025-12-09 12:04:12.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 7.278975963592529
2025-12-09 12:04:12.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 7.238102436065674
2025-12-09 12:04:12.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 7.2588911056518555
2025-12-09 12:04:12.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 7.321264743804932
2025-12-09 12:04:12.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 7.462705612182617
2025-12-09 12:04:12.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 7.317253112792969
2025-12-09 12:04:12.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 7.360952854156494
2025-12-09 12:04:12.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 7.3530378341674805
2025-12-09 12:04:12.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 7.305195331573486
2025-12-09 12:04:12.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 7.292384624481201
2025-12-09 12:04:12.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 7.310821056365967
2025-12-09 12:04:12.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 7.439445495605469
2025-12-09 12:04:12.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009698463103929543 Training loss: 7.373385906219482
2025-12-09 12:04:12.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00883022221559489 Training loss: 7.380871772766113
2025-12-09 12:04:12.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0075 Training loss: 7.446134090423584
2025-12-09 12:04:12.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.005868240888334653 Training loss: 7.297813415527344
2025-12-09 12:04:12.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0041317591116653484 Training loss: 7.462462902069092
2025-12-09 12:04:12.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0025000000000000014 Training loss: 7.315382480621338
2025-12-09 12:04:12.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0011697777844051104 Training loss: 7.153946399688721
2025-12-09 12:04:12.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00030153689607045843 Training loss: 7.102310657501221
2025-12-09 12:04:12.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 7.129591464996338
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:04:25.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 9.209644317626953
2025-12-09 12:04:25.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 9.209714889526367
2025-12-09 12:04:25.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 9.20905876159668
2025-12-09 12:04:25.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 9.209120750427246
2025-12-09 12:04:25.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 9.209829330444336
2025-12-09 12:04:25.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 9.209404945373535
2025-12-09 12:04:25.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 9.20901870727539
2025-12-09 12:04:25.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 9.2091064453125
2025-12-09 12:04:26.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 9.209918975830078
2025-12-09 12:04:26.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 9.209446907043457
2025-12-09 12:04:26.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 9.209345817565918
2025-12-09 12:04:26.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 9.209320068359375
2025-12-09 12:04:26.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 9.208237648010254
2025-12-09 12:04:26.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 9.208518981933594
2025-12-09 12:04:26.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 9.208145141601562
2025-12-09 12:04:26.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 9.2085599899292
2025-12-09 12:04:26.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 9.209213256835938
2025-12-09 12:04:26.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 9.208502769470215
2025-12-09 12:04:26.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 9.208298683166504
2025-12-09 12:04:26.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 9.208725929260254
2025-12-09 12:04:26.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 9.208651542663574
2025-12-09 12:04:26.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 9.207782745361328
2025-12-09 12:04:26.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 9.20805549621582
2025-12-09 12:04:26.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 9.208426475524902
2025-12-09 12:04:26.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 9.207901000976562
2025-12-09 12:04:26.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 9.208357810974121
2025-12-09 12:04:26.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 9.208651542663574
2025-12-09 12:04:26.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 9.206693649291992
2025-12-09 12:04:26.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 9.207845687866211
2025-12-09 12:04:26.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 9.207093238830566
2025-12-09 12:04:26.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 9.206916809082031
2025-12-09 12:04:26.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 9.20736026763916
2025-12-09 12:04:26.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 9.206315040588379
2025-12-09 12:04:26.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 9.206199645996094
2025-12-09 12:04:26.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 9.20627498626709
2025-12-09 12:04:26.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 9.206171035766602
2025-12-09 12:04:26.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 9.206101417541504
2025-12-09 12:04:26.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 9.206751823425293
2025-12-09 12:04:26.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 9.205167770385742
2025-12-09 12:04:26.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 9.204970359802246
2025-12-09 12:04:26.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 9.204676628112793
2025-12-09 12:04:26.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 9.205011367797852
2025-12-09 12:04:26.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 9.205023765563965
2025-12-09 12:04:26.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 9.204270362854004
2025-12-09 12:04:26.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 9.203763961791992
2025-12-09 12:04:26.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 9.203432083129883
2025-12-09 12:04:26.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 9.202577590942383
2025-12-09 12:04:26.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 9.20364761352539
2025-12-09 12:04:26.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 9.203619003295898
2025-12-09 12:04:26.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 9.203049659729004
2025-12-09 12:04:26.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 9.202397346496582
2025-12-09 12:04:26.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 9.201444625854492
2025-12-09 12:04:26.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 9.2021484375
2025-12-09 12:04:26.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 9.201743125915527
2025-12-09 12:04:26.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 9.201619148254395
2025-12-09 12:04:26.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 9.201549530029297
2025-12-09 12:04:26.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 9.201935768127441
2025-12-09 12:04:26.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 9.202043533325195
2025-12-09 12:04:26.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 9.200179100036621
2025-12-09 12:04:26.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 9.199420928955078
2025-12-09 12:04:26.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 9.199178695678711
2025-12-09 12:04:26.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 9.199477195739746
2025-12-09 12:04:26.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 9.199333190917969
2025-12-09 12:04:27.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 9.198615074157715
2025-12-09 12:04:27.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 9.198126792907715
2025-12-09 12:04:27.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 9.197563171386719
2025-12-09 12:04:27.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 9.197915077209473
2025-12-09 12:04:27.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 9.197680473327637
2025-12-09 12:04:27.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 9.197928428649902
2025-12-09 12:04:27.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 9.195914268493652
2025-12-09 12:04:27.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 9.1966552734375
2025-12-09 12:04:27.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 9.195870399475098
2025-12-09 12:04:27.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 9.195106506347656
2025-12-09 12:04:27.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 9.195870399475098
2025-12-09 12:04:27.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 9.195059776306152
2025-12-09 12:04:27.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 9.192647933959961
2025-12-09 12:04:27.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 9.193913459777832
2025-12-09 12:04:27.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 9.19375228881836
2025-12-09 12:04:27.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 9.193603515625
2025-12-09 12:04:27.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 9.19227409362793
2025-12-09 12:04:27.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 9.190899848937988
2025-12-09 12:04:27.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 9.190997123718262
2025-12-09 12:04:27.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 9.191049575805664
2025-12-09 12:04:27.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 9.189915657043457
2025-12-09 12:04:27.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 9.190652847290039
2025-12-09 12:04:27.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 9.188364028930664
2025-12-09 12:04:27.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 9.186989784240723
2025-12-09 12:04:27.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 9.188607215881348
2025-12-09 12:04:27.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 9.188634872436523
2025-12-09 12:04:27.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 9.18769359588623
2025-12-09 12:04:27.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 9.186058044433594
2025-12-09 12:04:27.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 9.185221672058105
2025-12-09 12:04:27.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 9.187432289123535
2025-12-09 12:04:27.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 9.185113906860352
2025-12-09 12:04:27.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 9.186395645141602
2025-12-09 12:04:27.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 9.186455726623535
2025-12-09 12:04:27.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 9.183403015136719
2025-12-09 12:04:27.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 9.183899879455566
2025-12-09 12:04:27.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 9.182343482971191
2025-12-09 12:04:27.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 9.18252182006836
2025-12-09 12:04:27.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.698463103929542e-05 Training loss: 9.182820320129395
2025-12-09 12:04:27.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 8.83022221559489e-05 Training loss: 9.179750442504883
2025-12-09 12:04:27.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 7.500000000000001e-05 Training loss: 9.180055618286133
2025-12-09 12:04:27.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 5.868240888334653e-05 Training loss: 9.181901931762695
2025-12-09 12:04:27.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 4.131759111665349e-05 Training loss: 9.180261611938477
2025-12-09 12:04:27.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 2.500000000000001e-05 Training loss: 9.179333686828613
2025-12-09 12:04:27.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 1.1697777844051105e-05 Training loss: 9.178815841674805
2025-12-09 12:04:27.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 3.0153689607045845e-06 Training loss: 9.179183959960938
2025-12-09 12:04:27.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 9.1808500289917
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:04:40.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 9.214483261108398
2025-12-09 12:04:40.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 9.215339660644531
2025-12-09 12:04:40.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 9.215664863586426
2025-12-09 12:04:40.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 9.215073585510254
2025-12-09 12:04:40.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 9.214529991149902
2025-12-09 12:04:40.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 9.214927673339844
2025-12-09 12:04:41.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 9.21426010131836
2025-12-09 12:04:41.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 9.215067863464355
2025-12-09 12:04:41.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 9.214740753173828
2025-12-09 12:04:41.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 9.21423053741455
2025-12-09 12:04:41.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 9.214510917663574
2025-12-09 12:04:41.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 9.21479320526123
2025-12-09 12:04:41.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 9.21377182006836
2025-12-09 12:04:41.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 9.213876724243164
2025-12-09 12:04:41.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 9.2136812210083
2025-12-09 12:04:41.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 9.2135009765625
2025-12-09 12:04:41.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 9.213239669799805
2025-12-09 12:04:41.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 9.212871551513672
2025-12-09 12:04:41.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 9.212533950805664
2025-12-09 12:04:41.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 9.212224960327148
2025-12-09 12:04:41.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 9.212156295776367
2025-12-09 12:04:41.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 9.211153030395508
2025-12-09 12:04:41.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 9.210943222045898
2025-12-09 12:04:41.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 9.210622787475586
2025-12-09 12:04:41.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 9.210515022277832
2025-12-09 12:04:41.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 9.209932327270508
2025-12-09 12:04:41.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 9.20986557006836
2025-12-09 12:04:41.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 9.208999633789062
2025-12-09 12:04:41.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 9.208955764770508
2025-12-09 12:04:41.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 9.207841873168945
2025-12-09 12:04:41.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 9.207893371582031
2025-12-09 12:04:41.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 9.207262992858887
2025-12-09 12:04:41.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 9.206971168518066
2025-12-09 12:04:41.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 9.206489562988281
2025-12-09 12:04:41.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 9.205711364746094
2025-12-09 12:04:41.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 9.205302238464355
2025-12-09 12:04:41.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 9.204537391662598
2025-12-09 12:04:41.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 9.204504013061523
2025-12-09 12:04:41.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 9.203428268432617
2025-12-09 12:04:41.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 9.2028226852417
2025-12-09 12:04:41.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 9.202515602111816
2025-12-09 12:04:41.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 9.20123291015625
2025-12-09 12:04:41.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 9.200417518615723
2025-12-09 12:04:41.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 9.199799537658691
2025-12-09 12:04:41.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 9.199917793273926
2025-12-09 12:04:41.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 9.198537826538086
2025-12-09 12:04:41.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 9.197699546813965
2025-12-09 12:04:41.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 9.196616172790527
2025-12-09 12:04:41.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 9.195481300354004
2025-12-09 12:04:41.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 9.194993019104004
2025-12-09 12:04:41.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 9.194281578063965
2025-12-09 12:04:41.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 9.193095207214355
2025-12-09 12:04:41.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 9.193645477294922
2025-12-09 12:04:41.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 9.191280364990234
2025-12-09 12:04:41.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 9.19084358215332
2025-12-09 12:04:41.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 9.190627098083496
2025-12-09 12:04:41.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 9.188633918762207
2025-12-09 12:04:41.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 9.18891429901123
2025-12-09 12:04:41.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 9.187501907348633
2025-12-09 12:04:41.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 9.18478775024414
2025-12-09 12:04:42.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 9.186189651489258
2025-12-09 12:04:42.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 9.183385848999023
2025-12-09 12:04:42.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 9.183296203613281
2025-12-09 12:04:42.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 9.18134880065918
2025-12-09 12:04:42.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 9.181385040283203
2025-12-09 12:04:42.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 9.18113899230957
2025-12-09 12:04:42.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 9.179003715515137
2025-12-09 12:04:42.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 9.176730155944824
2025-12-09 12:04:42.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 9.17731761932373
2025-12-09 12:04:42.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 9.175856590270996
2025-12-09 12:04:42.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 9.173502922058105
2025-12-09 12:04:42.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 9.173240661621094
2025-12-09 12:04:42.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 9.170836448669434
2025-12-09 12:04:42.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 9.169829368591309
2025-12-09 12:04:42.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 9.16817855834961
2025-12-09 12:04:42.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 9.165240287780762
2025-12-09 12:04:42.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 9.165356636047363
2025-12-09 12:04:42.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 9.165140151977539
2025-12-09 12:04:42.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 9.164169311523438
2025-12-09 12:04:42.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 9.15903091430664
2025-12-09 12:04:42.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 9.15799331665039
2025-12-09 12:04:42.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 9.157147407531738
2025-12-09 12:04:42.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 9.156033515930176
2025-12-09 12:04:42.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 9.152589797973633
2025-12-09 12:04:42.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 9.152776718139648
2025-12-09 12:04:42.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 9.149876594543457
2025-12-09 12:04:42.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 9.146342277526855
2025-12-09 12:04:42.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 9.149504661560059
2025-12-09 12:04:42.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 9.145686149597168
2025-12-09 12:04:42.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 9.142090797424316
2025-12-09 12:04:42.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 9.141707420349121
2025-12-09 12:04:42.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 9.138299942016602
2025-12-09 12:04:42.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 9.135589599609375
2025-12-09 12:04:42.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 9.138262748718262
2025-12-09 12:04:42.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 9.132377624511719
2025-12-09 12:04:42.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 9.133572578430176
2025-12-09 12:04:42.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 9.129109382629395
2025-12-09 12:04:42.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 9.12651538848877
2025-12-09 12:04:42.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 9.12702751159668
2025-12-09 12:04:42.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 9.118846893310547
2025-12-09 12:04:42.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0002909538931178862 Training loss: 9.120482444763184
2025-12-09 12:04:42.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00026490666646784665 Training loss: 9.123991012573242
2025-12-09 12:04:42.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000225 Training loss: 9.120641708374023
2025-12-09 12:04:42.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00017604722665003956 Training loss: 9.10910415649414
2025-12-09 12:04:42.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00012395277334996044 Training loss: 9.115778923034668
2025-12-09 12:04:42.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 7.500000000000002e-05 Training loss: 9.111594200134277
2025-12-09 12:04:42.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 3.509333353215331e-05 Training loss: 9.11138916015625
2025-12-09 12:04:42.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113751e-06 Training loss: 9.11166763305664
2025-12-09 12:04:42.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 9.113990783691406
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:04:55.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 9.20986557006836
2025-12-09 12:04:55.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 9.209885597229004
2025-12-09 12:04:55.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 9.210670471191406
2025-12-09 12:04:55.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 9.209357261657715
2025-12-09 12:04:55.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 9.209815979003906
2025-12-09 12:04:55.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 9.210168838500977
2025-12-09 12:04:56.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 9.209951400756836
2025-12-09 12:04:56.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 9.209392547607422
2025-12-09 12:04:56.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 9.208230018615723
2025-12-09 12:04:56.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 9.207984924316406
2025-12-09 12:04:56.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 9.208637237548828
2025-12-09 12:04:56.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 9.206437110900879
2025-12-09 12:04:56.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 9.206660270690918
2025-12-09 12:04:56.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 9.205323219299316
2025-12-09 12:04:56.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 9.204938888549805
2025-12-09 12:04:56.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 9.204834938049316
2025-12-09 12:04:56.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 9.204405784606934
2025-12-09 12:04:56.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 9.203251838684082
2025-12-09 12:04:56.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 9.202311515808105
2025-12-09 12:04:56.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 9.201807975769043
2025-12-09 12:04:56.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 9.19962215423584
2025-12-09 12:04:56.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 9.199010848999023
2025-12-09 12:04:56.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 9.198190689086914
2025-12-09 12:04:56.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 9.196310997009277
2025-12-09 12:04:56.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 9.195507049560547
2025-12-09 12:04:56.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 9.19357967376709
2025-12-09 12:04:56.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 9.191110610961914
2025-12-09 12:04:56.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 9.191511154174805
2025-12-09 12:04:56.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 9.188498497009277
2025-12-09 12:04:56.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 9.18739128112793
2025-12-09 12:04:56.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 9.187244415283203
2025-12-09 12:04:56.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 9.183589935302734
2025-12-09 12:04:56.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 9.183642387390137
2025-12-09 12:04:56.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 9.18204116821289
2025-12-09 12:04:56.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 9.1781644821167
2025-12-09 12:04:56.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 9.17562198638916
2025-12-09 12:04:56.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 9.172999382019043
2025-12-09 12:04:56.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 9.172351837158203
2025-12-09 12:04:56.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 9.170516967773438
2025-12-09 12:04:56.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 9.168116569519043
2025-12-09 12:04:56.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 9.167427062988281
2025-12-09 12:04:56.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 9.165847778320312
2025-12-09 12:04:56.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 9.162334442138672
2025-12-09 12:04:56.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 9.153227806091309
2025-12-09 12:04:56.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 9.157623291015625
2025-12-09 12:04:56.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 9.149405479431152
2025-12-09 12:04:56.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 9.150577545166016
2025-12-09 12:04:56.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 9.147282600402832
2025-12-09 12:04:56.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 9.142256736755371
2025-12-09 12:04:56.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 9.134761810302734
2025-12-09 12:04:56.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 9.1366605758667
2025-12-09 12:04:56.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 9.135385513305664
2025-12-09 12:04:56.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 9.128605842590332
2025-12-09 12:04:56.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 9.130030632019043
2025-12-09 12:04:56.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 9.121213912963867
2025-12-09 12:04:56.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 9.12011432647705
2025-12-09 12:04:56.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 9.110827445983887
2025-12-09 12:04:56.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 9.115679740905762
2025-12-09 12:04:56.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 9.102752685546875
2025-12-09 12:04:56.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 9.099503517150879
2025-12-09 12:04:56.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 9.095006942749023
2025-12-09 12:04:56.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 9.086748123168945
2025-12-09 12:04:57.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 9.080857276916504
2025-12-09 12:04:57.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 9.07855224609375
2025-12-09 12:04:57.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 9.07599925994873
2025-12-09 12:04:57.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 9.064007759094238
2025-12-09 12:04:57.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 9.056788444519043
2025-12-09 12:04:57.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 9.05146598815918
2025-12-09 12:04:57.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 9.035873413085938
2025-12-09 12:04:57.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 9.034204483032227
2025-12-09 12:04:57.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 9.022653579711914
2025-12-09 12:04:57.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 9.018049240112305
2025-12-09 12:04:57.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 9.02047348022461
2025-12-09 12:04:57.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 8.997529983520508
2025-12-09 12:04:57.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 9.003188133239746
2025-12-09 12:04:57.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 8.974532127380371
2025-12-09 12:04:57.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 8.97226333618164
2025-12-09 12:04:57.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 8.973663330078125
2025-12-09 12:04:57.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 8.95251750946045
2025-12-09 12:04:57.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 8.939626693725586
2025-12-09 12:04:57.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 8.923883438110352
2025-12-09 12:04:57.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 8.93244457244873
2025-12-09 12:04:57.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 8.917272567749023
2025-12-09 12:04:57.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 8.90630054473877
2025-12-09 12:04:57.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 8.885007858276367
2025-12-09 12:04:57.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 8.885083198547363
2025-12-09 12:04:57.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 8.8753662109375
2025-12-09 12:04:57.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 8.866058349609375
2025-12-09 12:04:57.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 8.844663619995117
2025-12-09 12:04:57.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 8.831707954406738
2025-12-09 12:04:57.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 8.804998397827148
2025-12-09 12:04:57.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 8.798495292663574
2025-12-09 12:04:57.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 8.78547477722168
2025-12-09 12:04:57.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 8.759178161621094
2025-12-09 12:04:57.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 8.768558502197266
2025-12-09 12:04:57.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 8.727803230285645
2025-12-09 12:04:57.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 8.693094253540039
2025-12-09 12:04:57.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 8.718737602233887
2025-12-09 12:04:57.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 8.667381286621094
2025-12-09 12:04:57.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 8.689367294311523
2025-12-09 12:04:57.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009698463103929542 Training loss: 8.6524019241333
2025-12-09 12:04:57.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.000883022221559489 Training loss: 8.610235214233398
2025-12-09 12:04:57.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.00075 Training loss: 8.615728378295898
2025-12-09 12:04:57.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0005868240888334653 Training loss: 8.591569900512695
2025-12-09 12:04:57.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00041317591116653486 Training loss: 8.581375122070312
2025-12-09 12:04:57.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002500000000000001 Training loss: 8.561566352844238
2025-12-09 12:04:57.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.00011697777844051105 Training loss: 8.52019214630127
2025-12-09 12:04:57.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 3.0153689607045842e-05 Training loss: 8.536534309387207
2025-12-09 12:04:57.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 8.47471809387207
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:05:10.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 9.210151672363281
2025-12-09 12:05:10.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 9.20954418182373
2025-12-09 12:05:11.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 9.21027946472168
2025-12-09 12:05:11.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 9.210590362548828
2025-12-09 12:05:11.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 9.208683013916016
2025-12-09 12:05:11.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 9.20920467376709
2025-12-09 12:05:11.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 9.207852363586426
2025-12-09 12:05:11.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 9.206955909729004
2025-12-09 12:05:11.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 9.206451416015625
2025-12-09 12:05:11.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 9.204996109008789
2025-12-09 12:05:11.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 9.202990531921387
2025-12-09 12:05:11.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 9.201594352722168
2025-12-09 12:05:11.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 9.199909210205078
2025-12-09 12:05:11.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 9.197595596313477
2025-12-09 12:05:11.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 9.196330070495605
2025-12-09 12:05:11.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 9.194538116455078
2025-12-09 12:05:11.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 9.189723014831543
2025-12-09 12:05:11.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 9.18807315826416
2025-12-09 12:05:11.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 9.183195114135742
2025-12-09 12:05:11.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 9.181389808654785
2025-12-09 12:05:11.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 9.178752899169922
2025-12-09 12:05:11.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 9.175820350646973
2025-12-09 12:05:11.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 9.170750617980957
2025-12-09 12:05:11.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 9.167020797729492
2025-12-09 12:05:11.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 9.16207504272461
2025-12-09 12:05:11.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 9.156201362609863
2025-12-09 12:05:11.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 9.152571678161621
2025-12-09 12:05:11.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 9.14694881439209
2025-12-09 12:05:11.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 9.139532089233398
2025-12-09 12:05:11.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 9.138635635375977
2025-12-09 12:05:11.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 9.130998611450195
2025-12-09 12:05:11.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 9.123586654663086
2025-12-09 12:05:11.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 9.11230182647705
2025-12-09 12:05:11.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 9.111957550048828
2025-12-09 12:05:11.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 9.107166290283203
2025-12-09 12:05:11.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 9.087074279785156
2025-12-09 12:05:11.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 9.08061695098877
2025-12-09 12:05:11.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 9.082479476928711
2025-12-09 12:05:11.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 9.05104923248291
2025-12-09 12:05:11.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 9.05200481414795
2025-12-09 12:05:11.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 9.047411918640137
2025-12-09 12:05:11.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 9.031377792358398
2025-12-09 12:05:11.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 9.022709846496582
2025-12-09 12:05:11.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 9.00292682647705
2025-12-09 12:05:11.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 8.994657516479492
2025-12-09 12:05:11.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 8.968361854553223
2025-12-09 12:05:11.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 8.953018188476562
2025-12-09 12:05:11.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 8.938533782958984
2025-12-09 12:05:11.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 8.911102294921875
2025-12-09 12:05:11.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 8.876639366149902
2025-12-09 12:05:11.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 8.856507301330566
2025-12-09 12:05:11.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 8.854846954345703
2025-12-09 12:05:11.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 8.810342788696289
2025-12-09 12:05:11.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 8.805413246154785
2025-12-09 12:05:11.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 8.779342651367188
2025-12-09 12:05:11.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 8.763955116271973
2025-12-09 12:05:12.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 8.711836814880371
2025-12-09 12:05:12.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 8.65783977508545
2025-12-09 12:05:12.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 8.65771198272705
2025-12-09 12:05:12.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 8.623629570007324
2025-12-09 12:05:12.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 8.561601638793945
2025-12-09 12:05:12.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 8.541531562805176
2025-12-09 12:05:12.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 8.508464813232422
2025-12-09 12:05:12.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 8.439845085144043
2025-12-09 12:05:12.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 8.430208206176758
2025-12-09 12:05:12.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 8.386890411376953
2025-12-09 12:05:12.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 8.360276222229004
2025-12-09 12:05:12.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 8.29568862915039
2025-12-09 12:05:12.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 8.253575325012207
2025-12-09 12:05:12.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 8.188616752624512
2025-12-09 12:05:12.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 8.177837371826172
2025-12-09 12:05:12.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 8.052769660949707
2025-12-09 12:05:12.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 8.004264831542969
2025-12-09 12:05:12.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 7.97055196762085
2025-12-09 12:05:12.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 7.948243141174316
2025-12-09 12:05:12.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 7.84149169921875
2025-12-09 12:05:12.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 7.7923479080200195
2025-12-09 12:05:12.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 7.663700580596924
2025-12-09 12:05:12.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 7.689327239990234
2025-12-09 12:05:12.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 7.6546454429626465
2025-12-09 12:05:12.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 7.697422027587891
2025-12-09 12:05:12.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 7.617502689361572
2025-12-09 12:05:12.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 7.58635139465332
2025-12-09 12:05:12.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 7.52396297454834
2025-12-09 12:05:12.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 7.371897220611572
2025-12-09 12:05:12.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 7.4474873542785645
2025-12-09 12:05:12.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 7.40054988861084
2025-12-09 12:05:12.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 7.380024433135986
2025-12-09 12:05:12.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 7.222132682800293
2025-12-09 12:05:12.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 7.3335957527160645
2025-12-09 12:05:12.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 7.249615669250488
2025-12-09 12:05:12.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 7.237180233001709
2025-12-09 12:05:12.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 7.201862335205078
2025-12-09 12:05:12.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 7.160663604736328
2025-12-09 12:05:12.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 7.132973670959473
2025-12-09 12:05:12.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 7.047317981719971
2025-12-09 12:05:12.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 6.9712371826171875
2025-12-09 12:05:12.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 6.941079616546631
2025-12-09 12:05:12.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 6.873414993286133
2025-12-09 12:05:12.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 7.040337085723877
2025-12-09 12:05:12.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.002909538931178863 Training loss: 6.847962856292725
2025-12-09 12:05:12.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002649066664678467 Training loss: 6.885359764099121
2025-12-09 12:05:12.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0022500000000000003 Training loss: 6.808192729949951
2025-12-09 12:05:12.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0017604722665003959 Training loss: 6.868605136871338
2025-12-09 12:05:12.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0012395277334996046 Training loss: 6.795557498931885
2025-12-09 12:05:12.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0007500000000000003 Training loss: 6.821191787719727
2025-12-09 12:05:12.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0003509333353215332 Training loss: 6.735558986663818
2025-12-09 12:05:12.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113752e-05 Training loss: 6.662150859832764
2025-12-09 12:05:12.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 6.690925598144531
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:05:25.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 9.203239440917969
2025-12-09 12:05:25.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 9.202942848205566
2025-12-09 12:05:25.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 9.202733993530273
2025-12-09 12:05:25.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 9.202313423156738
2025-12-09 12:05:25.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 9.200252532958984
2025-12-09 12:05:25.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 9.197815895080566
2025-12-09 12:05:26.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 9.195062637329102
2025-12-09 12:05:26.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 9.192245483398438
2025-12-09 12:05:26.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 9.189303398132324
2025-12-09 12:05:26.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 9.184635162353516
2025-12-09 12:05:26.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 9.178248405456543
2025-12-09 12:05:26.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 9.173714637756348
2025-12-09 12:05:26.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 9.168000221252441
2025-12-09 12:05:26.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 9.16197395324707
2025-12-09 12:05:26.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 9.147485733032227
2025-12-09 12:05:26.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 9.143495559692383
2025-12-09 12:05:26.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 9.131281852722168
2025-12-09 12:05:26.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 9.125595092773438
2025-12-09 12:05:26.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 9.11315631866455
2025-12-09 12:05:26.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 9.098722457885742
2025-12-09 12:05:26.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 9.089285850524902
2025-12-09 12:05:26.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 9.056537628173828
2025-12-09 12:05:26.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 9.046072959899902
2025-12-09 12:05:26.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 9.017667770385742
2025-12-09 12:05:26.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 9.012964248657227
2025-12-09 12:05:26.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 8.96566390991211
2025-12-09 12:05:26.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 8.935023307800293
2025-12-09 12:05:26.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 8.894574165344238
2025-12-09 12:05:26.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 8.861506462097168
2025-12-09 12:05:26.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 8.823013305664062
2025-12-09 12:05:26.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 8.762772560119629
2025-12-09 12:05:26.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 8.72768783569336
2025-12-09 12:05:26.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 8.68369197845459
2025-12-09 12:05:26.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 8.58271312713623
2025-12-09 12:05:26.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 8.56396484375
2025-12-09 12:05:26.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 8.492445945739746
2025-12-09 12:05:26.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 8.36238956451416
2025-12-09 12:05:26.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 8.299790382385254
2025-12-09 12:05:26.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 8.157927513122559
2025-12-09 12:05:26.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 8.105157852172852
2025-12-09 12:05:26.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 8.014663696289062
2025-12-09 12:05:26.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 7.975130558013916
2025-12-09 12:05:26.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 7.9013166427612305
2025-12-09 12:05:26.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 7.797182559967041
2025-12-09 12:05:26.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 7.75629186630249
2025-12-09 12:05:26.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 7.664514541625977
2025-12-09 12:05:26.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 7.579594135284424
2025-12-09 12:05:26.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 7.485896110534668
2025-12-09 12:05:26.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 7.404328346252441
2025-12-09 12:05:26.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 7.431312561035156
2025-12-09 12:05:26.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 7.20425271987915
2025-12-09 12:05:26.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 7.214107513427734
2025-12-09 12:05:26.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 7.114408016204834
2025-12-09 12:05:26.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 7.181487083435059
2025-12-09 12:05:26.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 6.978564262390137
2025-12-09 12:05:26.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 6.990139007568359
2025-12-09 12:05:26.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 6.84099817276001
2025-12-09 12:05:26.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 6.88386344909668
2025-12-09 12:05:26.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 6.762944221496582
2025-12-09 12:05:26.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 6.7264227867126465
2025-12-09 12:05:27.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 6.608733177185059
2025-12-09 12:05:27.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 6.647653579711914
2025-12-09 12:05:27.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 6.573606491088867
2025-12-09 12:05:27.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 6.670754432678223
2025-12-09 12:05:27.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 6.634359836578369
2025-12-09 12:05:27.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 6.529048442840576
2025-12-09 12:05:27.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 6.597766876220703
2025-12-09 12:05:27.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 6.375575065612793
2025-12-09 12:05:27.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 6.332215309143066
2025-12-09 12:05:27.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 6.380887031555176
2025-12-09 12:05:27.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 6.357466220855713
2025-12-09 12:05:27.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 6.295948028564453
2025-12-09 12:05:27.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 6.24399995803833
2025-12-09 12:05:27.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 6.300519943237305
2025-12-09 12:05:27.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 6.150449752807617
2025-12-09 12:05:27.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 6.117475509643555
2025-12-09 12:05:27.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 6.244584083557129
2025-12-09 12:05:27.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 6.1143622398376465
2025-12-09 12:05:27.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 6.062627792358398
2025-12-09 12:05:27.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 5.990518093109131
2025-12-09 12:05:27.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 6.050745964050293
2025-12-09 12:05:27.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 6.070287704467773
2025-12-09 12:05:27.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 5.904433727264404
2025-12-09 12:05:27.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 5.928668975830078
2025-12-09 12:05:27.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 6.081290245056152
2025-12-09 12:05:27.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 5.952468395233154
2025-12-09 12:05:27.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 5.886965274810791
2025-12-09 12:05:27.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 5.917882919311523
2025-12-09 12:05:27.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 5.940845489501953
2025-12-09 12:05:27.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 5.90283203125
2025-12-09 12:05:27.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 5.875065326690674
2025-12-09 12:05:27.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 5.82046365737915
2025-12-09 12:05:27.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 5.676299571990967
2025-12-09 12:05:27.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 5.7041707038879395
2025-12-09 12:05:27.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 5.847036838531494
2025-12-09 12:05:27.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 5.523550987243652
2025-12-09 12:05:27.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 5.771084785461426
2025-12-09 12:05:27.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 5.7648420333862305
2025-12-09 12:05:27.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 5.766242027282715
2025-12-09 12:05:27.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 5.654702186584473
2025-12-09 12:05:27.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009698463103929543 Training loss: 5.713146686553955
2025-12-09 12:05:27.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00883022221559489 Training loss: 5.64657735824585
2025-12-09 12:05:27.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0075 Training loss: 5.768609046936035
2025-12-09 12:05:27.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.005868240888334653 Training loss: 5.782599449157715
2025-12-09 12:05:27.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0041317591116653484 Training loss: 5.634528636932373
2025-12-09 12:05:27.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0025000000000000014 Training loss: 5.448480606079102
2025-12-09 12:05:27.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0011697777844051104 Training loss: 5.61607027053833
2025-12-09 12:05:27.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00030153689607045843 Training loss: 5.707545280456543
2025-12-09 12:05:27.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 5.657616138458252
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:05:40.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 9.216480255126953
2025-12-09 12:05:40.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 9.216216087341309
2025-12-09 12:05:40.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 9.216020584106445
2025-12-09 12:05:40.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 9.215725898742676
2025-12-09 12:05:40.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 9.215909957885742
2025-12-09 12:05:41.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 9.216475486755371
2025-12-09 12:05:41.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 9.21578311920166
2025-12-09 12:05:41.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 9.216353416442871
2025-12-09 12:05:41.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 9.216560363769531
2025-12-09 12:05:41.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 9.21619987487793
2025-12-09 12:05:41.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 9.216386795043945
2025-12-09 12:05:41.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 9.215383529663086
2025-12-09 12:05:41.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 9.215715408325195
2025-12-09 12:05:41.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 9.216937065124512
2025-12-09 12:05:41.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 9.215704917907715
2025-12-09 12:05:41.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 9.21628475189209
2025-12-09 12:05:41.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 9.216143608093262
2025-12-09 12:05:41.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 9.216523170471191
2025-12-09 12:05:41.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 9.215832710266113
2025-12-09 12:05:41.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 9.2161283493042
2025-12-09 12:05:41.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 9.215944290161133
2025-12-09 12:05:41.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 9.214885711669922
2025-12-09 12:05:41.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 9.21649169921875
2025-12-09 12:05:41.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 9.216652870178223
2025-12-09 12:05:41.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 9.216233253479004
2025-12-09 12:05:41.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 9.215529441833496
2025-12-09 12:05:41.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 9.216647148132324
2025-12-09 12:05:41.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 9.216316223144531
2025-12-09 12:05:41.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 9.216132164001465
2025-12-09 12:05:41.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 9.216156959533691
2025-12-09 12:05:41.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 9.21660041809082
2025-12-09 12:05:41.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 9.21608829498291
2025-12-09 12:05:41.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 9.215408325195312
2025-12-09 12:05:41.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 9.215876579284668
2025-12-09 12:05:41.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 9.216455459594727
2025-12-09 12:05:41.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 9.215259552001953
2025-12-09 12:05:41.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 9.216957092285156
2025-12-09 12:05:41.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 9.216453552246094
2025-12-09 12:05:41.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 9.215868949890137
2025-12-09 12:05:41.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 9.2156982421875
2025-12-09 12:05:41.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 9.216080665588379
2025-12-09 12:05:41.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 9.216156959533691
2025-12-09 12:05:41.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 9.216625213623047
2025-12-09 12:05:41.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 9.215767860412598
2025-12-09 12:05:41.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 9.216554641723633
2025-12-09 12:05:41.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 9.215950965881348
2025-12-09 12:05:41.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 9.21597957611084
2025-12-09 12:05:41.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 9.21618366241455
2025-12-09 12:05:41.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 9.2157621383667
2025-12-09 12:05:41.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 9.215936660766602
2025-12-09 12:05:41.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 9.215818405151367
2025-12-09 12:05:41.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 9.216057777404785
2025-12-09 12:05:41.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 9.215912818908691
2025-12-09 12:05:41.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 9.215713500976562
2025-12-09 12:05:41.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 9.216167449951172
2025-12-09 12:05:41.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 9.215851783752441
2025-12-09 12:05:41.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 9.215932846069336
2025-12-09 12:05:41.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 9.215731620788574
2025-12-09 12:05:41.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 9.215514183044434
2025-12-09 12:05:41.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 9.215889930725098
2025-12-09 12:05:41.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 9.215699195861816
2025-12-09 12:05:41.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 9.215511322021484
2025-12-09 12:05:41.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 9.216255187988281
2025-12-09 12:05:41.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 9.215851783752441
2025-12-09 12:05:41.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 9.216216087341309
2025-12-09 12:05:41.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 9.215901374816895
2025-12-09 12:05:41.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 9.215893745422363
2025-12-09 12:05:42.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 9.216304779052734
2025-12-09 12:05:42.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 9.216137886047363
2025-12-09 12:05:42.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 9.215920448303223
2025-12-09 12:05:42.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 9.21590518951416
2025-12-09 12:05:42.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 9.216132164001465
2025-12-09 12:05:42.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 9.21547794342041
2025-12-09 12:05:42.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 9.215553283691406
2025-12-09 12:05:42.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 9.215867042541504
2025-12-09 12:05:42.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 9.214908599853516
2025-12-09 12:05:42.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 9.215465545654297
2025-12-09 12:05:42.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 9.2164306640625
2025-12-09 12:05:42.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 9.215973854064941
2025-12-09 12:05:42.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 9.215299606323242
2025-12-09 12:05:42.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 9.21566104888916
2025-12-09 12:05:42.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 9.216715812683105
2025-12-09 12:05:42.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 9.215557098388672
2025-12-09 12:05:42.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 9.21540641784668
2025-12-09 12:05:42.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 9.215571403503418
2025-12-09 12:05:42.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 9.215167045593262
2025-12-09 12:05:42.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 9.21541690826416
2025-12-09 12:05:42.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 9.21593189239502
2025-12-09 12:05:42.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 9.215261459350586
2025-12-09 12:05:42.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 9.216241836547852
2025-12-09 12:05:42.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 9.215725898742676
2025-12-09 12:05:42.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 9.215301513671875
2025-12-09 12:05:42.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 9.215414047241211
2025-12-09 12:05:42.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 9.215306282043457
2025-12-09 12:05:42.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 9.21546745300293
2025-12-09 12:05:42.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 9.215767860412598
2025-12-09 12:05:42.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 9.216033935546875
2025-12-09 12:05:42.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 9.215570449829102
2025-12-09 12:05:42.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 9.215289115905762
2025-12-09 12:05:42.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 9.215738296508789
2025-12-09 12:05:42.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.698463103929542e-05 Training loss: 9.215045928955078
2025-12-09 12:05:42.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 8.83022221559489e-05 Training loss: 9.215328216552734
2025-12-09 12:05:42.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 7.500000000000001e-05 Training loss: 9.215535163879395
2025-12-09 12:05:42.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 5.868240888334653e-05 Training loss: 9.215384483337402
2025-12-09 12:05:42.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 4.131759111665349e-05 Training loss: 9.215279579162598
2025-12-09 12:05:42.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 2.500000000000001e-05 Training loss: 9.215476989746094
2025-12-09 12:05:42.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 1.1697777844051105e-05 Training loss: 9.215153694152832
2025-12-09 12:05:42.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 3.0153689607045845e-06 Training loss: 9.215136528015137
2025-12-09 12:05:42.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 9.215479850769043
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:05:55.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 9.213406562805176
2025-12-09 12:05:55.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 9.212957382202148
2025-12-09 12:05:55.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 9.212878227233887
2025-12-09 12:05:55.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 9.21306324005127
2025-12-09 12:05:55.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 9.21302604675293
2025-12-09 12:05:55.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 9.213062286376953
2025-12-09 12:05:55.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 9.213298797607422
2025-12-09 12:05:56.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 9.213385581970215
2025-12-09 12:05:56.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 9.213308334350586
2025-12-09 12:05:56.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 9.213101387023926
2025-12-09 12:05:56.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 9.213150024414062
2025-12-09 12:05:56.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 9.212858200073242
2025-12-09 12:05:56.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 9.213953018188477
2025-12-09 12:05:56.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 9.212505340576172
2025-12-09 12:05:56.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 9.212604522705078
2025-12-09 12:05:56.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 9.213565826416016
2025-12-09 12:05:56.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 9.212772369384766
2025-12-09 12:05:56.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 9.21329116821289
2025-12-09 12:05:56.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 9.21231460571289
2025-12-09 12:05:56.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 9.213364601135254
2025-12-09 12:05:56.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 9.212437629699707
2025-12-09 12:05:56.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 9.213123321533203
2025-12-09 12:05:56.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 9.212934494018555
2025-12-09 12:05:56.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 9.213221549987793
2025-12-09 12:05:56.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 9.212377548217773
2025-12-09 12:05:56.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 9.212800979614258
2025-12-09 12:05:56.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 9.212904930114746
2025-12-09 12:05:56.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 9.212611198425293
2025-12-09 12:05:56.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 9.212214469909668
2025-12-09 12:05:56.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 9.213440895080566
2025-12-09 12:05:56.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 9.212778091430664
2025-12-09 12:05:56.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 9.21281623840332
2025-12-09 12:05:56.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 9.212443351745605
2025-12-09 12:05:56.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 9.213152885437012
2025-12-09 12:05:56.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 9.212494850158691
2025-12-09 12:05:56.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 9.213357925415039
2025-12-09 12:05:56.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 9.213594436645508
2025-12-09 12:05:56.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 9.212959289550781
2025-12-09 12:05:56.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 9.212284088134766
2025-12-09 12:05:56.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 9.212980270385742
2025-12-09 12:05:56.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 9.213155746459961
2025-12-09 12:05:56.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 9.213152885437012
2025-12-09 12:05:56.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 9.212691307067871
2025-12-09 12:05:56.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 9.213240623474121
2025-12-09 12:05:56.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 9.212889671325684
2025-12-09 12:05:56.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 9.212604522705078
2025-12-09 12:05:56.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 9.212992668151855
2025-12-09 12:05:56.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 9.212678909301758
2025-12-09 12:05:56.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 9.212873458862305
2025-12-09 12:05:56.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 9.21241283416748
2025-12-09 12:05:56.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 9.213296890258789
2025-12-09 12:05:56.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 9.212596893310547
2025-12-09 12:05:56.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 9.212512016296387
2025-12-09 12:05:56.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 9.212469100952148
2025-12-09 12:05:56.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 9.212516784667969
2025-12-09 12:05:56.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 9.212868690490723
2025-12-09 12:05:56.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 9.211922645568848
2025-12-09 12:05:56.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 9.21279239654541
2025-12-09 12:05:56.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 9.212549209594727
2025-12-09 12:05:56.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 9.212141036987305
2025-12-09 12:05:56.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 9.21339225769043
2025-12-09 12:05:56.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 9.212337493896484
2025-12-09 12:05:56.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 9.212580680847168
2025-12-09 12:05:56.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 9.211901664733887
2025-12-09 12:05:56.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 9.211590766906738
2025-12-09 12:05:56.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 9.2122802734375
2025-12-09 12:05:56.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 9.212647438049316
2025-12-09 12:05:56.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 9.211946487426758
2025-12-09 12:05:56.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 9.21134090423584
2025-12-09 12:05:56.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 9.212160110473633
2025-12-09 12:05:57.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 9.211423873901367
2025-12-09 12:05:57.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 9.211854934692383
2025-12-09 12:05:57.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 9.211485862731934
2025-12-09 12:05:57.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 9.212435722351074
2025-12-09 12:05:57.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 9.21203327178955
2025-12-09 12:05:57.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 9.212258338928223
2025-12-09 12:05:57.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 9.211906433105469
2025-12-09 12:05:57.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 9.211371421813965
2025-12-09 12:05:57.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 9.211746215820312
2025-12-09 12:05:57.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 9.211563110351562
2025-12-09 12:05:57.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 9.211729049682617
2025-12-09 12:05:57.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 9.2110595703125
2025-12-09 12:05:57.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 9.211434364318848
2025-12-09 12:05:57.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 9.212080001831055
2025-12-09 12:05:57.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 9.211519241333008
2025-12-09 12:05:57.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 9.211006164550781
2025-12-09 12:05:57.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 9.211880683898926
2025-12-09 12:05:57.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 9.21142292022705
2025-12-09 12:05:57.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 9.211287498474121
2025-12-09 12:05:57.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 9.210411071777344
2025-12-09 12:05:57.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 9.211216926574707
2025-12-09 12:05:57.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 9.210859298706055
2025-12-09 12:05:57.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 9.211082458496094
2025-12-09 12:05:57.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 9.210892677307129
2025-12-09 12:05:57.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 9.211174011230469
2025-12-09 12:05:57.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 9.211050033569336
2025-12-09 12:05:57.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 9.210896492004395
2025-12-09 12:05:57.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 9.210919380187988
2025-12-09 12:05:57.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 9.210366249084473
2025-12-09 12:05:57.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 9.210972785949707
2025-12-09 12:05:57.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0002909538931178862 Training loss: 9.210627555847168
2025-12-09 12:05:57.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00026490666646784665 Training loss: 9.210921287536621
2025-12-09 12:05:57.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000225 Training loss: 9.210464477539062
2025-12-09 12:05:57.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00017604722665003956 Training loss: 9.211262702941895
2025-12-09 12:05:57.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00012395277334996044 Training loss: 9.210084915161133
2025-12-09 12:05:57.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 7.500000000000002e-05 Training loss: 9.210750579833984
2025-12-09 12:05:57.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 3.509333353215331e-05 Training loss: 9.210490226745605
2025-12-09 12:05:57.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113751e-06 Training loss: 9.210709571838379
2025-12-09 12:05:57.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 9.209187507629395
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:06:10.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 9.206876754760742
2025-12-09 12:06:10.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 9.206351280212402
2025-12-09 12:06:10.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 9.206207275390625
2025-12-09 12:06:10.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 9.206313133239746
2025-12-09 12:06:10.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 9.206485748291016
2025-12-09 12:06:10.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 9.206218719482422
2025-12-09 12:06:10.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 9.205831527709961
2025-12-09 12:06:10.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 9.206082344055176
2025-12-09 12:06:10.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 9.207072257995605
2025-12-09 12:06:10.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 9.206341743469238
2025-12-09 12:06:10.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 9.206046104431152
2025-12-09 12:06:10.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 9.206380844116211
2025-12-09 12:06:10.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 9.206282615661621
2025-12-09 12:06:10.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 9.206995010375977
2025-12-09 12:06:11.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 9.206212997436523
2025-12-09 12:06:11.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 9.206011772155762
2025-12-09 12:06:11.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 9.20606803894043
2025-12-09 12:06:11.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 9.206092834472656
2025-12-09 12:06:11.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 9.206725120544434
2025-12-09 12:06:11.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 9.205892562866211
2025-12-09 12:06:11.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 9.206048011779785
2025-12-09 12:06:11.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 9.205269813537598
2025-12-09 12:06:11.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 9.20545482635498
2025-12-09 12:06:11.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 9.205982208251953
2025-12-09 12:06:11.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 9.206063270568848
2025-12-09 12:06:11.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 9.20632266998291
2025-12-09 12:06:11.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 9.205892562866211
2025-12-09 12:06:11.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 9.205707550048828
2025-12-09 12:06:11.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 9.206028938293457
2025-12-09 12:06:11.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 9.205422401428223
2025-12-09 12:06:11.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 9.205460548400879
2025-12-09 12:06:11.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 9.205836296081543
2025-12-09 12:06:11.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 9.205754280090332
2025-12-09 12:06:11.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 9.206059455871582
2025-12-09 12:06:11.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 9.206026077270508
2025-12-09 12:06:11.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 9.205235481262207
2025-12-09 12:06:11.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 9.2050199508667
2025-12-09 12:06:11.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 9.204569816589355
2025-12-09 12:06:11.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 9.20505142211914
2025-12-09 12:06:11.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 9.205095291137695
2025-12-09 12:06:11.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 9.205799102783203
2025-12-09 12:06:11.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 9.205530166625977
2025-12-09 12:06:11.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 9.204809188842773
2025-12-09 12:06:11.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 9.20627212524414
2025-12-09 12:06:11.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 9.205501556396484
2025-12-09 12:06:11.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 9.20466136932373
2025-12-09 12:06:11.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 9.204520225524902
2025-12-09 12:06:11.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 9.204052925109863
2025-12-09 12:06:11.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 9.203927993774414
2025-12-09 12:06:11.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 9.203824043273926
2025-12-09 12:06:11.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 9.204754829406738
2025-12-09 12:06:11.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 9.203754425048828
2025-12-09 12:06:11.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 9.204029083251953
2025-12-09 12:06:11.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 9.20383071899414
2025-12-09 12:06:11.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 9.204373359680176
2025-12-09 12:06:11.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 9.2037992477417
2025-12-09 12:06:11.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 9.204781532287598
2025-12-09 12:06:11.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 9.202730178833008
2025-12-09 12:06:11.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 9.204054832458496
2025-12-09 12:06:11.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 9.20325756072998
2025-12-09 12:06:11.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 9.203513145446777
2025-12-09 12:06:11.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 9.203205108642578
2025-12-09 12:06:11.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 9.203441619873047
2025-12-09 12:06:11.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 9.203180313110352
2025-12-09 12:06:11.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 9.203012466430664
2025-12-09 12:06:11.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 9.20279312133789
2025-12-09 12:06:11.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 9.203445434570312
2025-12-09 12:06:11.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 9.202742576599121
2025-12-09 12:06:11.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 9.203317642211914
2025-12-09 12:06:11.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 9.202839851379395
2025-12-09 12:06:11.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 9.203027725219727
2025-12-09 12:06:11.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 9.202582359313965
2025-12-09 12:06:11.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 9.20238971710205
2025-12-09 12:06:11.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 9.202912330627441
2025-12-09 12:06:11.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 9.201623916625977
2025-12-09 12:06:11.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 9.201671600341797
2025-12-09 12:06:11.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 9.201393127441406
2025-12-09 12:06:11.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 9.201011657714844
2025-12-09 12:06:11.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 9.201615333557129
2025-12-09 12:06:12.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 9.203155517578125
2025-12-09 12:06:12.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 9.201201438903809
2025-12-09 12:06:12.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 9.201642990112305
2025-12-09 12:06:12.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 9.200433731079102
2025-12-09 12:06:12.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 9.20090389251709
2025-12-09 12:06:12.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 9.200797080993652
2025-12-09 12:06:12.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 9.200785636901855
2025-12-09 12:06:12.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 9.199981689453125
2025-12-09 12:06:12.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 9.200504302978516
2025-12-09 12:06:12.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 9.201117515563965
2025-12-09 12:06:12.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 9.200688362121582
2025-12-09 12:06:12.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 9.199512481689453
2025-12-09 12:06:12.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 9.199265480041504
2025-12-09 12:06:12.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 9.20095157623291
2025-12-09 12:06:12.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 9.199548721313477
2025-12-09 12:06:12.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 9.199079513549805
2025-12-09 12:06:12.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 9.199514389038086
2025-12-09 12:06:12.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 9.199511528015137
2025-12-09 12:06:12.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 9.198975563049316
2025-12-09 12:06:12.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 9.198731422424316
2025-12-09 12:06:12.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 9.198858261108398
2025-12-09 12:06:12.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009698463103929542 Training loss: 9.199006080627441
2025-12-09 12:06:12.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.000883022221559489 Training loss: 9.198145866394043
2025-12-09 12:06:12.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.00075 Training loss: 9.198347091674805
2025-12-09 12:06:12.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0005868240888334653 Training loss: 9.19830322265625
2025-12-09 12:06:12.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00041317591116653486 Training loss: 9.19803237915039
2025-12-09 12:06:12.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002500000000000001 Training loss: 9.19914436340332
2025-12-09 12:06:12.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.00011697777844051105 Training loss: 9.198724746704102
2025-12-09 12:06:12.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 3.0153689607045842e-05 Training loss: 9.19735050201416
2025-12-09 12:06:12.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 9.19536304473877
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:06:25.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 9.21070671081543
2025-12-09 12:06:25.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 9.211498260498047
2025-12-09 12:06:25.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 9.20983600616455
2025-12-09 12:06:25.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 9.210346221923828
2025-12-09 12:06:25.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 9.21045207977295
2025-12-09 12:06:25.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 9.210097312927246
2025-12-09 12:06:25.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 9.21127986907959
2025-12-09 12:06:25.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 9.210306167602539
2025-12-09 12:06:25.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 9.210771560668945
2025-12-09 12:06:25.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 9.209915161132812
2025-12-09 12:06:25.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 9.210298538208008
2025-12-09 12:06:25.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 9.210596084594727
2025-12-09 12:06:25.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 9.210412979125977
2025-12-09 12:06:25.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 9.209940910339355
2025-12-09 12:06:25.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 9.21056079864502
2025-12-09 12:06:25.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 9.209907531738281
2025-12-09 12:06:25.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 9.210492134094238
2025-12-09 12:06:25.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 9.21011734008789
2025-12-09 12:06:25.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 9.21005916595459
2025-12-09 12:06:25.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 9.209327697753906
2025-12-09 12:06:25.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 9.20950698852539
2025-12-09 12:06:25.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 9.210524559020996
2025-12-09 12:06:25.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 9.209683418273926
2025-12-09 12:06:25.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 9.208869934082031
2025-12-09 12:06:25.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 9.2097749710083
2025-12-09 12:06:25.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 9.210206985473633
2025-12-09 12:06:25.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 9.20969295501709
2025-12-09 12:06:25.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 9.208860397338867
2025-12-09 12:06:25.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 9.20798110961914
2025-12-09 12:06:25.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 9.208712577819824
2025-12-09 12:06:25.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 9.208771705627441
2025-12-09 12:06:26.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 9.20816421508789
2025-12-09 12:06:26.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 9.207880973815918
2025-12-09 12:06:26.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 9.207921981811523
2025-12-09 12:06:26.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 9.207528114318848
2025-12-09 12:06:26.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 9.207056045532227
2025-12-09 12:06:26.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 9.207371711730957
2025-12-09 12:06:26.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 9.207441329956055
2025-12-09 12:06:26.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 9.207262992858887
2025-12-09 12:06:26.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 9.207125663757324
2025-12-09 12:06:26.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 9.206172943115234
2025-12-09 12:06:26.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 9.206742286682129
2025-12-09 12:06:26.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 9.20680046081543
2025-12-09 12:06:26.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 9.20602798461914
2025-12-09 12:06:26.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 9.205410957336426
2025-12-09 12:06:26.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 9.205883979797363
2025-12-09 12:06:26.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 9.205810546875
2025-12-09 12:06:26.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 9.205482482910156
2025-12-09 12:06:26.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 9.205771446228027
2025-12-09 12:06:26.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 9.205453872680664
2025-12-09 12:06:26.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 9.205107688903809
2025-12-09 12:06:26.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 9.204365730285645
2025-12-09 12:06:26.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 9.20447826385498
2025-12-09 12:06:26.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 9.20429801940918
2025-12-09 12:06:26.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 9.2036714553833
2025-12-09 12:06:26.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 9.202563285827637
2025-12-09 12:06:26.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 9.203913688659668
2025-12-09 12:06:26.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 9.20321273803711
2025-12-09 12:06:26.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 9.202956199645996
2025-12-09 12:06:26.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 9.20206356048584
2025-12-09 12:06:26.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 9.202898025512695
2025-12-09 12:06:26.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 9.201386451721191
2025-12-09 12:06:26.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 9.201436042785645
2025-12-09 12:06:26.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 9.200857162475586
2025-12-09 12:06:26.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 9.200390815734863
2025-12-09 12:06:26.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 9.201284408569336
2025-12-09 12:06:26.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 9.200096130371094
2025-12-09 12:06:26.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 9.20004940032959
2025-12-09 12:06:26.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 9.199929237365723
2025-12-09 12:06:26.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 9.199563980102539
2025-12-09 12:06:26.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 9.199190139770508
2025-12-09 12:06:26.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 9.198697090148926
2025-12-09 12:06:26.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 9.198880195617676
2025-12-09 12:06:26.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 9.197277069091797
2025-12-09 12:06:26.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 9.197827339172363
2025-12-09 12:06:26.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 9.197293281555176
2025-12-09 12:06:26.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 9.1973295211792
2025-12-09 12:06:26.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 9.19669246673584
2025-12-09 12:06:26.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 9.196587562561035
2025-12-09 12:06:26.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 9.195343971252441
2025-12-09 12:06:26.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 9.19727897644043
2025-12-09 12:06:26.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 9.195211410522461
2025-12-09 12:06:26.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 9.194452285766602
2025-12-09 12:06:26.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 9.1947021484375
2025-12-09 12:06:26.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 9.19462776184082
2025-12-09 12:06:26.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 9.193660736083984
2025-12-09 12:06:26.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 9.193757057189941
2025-12-09 12:06:26.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 9.193964004516602
2025-12-09 12:06:26.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 9.19311809539795
2025-12-09 12:06:26.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 9.19212532043457
2025-12-09 12:06:26.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 9.193155288696289
2025-12-09 12:06:26.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 9.192254066467285
2025-12-09 12:06:26.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 9.190948486328125
2025-12-09 12:06:26.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 9.19106674194336
2025-12-09 12:06:26.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 9.19128704071045
2025-12-09 12:06:26.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 9.190295219421387
2025-12-09 12:06:27.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 9.18956470489502
2025-12-09 12:06:27.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 9.190181732177734
2025-12-09 12:06:27.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 9.189826965332031
2025-12-09 12:06:27.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 9.189397811889648
2025-12-09 12:06:27.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.002909538931178863 Training loss: 9.18880558013916
2025-12-09 12:06:27.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002649066664678467 Training loss: 9.18875503540039
2025-12-09 12:06:27.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0022500000000000003 Training loss: 9.188403129577637
2025-12-09 12:06:27.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0017604722665003959 Training loss: 9.18795394897461
2025-12-09 12:06:27.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0012395277334996046 Training loss: 9.18618392944336
2025-12-09 12:06:27.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0007500000000000003 Training loss: 9.188094139099121
2025-12-09 12:06:27.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0003509333353215332 Training loss: 9.18581485748291
2025-12-09 12:06:27.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113752e-05 Training loss: 9.187644958496094
2025-12-09 12:06:27.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 9.188776016235352
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:06:40.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 9.209999084472656
2025-12-09 12:06:40.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 9.21146011352539
2025-12-09 12:06:40.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 9.210366249084473
2025-12-09 12:06:40.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 9.210699081420898
2025-12-09 12:06:40.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 9.210087776184082
2025-12-09 12:06:40.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 9.210454940795898
2025-12-09 12:06:40.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 9.21002197265625
2025-12-09 12:06:40.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 9.209879875183105
2025-12-09 12:06:40.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 9.210851669311523
2025-12-09 12:06:40.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 9.209966659545898
2025-12-09 12:06:40.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 9.209686279296875
2025-12-09 12:06:40.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 9.209589004516602
2025-12-09 12:06:40.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 9.209878921508789
2025-12-09 12:06:40.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 9.209556579589844
2025-12-09 12:06:40.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 9.209092140197754
2025-12-09 12:06:40.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 9.209318161010742
2025-12-09 12:06:40.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 9.209025382995605
2025-12-09 12:06:40.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 9.208483695983887
2025-12-09 12:06:40.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 9.2069673538208
2025-12-09 12:06:40.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 9.207907676696777
2025-12-09 12:06:40.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 9.20862102508545
2025-12-09 12:06:40.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 9.208059310913086
2025-12-09 12:06:40.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 9.206680297851562
2025-12-09 12:06:40.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 9.206539154052734
2025-12-09 12:06:40.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 9.206056594848633
2025-12-09 12:06:40.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 9.206703186035156
2025-12-09 12:06:40.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 9.206052780151367
2025-12-09 12:06:40.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 9.205138206481934
2025-12-09 12:06:40.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 9.204345703125
2025-12-09 12:06:40.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 9.203618049621582
2025-12-09 12:06:40.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 9.204437255859375
2025-12-09 12:06:40.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 9.203662872314453
2025-12-09 12:06:40.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 9.202448844909668
2025-12-09 12:06:40.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 9.201532363891602
2025-12-09 12:06:40.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 9.202292442321777
2025-12-09 12:06:40.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 9.201266288757324
2025-12-09 12:06:41.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 9.200623512268066
2025-12-09 12:06:41.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 9.200783729553223
2025-12-09 12:06:41.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 9.19987678527832
2025-12-09 12:06:41.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 9.199650764465332
2025-12-09 12:06:41.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 9.198962211608887
2025-12-09 12:06:41.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 9.19703483581543
2025-12-09 12:06:41.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 9.198054313659668
2025-12-09 12:06:41.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 9.197697639465332
2025-12-09 12:06:41.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 9.195648193359375
2025-12-09 12:06:41.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 9.194279670715332
2025-12-09 12:06:41.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 9.194808006286621
2025-12-09 12:06:41.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 9.194293975830078
2025-12-09 12:06:41.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 9.193161964416504
2025-12-09 12:06:41.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 9.192587852478027
2025-12-09 12:06:41.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 9.192758560180664
2025-12-09 12:06:41.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 9.192192077636719
2025-12-09 12:06:41.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 9.191022872924805
2025-12-09 12:06:41.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 9.190803527832031
2025-12-09 12:06:41.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 9.189075469970703
2025-12-09 12:06:41.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 9.189713478088379
2025-12-09 12:06:41.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 9.188621520996094
2025-12-09 12:06:41.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 9.187103271484375
2025-12-09 12:06:41.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 9.185853004455566
2025-12-09 12:06:41.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 9.185774803161621
2025-12-09 12:06:41.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 9.18369197845459
2025-12-09 12:06:41.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 9.185553550720215
2025-12-09 12:06:41.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 9.184773445129395
2025-12-09 12:06:41.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 9.18313980102539
2025-12-09 12:06:41.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 9.181711196899414
2025-12-09 12:06:41.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 9.181602478027344
2025-12-09 12:06:41.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 9.181554794311523
2025-12-09 12:06:41.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 9.17831039428711
2025-12-09 12:06:41.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 9.178744316101074
2025-12-09 12:06:41.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 9.177574157714844
2025-12-09 12:06:41.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 9.17864990234375
2025-12-09 12:06:41.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 9.175681114196777
2025-12-09 12:06:41.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 9.177225112915039
2025-12-09 12:06:41.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 9.175692558288574
2025-12-09 12:06:41.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 9.174171447753906
2025-12-09 12:06:41.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 9.17521858215332
2025-12-09 12:06:41.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 9.171536445617676
2025-12-09 12:06:41.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 9.170650482177734
2025-12-09 12:06:41.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 9.169734954833984
2025-12-09 12:06:41.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 9.169398307800293
2025-12-09 12:06:41.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 9.169102668762207
2025-12-09 12:06:41.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 9.169157981872559
2025-12-09 12:06:41.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 9.167255401611328
2025-12-09 12:06:41.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 9.166399955749512
2025-12-09 12:06:41.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 9.164761543273926
2025-12-09 12:06:41.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 9.167752265930176
2025-12-09 12:06:41.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 9.164509773254395
2025-12-09 12:06:41.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 9.162942886352539
2025-12-09 12:06:41.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 9.162818908691406
2025-12-09 12:06:41.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 9.165106773376465
2025-12-09 12:06:41.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 9.161785125732422
2025-12-09 12:06:41.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 9.163398742675781
2025-12-09 12:06:41.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 9.160959243774414
2025-12-09 12:06:41.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 9.161722183227539
2025-12-09 12:06:41.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 9.157424926757812
2025-12-09 12:06:41.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 9.157280921936035
2025-12-09 12:06:41.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 9.154627799987793
2025-12-09 12:06:41.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 9.157463073730469
2025-12-09 12:06:41.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 9.154134750366211
2025-12-09 12:06:41.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 9.156082153320312
2025-12-09 12:06:41.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009698463103929543 Training loss: 9.155733108520508
2025-12-09 12:06:41.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00883022221559489 Training loss: 9.153365135192871
2025-12-09 12:06:42.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0075 Training loss: 9.152853965759277
2025-12-09 12:06:42.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.005868240888334653 Training loss: 9.154276847839355
2025-12-09 12:06:42.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0041317591116653484 Training loss: 9.153300285339355
2025-12-09 12:06:42.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0025000000000000014 Training loss: 9.152152061462402
2025-12-09 12:06:42.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0011697777844051104 Training loss: 9.152538299560547
2025-12-09 12:06:42.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00030153689607045843 Training loss: 9.1519775390625
2025-12-09 12:06:42.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 9.151529312133789
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:06:57.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 4.854283809661865
2025-12-09 12:06:57.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 4.831978797912598
2025-12-09 12:06:57.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 4.912419319152832
2025-12-09 12:06:57.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 4.823228359222412
2025-12-09 12:06:57.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 4.990100860595703
2025-12-09 12:06:57.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 4.875051498413086
2025-12-09 12:06:57.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 4.937801361083984
2025-12-09 12:06:57.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 4.871363639831543
2025-12-09 12:06:57.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 4.812675476074219
2025-12-09 12:06:57.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 4.856395244598389
2025-12-09 12:06:57.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 4.853599548339844
2025-12-09 12:06:57.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 4.83686637878418
2025-12-09 12:06:57.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 4.812750339508057
2025-12-09 12:06:57.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 4.813241481781006
2025-12-09 12:06:57.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 4.678197860717773
2025-12-09 12:06:57.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 4.876860618591309
2025-12-09 12:06:57.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 4.928826808929443
2025-12-09 12:06:57.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 4.762227535247803
2025-12-09 12:06:57.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 4.854517936706543
2025-12-09 12:06:57.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 4.845731735229492
2025-12-09 12:06:57.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 4.890653133392334
2025-12-09 12:06:57.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 4.8168134689331055
2025-12-09 12:06:58.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 4.880712985992432
2025-12-09 12:06:58.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 4.927534103393555
2025-12-09 12:06:58.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 4.8227410316467285
2025-12-09 12:06:58.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 4.783098220825195
2025-12-09 12:06:58.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 4.806928634643555
2025-12-09 12:06:58.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 4.772195339202881
2025-12-09 12:06:58.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 4.8470659255981445
2025-12-09 12:06:58.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 4.765686511993408
2025-12-09 12:06:58.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 4.860014915466309
2025-12-09 12:06:58.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 4.730714321136475
2025-12-09 12:06:58.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 4.9148383140563965
2025-12-09 12:06:58.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 4.8553547859191895
2025-12-09 12:06:58.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 4.727014064788818
2025-12-09 12:06:58.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 4.723910331726074
2025-12-09 12:06:58.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 4.726020812988281
2025-12-09 12:06:58.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 4.777405738830566
2025-12-09 12:06:58.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 4.742199897766113
2025-12-09 12:06:58.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 4.7550458908081055
2025-12-09 12:06:58.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 4.630533218383789
2025-12-09 12:06:58.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 4.782924175262451
2025-12-09 12:06:58.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 4.612030982971191
2025-12-09 12:06:58.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 4.677438259124756
2025-12-09 12:06:58.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 4.740228176116943
2025-12-09 12:06:58.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 4.6700873374938965
2025-12-09 12:06:58.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 4.603530406951904
2025-12-09 12:06:58.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 4.743465900421143
2025-12-09 12:06:58.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 4.639352798461914
2025-12-09 12:06:58.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 4.618043422698975
2025-12-09 12:06:58.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 4.5150041580200195
2025-12-09 12:06:58.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 4.624542236328125
2025-12-09 12:06:58.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 4.580234527587891
2025-12-09 12:06:58.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 4.729524612426758
2025-12-09 12:06:58.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 4.5885396003723145
2025-12-09 12:06:58.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 4.616948127746582
2025-12-09 12:06:58.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 4.5866312980651855
2025-12-09 12:06:58.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 4.673373222351074
2025-12-09 12:06:58.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 4.635913848876953
2025-12-09 12:06:58.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 4.673069953918457
2025-12-09 12:06:58.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 4.536548137664795
2025-12-09 12:06:58.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 4.646340847015381
2025-12-09 12:06:58.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 4.501505374908447
2025-12-09 12:06:58.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 4.542118072509766
2025-12-09 12:06:58.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 4.6350297927856445
2025-12-09 12:06:58.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 4.62124490737915
2025-12-09 12:06:58.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 4.595634460449219
2025-12-09 12:06:58.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 4.492221832275391
2025-12-09 12:06:58.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 4.547300815582275
2025-12-09 12:06:58.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 4.410206317901611
2025-12-09 12:06:58.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 4.533703804016113
2025-12-09 12:06:58.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 4.44480562210083
2025-12-09 12:06:58.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 4.410390853881836
2025-12-09 12:06:58.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 4.534135341644287
2025-12-09 12:06:58.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 4.423419952392578
2025-12-09 12:06:58.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 4.519959449768066
2025-12-09 12:06:58.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 4.436923980712891
2025-12-09 12:06:58.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 4.469552516937256
2025-12-09 12:06:58.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 4.459731101989746
2025-12-09 12:06:58.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 4.340163230895996
2025-12-09 12:06:58.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 4.40826940536499
2025-12-09 12:06:58.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 4.270868301391602
2025-12-09 12:06:58.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 4.3908586502075195
2025-12-09 12:06:58.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 4.452849864959717
2025-12-09 12:06:58.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 4.4256110191345215
2025-12-09 12:06:58.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 4.452824592590332
2025-12-09 12:06:58.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 4.476365566253662
2025-12-09 12:06:58.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 4.443815231323242
2025-12-09 12:06:58.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 4.311321258544922
2025-12-09 12:06:58.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 4.3514790534973145
2025-12-09 12:06:58.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 4.534542083740234
2025-12-09 12:06:58.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 4.36632776260376
2025-12-09 12:06:58.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 4.442415714263916
2025-12-09 12:06:58.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 4.4687066078186035
2025-12-09 12:06:58.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 4.4546709060668945
2025-12-09 12:06:58.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 4.319267749786377
2025-12-09 12:06:58.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 4.408821105957031
2025-12-09 12:06:58.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 4.424880027770996
2025-12-09 12:06:58.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 4.384862422943115
2025-12-09 12:06:58.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 4.300318241119385
2025-12-09 12:06:58.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.999708626830618e-05 Training loss: 4.329293727874756
2025-12-09 12:06:58.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 9.998834541281798e-05 Training loss: 4.3628153800964355
2025-12-09 12:06:58.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 9.997377845227576e-05 Training loss: 4.249407768249512
2025-12-09 12:06:58.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 9.995338708444804e-05 Training loss: 4.1984734535217285
2025-12-09 12:06:58.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 9.992717368593385e-05 Training loss: 4.306155204772949
2025-12-09 12:06:58.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 9.989514131188559e-05 Training loss: 4.290853500366211
2025-12-09 12:06:58.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 9.985729369565299e-05 Training loss: 4.308109283447266
2025-12-09 12:06:58.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.9813635248348e-05 Training loss: 4.20418119430542
2025-12-09 12:06:58.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 9.97641710583307e-05 Training loss: 4.1398186683654785
2025-12-09 12:06:58.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 9.970890689061622e-05 Training loss: 4.166777610778809
2025-12-09 12:06:58.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 9.964784918620282e-05 Training loss: 4.174644947052002
2025-12-09 12:06:58.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 9.958100506132127e-05 Training loss: 4.178516864776611
2025-12-09 12:06:58.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 9.950838230660534e-05 Training loss: 4.236655235290527
2025-12-09 12:06:58.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 9.942998938618394e-05 Training loss: 4.278528690338135
2025-12-09 12:06:58.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 9.934583543669453e-05 Training loss: 4.132211685180664
2025-12-09 12:06:58.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 9.925593026621833e-05 Training loss: 4.113114356994629
2025-12-09 12:06:58.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 9.916028435313708e-05 Training loss: 4.19613790512085
2025-12-09 12:06:58.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 9.905890884491195e-05 Training loss: 4.395435333251953
2025-12-09 12:06:58.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 9.895181555678418e-05 Training loss: 4.3150529861450195
2025-12-09 12:06:58.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 9.883901697039808e-05 Training loss: 4.23065710067749
2025-12-09 12:06:58.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 9.872052623234632e-05 Training loss: 4.1830644607543945
2025-12-09 12:06:58.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 9.85963571526376e-05 Training loss: 4.29765510559082
2025-12-09 12:06:58.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 9.846652420308728e-05 Training loss: 4.216587066650391
2025-12-09 12:06:58.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 9.833104251563056e-05 Training loss: 4.136542320251465
2025-12-09 12:06:58.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 9.818992788055889e-05 Training loss: 4.1336669921875
2025-12-09 12:06:58.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 9.80431967446797e-05 Training loss: 4.301205635070801
2025-12-09 12:06:58.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 9.789086620939936e-05 Training loss: 4.1358113288879395
2025-12-09 12:06:58.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 9.773295402873026e-05 Training loss: 4.085188388824463
2025-12-09 12:06:58.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 9.756947860722143e-05 Training loss: 4.11769962310791
2025-12-09 12:06:58.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 9.740045899781352e-05 Training loss: 4.02972936630249
2025-12-09 12:06:58.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 9.722591489961827e-05 Training loss: 4.1487135887146
2025-12-09 12:06:58.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 9.70458666556225e-05 Training loss: 4.111783027648926
2025-12-09 12:06:58.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 9.686033525031719e-05 Training loss: 4.20424222946167
2025-12-09 12:06:58.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 9.66693423072518e-05 Training loss: 4.204248428344727
2025-12-09 12:06:58.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 9.647291008651398e-05 Training loss: 4.230284214019775
2025-12-09 12:06:59.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 9.627106148213522e-05 Training loss: 4.202953815460205
2025-12-09 12:06:59.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 9.606382001942255e-05 Training loss: 4.071052074432373
2025-12-09 12:06:59.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 9.585120985221671e-05 Training loss: 4.074645042419434
2025-12-09 12:06:59.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 9.563325576007701e-05 Training loss: 4.087821960449219
2025-12-09 12:06:59.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 9.540998314539328e-05 Training loss: 4.113432884216309
2025-12-09 12:06:59.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 9.518141803042527e-05 Training loss: 4.123684406280518
2025-12-09 12:06:59.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 9.494758705426978e-05 Training loss: 3.963097333908081
2025-12-09 12:06:59.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 9.470851746975582e-05 Training loss: 4.1621785163879395
2025-12-09 12:06:59.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 9.446423714026846e-05 Training loss: 4.037012100219727
2025-12-09 12:06:59.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 9.421477453650118e-05 Training loss: 3.975029468536377
2025-12-09 12:06:59.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 9.396015873313781e-05 Training loss: 4.211562633514404
2025-12-09 12:06:59.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 9.37004194054638e-05 Training loss: 3.9974281787872314
2025-12-09 12:06:59.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 9.343558682590756e-05 Training loss: 4.17371940612793
2025-12-09 12:06:59.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 9.316569186051234e-05 Training loss: 4.0117363929748535
2025-12-09 12:06:59.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 9.289076596533872e-05 Training loss: 4.102156162261963
2025-12-09 12:06:59.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 9.261084118279847e-05 Training loss: 4.0645647048950195
2025-12-09 12:06:59.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 9.232595013792002e-05 Training loss: 3.945413589477539
2025-12-09 12:06:59.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 9.203612603454604e-05 Training loss: 4.094845294952393
2025-12-09 12:06:59.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 9.174140265146356e-05 Training loss: 3.968907594680786
2025-12-09 12:06:59.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 9.144181433846707e-05 Training loss: 4.028271675109863
2025-12-09 12:06:59.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 9.113739601235507e-05 Training loss: 4.012262344360352
2025-12-09 12:06:59.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 9.082818315286055e-05 Training loss: 3.944751739501953
2025-12-09 12:06:59.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 9.051421179851588e-05 Training loss: 3.931450366973877
2025-12-09 12:06:59.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 9.01955185424525e-05 Training loss: 3.851182460784912
2025-12-09 12:06:59.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 8.987214052813604e-05 Training loss: 4.0422515869140625
2025-12-09 12:06:59.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 8.954411544503729e-05 Training loss: 3.963252067565918
2025-12-09 12:06:59.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 8.921148152423946e-05 Training loss: 3.9542598724365234
2025-12-09 12:06:59.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 8.887427753398248e-05 Training loss: 4.071305751800537
2025-12-09 12:06:59.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 8.853254277514446e-05 Training loss: 4.010483264923096
2025-12-09 12:06:59.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 8.818631707666135e-05 Training loss: 4.088729381561279
2025-12-09 12:06:59.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 8.783564079088477e-05 Training loss: 4.153090476989746
2025-12-09 12:06:59.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 8.748055478887904e-05 Training loss: 4.0196943283081055
2025-12-09 12:06:59.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 8.712110045565768e-05 Training loss: 3.919039011001587
2025-12-09 12:06:59.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 8.675731968536002e-05 Training loss: 3.8949778079986572
2025-12-09 12:06:59.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 8.638925487636848e-05 Training loss: 3.982841730117798
2025-12-09 12:06:59.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 8.6016948926367e-05 Training loss: 3.849442958831787
2025-12-09 12:06:59.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 8.564044522734147e-05 Training loss: 4.0160980224609375
2025-12-09 12:06:59.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 8.52597876605223e-05 Training loss: 3.834015369415283
2025-12-09 12:06:59.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 8.487502059127015e-05 Training loss: 3.97245454788208
2025-12-09 12:06:59.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 8.448618886390522e-05 Training loss: 3.8875842094421387
2025-12-09 12:06:59.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 8.40933377964806e-05 Training loss: 3.8656091690063477
2025-12-09 12:06:59.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 8.369651317550054e-05 Training loss: 3.9063503742218018
2025-12-09 12:06:59.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 8.329576125058406e-05 Training loss: 3.8623619079589844
2025-12-09 12:06:59.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 8.289112872907454e-05 Training loss: 4.019557476043701
2025-12-09 12:06:59.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 8.248266277059607e-05 Training loss: 4.024027347564697
2025-12-09 12:06:59.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 8.2070410981557e-05 Training loss: 3.846798896789551
2025-12-09 12:06:59.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 8.16544214096015e-05 Training loss: 3.9575424194335938
2025-12-09 12:06:59.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 8.123474253800957e-05 Training loss: 3.769230604171753
2025-12-09 12:06:59.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 8.081142328004637e-05 Training loss: 3.887059211730957
2025-12-09 12:06:59.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 8.038451297326145e-05 Training loss: 3.9429121017456055
2025-12-09 12:06:59.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 7.995406137373846e-05 Training loss: 3.939791202545166
2025-12-09 12:06:59.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 7.952011865029614e-05 Training loss: 3.8799846172332764
2025-12-09 12:06:59.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 7.908273537864113e-05 Training loss: 4.019714832305908
2025-12-09 12:06:59.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 7.86419625354735e-05 Training loss: 3.843060255050659
2025-12-09 12:06:59.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 7.819785149254532e-05 Training loss: 4.0061492919921875
2025-12-09 12:06:59.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 7.77504540106735e-05 Training loss: 3.7788784503936768
2025-12-09 12:06:59.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 7.729982223370691e-05 Training loss: 3.73054575920105
2025-12-09 12:06:59.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 7.68460086824492e-05 Training loss: 3.8469388484954834
2025-12-09 12:06:59.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 7.638906624853743e-05 Training loss: 3.7584152221679688
2025-12-09 12:06:59.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 7.592904818827775e-05 Training loss: 3.9083900451660156
2025-12-09 12:06:59.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 7.546600811643816e-05 Training loss: 3.9068562984466553
2025-12-09 12:06:59.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 7.500000000000001e-05 Training loss: 3.9516279697418213
2025-12-09 12:06:59.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 7.453107815186803e-05 Training loss: 3.8569607734680176
2025-12-09 12:06:59.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 7.405929722454026e-05 Training loss: 3.838956832885742
2025-12-09 12:06:59.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 7.358471220373832e-05 Training loss: 3.741452217102051
2025-12-09 12:06:59.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 7.310737840199885e-05 Training loss: 3.893106460571289
2025-12-09 12:06:59.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 7.262735145222696e-05 Training loss: 3.786752462387085
2025-12-09 12:06:59.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 7.214468730121208e-05 Training loss: 3.843427896499634
2025-12-09 12:06:59.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 7.165944220310767e-05 Training loss: 3.7135894298553467
2025-12-09 12:06:59.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 7.117167271287453e-05 Training loss: 3.890105962753296
2025-12-09 12:06:59.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 7.068143567968957e-05 Training loss: 3.986466884613037
2025-12-09 12:06:59.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 7.018878824032009e-05 Training loss: 3.6334409713745117
2025-12-09 12:06:59.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 6.969378781246436e-05 Training loss: 3.9476349353790283
2025-12-09 12:06:59.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 6.919649208805981e-05 Training loss: 3.7053730487823486
2025-12-09 12:06:59.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 6.869695902655897e-05 Training loss: 3.8539552688598633
2025-12-09 12:06:59.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 6.819524684817438e-05 Training loss: 3.9609148502349854
2025-12-09 12:06:59.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 6.769141402709305e-05 Training loss: 3.7822141647338867
2025-12-09 12:06:59.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 6.718551928466132e-05 Training loss: 3.9322547912597656
2025-12-09 12:06:59.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 6.667762158254104e-05 Training loss: 3.6904714107513428
2025-12-09 12:06:59.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 6.616778011583743e-05 Training loss: 3.812622308731079
2025-12-09 12:06:59.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 6.565605430620013e-05 Training loss: 3.7843427658081055
2025-12-09 12:06:59.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 6.514250379489753e-05 Training loss: 3.8327596187591553
2025-12-09 12:06:59.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 6.462718843586571e-05 Training loss: 3.924203634262085
2025-12-09 12:06:59.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 6.411016828873239e-05 Training loss: 3.723508358001709
2025-12-09 12:06:59.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 6.359150361181715e-05 Training loss: 3.7187492847442627
2025-12-09 12:06:59.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 6.307125485510828e-05 Training loss: 3.880784511566162
2025-12-09 12:06:59.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 6.254948265321744e-05 Training loss: 3.8366312980651855
2025-12-09 12:06:59.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 6.202624781831268e-05 Training loss: 3.8977701663970947
2025-12-09 12:06:59.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 6.150161133303089e-05 Training loss: 3.944199562072754
2025-12-09 12:06:59.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 6.0975634343370256e-05 Training loss: 3.8944973945617676
2025-12-09 12:06:59.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 6.044837815156377e-05 Training loss: 3.716460943222046
2025-12-09 12:06:59.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 5.99199042089345e-05 Training loss: 3.700043201446533
2025-12-09 12:06:59.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 5.939027410873351e-05 Training loss: 3.95253586769104
2025-12-09 12:06:59.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 5.885954957896115e-05 Training loss: 3.778898239135742
2025-12-09 12:06:59.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 5.832779247517273e-05 Training loss: 3.677168607711792
2025-12-09 12:06:59.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 5.779506477326933e-05 Training loss: 3.9953055381774902
2025-12-09 12:06:59.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 5.726142856227452e-05 Training loss: 3.6967644691467285
2025-12-09 12:06:59.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 5.672694603709794e-05 Training loss: 3.826378345489502
2025-12-09 12:06:59.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 5.619167949128652e-05 Training loss: 3.9094812870025635
2025-12-09 12:06:59.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 5.565569130976422e-05 Training loss: 3.7931602001190186
2025-12-09 12:06:59.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 5.5119043961561136e-05 Training loss: 3.652776002883911
2025-12-09 12:06:59.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 5.458179999253275e-05 Training loss: 3.7264468669891357
2025-12-09 12:06:59.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 5.4044022018070214e-05 Training loss: 3.7107253074645996
2025-12-09 12:06:59.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 5.3505772715802704e-05 Training loss: 3.7501723766326904
2025-12-09 12:06:59.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 5.296711481829226e-05 Training loss: 3.7656893730163574
2025-12-09 12:06:59.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 5.242811110572242e-05 Training loss: 3.895786762237549
2025-12-09 12:06:59.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 5.188882439858117e-05 Training loss: 3.6707522869110107
2025-12-09 12:06:59.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 5.134931755033936e-05 Training loss: 3.7746975421905518
2025-12-09 12:06:59.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 5.080965344012508e-05 Training loss: 3.551265001296997
2025-12-09 12:06:59.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 5.0269894965395225e-05 Training loss: 3.951277256011963
2025-12-09 12:06:59.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 4.973010503460479e-05 Training loss: 3.804276943206787
2025-12-09 12:06:59.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 4.919034655987493e-05 Training loss: 3.545405387878418
2025-12-09 12:06:59.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 4.865068244966066e-05 Training loss: 3.7713100910186768
2025-12-09 12:06:59.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 4.8111175601418844e-05 Training loss: 3.875871181488037
2025-12-09 12:06:59.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 4.7571888894277604e-05 Training loss: 3.611025094985962
2025-12-09 12:06:59.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 4.703288518170774e-05 Training loss: 3.762186050415039
2025-12-09 12:07:00.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 4.6494227284197294e-05 Training loss: 3.724543809890747
2025-12-09 12:07:00.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 4.59559779819298e-05 Training loss: 3.8853273391723633
2025-12-09 12:07:00.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 4.541820000746727e-05 Training loss: 3.7056593894958496
2025-12-09 12:07:00.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 4.4880956038438876e-05 Training loss: 3.8218002319335938
2025-12-09 12:07:00.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 4.434430869023579e-05 Training loss: 3.714262008666992
2025-12-09 12:07:00.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 4.38083205087135e-05 Training loss: 3.831036329269409
2025-12-09 12:07:00.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 4.3273053962902076e-05 Training loss: 3.831557512283325
2025-12-09 12:07:00.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 4.27385714377255e-05 Training loss: 3.6734840869903564
2025-12-09 12:07:00.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 4.220493522673067e-05 Training loss: 3.716331720352173
2025-12-09 12:07:00.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 4.1672207524827275e-05 Training loss: 3.7641499042510986
2025-12-09 12:07:00.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 4.114045042103887e-05 Training loss: 3.6201446056365967
2025-12-09 12:07:00.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 4.06097258912665e-05 Training loss: 3.820664882659912
2025-12-09 12:07:00.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 4.0080095791065505e-05 Training loss: 3.4889943599700928
2025-12-09 12:07:00.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 3.955162184843625e-05 Training loss: 3.685731887817383
2025-12-09 12:07:00.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 3.902436565662977e-05 Training loss: 3.8335418701171875
2025-12-09 12:07:00.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 3.849838866696913e-05 Training loss: 3.85538387298584
2025-12-09 12:07:00.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 3.7973752181687335e-05 Training loss: 3.580583095550537
2025-12-09 12:07:00.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 3.745051734678256e-05 Training loss: 3.660168409347534
2025-12-09 12:07:00.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 3.692874514489173e-05 Training loss: 3.7640810012817383
2025-12-09 12:07:00.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 3.640849638818286e-05 Training loss: 3.6196866035461426
2025-12-09 12:07:00.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 3.588983171126762e-05 Training loss: 3.8280365467071533
2025-12-09 12:07:00.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 3.53728115641343e-05 Training loss: 3.6899044513702393
2025-12-09 12:07:00.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 3.4857496205102474e-05 Training loss: 3.6950857639312744
2025-12-09 12:07:00.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 3.434394569379988e-05 Training loss: 3.75325608253479
2025-12-09 12:07:00.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 3.3832219884162585e-05 Training loss: 3.621522903442383
2025-12-09 12:07:00.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 3.332237841745898e-05 Training loss: 3.67728853225708
2025-12-09 12:07:00.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 3.281448071533867e-05 Training loss: 3.6723499298095703
2025-12-09 12:07:00.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 3.2308585972906966e-05 Training loss: 3.67236590385437
2025-12-09 12:07:00.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 3.180475315182563e-05 Training loss: 3.797400951385498
2025-12-09 12:07:00.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 3.130304097344103e-05 Training loss: 3.601317882537842
2025-12-09 12:07:00.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 3.080350791194019e-05 Training loss: 3.6594157218933105
2025-12-09 12:07:00.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 3.0306212187535653e-05 Training loss: 3.7491705417633057
2025-12-09 12:07:00.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 2.9811211759679924e-05 Training loss: 3.643538236618042
2025-12-09 12:07:00.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 2.9318564320310444e-05 Training loss: 3.5947935581207275
2025-12-09 12:07:00.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 2.882832728712551e-05 Training loss: 3.752751111984253
2025-12-09 12:07:00.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 2.8340557796892354e-05 Training loss: 3.8017578125
2025-12-09 12:07:00.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 2.7855312698787904e-05 Training loss: 3.7837672233581543
2025-12-09 12:07:00.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 2.737264854777306e-05 Training loss: 3.7636027336120605
2025-12-09 12:07:00.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 2.6892621598001156e-05 Training loss: 3.8730788230895996
2025-12-09 12:07:00.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 2.6415287796261706e-05 Training loss: 3.583857297897339
2025-12-09 12:07:00.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 2.5940702775459747e-05 Training loss: 3.5857157707214355
2025-12-09 12:07:00.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 2.5468921848131983e-05 Training loss: 3.8029580116271973
2025-12-09 12:07:00.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 2.500000000000001e-05 Training loss: 3.5407862663269043
2025-12-09 12:07:00.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 2.4533991883561868e-05 Training loss: 3.5353102684020996
2025-12-09 12:07:00.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 2.407095181172227e-05 Training loss: 3.8530774116516113
2025-12-09 12:07:00.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 2.3610933751462553e-05 Training loss: 3.6548237800598145
2025-12-09 12:07:00.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 2.315399131755081e-05 Training loss: 3.751981258392334
2025-12-09 12:07:00.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 2.2700177766293096e-05 Training loss: 3.517580032348633
2025-12-09 12:07:00.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 2.2249545989326514e-05 Training loss: 3.7504076957702637
2025-12-09 12:07:00.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 2.180214850745467e-05 Training loss: 3.6773765087127686
2025-12-09 12:07:00.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 2.1358037464526515e-05 Training loss: 3.6175343990325928
2025-12-09 12:07:00.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 2.091726462135888e-05 Training loss: 3.805619239807129
2025-12-09 12:07:00.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 2.0479881349703883e-05 Training loss: 3.744575262069702
2025-12-09 12:07:00.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 2.0045938626261546e-05 Training loss: 3.595315456390381
2025-12-09 12:07:00.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 1.9615487026738543e-05 Training loss: 3.4882025718688965
2025-12-09 12:07:00.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 1.9188576719953633e-05 Training loss: 3.4987003803253174
2025-12-09 12:07:00.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 1.8765257461990442e-05 Training loss: 3.597715139389038
2025-12-09 12:07:00.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 1.834557859039851e-05 Training loss: 3.599313497543335
2025-12-09 12:07:00.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 1.7929589018443016e-05 Training loss: 3.5529394149780273
2025-12-09 12:07:00.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 1.7517337229403946e-05 Training loss: 3.621839761734009
2025-12-09 12:07:00.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 1.710887127092548e-05 Training loss: 3.5443692207336426
2025-12-09 12:07:00.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 1.6704238749415957e-05 Training loss: 3.789057493209839
2025-12-09 12:07:00.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 1.6303486824499458e-05 Training loss: 3.503645658493042
2025-12-09 12:07:00.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 1.5906662203519412e-05 Training loss: 3.7386395931243896
2025-12-09 12:07:00.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 1.5513811136094787e-05 Training loss: 3.6860828399658203
2025-12-09 12:07:00.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 1.5124979408729861e-05 Training loss: 3.704669237136841
2025-12-09 12:07:00.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 1.4740212339477721e-05 Training loss: 3.589751720428467
2025-12-09 12:07:00.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 1.4359554772658552e-05 Training loss: 3.581062078475952
2025-12-09 12:07:00.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 1.3983051073632997e-05 Training loss: 3.784985065460205
2025-12-09 12:07:00.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 1.3610745123631535e-05 Training loss: 3.6087419986724854
2025-12-09 12:07:00.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 1.3242680314639993e-05 Training loss: 3.7666008472442627
2025-12-09 12:07:00.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 1.2878899544342327e-05 Training loss: 3.701892375946045
2025-12-09 12:07:00.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 1.2519445211120979e-05 Training loss: 3.724440574645996
2025-12-09 12:07:00.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 1.2164359209115234e-05 Training loss: 3.7106857299804688
2025-12-09 12:07:00.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 1.1813682923338653e-05 Training loss: 3.818887948989868
2025-12-09 12:07:00.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 1.1467457224855544e-05 Training loss: 3.6455531120300293
2025-12-09 12:07:00.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 1.1125722466017547e-05 Training loss: 3.3413949012756348
2025-12-09 12:07:00.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 1.0788518475760545e-05 Training loss: 3.5413219928741455
2025-12-09 12:07:00.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 1.0455884554962725e-05 Training loss: 3.826908588409424
2025-12-09 12:07:00.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 1.012785947186397e-05 Training loss: 3.778160810470581
2025-12-09 12:07:00.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.804481457547498e-06 Training loss: 3.5508689880371094
2025-12-09 12:07:00.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.485788201484126e-06 Training loss: 3.472651720046997
2025-12-09 12:07:00.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.171816847139448e-06 Training loss: 3.7172958850860596
2025-12-09 12:07:00.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 8.86260398764494e-06 Training loss: 3.7445878982543945
2025-12-09 12:07:00.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 8.558185661532941e-06 Training loss: 3.723233699798584
2025-12-09 12:07:00.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 8.25859734853645e-06 Training loss: 3.6651811599731445
2025-12-09 12:07:00.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 7.96387396545396e-06 Training loss: 3.835040807723999
2025-12-09 12:07:00.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 7.67404986207999e-06 Training loss: 3.6985628604888916
2025-12-09 12:07:00.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 7.389158817201542e-06 Training loss: 3.7781875133514404
2025-12-09 12:07:00.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 7.109234034661289e-06 Training loss: 3.700387477874756
2025-12-09 12:07:00.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 6.8343081394876715e-06 Training loss: 3.608898162841797
2025-12-09 12:07:00.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 6.564413174092443e-06 Training loss: 3.6532928943634033
2025-12-09 12:07:00.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 6.299580594536214e-06 Training loss: 3.519148111343384
2025-12-09 12:07:00.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 6.0398412668621895e-06 Training loss: 3.609532117843628
2025-12-09 12:07:00.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 5.785225463498828e-06 Training loss: 3.7017219066619873
2025-12-09 12:07:00.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 5.535762859731547e-06 Training loss: 3.503455877304077
2025-12-09 12:07:00.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 5.291482530244179e-06 Training loss: 3.591597318649292
2025-12-09 12:07:00.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 5.05241294573024e-06 Training loss: 3.5372865200042725
2025-12-09 12:07:00.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 4.818581969574742e-06 Training loss: 3.7487096786499023
2025-12-09 12:07:00.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 4.590016854606727e-06 Training loss: 3.4798102378845215
2025-12-09 12:07:00.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 4.366744239922998e-06 Training loss: 3.6034421920776367
2025-12-09 12:07:00.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 4.148790147783288e-06 Training loss: 3.650090456008911
2025-12-09 12:07:00.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 3.936179980577453e-06 Training loss: 3.7679970264434814
2025-12-09 12:07:00.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 3.728938517864794e-06 Training loss: 3.7263715267181396
2025-12-09 12:07:00.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 3.527089913486037e-06 Training loss: 3.6965670585632324
2025-12-09 12:07:00.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 3.3306576927482126e-06 Training loss: 3.5628654956817627
2025-12-09 12:07:00.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 3.1396647496828247e-06 Training loss: 3.606773614883423
2025-12-09 12:07:00.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 2.9541333443775243e-06 Training loss: 3.7330570220947266
2025-12-09 12:07:00.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 2.774085100381735e-06 Training loss: 3.786121368408203
2025-12-09 12:07:00.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 2.5995410021864787e-06 Training loss: 3.856532096862793
2025-12-09 12:07:00.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 2.430521392778573e-06 Training loss: 3.7683207988739014
2025-12-09 12:07:00.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 2.2670459712697377e-06 Training loss: 3.5283124446868896
2025-12-09 12:07:00.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 2.1091337906006482e-06 Training loss: 3.6132991313934326
2025-12-09 12:07:00.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 1.956803255320322e-06 Training loss: 3.53914737701416
2025-12-09 12:07:00.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 1.810072119441103e-06 Training loss: 3.818727493286133
2025-12-09 12:07:01.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 1.6689574843694433e-06 Training loss: 3.6142096519470215
2025-12-09 12:07:01.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 1.53347579691272e-06 Training loss: 3.715486764907837
2025-12-09 12:07:01.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 1.4036428473624019e-06 Training loss: 3.986616849899292
2025-12-09 12:07:01.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 1.2794737676536994e-06 Training loss: 3.6956632137298584
2025-12-09 12:07:01.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 1.1609830296019143e-06 Training loss: 3.6008927822113037
2025-12-09 12:07:01.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 1.0481844432158161e-06 Training loss: 3.5671794414520264
2025-12-09 12:07:01.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880475e-07 Training loss: 3.4820146560668945
2025-12-09 12:07:01.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629208e-07 Training loss: 3.711705207824707
2025-12-09 12:07:01.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.44069733781677e-07 Training loss: 3.4729866981506348
2025-12-09 12:07:01.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.54164563305465e-07 Training loss: 3.7292182445526123
2025-12-09 12:07:01.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.700106138160688e-07 Training loss: 3.628969430923462
2025-12-09 12:07:01.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946693e-07 Training loss: 3.5645384788513184
2025-12-09 12:07:01.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.189949386787462e-07 Training loss: 3.6276140213012695
2025-12-09 12:07:01.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.5215081379718074e-07 Training loss: 3.6357223987579346
2025-12-09 12:07:01.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378877e-07 Training loss: 3.668879508972168
2025-12-09 12:07:01.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.3582894166930268e-07 Training loss: 3.580132484436035
2025-12-09 12:07:01.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.8636475165200174e-07 Training loss: 3.7303192615509033
2025-12-09 12:07:01.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.427063043470178e-07 Training loss: 3.610105276107788
2025-12-09 12:07:01.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441757e-07 Training loss: 3.772099018096924
2025-12-09 12:07:01.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.282631406615447e-08 Training loss: 3.662510871887207
2025-12-09 12:07:01.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.661291555196345e-08 Training loss: 3.724213123321533
2025-12-09 12:07:01.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253337e-08 Training loss: 3.5079431533813477
2025-12-09 12:07:01.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-08 Training loss: 3.7243452072143555
2025-12-09 12:07:01.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265825e-09 Training loss: 3.624079465866089
2025-12-09 12:07:01.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 3.345655918121338
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:07:14.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 4.675591945648193
2025-12-09 12:07:14.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 4.885908126831055
2025-12-09 12:07:14.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 4.871219635009766
2025-12-09 12:07:14.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 4.859221935272217
2025-12-09 12:07:14.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 4.885030269622803
2025-12-09 12:07:14.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 4.951552391052246
2025-12-09 12:07:14.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 4.943704128265381
2025-12-09 12:07:14.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 4.799649715423584
2025-12-09 12:07:14.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 4.951313018798828
2025-12-09 12:07:14.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 4.83967924118042
2025-12-09 12:07:14.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 4.796036243438721
2025-12-09 12:07:14.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 4.709758281707764
2025-12-09 12:07:14.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 4.8590898513793945
2025-12-09 12:07:14.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 4.81242561340332
2025-12-09 12:07:14.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 4.780944347381592
2025-12-09 12:07:14.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 4.86430549621582
2025-12-09 12:07:14.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 4.801452159881592
2025-12-09 12:07:14.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 4.829966068267822
2025-12-09 12:07:14.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 4.784231185913086
2025-12-09 12:07:14.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 4.927567481994629
2025-12-09 12:07:14.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 4.799587249755859
2025-12-09 12:07:14.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 4.862672805786133
2025-12-09 12:07:14.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 4.808459758758545
2025-12-09 12:07:14.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 4.700407981872559
2025-12-09 12:07:14.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 4.737342834472656
2025-12-09 12:07:14.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 4.76406192779541
2025-12-09 12:07:14.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 4.642794132232666
2025-12-09 12:07:14.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 4.62866735458374
2025-12-09 12:07:14.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 4.593667507171631
2025-12-09 12:07:14.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 4.638008117675781
2025-12-09 12:07:14.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 4.730047702789307
2025-12-09 12:07:14.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 4.675140857696533
2025-12-09 12:07:14.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 4.692230701446533
2025-12-09 12:07:15.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 4.563708305358887
2025-12-09 12:07:15.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 4.5642805099487305
2025-12-09 12:07:15.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 4.5718607902526855
2025-12-09 12:07:15.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 4.665913105010986
2025-12-09 12:07:15.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 4.751561641693115
2025-12-09 12:07:15.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 4.510075569152832
2025-12-09 12:07:15.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 4.514347076416016
2025-12-09 12:07:15.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 4.428974628448486
2025-12-09 12:07:15.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 4.663456439971924
2025-12-09 12:07:15.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 4.4891204833984375
2025-12-09 12:07:15.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 4.596987247467041
2025-12-09 12:07:15.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 4.501766681671143
2025-12-09 12:07:15.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 4.524409770965576
2025-12-09 12:07:15.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 4.363064289093018
2025-12-09 12:07:15.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 4.421557426452637
2025-12-09 12:07:15.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 4.325135707855225
2025-12-09 12:07:15.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 4.521927356719971
2025-12-09 12:07:15.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 4.510693073272705
2025-12-09 12:07:15.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 4.414879322052002
2025-12-09 12:07:15.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 4.469440460205078
2025-12-09 12:07:15.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 4.390416622161865
2025-12-09 12:07:15.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 4.291649341583252
2025-12-09 12:07:15.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 4.371419906616211
2025-12-09 12:07:15.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 4.419365406036377
2025-12-09 12:07:15.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 4.466007709503174
2025-12-09 12:07:15.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 4.29286003112793
2025-12-09 12:07:15.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 4.347824573516846
2025-12-09 12:07:15.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 4.313650131225586
2025-12-09 12:07:15.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 4.2996320724487305
2025-12-09 12:07:15.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 4.411309242248535
2025-12-09 12:07:15.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 4.392375469207764
2025-12-09 12:07:15.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 4.342013835906982
2025-12-09 12:07:15.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 4.136110305786133
2025-12-09 12:07:15.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 4.322570323944092
2025-12-09 12:07:15.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 4.504336357116699
2025-12-09 12:07:15.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 4.359325885772705
2025-12-09 12:07:15.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 4.267481327056885
2025-12-09 12:07:15.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 4.240442276000977
2025-12-09 12:07:15.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 4.269577980041504
2025-12-09 12:07:15.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 4.3126220703125
2025-12-09 12:07:15.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 4.264740943908691
2025-12-09 12:07:15.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 4.304041385650635
2025-12-09 12:07:15.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 4.342585563659668
2025-12-09 12:07:15.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 4.225215435028076
2025-12-09 12:07:15.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 4.238094329833984
2025-12-09 12:07:15.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 4.1013641357421875
2025-12-09 12:07:15.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 4.137601375579834
2025-12-09 12:07:15.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 4.208112716674805
2025-12-09 12:07:15.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 4.042497634887695
2025-12-09 12:07:15.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 4.082161903381348
2025-12-09 12:07:15.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 4.021085739135742
2025-12-09 12:07:15.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 4.117061614990234
2025-12-09 12:07:15.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 4.101733684539795
2025-12-09 12:07:15.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 4.1793718338012695
2025-12-09 12:07:15.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 4.069555759429932
2025-12-09 12:07:15.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 4.142802715301514
2025-12-09 12:07:15.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 4.091071605682373
2025-12-09 12:07:15.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 4.003241062164307
2025-12-09 12:07:15.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 4.205959796905518
2025-12-09 12:07:15.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 4.085240840911865
2025-12-09 12:07:15.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 3.9423696994781494
2025-12-09 12:07:15.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 3.976818799972534
2025-12-09 12:07:15.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 3.844233989715576
2025-12-09 12:07:15.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 4.122962474822998
2025-12-09 12:07:15.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 4.012948513031006
2025-12-09 12:07:15.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 4.084410190582275
2025-12-09 12:07:15.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 4.006274223327637
2025-12-09 12:07:15.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999125880491846 Training loss: 3.9180312156677246
2025-12-09 12:07:15.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029996503623845393 Training loss: 4.157601833343506
2025-12-09 12:07:15.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.00029992133535682725 Training loss: 3.7917330265045166
2025-12-09 12:07:15.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00029986016125334406 Training loss: 4.022398471832275
2025-12-09 12:07:15.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0002997815210578015 Training loss: 3.9028992652893066
2025-12-09 12:07:15.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002996854239356567 Training loss: 3.6368207931518555
2025-12-09 12:07:15.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0002995718810869589 Training loss: 3.9158775806427
2025-12-09 12:07:15.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00029944090574504395 Training loss: 3.997800350189209
2025-12-09 12:07:15.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0002992925131749921 Training loss: 3.922684669494629
2025-12-09 12:07:15.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0002991267206718486 Training loss: 3.997042417526245
2025-12-09 12:07:15.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029894354755860845 Training loss: 3.983301877975464
2025-12-09 12:07:15.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.00029874301518396376 Training loss: 3.9115090370178223
2025-12-09 12:07:15.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.000298525146919816 Training loss: 3.784433603286743
2025-12-09 12:07:15.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0002982899681585518 Training loss: 3.9330942630767822
2025-12-09 12:07:15.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00029803750631008356 Training loss: 4.054771900177002
2025-12-09 12:07:15.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00029776779079865496 Training loss: 3.946742534637451
2025-12-09 12:07:15.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029748085305941123 Training loss: 3.8163058757781982
2025-12-09 12:07:15.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0002971767265347358 Training loss: 3.8733677864074707
2025-12-09 12:07:15.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002968554466703525 Training loss: 4.033291339874268
2025-12-09 12:07:15.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0002965170509111942 Training loss: 3.871288299560547
2025-12-09 12:07:15.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0002961615786970389 Training loss: 3.8288285732269287
2025-12-09 12:07:15.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00029578907145791274 Training loss: 3.8580048084259033
2025-12-09 12:07:15.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00029539957260926183 Training loss: 4.093703269958496
2025-12-09 12:07:15.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0002949931275468917 Training loss: 3.7587690353393555
2025-12-09 12:07:15.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002945697836416767 Training loss: 3.7400197982788086
2025-12-09 12:07:15.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.000294129590234039 Training loss: 4.073376178741455
2025-12-09 12:07:15.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.00029367259862819804 Training loss: 3.834859609603882
2025-12-09 12:07:15.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00029319886208619073 Training loss: 3.784738063812256
2025-12-09 12:07:15.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.00029270843582166427 Training loss: 3.941598415374756
2025-12-09 12:07:15.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029220137699344055 Training loss: 3.6843490600585938
2025-12-09 12:07:15.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0002916777446988548 Training loss: 3.738886594772339
2025-12-09 12:07:15.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.00029113759996686743 Training loss: 3.91898250579834
2025-12-09 12:07:15.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0002905810057509515 Training loss: 3.7622387409210205
2025-12-09 12:07:15.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.00029000802692175537 Training loss: 3.910658359527588
2025-12-09 12:07:15.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0002894187302595419 Training loss: 3.9051711559295654
2025-12-09 12:07:15.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0002888131844464056 Training loss: 3.9172677993774414
2025-12-09 12:07:15.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002881914600582676 Training loss: 4.010610103607178
2025-12-09 12:07:15.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0002875536295566501 Training loss: 3.8408126831054688
2025-12-09 12:07:15.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.000286899767280231 Training loss: 3.7234044075012207
2025-12-09 12:07:15.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0002862299494361798 Training loss: 3.644010066986084
2025-12-09 12:07:15.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0002855442540912758 Training loss: 3.721810817718506
2025-12-09 12:07:15.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00028484276116280926 Training loss: 3.6394424438476562
2025-12-09 12:07:15.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0002841255524092674 Training loss: 3.8273167610168457
2025-12-09 12:07:15.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.00028339271142080534 Training loss: 3.5553746223449707
2025-12-09 12:07:16.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.00028264432360950353 Training loss: 3.6868467330932617
2025-12-09 12:07:16.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00028188047619941343 Training loss: 3.766221046447754
2025-12-09 12:07:16.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0002811012582163913 Training loss: 3.6361472606658936
2025-12-09 12:07:16.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.00028030676047772265 Training loss: 3.6413087844848633
2025-12-09 12:07:16.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.000279497075581537 Training loss: 3.742798089981079
2025-12-09 12:07:16.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002786722978960161 Training loss: 3.711651086807251
2025-12-09 12:07:16.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0002778325235483954 Training loss: 3.7389109134674072
2025-12-09 12:07:16.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00027697785041376006 Training loss: 3.918407917022705
2025-12-09 12:07:16.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0002761083781036381 Training loss: 3.6649069786071777
2025-12-09 12:07:16.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00027522420795439065 Training loss: 3.6707448959350586
2025-12-09 12:07:16.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0002743254430154012 Training loss: 3.7214224338531494
2025-12-09 12:07:16.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0002734121880370652 Training loss: 3.57132887840271
2025-12-09 12:07:16.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0002724845494585816 Training loss: 3.7085087299346924
2025-12-09 12:07:16.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002715426353955476 Training loss: 3.713063955307007
2025-12-09 12:07:16.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0002705865556273575 Training loss: 3.743959426879883
2025-12-09 12:07:16.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0002696164215844081 Training loss: 3.7432546615600586
2025-12-09 12:07:16.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00026863234633511183 Training loss: 3.6230833530426025
2025-12-09 12:07:16.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00026763444457271837 Training loss: 3.714338779449463
2025-12-09 12:07:16.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0002666228326019474 Training loss: 3.6829123497009277
2025-12-09 12:07:16.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00026559762832543336 Training loss: 3.607056140899658
2025-12-09 12:07:16.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.000264558951229984 Training loss: 3.5092387199401855
2025-12-09 12:07:16.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.00026350692237265427 Training loss: 3.6232190132141113
2025-12-09 12:07:16.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0002624416643666371 Training loss: 3.6281826496124268
2025-12-09 12:07:16.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.000261363301366973 Training loss: 3.4754631519317627
2025-12-09 12:07:16.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00026027195905608006 Training loss: 3.43827223777771
2025-12-09 12:07:16.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0002591677646291054 Training loss: 3.5617356300354004
2025-12-09 12:07:16.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00025805084677910095 Training loss: 3.7357285022735596
2025-12-09 12:07:16.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0002569213356820244 Training loss: 3.3817296028137207
2025-12-09 12:07:16.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0002557793629815669 Training loss: 3.6034584045410156
2025-12-09 12:07:16.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00025462506177381043 Training loss: 3.505260944366455
2025-12-09 12:07:16.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.00025345856659171563 Training loss: 3.5239908695220947
2025-12-09 12:07:16.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00025228001338944175 Training loss: 3.4501819610595703
2025-12-09 12:07:16.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0002510895395265016 Training loss: 3.57647442817688
2025-12-09 12:07:16.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00024988728375175214 Training loss: 3.5000205039978027
2025-12-09 12:07:16.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00024867338618722357 Training loss: 3.7064850330352783
2025-12-09 12:07:16.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0002474479883117882 Training loss: 3.4890758991241455
2025-12-09 12:07:16.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00024621123294467096 Training loss: 3.7049951553344727
2025-12-09 12:07:16.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.0002449632642288045 Training loss: 3.786771297454834
2025-12-09 12:07:16.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.00024370422761402867 Training loss: 3.547865867614746
2025-12-09 12:07:16.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0002424342698401391 Training loss: 3.3763506412506104
2025-12-09 12:07:16.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00024115353891978431 Training loss: 3.650451421737671
2025-12-09 12:07:16.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.00023986218412121537 Training loss: 3.5093817710876465
2025-12-09 12:07:16.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.00023856035595088839 Training loss: 3.707420587539673
2025-12-09 12:07:16.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00023724820613592337 Training loss: 3.623662233352661
2025-12-09 12:07:16.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.00023592588760642044 Training loss: 3.688542366027832
2025-12-09 12:07:16.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00023459355447763596 Training loss: 3.3492114543914795
2025-12-09 12:07:16.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.00023325136203202049 Training loss: 3.6206002235412598
2025-12-09 12:07:16.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.00023189946670112069 Training loss: 3.5046353340148926
2025-12-09 12:07:16.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.00023053802604734757 Training loss: 3.539571762084961
2025-12-09 12:07:16.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.00022916719874561226 Training loss: 3.1859757900238037
2025-12-09 12:07:16.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0002277871445648332 Training loss: 3.8380937576293945
2025-12-09 12:07:16.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.00022639802434931444 Training loss: 3.492570161819458
2025-12-09 12:07:16.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.000225 Training loss: 3.5640487670898438
2025-12-09 12:07:16.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00022359323445560406 Training loss: 3.5004935264587402
2025-12-09 12:07:16.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.00022217789167362073 Training loss: 3.6436283588409424
2025-12-09 12:07:16.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.00022075413661121492 Training loss: 3.6535794734954834
2025-12-09 12:07:16.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.00021932213520599653 Training loss: 3.585606336593628
2025-12-09 12:07:16.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00021788205435668083 Training loss: 3.8735604286193848
2025-12-09 12:07:16.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00021643406190363624 Training loss: 3.563248634338379
2025-12-09 12:07:16.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.00021497832660932295 Training loss: 3.446505069732666
2025-12-09 12:07:16.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.00021351501813862356 Training loss: 3.572026252746582
2025-12-09 12:07:16.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0002120443070390687 Training loss: 3.4428024291992188
2025-12-09 12:07:16.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00021056636472096025 Training loss: 3.5409460067749023
2025-12-09 12:07:16.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00020908136343739307 Training loss: 3.503737211227417
2025-12-09 12:07:16.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00020758947626417943 Training loss: 3.42832612991333
2025-12-09 12:07:16.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0002060908770796769 Training loss: 3.4298810958862305
2025-12-09 12:07:16.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.00020458574054452313 Training loss: 3.5509212017059326
2025-12-09 12:07:16.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00020307424208127912 Training loss: 3.212313652038574
2025-12-09 12:07:16.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.00020155655785398393 Training loss: 3.427323818206787
2025-12-09 12:07:16.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0002000328647476231 Training loss: 3.3637020587921143
2025-12-09 12:07:16.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00019850334034751226 Training loss: 3.3944292068481445
2025-12-09 12:07:16.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00019696816291860038 Training loss: 3.2389044761657715
2025-12-09 12:07:16.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0001954275113846926 Training loss: 3.3001134395599365
2025-12-09 12:07:16.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00019388156530759712 Training loss: 3.5598580837249756
2025-12-09 12:07:16.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00019233050486619713 Training loss: 3.4889798164367676
2025-12-09 12:07:16.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0001907745108354514 Training loss: 3.4471163749694824
2025-12-09 12:07:16.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.00018921376456532482 Training loss: 3.335768938064575
2025-12-09 12:07:16.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.00018764844795965229 Training loss: 3.608708381652832
2025-12-09 12:07:16.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.00018607874345493805 Training loss: 3.3905012607574463
2025-12-09 12:07:16.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.00018450483399909263 Training loss: 3.3243699073791504
2025-12-09 12:07:16.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.00018292690303011076 Training loss: 3.3811445236206055
2025-12-09 12:07:16.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00018134513445469127 Training loss: 3.4462738037109375
2025-12-09 12:07:16.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00017975971262680347 Training loss: 3.2401247024536133
2025-12-09 12:07:16.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00017817082232620052 Training loss: 3.3246517181396484
2025-12-09 12:07:16.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.00017657864873688343 Training loss: 3.7276837825775146
2025-12-09 12:07:16.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.00017498337742551817 Training loss: 3.233639717102051
2025-12-09 12:07:16.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00017338519431980796 Training loss: 3.4738574028015137
2025-12-09 12:07:16.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.00017178428568682353 Training loss: 3.36116623878479
2025-12-09 12:07:16.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0001701808381112938 Training loss: 3.4222259521484375
2025-12-09 12:07:16.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00016857503847385953 Training loss: 3.37644100189209
2025-12-09 12:07:16.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.00016696707392929266 Training loss: 3.317875385284424
2025-12-09 12:07:16.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0001653571318846834 Training loss: 3.5582714080810547
2025-12-09 12:07:16.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00016374539997759821 Training loss: 3.1501996517181396
2025-12-09 12:07:16.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.00016213206605421063 Training loss: 3.2689919471740723
2025-12-09 12:07:16.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0001605173181474081 Training loss: 3.410737991333008
2025-12-09 12:07:16.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.00015890134445487676 Training loss: 3.473517417907715
2025-12-09 12:07:16.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.00015728433331716724 Training loss: 3.31606125831604
2025-12-09 12:07:16.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0001556664731957435 Training loss: 3.332010269165039
2025-12-09 12:07:16.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00015404795265101806 Training loss: 3.5092790126800537
2025-12-09 12:07:16.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.00015242896032037522 Training loss: 3.1533138751983643
2025-12-09 12:07:16.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.00015080968489618565 Training loss: 3.274678945541382
2025-12-09 12:07:16.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.00014919031510381435 Training loss: 3.403329372406006
2025-12-09 12:07:16.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00014757103967962475 Training loss: 3.2997899055480957
2025-12-09 12:07:16.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.00014595204734898197 Training loss: 3.430919647216797
2025-12-09 12:07:16.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0001443335268042565 Training loss: 3.3933589458465576
2025-12-09 12:07:16.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0001427156666828328 Training loss: 3.3401997089385986
2025-12-09 12:07:16.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.00014109865554512319 Training loss: 3.2582006454467773
2025-12-09 12:07:16.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00013948268185259188 Training loss: 3.009890079498291
2025-12-09 12:07:16.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00013786793394578937 Training loss: 3.4422214031219482
2025-12-09 12:07:17.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0001362546000224018 Training loss: 3.329972505569458
2025-12-09 12:07:17.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00013464286811531661 Training loss: 3.340825080871582
2025-12-09 12:07:17.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.00013303292607070737 Training loss: 3.4059219360351562
2025-12-09 12:07:17.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.0001314249615261405 Training loss: 3.3284530639648438
2025-12-09 12:07:17.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0001298191618887062 Training loss: 3.42215633392334
2025-12-09 12:07:17.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00012821571431317647 Training loss: 3.2409703731536865
2025-12-09 12:07:17.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00012661480568019201 Training loss: 3.274017810821533
2025-12-09 12:07:17.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0001250166225744818 Training loss: 3.136139392852783
2025-12-09 12:07:17.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0001234213512631166 Training loss: 3.44783353805542
2025-12-09 12:07:17.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00012182917767379948 Training loss: 3.385079860687256
2025-12-09 12:07:17.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.00012024028737319652 Training loss: 3.3134114742279053
2025-12-09 12:07:17.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.00011865486554530873 Training loss: 3.404299259185791
2025-12-09 12:07:17.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0001170730969698893 Training loss: 3.097822666168213
2025-12-09 12:07:17.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.00011549516600090737 Training loss: 3.442605495452881
2025-12-09 12:07:17.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00011392125654506198 Training loss: 3.3552534580230713
2025-12-09 12:07:17.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00011235155204034767 Training loss: 3.2098329067230225
2025-12-09 12:07:17.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00011078623543467518 Training loss: 3.627460479736328
2025-12-09 12:07:17.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00010922548916454855 Training loss: 2.9352195262908936
2025-12-09 12:07:17.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00010766949513380284 Training loss: 3.3836233615875244
2025-12-09 12:07:17.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00010611843469240288 Training loss: 3.159932851791382
2025-12-09 12:07:17.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00010457248861530741 Training loss: 3.156075954437256
2025-12-09 12:07:17.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00010303183708139964 Training loss: 3.0727531909942627
2025-12-09 12:07:17.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00010149665965248775 Training loss: 3.3273932933807373
2025-12-09 12:07:17.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 9.996713525237694e-05 Training loss: 3.176445722579956
2025-12-09 12:07:17.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 9.8443442146016e-05 Training loss: 3.164435863494873
2025-12-09 12:07:17.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 9.692575791872089e-05 Training loss: 3.3897275924682617
2025-12-09 12:07:17.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 9.541425945547687e-05 Training loss: 3.207587957382202
2025-12-09 12:07:17.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 9.390912292032309e-05 Training loss: 3.2820281982421875
2025-12-09 12:07:17.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 9.241052373582057e-05 Training loss: 3.1047592163085938
2025-12-09 12:07:17.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 9.091863656260695e-05 Training loss: 3.279874086380005
2025-12-09 12:07:17.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 8.943363527903976e-05 Training loss: 3.1794703006744385
2025-12-09 12:07:17.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 8.795569296093132e-05 Training loss: 3.3596737384796143
2025-12-09 12:07:17.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 8.648498186137653e-05 Training loss: 3.430663824081421
2025-12-09 12:07:17.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 8.502167339067705e-05 Training loss: 3.311479091644287
2025-12-09 12:07:17.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 8.356593809636371e-05 Training loss: 3.2098934650421143
2025-12-09 12:07:17.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 8.211794564331917e-05 Training loss: 3.5889248847961426
2025-12-09 12:07:17.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 8.067786479400346e-05 Training loss: 3.2645957469940186
2025-12-09 12:07:17.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 7.924586338878511e-05 Training loss: 3.1882035732269287
2025-12-09 12:07:17.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 7.782210832637923e-05 Training loss: 3.295732259750366
2025-12-09 12:07:17.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 7.640676554439594e-05 Training loss: 3.103464126586914
2025-12-09 12:07:17.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 7.500000000000002e-05 Training loss: 3.2837419509887695
2025-12-09 12:07:17.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 7.36019756506856e-05 Training loss: 3.3652451038360596
2025-12-09 12:07:17.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 7.22128554351668e-05 Training loss: 3.365544080734253
2025-12-09 12:07:17.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 7.083280125438766e-05 Training loss: 3.2252843379974365
2025-12-09 12:07:17.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 6.946197395265242e-05 Training loss: 3.379284143447876
2025-12-09 12:07:17.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 6.810053329887928e-05 Training loss: 3.1630499362945557
2025-12-09 12:07:17.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 6.674863796797953e-05 Training loss: 3.2435319423675537
2025-12-09 12:07:17.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 6.540644552236401e-05 Training loss: 3.264002561569214
2025-12-09 12:07:17.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 6.407411239357953e-05 Training loss: 3.182868003845215
2025-12-09 12:07:17.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 6.275179386407663e-05 Training loss: 3.306928873062134
2025-12-09 12:07:17.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 6.143964404911164e-05 Training loss: 3.358489990234375
2025-12-09 12:07:17.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 6.013781587878463e-05 Training loss: 3.233064889907837
2025-12-09 12:07:17.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 5.8846461080215626e-05 Training loss: 3.3502564430236816
2025-12-09 12:07:17.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 5.756573015986089e-05 Training loss: 3.2924578189849854
2025-12-09 12:07:17.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 5.629577238597132e-05 Training loss: 3.278231620788574
2025-12-09 12:07:17.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 5.503673577119552e-05 Training loss: 3.0899147987365723
2025-12-09 12:07:17.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 5.378876705532904e-05 Training loss: 3.2839109897613525
2025-12-09 12:07:17.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 5.2552011688211835e-05 Training loss: 3.3940322399139404
2025-12-09 12:07:17.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 5.1326613812776434e-05 Training loss: 3.0551414489746094
2025-12-09 12:07:17.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 5.011271624824786e-05 Training loss: 3.181072950363159
2025-12-09 12:07:17.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 4.891046047349837e-05 Training loss: 3.219020366668701
2025-12-09 12:07:17.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 4.7719986610558234e-05 Training loss: 3.3582167625427246
2025-12-09 12:07:17.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 4.654143340828435e-05 Training loss: 3.156688928604126
2025-12-09 12:07:17.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 4.537493822618958e-05 Training loss: 3.090094804763794
2025-12-09 12:07:17.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 4.422063701843316e-05 Training loss: 3.0830931663513184
2025-12-09 12:07:17.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 4.3078664317975646e-05 Training loss: 3.2513034343719482
2025-12-09 12:07:17.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 4.194915322089898e-05 Training loss: 3.3742527961730957
2025-12-09 12:07:17.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 4.08322353708946e-05 Training loss: 3.1681063175201416
2025-12-09 12:07:17.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 3.972804094391998e-05 Training loss: 3.2358336448669434
2025-12-09 12:07:17.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 3.863669863302697e-05 Training loss: 3.361556053161621
2025-12-09 12:07:17.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 3.755833563336293e-05 Training loss: 3.0888140201568604
2025-12-09 12:07:17.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 3.64930776273457e-05 Training loss: 3.2408623695373535
2025-12-09 12:07:17.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 3.5441048770015954e-05 Training loss: 3.3154890537261963
2025-12-09 12:07:17.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 3.4402371674566626e-05 Training loss: 3.0793747901916504
2025-12-09 12:07:17.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 3.3377167398052636e-05 Training loss: 3.0461583137512207
2025-12-09 12:07:17.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 3.2365555427281634e-05 Training loss: 3.3208065032958984
2025-12-09 12:07:17.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 3.136765366488817e-05 Training loss: 3.23040771484375
2025-12-09 12:07:17.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 3.038357841559191e-05 Training loss: 3.213009834289551
2025-12-09 12:07:17.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 2.941344437264249e-05 Training loss: 3.1196043491363525
2025-12-09 12:07:17.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 2.8457364604452372e-05 Training loss: 3.2254700660705566
2025-12-09 12:07:17.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 2.7515450541418338e-05 Training loss: 3.2013378143310547
2025-12-09 12:07:17.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 2.658781196293482e-05 Training loss: 2.945838212966919
2025-12-09 12:07:17.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 2.5674556984598822e-05 Training loss: 3.0646207332611084
2025-12-09 12:07:17.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 2.477579204560935e-05 Training loss: 3.1705214977264404
2025-12-09 12:07:17.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 2.389162189636188e-05 Training loss: 3.3946847915649414
2025-12-09 12:07:17.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 2.3022149586239968e-05 Training loss: 3.045461893081665
2025-12-09 12:07:17.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 2.216747645160462e-05 Training loss: 3.0493438243865967
2025-12-09 12:07:17.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 2.1327702103983863e-05 Training loss: 3.226665735244751
2025-12-09 12:07:17.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 2.0502924418463013e-05 Training loss: 3.142559051513672
2025-12-09 12:07:17.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 1.9693239522277327e-05 Training loss: 3.317655563354492
2025-12-09 12:07:17.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 1.889874178360864e-05 Training loss: 2.8801205158233643
2025-12-09 12:07:17.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 1.8119523800586568e-05 Training loss: 3.26218318939209
2025-12-09 12:07:17.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 1.735567639049648e-05 Training loss: 3.2514638900756836
2025-12-09 12:07:17.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 1.6607288579194638e-05 Training loss: 3.08738112449646
2025-12-09 12:07:17.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 1.5874447590732538e-05 Training loss: 3.3183889389038086
2025-12-09 12:07:17.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 1.5157238837190716e-05 Training loss: 3.5364527702331543
2025-12-09 12:07:17.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 1.4455745908724226e-05 Training loss: 3.133774995803833
2025-12-09 12:07:17.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 1.3770050563820179e-05 Training loss: 3.23453688621521
2025-12-09 12:07:17.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 1.3100232719768994e-05 Training loss: 3.4116036891937256
2025-12-09 12:07:17.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 1.2446370443349863e-05 Training loss: 3.03609561920166
2025-12-09 12:07:17.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 1.180853994173236e-05 Training loss: 3.242274761199951
2025-12-09 12:07:17.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 1.118681555359438e-05 Training loss: 3.4171640872955322
2025-12-09 12:07:17.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 1.058126974045811e-05 Training loss: 3.5130434036254883
2025-12-09 12:07:17.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.991973078244636e-06 Training loss: 3.07549786567688
2025-12-09 12:07:17.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.418994249048472e-06 Training loss: 3.2387588024139404
2025-12-09 12:07:17.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 8.862400033132571e-06 Training loss: 3.012404680252075
2025-12-09 12:07:17.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 8.322255301145204e-06 Training loss: 3.19584321975708
2025-12-09 12:07:17.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 7.798623006559435e-06 Training loss: 3.02966046333313
2025-12-09 12:07:17.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 7.291564178335718e-06 Training loss: 3.231389284133911
2025-12-09 12:07:17.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 6.801137913809213e-06 Training loss: 3.3831729888916016
2025-12-09 12:07:17.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 6.3274013718019434e-06 Training loss: 3.420488119125366
2025-12-09 12:07:17.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 5.870409765960965e-06 Training loss: 3.252845525741577
2025-12-09 12:07:18.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 5.430216358323309e-06 Training loss: 3.122375965118408
2025-12-09 12:07:18.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 5.006872453108329e-06 Training loss: 3.264364719390869
2025-12-09 12:07:18.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 4.600427390738159e-06 Training loss: 3.3171768188476562
2025-12-09 12:07:18.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 4.210928542087206e-06 Training loss: 3.3729772567749023
2025-12-09 12:07:18.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 3.838421302961098e-06 Training loss: 3.2765159606933594
2025-12-09 12:07:18.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 3.482949088805742e-06 Training loss: 2.9655325412750244
2025-12-09 12:07:18.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 3.1445533296474478e-06 Training loss: 3.0852444171905518
2025-12-09 12:07:18.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 2.823273465264142e-06 Training loss: 3.1815950870513916
2025-12-09 12:07:18.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 2.519146940588762e-06 Training loss: 2.994985818862915
2025-12-09 12:07:18.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 2.232209201345031e-06 Training loss: 3.2087788581848145
2025-12-09 12:07:18.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 1.9624936899163945e-06 Training loss: 3.097398281097412
2025-12-09 12:07:18.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 1.7100318414482061e-06 Training loss: 3.1446478366851807
2025-12-09 12:07:18.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 1.4748530801840074e-06 Training loss: 3.2366440296173096
2025-12-09 12:07:18.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 1.2569848160362384e-06 Training loss: 3.0719645023345947
2025-12-09 12:07:18.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 1.056452441391542e-06 Training loss: 2.9776196479797363
2025-12-09 12:07:18.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 8.732793281513661e-07 Training loss: 3.138159990310669
2025-12-09 12:07:18.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 7.07486825007908e-07 Training loss: 3.2798709869384766
2025-12-09 12:07:18.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 5.590942549560051e-07 Training loss: 3.14107084274292
2025-12-09 12:07:18.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 4.281189130410534e-07 Training loss: 3.123828649520874
2025-12-09 12:07:18.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 3.1457606434325266e-07 Training loss: 3.232196807861328
2025-12-09 12:07:18.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 2.184789421984634e-07 Training loss: 3.1917500495910645
2025-12-09 12:07:18.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 1.3983874665589035e-07 Training loss: 3.0976192951202393
2025-12-09 12:07:18.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 7.866464317276e-08 Training loss: 3.381108522415161
2025-12-09 12:07:18.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 3.4963761546041855e-08 Training loss: 3.2190427780151367
2025-12-09 12:07:18.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 8.741195081479747e-09 Training loss: 3.3721187114715576
2025-12-09 12:07:18.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 3.5541114807128906
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:07:31.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 4.902071952819824
2025-12-09 12:07:31.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 4.799776077270508
2025-12-09 12:07:31.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 4.992262840270996
2025-12-09 12:07:31.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 5.050384044647217
2025-12-09 12:07:31.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 4.758328437805176
2025-12-09 12:07:31.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 4.82920503616333
2025-12-09 12:07:31.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 4.878856658935547
2025-12-09 12:07:31.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 4.86288595199585
2025-12-09 12:07:31.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 4.855072021484375
2025-12-09 12:07:31.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 4.8631062507629395
2025-12-09 12:07:31.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 4.797866344451904
2025-12-09 12:07:31.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 4.8048882484436035
2025-12-09 12:07:31.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 4.836832046508789
2025-12-09 12:07:31.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 4.742440700531006
2025-12-09 12:07:31.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 4.65506649017334
2025-12-09 12:07:31.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 4.698196887969971
2025-12-09 12:07:31.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 4.635758399963379
2025-12-09 12:07:31.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 4.68140172958374
2025-12-09 12:07:31.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 4.605752468109131
2025-12-09 12:07:31.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 4.508063316345215
2025-12-09 12:07:31.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 4.665602684020996
2025-12-09 12:07:31.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 4.571895599365234
2025-12-09 12:07:31.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 4.519351005554199
2025-12-09 12:07:31.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 4.673489093780518
2025-12-09 12:07:31.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 4.424766540527344
2025-12-09 12:07:31.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 4.508998394012451
2025-12-09 12:07:31.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 4.507012367248535
2025-12-09 12:07:31.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 4.533949851989746
2025-12-09 12:07:31.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 4.470733165740967
2025-12-09 12:07:31.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 4.5855393409729
2025-12-09 12:07:31.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 4.443317890167236
2025-12-09 12:07:31.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 4.479922294616699
2025-12-09 12:07:31.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 4.300083637237549
2025-12-09 12:07:31.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 4.3665452003479
2025-12-09 12:07:31.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 4.39802885055542
2025-12-09 12:07:31.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 4.41067361831665
2025-12-09 12:07:31.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 4.424589157104492
2025-12-09 12:07:31.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 4.439002513885498
2025-12-09 12:07:31.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 4.2829084396362305
2025-12-09 12:07:31.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 4.3343915939331055
2025-12-09 12:07:31.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 4.1714348793029785
2025-12-09 12:07:31.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 4.34212064743042
2025-12-09 12:07:32.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 4.290355682373047
2025-12-09 12:07:32.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 4.406133651733398
2025-12-09 12:07:32.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 4.218841552734375
2025-12-09 12:07:32.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 4.251500129699707
2025-12-09 12:07:32.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 4.151042461395264
2025-12-09 12:07:32.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 4.186878681182861
2025-12-09 12:07:32.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 4.232626914978027
2025-12-09 12:07:32.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 4.214980125427246
2025-12-09 12:07:32.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 4.12679386138916
2025-12-09 12:07:32.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 4.156745910644531
2025-12-09 12:07:32.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 4.166407108306885
2025-12-09 12:07:32.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 4.251073360443115
2025-12-09 12:07:32.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 4.151442050933838
2025-12-09 12:07:32.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 4.050533771514893
2025-12-09 12:07:32.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 4.069966793060303
2025-12-09 12:07:32.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 4.084425449371338
2025-12-09 12:07:32.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 3.9431278705596924
2025-12-09 12:07:32.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 4.016781806945801
2025-12-09 12:07:32.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 4.15385627746582
2025-12-09 12:07:32.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 4.068385601043701
2025-12-09 12:07:32.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 4.188042163848877
2025-12-09 12:07:32.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 4.020360469818115
2025-12-09 12:07:32.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 4.270858287811279
2025-12-09 12:07:32.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 3.9194998741149902
2025-12-09 12:07:32.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 4.005044937133789
2025-12-09 12:07:32.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 4.121378421783447
2025-12-09 12:07:32.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 3.8430192470550537
2025-12-09 12:07:32.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 3.8720884323120117
2025-12-09 12:07:32.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 4.068196773529053
2025-12-09 12:07:32.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 3.9437224864959717
2025-12-09 12:07:32.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 3.8922417163848877
2025-12-09 12:07:32.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 4.019424915313721
2025-12-09 12:07:32.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 3.8553712368011475
2025-12-09 12:07:32.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 3.9048686027526855
2025-12-09 12:07:32.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 3.9831953048706055
2025-12-09 12:07:32.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 3.9678516387939453
2025-12-09 12:07:32.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 3.9253125190734863
2025-12-09 12:07:32.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 4.031107425689697
2025-12-09 12:07:32.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 4.0098557472229
2025-12-09 12:07:32.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 3.8629631996154785
2025-12-09 12:07:32.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 3.915043592453003
2025-12-09 12:07:32.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 3.8765454292297363
2025-12-09 12:07:32.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 3.653426170349121
2025-12-09 12:07:32.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 3.839121103286743
2025-12-09 12:07:32.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 3.8970108032226562
2025-12-09 12:07:32.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 3.821587324142456
2025-12-09 12:07:32.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 3.9726500511169434
2025-12-09 12:07:32.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 3.8340916633605957
2025-12-09 12:07:32.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 3.8894848823547363
2025-12-09 12:07:32.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 3.879199743270874
2025-12-09 12:07:32.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 3.767333984375
2025-12-09 12:07:32.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 3.7901198863983154
2025-12-09 12:07:32.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 3.7636473178863525
2025-12-09 12:07:32.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 3.8814499378204346
2025-12-09 12:07:32.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 3.861192226409912
2025-12-09 12:07:32.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 3.784872055053711
2025-12-09 12:07:32.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 3.7253994941711426
2025-12-09 12:07:32.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 3.7716281414031982
2025-12-09 12:07:32.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009999708626830617 Training loss: 3.741147041320801
2025-12-09 12:07:32.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0009998834541281797 Training loss: 3.9739911556243896
2025-12-09 12:07:32.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0009997377845227576 Training loss: 3.6909263134002686
2025-12-09 12:07:32.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0009995338708444802 Training loss: 3.8639328479766846
2025-12-09 12:07:32.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0009992717368593384 Training loss: 3.897185802459717
2025-12-09 12:07:32.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0009989514131188558 Training loss: 3.6808369159698486
2025-12-09 12:07:32.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0009985729369565298 Training loss: 3.7451817989349365
2025-12-09 12:07:32.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00099813635248348 Training loss: 3.9761056900024414
2025-12-09 12:07:32.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0009976417105833069 Training loss: 3.8694984912872314
2025-12-09 12:07:32.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.000997089068906162 Training loss: 3.8875248432159424
2025-12-09 12:07:32.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0009964784918620282 Training loss: 3.9154176712036133
2025-12-09 12:07:32.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0009958100506132126 Training loss: 3.5804011821746826
2025-12-09 12:07:32.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0009950838230660534 Training loss: 3.7718183994293213
2025-12-09 12:07:32.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0009942998938618393 Training loss: 3.785996675491333
2025-12-09 12:07:32.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0009934583543669453 Training loss: 3.6971018314361572
2025-12-09 12:07:32.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0009925593026621834 Training loss: 3.8506393432617188
2025-12-09 12:07:32.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.000991602843531371 Training loss: 3.7200770378112793
2025-12-09 12:07:32.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0009905890884491196 Training loss: 3.5929131507873535
2025-12-09 12:07:32.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0009895181555678418 Training loss: 3.6624624729156494
2025-12-09 12:07:32.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0009883901697039807 Training loss: 3.5289690494537354
2025-12-09 12:07:32.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.000987205262323463 Training loss: 3.9420175552368164
2025-12-09 12:07:32.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.000985963571526376 Training loss: 3.739933967590332
2025-12-09 12:07:32.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0009846652420308728 Training loss: 3.7938551902770996
2025-12-09 12:07:32.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0009833104251563056 Training loss: 3.7096595764160156
2025-12-09 12:07:32.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.000981899278805589 Training loss: 3.8643407821655273
2025-12-09 12:07:32.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0009804319674467969 Training loss: 3.9117846488952637
2025-12-09 12:07:32.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0009789086620939935 Training loss: 3.689323663711548
2025-12-09 12:07:32.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0009773295402873026 Training loss: 3.6356115341186523
2025-12-09 12:07:32.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0009756947860722143 Training loss: 3.6520941257476807
2025-12-09 12:07:32.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0009740045899781352 Training loss: 4.030992031097412
2025-12-09 12:07:32.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0009722591489961827 Training loss: 3.717789649963379
2025-12-09 12:07:32.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0009704586665562249 Training loss: 3.891408920288086
2025-12-09 12:07:32.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0009686033525031719 Training loss: 3.779996871948242
2025-12-09 12:07:32.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0009666934230725179 Training loss: 3.6771955490112305
2025-12-09 12:07:32.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0009647291008651398 Training loss: 3.6876509189605713
2025-12-09 12:07:32.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0009627106148213521 Training loss: 3.826122283935547
2025-12-09 12:07:32.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0009606382001942255 Training loss: 3.8568434715270996
2025-12-09 12:07:32.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0009585120985221671 Training loss: 3.6136832237243652
2025-12-09 12:07:32.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0009563325576007701 Training loss: 3.8320746421813965
2025-12-09 12:07:32.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0009540998314539327 Training loss: 3.6768171787261963
2025-12-09 12:07:32.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0009518141803042527 Training loss: 3.713090419769287
2025-12-09 12:07:32.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0009494758705426977 Training loss: 3.8929762840270996
2025-12-09 12:07:32.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0009470851746975581 Training loss: 3.8304991722106934
2025-12-09 12:07:32.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0009446423714026846 Training loss: 3.806478261947632
2025-12-09 12:07:32.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0009421477453650118 Training loss: 3.9078328609466553
2025-12-09 12:07:32.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0009396015873313782 Training loss: 3.637789726257324
2025-12-09 12:07:32.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0009370041940546379 Training loss: 3.913346767425537
2025-12-09 12:07:32.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0009343558682590756 Training loss: 3.68160080909729
2025-12-09 12:07:32.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0009316569186051234 Training loss: 3.6968154907226562
2025-12-09 12:07:32.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0009289076596533872 Training loss: 3.626406192779541
2025-12-09 12:07:32.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0009261084118279846 Training loss: 3.6404247283935547
2025-12-09 12:07:32.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0009232595013792003 Training loss: 3.551931619644165
2025-12-09 12:07:32.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0009203612603454604 Training loss: 3.6788413524627686
2025-12-09 12:07:33.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0009174140265146356 Training loss: 3.3855578899383545
2025-12-09 12:07:33.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0009144181433846706 Training loss: 3.3127150535583496
2025-12-09 12:07:33.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0009113739601235507 Training loss: 3.7886428833007812
2025-12-09 12:07:33.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0009082818315286055 Training loss: 3.611888885498047
2025-12-09 12:07:33.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0009051421179851588 Training loss: 3.609332323074341
2025-12-09 12:07:33.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.000901955185424525 Training loss: 3.460726022720337
2025-12-09 12:07:33.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0008987214052813603 Training loss: 3.6199934482574463
2025-12-09 12:07:33.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0008954411544503729 Training loss: 3.5334365367889404
2025-12-09 12:07:33.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0008921148152423946 Training loss: 3.396487236022949
2025-12-09 12:07:33.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0008887427753398248 Training loss: 3.5967626571655273
2025-12-09 12:07:33.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0008853254277514447 Training loss: 3.669004201889038
2025-12-09 12:07:33.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0008818631707666135 Training loss: 3.774049997329712
2025-12-09 12:07:33.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0008783564079088476 Training loss: 3.629426956176758
2025-12-09 12:07:33.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0008748055478887904 Training loss: 3.642237901687622
2025-12-09 12:07:33.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0008712110045565768 Training loss: 3.5818307399749756
2025-12-09 12:07:33.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0008675731968536002 Training loss: 3.5439229011535645
2025-12-09 12:07:33.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0008638925487636848 Training loss: 3.3817718029022217
2025-12-09 12:07:33.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00086016948926367 Training loss: 3.6006405353546143
2025-12-09 12:07:33.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0008564044522734146 Training loss: 3.5886988639831543
2025-12-09 12:07:33.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.000852597876605223 Training loss: 3.5199687480926514
2025-12-09 12:07:33.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0008487502059127015 Training loss: 3.5125889778137207
2025-12-09 12:07:33.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0008448618886390522 Training loss: 3.5646400451660156
2025-12-09 12:07:33.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.0008409333779648059 Training loss: 3.669891595840454
2025-12-09 12:07:33.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0008369651317550054 Training loss: 3.3731887340545654
2025-12-09 12:07:33.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0008329576125058406 Training loss: 3.3741512298583984
2025-12-09 12:07:33.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0008289112872907454 Training loss: 3.670724630355835
2025-12-09 12:07:33.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0008248266277059606 Training loss: 3.4812803268432617
2025-12-09 12:07:33.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00082070410981557 Training loss: 3.3980023860931396
2025-12-09 12:07:33.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.000816544214096015 Training loss: 3.5198159217834473
2025-12-09 12:07:33.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0008123474253800957 Training loss: 3.5558996200561523
2025-12-09 12:07:33.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0008081142328004637 Training loss: 3.546647548675537
2025-12-09 12:07:33.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0008038451297326145 Training loss: 3.597137689590454
2025-12-09 12:07:33.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0007995406137373846 Training loss: 3.5242116451263428
2025-12-09 12:07:33.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0007952011865029613 Training loss: 3.5761287212371826
2025-12-09 12:07:33.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0007908273537864113 Training loss: 3.492710590362549
2025-12-09 12:07:33.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0007864196253547349 Training loss: 3.2370169162750244
2025-12-09 12:07:33.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0007819785149254532 Training loss: 3.472161054611206
2025-12-09 12:07:33.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.000777504540106735 Training loss: 3.4965596199035645
2025-12-09 12:07:33.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0007729982223370691 Training loss: 3.6133346557617188
2025-12-09 12:07:33.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0007684600868244919 Training loss: 3.3403992652893066
2025-12-09 12:07:33.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0007638906624853743 Training loss: 3.4745049476623535
2025-12-09 12:07:33.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0007592904818827774 Training loss: 3.4480197429656982
2025-12-09 12:07:33.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0007546600811643815 Training loss: 3.3990774154663086
2025-12-09 12:07:33.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.00075 Training loss: 3.5228612422943115
2025-12-09 12:07:33.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0007453107815186803 Training loss: 3.4305927753448486
2025-12-09 12:07:33.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0007405929722454026 Training loss: 3.531097650527954
2025-12-09 12:07:33.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0007358471220373831 Training loss: 3.501842498779297
2025-12-09 12:07:33.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0007310737840199885 Training loss: 3.4230289459228516
2025-12-09 12:07:33.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0007262735145222696 Training loss: 3.1712467670440674
2025-12-09 12:07:33.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0007214468730121209 Training loss: 3.1885764598846436
2025-12-09 12:07:33.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0007165944220310766 Training loss: 3.451148271560669
2025-12-09 12:07:33.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0007117167271287453 Training loss: 3.336209535598755
2025-12-09 12:07:33.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0007068143567968958 Training loss: 3.4033565521240234
2025-12-09 12:07:33.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0007018878824032009 Training loss: 3.348346710205078
2025-12-09 12:07:33.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.0006969378781246436 Training loss: 3.6716818809509277
2025-12-09 12:07:33.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0006919649208805981 Training loss: 3.289508819580078
2025-12-09 12:07:33.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0006869695902655897 Training loss: 3.453125
2025-12-09 12:07:33.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0006819524684817438 Training loss: 3.4916670322418213
2025-12-09 12:07:33.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0006769141402709304 Training loss: 3.2673158645629883
2025-12-09 12:07:33.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0006718551928466132 Training loss: 3.3703558444976807
2025-12-09 12:07:33.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0006667762158254104 Training loss: 3.3901009559631348
2025-12-09 12:07:33.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.0006616778011583743 Training loss: 3.451183795928955
2025-12-09 12:07:33.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0006565605430620013 Training loss: 3.481969118118286
2025-12-09 12:07:33.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0006514250379489753 Training loss: 3.3176989555358887
2025-12-09 12:07:33.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0006462718843586572 Training loss: 3.5554301738739014
2025-12-09 12:07:33.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0006411016828873239 Training loss: 3.3925886154174805
2025-12-09 12:07:33.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0006359150361181715 Training loss: 3.2628703117370605
2025-12-09 12:07:33.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0006307125485510829 Training loss: 3.4819705486297607
2025-12-09 12:07:33.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0006254948265321744 Training loss: 3.523745536804199
2025-12-09 12:07:33.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0006202624781831269 Training loss: 3.459033250808716
2025-12-09 12:07:33.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0006150161133303088 Training loss: 3.4526259899139404
2025-12-09 12:07:33.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0006097563434337025 Training loss: 3.4287848472595215
2025-12-09 12:07:33.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0006044837815156376 Training loss: 3.3711233139038086
2025-12-09 12:07:33.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0005991990420893449 Training loss: 3.200705051422119
2025-12-09 12:07:33.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0005939027410873352 Training loss: 3.39522123336792
2025-12-09 12:07:33.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0005885954957896114 Training loss: 3.4457316398620605
2025-12-09 12:07:33.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0005832779247517272 Training loss: 3.2448132038116455
2025-12-09 12:07:33.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0005779506477326933 Training loss: 3.4456839561462402
2025-12-09 12:07:33.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0005726142856227452 Training loss: 3.2389883995056152
2025-12-09 12:07:33.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0005672694603709794 Training loss: 3.6066248416900635
2025-12-09 12:07:33.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0005619167949128652 Training loss: 3.3394088745117188
2025-12-09 12:07:33.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0005565569130976422 Training loss: 3.249512195587158
2025-12-09 12:07:33.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0005511904396156113 Training loss: 3.2793779373168945
2025-12-09 12:07:33.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0005458179999253274 Training loss: 3.440136432647705
2025-12-09 12:07:33.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0005404402201807021 Training loss: 3.2955827713012695
2025-12-09 12:07:33.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0005350577271580271 Training loss: 3.3218133449554443
2025-12-09 12:07:33.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0005296711481829226 Training loss: 3.3483831882476807
2025-12-09 12:07:33.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0005242811110572242 Training loss: 3.511014699935913
2025-12-09 12:07:33.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0005188882439858117 Training loss: 3.1974544525146484
2025-12-09 12:07:33.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0005134931755033936 Training loss: 3.3339221477508545
2025-12-09 12:07:33.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0005080965344012508 Training loss: 2.998211145401001
2025-12-09 12:07:33.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0005026989496539523 Training loss: 3.3404312133789062
2025-12-09 12:07:33.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0004973010503460479 Training loss: 3.243499755859375
2025-12-09 12:07:33.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0004919034655987492 Training loss: 3.2867629528045654
2025-12-09 12:07:33.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0004865068244966066 Training loss: 3.2134716510772705
2025-12-09 12:07:33.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0004811117560141884 Training loss: 3.1450016498565674
2025-12-09 12:07:33.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.000475718888942776 Training loss: 3.2863786220550537
2025-12-09 12:07:33.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0004703288518170774 Training loss: 3.3414361476898193
2025-12-09 12:07:33.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00046494227284197295 Training loss: 3.089836835861206
2025-12-09 12:07:33.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00045955977981929796 Training loss: 3.1444413661956787
2025-12-09 12:07:33.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0004541820000746727 Training loss: 3.278970956802368
2025-12-09 12:07:33.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00044880956038438873 Training loss: 3.408684253692627
2025-12-09 12:07:33.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0004434430869023579 Training loss: 3.313790798187256
2025-12-09 12:07:33.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.000438083205087135 Training loss: 3.121086597442627
2025-12-09 12:07:33.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.00043273053962902076 Training loss: 3.0750513076782227
2025-12-09 12:07:33.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.000427385714377255 Training loss: 3.093784809112549
2025-12-09 12:07:33.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0004220493522673067 Training loss: 3.134849786758423
2025-12-09 12:07:33.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0004167220752482728 Training loss: 3.134831666946411
2025-12-09 12:07:33.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.00041140450421038864 Training loss: 3.408961772918701
2025-12-09 12:07:33.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.000406097258912665 Training loss: 3.1583919525146484
2025-12-09 12:07:34.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0004008009579106551 Training loss: 3.1101458072662354
2025-12-09 12:07:34.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0003955162184843625 Training loss: 3.233851432800293
2025-12-09 12:07:34.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.00039024365656629766 Training loss: 3.276808261871338
2025-12-09 12:07:34.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0003849838866696913 Training loss: 3.0939323902130127
2025-12-09 12:07:34.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00037973752181687335 Training loss: 3.0543413162231445
2025-12-09 12:07:34.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00037450517346782563 Training loss: 3.1654882431030273
2025-12-09 12:07:34.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0003692874514489173 Training loss: 3.0670859813690186
2025-12-09 12:07:34.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00036408496388182855 Training loss: 2.98134446144104
2025-12-09 12:07:34.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0003588983171126762 Training loss: 3.377030611038208
2025-12-09 12:07:34.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.000353728115641343 Training loss: 3.0019333362579346
2025-12-09 12:07:34.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0003485749620510247 Training loss: 3.0327394008636475
2025-12-09 12:07:34.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00034343945693799885 Training loss: 2.9816861152648926
2025-12-09 12:07:34.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00033832219884162584 Training loss: 3.2179388999938965
2025-12-09 12:07:34.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0003332237841745898 Training loss: 3.239443778991699
2025-12-09 12:07:34.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00032814480715338666 Training loss: 3.265049457550049
2025-12-09 12:07:34.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0003230858597290697 Training loss: 2.9570541381835938
2025-12-09 12:07:34.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0003180475315182563 Training loss: 2.983248233795166
2025-12-09 12:07:34.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0003130304097344103 Training loss: 3.1104633808135986
2025-12-09 12:07:34.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0003080350791194019 Training loss: 3.022446870803833
2025-12-09 12:07:34.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.00030306212187535653 Training loss: 3.060664415359497
2025-12-09 12:07:34.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002981121175967992 Training loss: 2.8824377059936523
2025-12-09 12:07:34.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.00029318564320310444 Training loss: 3.129668951034546
2025-12-09 12:07:34.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0002882832728712551 Training loss: 2.8682472705841064
2025-12-09 12:07:34.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0002834055779689235 Training loss: 2.85302472114563
2025-12-09 12:07:34.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.00027855312698787905 Training loss: 3.3352460861206055
2025-12-09 12:07:34.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0002737264854777306 Training loss: 2.9033279418945312
2025-12-09 12:07:34.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.00026892621598001155 Training loss: 3.2355682849884033
2025-12-09 12:07:34.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0002641528779626171 Training loss: 3.3294472694396973
2025-12-09 12:07:34.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.00025940702775459747 Training loss: 3.1424753665924072
2025-12-09 12:07:34.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.00025468921848131984 Training loss: 3.0940208435058594
2025-12-09 12:07:34.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0002500000000000001 Training loss: 3.155986785888672
2025-12-09 12:07:34.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.00024533991883561866 Training loss: 2.9337189197540283
2025-12-09 12:07:34.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.00024070951811722268 Training loss: 3.17521595954895
2025-12-09 12:07:34.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00023610933751462554 Training loss: 2.80256986618042
2025-12-09 12:07:34.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0002315399131755081 Training loss: 2.9599521160125732
2025-12-09 12:07:34.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00022700177766293096 Training loss: 3.1635944843292236
2025-12-09 12:07:34.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.00022249545989326514 Training loss: 3.039517402648926
2025-12-09 12:07:34.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002180214850745467 Training loss: 2.929443359375
2025-12-09 12:07:34.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.00021358037464526514 Training loss: 3.0686919689178467
2025-12-09 12:07:34.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.00020917264621358878 Training loss: 2.8090250492095947
2025-12-09 12:07:34.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.00020479881349703882 Training loss: 2.9899306297302246
2025-12-09 12:07:34.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.00020045938626261545 Training loss: 3.2260749340057373
2025-12-09 12:07:34.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.00019615487026738542 Training loss: 3.170473098754883
2025-12-09 12:07:34.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.00019188576719953633 Training loss: 3.0923209190368652
2025-12-09 12:07:34.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.00018765257461990443 Training loss: 2.988070011138916
2025-12-09 12:07:34.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0001834557859039851 Training loss: 2.8492422103881836
2025-12-09 12:07:34.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.00017929589018443015 Training loss: 3.1352341175079346
2025-12-09 12:07:34.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.00017517337229403947 Training loss: 2.9785664081573486
2025-12-09 12:07:34.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0001710887127092548 Training loss: 3.055682420730591
2025-12-09 12:07:34.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.00016704238749415957 Training loss: 3.1732125282287598
2025-12-09 12:07:34.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0001630348682449946 Training loss: 2.9174740314483643
2025-12-09 12:07:34.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00015906662203519413 Training loss: 2.8585493564605713
2025-12-09 12:07:34.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00015513811136094787 Training loss: 3.115839958190918
2025-12-09 12:07:34.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.0001512497940872986 Training loss: 3.167027711868286
2025-12-09 12:07:34.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0001474021233947772 Training loss: 3.155799150466919
2025-12-09 12:07:34.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00014359554772658552 Training loss: 2.4998586177825928
2025-12-09 12:07:34.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.00013983051073632996 Training loss: 3.2807888984680176
2025-12-09 12:07:34.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00013610745123631535 Training loss: 2.785172462463379
2025-12-09 12:07:34.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00013242680314639994 Training loss: 3.02824068069458
2025-12-09 12:07:34.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00012878899544342326 Training loss: 2.956393241882324
2025-12-09 12:07:34.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00012519445211120977 Training loss: 2.731273889541626
2025-12-09 12:07:34.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00012164359209115234 Training loss: 3.0263757705688477
2025-12-09 12:07:34.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00011813682923338653 Training loss: 2.9982717037200928
2025-12-09 12:07:34.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.00011467457224855543 Training loss: 3.1267337799072266
2025-12-09 12:07:34.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.00011125722466017545 Training loss: 3.0102462768554688
2025-12-09 12:07:34.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00010788518475760544 Training loss: 2.774111032485962
2025-12-09 12:07:34.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00010455884554962725 Training loss: 3.014448404312134
2025-12-09 12:07:34.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0001012785947186397 Training loss: 3.043984889984131
2025-12-09 12:07:34.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.804481457547498e-05 Training loss: 3.147935628890991
2025-12-09 12:07:34.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.485788201484124e-05 Training loss: 3.1863808631896973
2025-12-09 12:07:34.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.171816847139447e-05 Training loss: 2.8953685760498047
2025-12-09 12:07:34.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 8.862603987644941e-05 Training loss: 3.129934072494507
2025-12-09 12:07:34.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 8.558185661532942e-05 Training loss: 2.962118148803711
2025-12-09 12:07:34.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 8.258597348536451e-05 Training loss: 3.126514434814453
2025-12-09 12:07:34.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 7.96387396545396e-05 Training loss: 3.1222076416015625
2025-12-09 12:07:34.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 7.674049862079991e-05 Training loss: 3.0825443267822266
2025-12-09 12:07:34.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 7.38915881720154e-05 Training loss: 2.931342363357544
2025-12-09 12:07:34.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 7.109234034661289e-05 Training loss: 2.919646978378296
2025-12-09 12:07:34.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 6.834308139487671e-05 Training loss: 2.9157845973968506
2025-12-09 12:07:34.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 6.564413174092443e-05 Training loss: 2.805983304977417
2025-12-09 12:07:34.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 6.299580594536214e-05 Training loss: 2.9649887084960938
2025-12-09 12:07:34.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 6.0398412668621897e-05 Training loss: 2.9350969791412354
2025-12-09 12:07:34.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 5.785225463498828e-05 Training loss: 2.889084577560425
2025-12-09 12:07:34.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 5.535762859731547e-05 Training loss: 3.14776611328125
2025-12-09 12:07:34.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 5.291482530244179e-05 Training loss: 2.9472665786743164
2025-12-09 12:07:34.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 5.0524129457302394e-05 Training loss: 2.9735045433044434
2025-12-09 12:07:34.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 4.818581969574742e-05 Training loss: 3.002448320388794
2025-12-09 12:07:34.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 4.5900168546067264e-05 Training loss: 2.919330596923828
2025-12-09 12:07:34.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 4.366744239922998e-05 Training loss: 2.817737579345703
2025-12-09 12:07:34.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 4.148790147783288e-05 Training loss: 2.9035611152648926
2025-12-09 12:07:34.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 3.936179980577453e-05 Training loss: 3.0014572143554688
2025-12-09 12:07:34.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 3.728938517864794e-05 Training loss: 2.9297688007354736
2025-12-09 12:07:34.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 3.527089913486037e-05 Training loss: 2.922882318496704
2025-12-09 12:07:34.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 3.330657692748212e-05 Training loss: 2.637202024459839
2025-12-09 12:07:34.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 3.1396647496828245e-05 Training loss: 2.9470982551574707
2025-12-09 12:07:34.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 2.9541333443775243e-05 Training loss: 2.9105961322784424
2025-12-09 12:07:34.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 2.7740851003817347e-05 Training loss: 2.7872514724731445
2025-12-09 12:07:34.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 2.5995410021864786e-05 Training loss: 2.690641403198242
2025-12-09 12:07:34.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 2.430521392778573e-05 Training loss: 2.900927782058716
2025-12-09 12:07:34.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 2.2670459712697378e-05 Training loss: 2.949101686477661
2025-12-09 12:07:34.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 2.109133790600648e-05 Training loss: 3.001410722732544
2025-12-09 12:07:34.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 1.956803255320322e-05 Training loss: 2.884272336959839
2025-12-09 12:07:34.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 1.810072119441103e-05 Training loss: 3.1464240550994873
2025-12-09 12:07:34.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 1.6689574843694434e-05 Training loss: 2.938363790512085
2025-12-09 12:07:34.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 1.53347579691272e-05 Training loss: 3.122175931930542
2025-12-09 12:07:34.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 1.4036428473624019e-05 Training loss: 2.916015625
2025-12-09 12:07:34.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 1.2794737676536993e-05 Training loss: 2.803297519683838
2025-12-09 12:07:34.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 1.1609830296019142e-05 Training loss: 2.8586740493774414
2025-12-09 12:07:34.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 1.048184443215816e-05 Training loss: 3.139516592025757
2025-12-09 12:07:34.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880474e-06 Training loss: 3.0074973106384277
2025-12-09 12:07:34.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629207e-06 Training loss: 2.8618204593658447
2025-12-09 12:07:34.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.440697337816771e-06 Training loss: 2.9199278354644775
2025-12-09 12:07:35.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.541645633054649e-06 Training loss: 3.118321657180786
2025-12-09 12:07:35.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.700106138160688e-06 Training loss: 2.811218738555908
2025-12-09 12:07:35.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946692e-06 Training loss: 2.9558186531066895
2025-12-09 12:07:35.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.189949386787462e-06 Training loss: 2.941944122314453
2025-12-09 12:07:35.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.521508137971807e-06 Training loss: 2.940264940261841
2025-12-09 12:07:35.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378875e-06 Training loss: 2.810718297958374
2025-12-09 12:07:35.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.3582894166930268e-06 Training loss: 3.0499019622802734
2025-12-09 12:07:35.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.8636475165200173e-06 Training loss: 3.021050214767456
2025-12-09 12:07:35.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.4270630434701782e-06 Training loss: 2.9467933177948
2025-12-09 12:07:35.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441758e-06 Training loss: 2.797092914581299
2025-12-09 12:07:35.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.282631406615448e-07 Training loss: 3.004603147506714
2025-12-09 12:07:35.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.6612915551963455e-07 Training loss: 2.764779806137085
2025-12-09 12:07:35.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253336e-07 Training loss: 2.968517541885376
2025-12-09 12:07:35.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-07 Training loss: 2.9317970275878906
2025-12-09 12:07:35.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265823e-08 Training loss: 2.877230644226074
2025-12-09 12:07:35.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 3.0553176403045654
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:07:48.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 4.976489067077637
2025-12-09 12:07:48.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 4.871828079223633
2025-12-09 12:07:48.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 4.833873271942139
2025-12-09 12:07:48.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 4.796751976013184
2025-12-09 12:07:48.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 4.9385504722595215
2025-12-09 12:07:48.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 4.71798849105835
2025-12-09 12:07:48.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 4.780460834503174
2025-12-09 12:07:48.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 4.777891635894775
2025-12-09 12:07:48.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 4.770389556884766
2025-12-09 12:07:48.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 4.764874458312988
2025-12-09 12:07:48.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 4.824403762817383
2025-12-09 12:07:48.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 4.593798637390137
2025-12-09 12:07:48.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 4.676577568054199
2025-12-09 12:07:48.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 4.592828273773193
2025-12-09 12:07:48.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 4.62206506729126
2025-12-09 12:07:48.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 4.566178321838379
2025-12-09 12:07:48.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 4.4727067947387695
2025-12-09 12:07:48.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 4.373369216918945
2025-12-09 12:07:48.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 4.412664890289307
2025-12-09 12:07:48.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 4.519961833953857
2025-12-09 12:07:48.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 4.466067790985107
2025-12-09 12:07:48.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 4.522732257843018
2025-12-09 12:07:48.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 4.431121349334717
2025-12-09 12:07:48.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 4.385066986083984
2025-12-09 12:07:48.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 4.540459632873535
2025-12-09 12:07:48.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 4.4341959953308105
2025-12-09 12:07:48.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 4.327855110168457
2025-12-09 12:07:48.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 4.465521335601807
2025-12-09 12:07:48.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 4.40708589553833
2025-12-09 12:07:48.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 4.438540935516357
2025-12-09 12:07:48.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 4.331910610198975
2025-12-09 12:07:48.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 4.305478572845459
2025-12-09 12:07:48.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 4.286745548248291
2025-12-09 12:07:48.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 4.270376682281494
2025-12-09 12:07:48.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 4.195029258728027
2025-12-09 12:07:48.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 4.151246547698975
2025-12-09 12:07:48.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 4.074104309082031
2025-12-09 12:07:48.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 4.164137363433838
2025-12-09 12:07:48.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 4.202669620513916
2025-12-09 12:07:48.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 4.164398193359375
2025-12-09 12:07:48.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 4.131859302520752
2025-12-09 12:07:48.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 3.944741725921631
2025-12-09 12:07:48.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 4.134754180908203
2025-12-09 12:07:48.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 4.083258628845215
2025-12-09 12:07:48.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 4.041276931762695
2025-12-09 12:07:48.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 4.092491149902344
2025-12-09 12:07:48.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 4.25663423538208
2025-12-09 12:07:48.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 4.144016265869141
2025-12-09 12:07:48.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 3.987274169921875
2025-12-09 12:07:48.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 3.9252512454986572
2025-12-09 12:07:48.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 3.997861385345459
2025-12-09 12:07:48.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 4.01151704788208
2025-12-09 12:07:49.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 4.1146697998046875
2025-12-09 12:07:49.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 4.101168632507324
2025-12-09 12:07:49.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 4.044936656951904
2025-12-09 12:07:49.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 4.238252639770508
2025-12-09 12:07:49.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 4.293873310089111
2025-12-09 12:07:49.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 3.9733498096466064
2025-12-09 12:07:49.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 4.067327976226807
2025-12-09 12:07:49.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 4.072841644287109
2025-12-09 12:07:49.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 3.7088842391967773
2025-12-09 12:07:49.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 3.972538948059082
2025-12-09 12:07:49.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 4.171774387359619
2025-12-09 12:07:49.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 4.108836650848389
2025-12-09 12:07:49.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 4.00313138961792
2025-12-09 12:07:49.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 3.922978639602661
2025-12-09 12:07:49.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 3.979597806930542
2025-12-09 12:07:49.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 4.070082664489746
2025-12-09 12:07:49.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 3.8773226737976074
2025-12-09 12:07:49.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 3.997455596923828
2025-12-09 12:07:49.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 4.16917085647583
2025-12-09 12:07:49.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 4.026566028594971
2025-12-09 12:07:49.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 4.110862731933594
2025-12-09 12:07:49.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 3.8913488388061523
2025-12-09 12:07:49.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 3.8904716968536377
2025-12-09 12:07:49.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 4.198330402374268
2025-12-09 12:07:49.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 3.949608325958252
2025-12-09 12:07:49.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 3.9902842044830322
2025-12-09 12:07:49.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 3.9625163078308105
2025-12-09 12:07:49.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 3.9247171878814697
2025-12-09 12:07:49.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 3.9613633155822754
2025-12-09 12:07:49.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 3.936459541320801
2025-12-09 12:07:49.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 3.9434335231781006
2025-12-09 12:07:49.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 4.003429889678955
2025-12-09 12:07:49.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 4.045229911804199
2025-12-09 12:07:49.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 4.1507768630981445
2025-12-09 12:07:49.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 3.9612579345703125
2025-12-09 12:07:49.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 4.141915798187256
2025-12-09 12:07:49.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 4.057850360870361
2025-12-09 12:07:49.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 3.9975638389587402
2025-12-09 12:07:49.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 3.9337759017944336
2025-12-09 12:07:49.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 3.741051197052002
2025-12-09 12:07:49.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 3.8111133575439453
2025-12-09 12:07:49.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 3.90574049949646
2025-12-09 12:07:49.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 3.8241982460021973
2025-12-09 12:07:49.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 4.099527835845947
2025-12-09 12:07:49.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 4.217360973358154
2025-12-09 12:07:49.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 3.933560848236084
2025-12-09 12:07:49.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 3.8104746341705322
2025-12-09 12:07:49.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 3.862635850906372
2025-12-09 12:07:49.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999125880491853 Training loss: 3.8321566581726074
2025-12-09 12:07:49.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0029996503623845394 Training loss: 3.9852561950683594
2025-12-09 12:07:49.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029992133535682725 Training loss: 3.91964054107666
2025-12-09 12:07:49.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.002998601612533441 Training loss: 3.9017205238342285
2025-12-09 12:07:49.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0029978152105780156 Training loss: 3.7395377159118652
2025-12-09 12:07:49.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0029968542393565677 Training loss: 3.8708083629608154
2025-12-09 12:07:49.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029957188108695894 Training loss: 4.130891799926758
2025-12-09 12:07:49.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00299440905745044 Training loss: 3.8461875915527344
2025-12-09 12:07:49.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.002992925131749921 Training loss: 3.9801337718963623
2025-12-09 12:07:49.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0029912672067184862 Training loss: 4.060948371887207
2025-12-09 12:07:49.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0029894354755860848 Training loss: 3.867499351501465
2025-12-09 12:07:49.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0029874301518396378 Training loss: 3.798621654510498
2025-12-09 12:07:49.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029852514691981603 Training loss: 3.9012553691864014
2025-12-09 12:07:49.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029828996815855183 Training loss: 3.852998733520508
2025-12-09 12:07:49.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.002980375063100836 Training loss: 3.7366113662719727
2025-12-09 12:07:49.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00297767790798655 Training loss: 3.8473942279815674
2025-12-09 12:07:49.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.0029748085305941127 Training loss: 3.7413647174835205
2025-12-09 12:07:49.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.002971767265347359 Training loss: 3.5863656997680664
2025-12-09 12:07:49.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029685544667035256 Training loss: 3.7863004207611084
2025-12-09 12:07:49.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0029651705091119423 Training loss: 3.8103599548339844
2025-12-09 12:07:49.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0029616157869703894 Training loss: 3.838618040084839
2025-12-09 12:07:49.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002957890714579128 Training loss: 3.9554972648620605
2025-12-09 12:07:49.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0029539957260926184 Training loss: 3.5868284702301025
2025-12-09 12:07:49.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.002949931275468917 Training loss: 3.9129018783569336
2025-12-09 12:07:49.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029456978364167667 Training loss: 3.7272021770477295
2025-12-09 12:07:49.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0029412959023403904 Training loss: 3.950617551803589
2025-12-09 12:07:49.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.002936725986281981 Training loss: 3.8272273540496826
2025-12-09 12:07:49.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.002931988620861908 Training loss: 3.837439775466919
2025-12-09 12:07:49.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.002927084358216643 Training loss: 3.726689100265503
2025-12-09 12:07:49.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0029220137699344055 Training loss: 3.6944782733917236
2025-12-09 12:07:49.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0029167774469885483 Training loss: 3.8530449867248535
2025-12-09 12:07:49.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002911375999668675 Training loss: 3.7947661876678467
2025-12-09 12:07:49.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.002905810057509516 Training loss: 3.6937084197998047
2025-12-09 12:07:49.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.002900080269217554 Training loss: 3.8164548873901367
2025-12-09 12:07:49.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.002894187302595419 Training loss: 3.641418933868408
2025-12-09 12:07:49.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0028881318444640564 Training loss: 3.855508327484131
2025-12-09 12:07:49.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0028819146005826766 Training loss: 3.802382469177246
2025-12-09 12:07:49.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0028755362955665015 Training loss: 3.8086493015289307
2025-12-09 12:07:49.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0028689976728023103 Training loss: 3.863269805908203
2025-12-09 12:07:49.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0028622994943617985 Training loss: 3.9153971672058105
2025-12-09 12:07:49.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0028554425409127583 Training loss: 3.833392858505249
2025-12-09 12:07:49.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.002848427611628093 Training loss: 3.668972969055176
2025-12-09 12:07:49.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0028412555240926746 Training loss: 3.7550246715545654
2025-12-09 12:07:49.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.002833927114208054 Training loss: 3.79451322555542
2025-12-09 12:07:49.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0028264432360950355 Training loss: 3.7942068576812744
2025-12-09 12:07:49.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0028188047619941343 Training loss: 3.856816530227661
2025-12-09 12:07:49.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0028110125821639137 Training loss: 3.8927440643310547
2025-12-09 12:07:49.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.002803067604777227 Training loss: 3.9720232486724854
2025-12-09 12:07:49.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0027949707558153703 Training loss: 3.807439088821411
2025-12-09 12:07:49.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.002786722978960162 Training loss: 3.986947536468506
2025-12-09 12:07:49.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.002778325235483954 Training loss: 3.92338490486145
2025-12-09 12:07:49.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0027697785041376007 Training loss: 3.5982165336608887
2025-12-09 12:07:49.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.002761083781036381 Training loss: 3.842644453048706
2025-12-09 12:07:49.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.002752242079543907 Training loss: 3.6788625717163086
2025-12-09 12:07:49.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002743254430154012 Training loss: 3.8541183471679688
2025-12-09 12:07:49.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.002734121880370652 Training loss: 3.9690964221954346
2025-12-09 12:07:49.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0027248454945858164 Training loss: 3.681166887283325
2025-12-09 12:07:49.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0027154263539554764 Training loss: 3.7886128425598145
2025-12-09 12:07:49.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.002705865556273575 Training loss: 3.6888587474823
2025-12-09 12:07:50.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.002696164215844081 Training loss: 3.484968900680542
2025-12-09 12:07:50.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0026863234633511188 Training loss: 3.666774272918701
2025-12-09 12:07:50.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0026763444457271837 Training loss: 3.6661553382873535
2025-12-09 12:07:50.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.002666228326019474 Training loss: 3.681952476501465
2025-12-09 12:07:50.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.002655976283254334 Training loss: 3.438758134841919
2025-12-09 12:07:50.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0026455895122998404 Training loss: 3.7454261779785156
2025-12-09 12:07:50.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002635069223726543 Training loss: 3.5808801651000977
2025-12-09 12:07:50.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.002624416643666371 Training loss: 3.743238925933838
2025-12-09 12:07:50.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0026136330136697305 Training loss: 3.75632381439209
2025-12-09 12:07:50.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0026027195905608006 Training loss: 3.607544183731079
2025-12-09 12:07:50.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0025916776462910542 Training loss: 3.4888787269592285
2025-12-09 12:07:50.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00258050846779101 Training loss: 3.551607131958008
2025-12-09 12:07:50.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0025692133568202442 Training loss: 3.6547861099243164
2025-12-09 12:07:50.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.002557793629815669 Training loss: 3.627504825592041
2025-12-09 12:07:50.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0025462506177381045 Training loss: 3.5860109329223633
2025-12-09 12:07:50.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0025345856659171567 Training loss: 3.698561668395996
2025-12-09 12:07:50.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002522800133894418 Training loss: 3.6210241317749023
2025-12-09 12:07:50.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0025108953952650164 Training loss: 3.671583414077759
2025-12-09 12:07:50.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0024988728375175216 Training loss: 3.8112857341766357
2025-12-09 12:07:50.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.002486733861872236 Training loss: 3.6273434162139893
2025-12-09 12:07:50.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0024744798831178817 Training loss: 3.6662676334381104
2025-12-09 12:07:50.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0024621123294467097 Training loss: 3.6455135345458984
2025-12-09 12:07:50.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.002449632642288045 Training loss: 4.05660343170166
2025-12-09 12:07:50.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002437042276140287 Training loss: 3.6306304931640625
2025-12-09 12:07:50.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0024243426984013913 Training loss: 3.6869640350341797
2025-12-09 12:07:50.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0024115353891978435 Training loss: 3.7590556144714355
2025-12-09 12:07:50.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.002398621841212154 Training loss: 3.801476001739502
2025-12-09 12:07:50.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.002385603559508884 Training loss: 3.657881498336792
2025-12-09 12:07:50.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.002372482061359234 Training loss: 3.627887487411499
2025-12-09 12:07:50.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0023592588760642046 Training loss: 3.8806252479553223
2025-12-09 12:07:50.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00234593554477636 Training loss: 3.7899057865142822
2025-12-09 12:07:50.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.002332513620320205 Training loss: 3.545201539993286
2025-12-09 12:07:50.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.002318994667011207 Training loss: 3.51725435256958
2025-12-09 12:07:50.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.002305380260473476 Training loss: 3.5522685050964355
2025-12-09 12:07:50.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002291671987456123 Training loss: 3.3618643283843994
2025-12-09 12:07:50.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0022778714456483324 Training loss: 3.755150079727173
2025-12-09 12:07:50.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0022639802434931446 Training loss: 3.6214025020599365
2025-12-09 12:07:50.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0022500000000000003 Training loss: 3.8280460834503174
2025-12-09 12:07:50.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0022359323445560408 Training loss: 3.423414468765259
2025-12-09 12:07:50.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0022217789167362076 Training loss: 3.43143630027771
2025-12-09 12:07:50.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.002207541366112149 Training loss: 3.6678214073181152
2025-12-09 12:07:50.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0021932213520599654 Training loss: 3.6339993476867676
2025-12-09 12:07:50.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0021788205435668085 Training loss: 3.649966239929199
2025-12-09 12:07:50.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0021643406190363624 Training loss: 3.6059305667877197
2025-12-09 12:07:50.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0021497832660932296 Training loss: 3.5922510623931885
2025-12-09 12:07:50.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0021351501813862356 Training loss: 3.4891762733459473
2025-12-09 12:07:50.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0021204430703906873 Training loss: 3.531851053237915
2025-12-09 12:07:50.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0021056636472096026 Training loss: 3.716019630432129
2025-12-09 12:07:50.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002090813634373931 Training loss: 3.477907180786133
2025-12-09 12:07:50.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0020758947626417943 Training loss: 3.973571300506592
2025-12-09 12:07:50.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.002060908770796769 Training loss: 3.3784096240997314
2025-12-09 12:07:50.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0020458574054452315 Training loss: 3.5443763732910156
2025-12-09 12:07:50.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.002030742420812791 Training loss: 3.5410265922546387
2025-12-09 12:07:50.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0020155655785398397 Training loss: 3.608571767807007
2025-12-09 12:07:50.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.002000328647476231 Training loss: 3.3663411140441895
2025-12-09 12:07:50.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.001985033403475123 Training loss: 3.589420795440674
2025-12-09 12:07:50.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.001969681629186004 Training loss: 3.4118247032165527
2025-12-09 12:07:50.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.001954275113846926 Training loss: 3.4628167152404785
2025-12-09 12:07:50.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0019388156530759713 Training loss: 3.3815696239471436
2025-12-09 12:07:50.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0019233050486619715 Training loss: 3.5092644691467285
2025-12-09 12:07:50.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0019077451083545144 Training loss: 3.6392972469329834
2025-12-09 12:07:50.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0018921376456532484 Training loss: 3.2417285442352295
2025-12-09 12:07:50.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0018764844795965232 Training loss: 3.7119765281677246
2025-12-09 12:07:50.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0018607874345493807 Training loss: 3.5939512252807617
2025-12-09 12:07:50.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0018450483399909264 Training loss: 3.4400367736816406
2025-12-09 12:07:50.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0018292690303011077 Training loss: 3.6355886459350586
2025-12-09 12:07:50.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.001813451344546913 Training loss: 3.464183807373047
2025-12-09 12:07:50.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0017975971262680348 Training loss: 3.3029751777648926
2025-12-09 12:07:50.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0017817082232620054 Training loss: 3.3171534538269043
2025-12-09 12:07:50.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0017657864873688344 Training loss: 3.404851198196411
2025-12-09 12:07:50.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0017498337742551818 Training loss: 3.351013422012329
2025-12-09 12:07:50.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0017338519431980798 Training loss: 3.7146453857421875
2025-12-09 12:07:50.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0017178428568682357 Training loss: 3.5499842166900635
2025-12-09 12:07:50.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.001701808381112938 Training loss: 3.5463411808013916
2025-12-09 12:07:50.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0016857503847385955 Training loss: 3.6728577613830566
2025-12-09 12:07:50.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0016696707392929266 Training loss: 3.2027735710144043
2025-12-09 12:07:50.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.001653571318846834 Training loss: 3.4992568492889404
2025-12-09 12:07:50.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0016374539997759824 Training loss: 3.324655294418335
2025-12-09 12:07:50.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0016213206605421066 Training loss: 3.1080174446105957
2025-12-09 12:07:50.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.001605173181474081 Training loss: 3.526928186416626
2025-12-09 12:07:50.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0015890134445487678 Training loss: 3.363774061203003
2025-12-09 12:07:50.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0015728433331716725 Training loss: 3.6196000576019287
2025-12-09 12:07:50.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0015566647319574351 Training loss: 3.003631114959717
2025-12-09 12:07:50.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0015404795265101807 Training loss: 3.5254693031311035
2025-12-09 12:07:50.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0015242896032037524 Training loss: 3.2831826210021973
2025-12-09 12:07:50.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0015080968489618568 Training loss: 3.3955256938934326
2025-12-09 12:07:50.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0014919031510381437 Training loss: 3.441046714782715
2025-12-09 12:07:50.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0014757103967962479 Training loss: 3.4102895259857178
2025-12-09 12:07:50.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0014595204734898198 Training loss: 3.1476733684539795
2025-12-09 12:07:50.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0014433352680425654 Training loss: 3.4704833030700684
2025-12-09 12:07:50.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0014271566668283282 Training loss: 3.168666124343872
2025-12-09 12:07:50.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.001410986555451232 Training loss: 3.262883424758911
2025-12-09 12:07:50.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0013948268185259188 Training loss: 3.401083469390869
2025-12-09 12:07:50.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.001378679339457894 Training loss: 3.1510417461395264
2025-12-09 12:07:50.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0013625460002240181 Training loss: 3.463719129562378
2025-12-09 12:07:50.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0013464286811531662 Training loss: 3.3561384677886963
2025-12-09 12:07:50.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0013303292607070737 Training loss: 3.2139787673950195
2025-12-09 12:07:50.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.001314249615261405 Training loss: 3.454230785369873
2025-12-09 12:07:50.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0012981916188870622 Training loss: 3.3416213989257812
2025-12-09 12:07:50.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.001282157143131765 Training loss: 3.2024433612823486
2025-12-09 12:07:50.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00126614805680192 Training loss: 3.20147967338562
2025-12-09 12:07:50.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0012501662257448183 Training loss: 3.5318820476531982
2025-12-09 12:07:50.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.001234213512631166 Training loss: 3.2932732105255127
2025-12-09 12:07:50.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.001218291776737995 Training loss: 3.3624932765960693
2025-12-09 12:07:50.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0012024028737319652 Training loss: 3.1245036125183105
2025-12-09 12:07:51.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0011865486554530874 Training loss: 3.193692207336426
2025-12-09 12:07:51.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.001170730969698893 Training loss: 3.4519474506378174
2025-12-09 12:07:51.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0011549516600090739 Training loss: 3.3076059818267822
2025-12-09 12:07:51.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00113921256545062 Training loss: 3.2571864128112793
2025-12-09 12:07:51.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0011235155204034769 Training loss: 3.245718479156494
2025-12-09 12:07:51.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0011078623543467519 Training loss: 3.3094482421875
2025-12-09 12:07:51.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0010922548916454857 Training loss: 3.1434876918792725
2025-12-09 12:07:51.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0010766949513380285 Training loss: 3.627774953842163
2025-12-09 12:07:51.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.001061184346924029 Training loss: 3.2959580421447754
2025-12-09 12:07:51.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0010457248861530741 Training loss: 3.240569829940796
2025-12-09 12:07:51.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0010303183708139964 Training loss: 3.1962645053863525
2025-12-09 12:07:51.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0010149665965248776 Training loss: 3.3213586807250977
2025-12-09 12:07:51.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009996713525237694 Training loss: 3.196148633956909
2025-12-09 12:07:51.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0009844344214601601 Training loss: 3.107743501663208
2025-12-09 12:07:51.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.000969257579187209 Training loss: 3.2953882217407227
2025-12-09 12:07:51.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009541425945547689 Training loss: 3.0771117210388184
2025-12-09 12:07:51.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.000939091229203231 Training loss: 3.1785495281219482
2025-12-09 12:07:51.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0009241052373582058 Training loss: 3.2789463996887207
2025-12-09 12:07:51.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009091863656260695 Training loss: 2.9500136375427246
2025-12-09 12:07:51.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0008943363527903977 Training loss: 3.362035036087036
2025-12-09 12:07:51.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0008795569296093132 Training loss: 3.1423139572143555
2025-12-09 12:07:51.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0008648498186137653 Training loss: 3.394465446472168
2025-12-09 12:07:51.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0008502167339067705 Training loss: 3.0915184020996094
2025-12-09 12:07:51.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0008356593809636371 Training loss: 3.09486985206604
2025-12-09 12:07:51.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0008211794564331918 Training loss: 3.1626739501953125
2025-12-09 12:07:51.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0008067786479400346 Training loss: 3.14481782913208
2025-12-09 12:07:51.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0007924586338878512 Training loss: 3.104214668273926
2025-12-09 12:07:51.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0007782210832637924 Training loss: 3.460597515106201
2025-12-09 12:07:51.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0007640676554439594 Training loss: 3.1590278148651123
2025-12-09 12:07:51.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0007500000000000003 Training loss: 3.2370123863220215
2025-12-09 12:07:51.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.000736019756506856 Training loss: 3.2514142990112305
2025-12-09 12:07:51.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.000722128554351668 Training loss: 3.0821373462677
2025-12-09 12:07:51.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0007083280125438767 Training loss: 2.979484796524048
2025-12-09 12:07:51.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0006946197395265243 Training loss: 3.1978814601898193
2025-12-09 12:07:51.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0006810053329887928 Training loss: 3.174119710922241
2025-12-09 12:07:51.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0006674863796797954 Training loss: 3.323975086212158
2025-12-09 12:07:51.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0006540644552236401 Training loss: 3.172330856323242
2025-12-09 12:07:51.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0006407411239357954 Training loss: 3.1237635612487793
2025-12-09 12:07:51.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0006275179386407663 Training loss: 2.9941179752349854
2025-12-09 12:07:51.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0006143964404911165 Training loss: 3.011157274246216
2025-12-09 12:07:51.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0006013781587878463 Training loss: 3.115248918533325
2025-12-09 12:07:51.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0005884646108021563 Training loss: 2.976708173751831
2025-12-09 12:07:51.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.000575657301598609 Training loss: 3.0971598625183105
2025-12-09 12:07:51.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0005629577238597133 Training loss: 3.074589729309082
2025-12-09 12:07:51.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0005503673577119553 Training loss: 3.3040366172790527
2025-12-09 12:07:51.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0005378876705532904 Training loss: 2.780287742614746
2025-12-09 12:07:51.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0005255201168821184 Training loss: 3.1740546226501465
2025-12-09 12:07:51.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0005132661381277644 Training loss: 3.069525718688965
2025-12-09 12:07:51.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0005011271624824787 Training loss: 3.189584255218506
2025-12-09 12:07:51.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0004891046047349837 Training loss: 2.6287760734558105
2025-12-09 12:07:51.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00047719986610558236 Training loss: 3.047875165939331
2025-12-09 12:07:51.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00046541433408284357 Training loss: 2.810731887817383
2025-12-09 12:07:51.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00045374938226189584 Training loss: 3.128237009048462
2025-12-09 12:07:51.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.00044220637018433165 Training loss: 3.0452709197998047
2025-12-09 12:07:51.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00043078664317975653 Training loss: 3.083146572113037
2025-12-09 12:07:51.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0004194915322089899 Training loss: 2.72196102142334
2025-12-09 12:07:51.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00040832235370894606 Training loss: 3.1087398529052734
2025-12-09 12:07:51.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0003972804094391998 Training loss: 3.0713963508605957
2025-12-09 12:07:51.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0003863669863302698 Training loss: 3.055866241455078
2025-12-09 12:07:51.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00037558335633362935 Training loss: 3.103181838989258
2025-12-09 12:07:51.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.000364930776273457 Training loss: 2.802208185195923
2025-12-09 12:07:51.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0003544104877001596 Training loss: 3.0502665042877197
2025-12-09 12:07:51.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0003440237167456663 Training loss: 3.0144670009613037
2025-12-09 12:07:51.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0003337716739805264 Training loss: 2.902317762374878
2025-12-09 12:07:51.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00032365555427281634 Training loss: 2.9626381397247314
2025-12-09 12:07:51.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00031367653664888173 Training loss: 2.7841696739196777
2025-12-09 12:07:51.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0003038357841559191 Training loss: 2.9107303619384766
2025-12-09 12:07:51.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00029413444372642495 Training loss: 3.1840641498565674
2025-12-09 12:07:51.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.00028457364604452375 Training loss: 3.001225233078003
2025-12-09 12:07:51.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.00027515450541418343 Training loss: 3.0896966457366943
2025-12-09 12:07:51.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.0002658781196293482 Training loss: 2.853977918624878
2025-12-09 12:07:51.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0002567455698459882 Training loss: 2.9120564460754395
2025-12-09 12:07:51.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00024775792045609354 Training loss: 2.9984946250915527
2025-12-09 12:07:51.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.00023891621896361882 Training loss: 3.031127691268921
2025-12-09 12:07:51.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.00023022149586239971 Training loss: 3.0089004039764404
2025-12-09 12:07:51.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.00022167476451604625 Training loss: 2.8707549571990967
2025-12-09 12:07:51.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.00021327702103983865 Training loss: 3.1487033367156982
2025-12-09 12:07:51.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.00020502924418463014 Training loss: 3.3257360458374023
2025-12-09 12:07:51.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0001969323952227733 Training loss: 2.7326877117156982
2025-12-09 12:07:51.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.00018898741783608642 Training loss: 2.9054930210113525
2025-12-09 12:07:51.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00018119523800586568 Training loss: 2.6401567459106445
2025-12-09 12:07:51.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.00017355676390496484 Training loss: 3.1470718383789062
2025-12-09 12:07:51.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0001660728857919464 Training loss: 3.0091798305511475
2025-12-09 12:07:51.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00015874447590732537 Training loss: 2.843773365020752
2025-12-09 12:07:51.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0001515723883719072 Training loss: 2.922555446624756
2025-12-09 12:07:51.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00014455745908724228 Training loss: 2.998190402984619
2025-12-09 12:07:51.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0001377005056382018 Training loss: 3.0436513423919678
2025-12-09 12:07:51.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00013100232719768994 Training loss: 2.9133145809173584
2025-12-09 12:07:51.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00012446370443349862 Training loss: 3.0233964920043945
2025-12-09 12:07:51.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0001180853994173236 Training loss: 2.886996030807495
2025-12-09 12:07:51.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.00011186815553594382 Training loss: 2.9829773902893066
2025-12-09 12:07:51.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0001058126974045811 Training loss: 3.1250338554382324
2025-12-09 12:07:51.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.991973078244637e-05 Training loss: 2.5960426330566406
2025-12-09 12:07:51.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.418994249048473e-05 Training loss: 3.032918930053711
2025-12-09 12:07:51.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 8.862400033132573e-05 Training loss: 2.785698890686035
2025-12-09 12:07:51.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 8.322255301145204e-05 Training loss: 2.8610129356384277
2025-12-09 12:07:51.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 7.798623006559435e-05 Training loss: 2.834228038787842
2025-12-09 12:07:51.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 7.291564178335719e-05 Training loss: 2.9522299766540527
2025-12-09 12:07:51.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 6.801137913809213e-05 Training loss: 2.781555652618408
2025-12-09 12:07:51.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 6.327401371801944e-05 Training loss: 3.062258243560791
2025-12-09 12:07:51.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 5.870409765960966e-05 Training loss: 2.7055156230926514
2025-12-09 12:07:51.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 5.430216358323309e-05 Training loss: 3.1717772483825684
2025-12-09 12:07:51.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 5.00687245310833e-05 Training loss: 2.979220390319824
2025-12-09 12:07:51.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 4.60042739073816e-05 Training loss: 3.0882859230041504
2025-12-09 12:07:51.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 4.2109285420872056e-05 Training loss: 3.0551676750183105
2025-12-09 12:07:51.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 3.8384213029610984e-05 Training loss: 2.861394166946411
2025-12-09 12:07:51.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 3.4829490888057425e-05 Training loss: 3.3885231018066406
2025-12-09 12:07:51.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 3.144553329647448e-05 Training loss: 2.745482921600342
2025-12-09 12:07:52.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 2.823273465264142e-05 Training loss: 3.061140537261963
2025-12-09 12:07:52.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 2.5191469405887624e-05 Training loss: 2.8795626163482666
2025-12-09 12:07:52.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 2.2322092013450313e-05 Training loss: 2.911536931991577
2025-12-09 12:07:52.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 1.9624936899163947e-05 Training loss: 2.901172161102295
2025-12-09 12:07:52.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 1.7100318414482063e-05 Training loss: 2.9538931846618652
2025-12-09 12:07:52.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 1.4748530801840077e-05 Training loss: 2.716857671737671
2025-12-09 12:07:52.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 1.2569848160362384e-05 Training loss: 2.918416738510132
2025-12-09 12:07:52.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 1.0564524413915422e-05 Training loss: 2.8739700317382812
2025-12-09 12:07:52.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 8.732793281513663e-06 Training loss: 3.027895212173462
2025-12-09 12:07:52.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 7.07486825007908e-06 Training loss: 2.9787421226501465
2025-12-09 12:07:52.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 5.590942549560052e-06 Training loss: 2.757899045944214
2025-12-09 12:07:52.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 4.281189130410535e-06 Training loss: 2.829373836517334
2025-12-09 12:07:52.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 3.145760643432527e-06 Training loss: 3.0382795333862305
2025-12-09 12:07:52.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 2.184789421984634e-06 Training loss: 2.9295363426208496
2025-12-09 12:07:52.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 1.3983874665589036e-06 Training loss: 3.085521936416626
2025-12-09 12:07:52.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 7.866464317276001e-07 Training loss: 2.836246967315674
2025-12-09 12:07:52.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 3.496376154604186e-07 Training loss: 3.048431396484375
2025-12-09 12:07:52.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 8.741195081479747e-08 Training loss: 2.848733901977539
2025-12-09 12:07:52.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 2.9003043174743652
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:08:05.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 4.964031219482422
2025-12-09 12:08:05.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 4.920886516571045
2025-12-09 12:08:05.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 4.867537021636963
2025-12-09 12:08:05.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 4.74768590927124
2025-12-09 12:08:05.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 4.790375709533691
2025-12-09 12:08:05.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 4.520353317260742
2025-12-09 12:08:05.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 4.726772785186768
2025-12-09 12:08:05.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 4.708503246307373
2025-12-09 12:08:05.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 4.577335834503174
2025-12-09 12:08:05.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 4.390108108520508
2025-12-09 12:08:05.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 4.48075008392334
2025-12-09 12:08:05.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 4.487858295440674
2025-12-09 12:08:05.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 4.5411696434021
2025-12-09 12:08:06.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 4.509962558746338
2025-12-09 12:08:06.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 4.470780372619629
2025-12-09 12:08:06.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 4.439901351928711
2025-12-09 12:08:06.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 4.492313385009766
2025-12-09 12:08:06.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 4.353745460510254
2025-12-09 12:08:06.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 4.371682167053223
2025-12-09 12:08:06.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 4.548332214355469
2025-12-09 12:08:06.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 4.352470874786377
2025-12-09 12:08:06.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 4.238433837890625
2025-12-09 12:08:06.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 4.166964530944824
2025-12-09 12:08:06.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 4.344674587249756
2025-12-09 12:08:06.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 4.0654683113098145
2025-12-09 12:08:06.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 4.377280235290527
2025-12-09 12:08:06.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 4.332311630249023
2025-12-09 12:08:06.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 4.134580612182617
2025-12-09 12:08:06.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 4.546237945556641
2025-12-09 12:08:06.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 4.274167060852051
2025-12-09 12:08:06.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 4.218870639801025
2025-12-09 12:08:06.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 4.270751476287842
2025-12-09 12:08:06.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 4.174614906311035
2025-12-09 12:08:06.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 4.156720161437988
2025-12-09 12:08:06.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 4.2336106300354
2025-12-09 12:08:06.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 4.240270137786865
2025-12-09 12:08:06.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 4.175465106964111
2025-12-09 12:08:06.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 4.201331615447998
2025-12-09 12:08:06.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 4.630127429962158
2025-12-09 12:08:06.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 4.447897434234619
2025-12-09 12:08:06.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 4.183568000793457
2025-12-09 12:08:06.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 4.270409107208252
2025-12-09 12:08:06.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 4.305760383605957
2025-12-09 12:08:06.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 4.094882011413574
2025-12-09 12:08:06.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 4.185548305511475
2025-12-09 12:08:06.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 3.9459304809570312
2025-12-09 12:08:06.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 4.113375186920166
2025-12-09 12:08:06.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 4.355465888977051
2025-12-09 12:08:06.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 4.125142574310303
2025-12-09 12:08:06.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 4.05518913269043
2025-12-09 12:08:06.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 4.163570404052734
2025-12-09 12:08:06.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 4.268872261047363
2025-12-09 12:08:06.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 4.1108527183532715
2025-12-09 12:08:06.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 3.8671607971191406
2025-12-09 12:08:06.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 4.105704307556152
2025-12-09 12:08:06.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 4.055304527282715
2025-12-09 12:08:06.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 4.247432708740234
2025-12-09 12:08:06.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 3.946279525756836
2025-12-09 12:08:06.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 4.032462120056152
2025-12-09 12:08:06.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 4.004905700683594
2025-12-09 12:08:06.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 4.1150922775268555
2025-12-09 12:08:06.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 4.061941146850586
2025-12-09 12:08:06.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 4.075565814971924
2025-12-09 12:08:06.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 4.225064277648926
2025-12-09 12:08:06.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 4.074202060699463
2025-12-09 12:08:06.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 4.1024298667907715
2025-12-09 12:08:06.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 4.112710952758789
2025-12-09 12:08:06.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 4.057206153869629
2025-12-09 12:08:06.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 4.210938930511475
2025-12-09 12:08:06.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 4.734240531921387
2025-12-09 12:08:06.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 4.301483154296875
2025-12-09 12:08:06.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 4.171261787414551
2025-12-09 12:08:06.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 4.048961639404297
2025-12-09 12:08:06.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 3.9773144721984863
2025-12-09 12:08:06.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 4.237824440002441
2025-12-09 12:08:06.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 4.287691593170166
2025-12-09 12:08:06.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 4.072853088378906
2025-12-09 12:08:06.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 4.278404235839844
2025-12-09 12:08:06.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 4.3817458152771
2025-12-09 12:08:06.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 4.484127044677734
2025-12-09 12:08:06.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 4.053979396820068
2025-12-09 12:08:06.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 3.9456863403320312
2025-12-09 12:08:06.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 4.202836036682129
2025-12-09 12:08:06.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 4.2553510665893555
2025-12-09 12:08:06.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 4.121187686920166
2025-12-09 12:08:06.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 4.673640251159668
2025-12-09 12:08:06.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 4.514811038970947
2025-12-09 12:08:06.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 4.536930084228516
2025-12-09 12:08:06.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 4.489092826843262
2025-12-09 12:08:06.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 4.302606582641602
2025-12-09 12:08:06.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 4.5816264152526855
2025-12-09 12:08:06.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 4.411131858825684
2025-12-09 12:08:06.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 4.292599678039551
2025-12-09 12:08:06.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 4.29238748550415
2025-12-09 12:08:06.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 4.079727649688721
2025-12-09 12:08:06.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 4.328945159912109
2025-12-09 12:08:06.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 4.143002033233643
2025-12-09 12:08:06.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 4.302794456481934
2025-12-09 12:08:06.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 4.033949375152588
2025-12-09 12:08:06.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 4.08897590637207
2025-12-09 12:08:06.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999708626830616 Training loss: 4.355571269989014
2025-12-09 12:08:06.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.009998834541281799 Training loss: 4.295639991760254
2025-12-09 12:08:06.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009997377845227575 Training loss: 4.243244647979736
2025-12-09 12:08:06.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009995338708444804 Training loss: 4.687609672546387
2025-12-09 12:08:06.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009992717368593385 Training loss: 4.041380405426025
2025-12-09 12:08:06.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009989514131188558 Training loss: 4.899167537689209
2025-12-09 12:08:06.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009985729369565299 Training loss: 4.04181432723999
2025-12-09 12:08:06.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0099813635248348 Training loss: 4.433789253234863
2025-12-09 12:08:06.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.00997641710583307 Training loss: 4.390942096710205
2025-12-09 12:08:06.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.009970890689061622 Training loss: 4.350428581237793
2025-12-09 12:08:06.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009964784918620281 Training loss: 4.206364631652832
2025-12-09 12:08:06.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.009958100506132127 Training loss: 4.029748916625977
2025-12-09 12:08:06.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009950838230660534 Training loss: 4.242629051208496
2025-12-09 12:08:06.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009942998938618395 Training loss: 4.402425289154053
2025-12-09 12:08:06.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.009934583543669454 Training loss: 4.1653218269348145
2025-12-09 12:08:06.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009925593026621833 Training loss: 4.261630535125732
2025-12-09 12:08:06.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.009916028435313709 Training loss: 3.9592783451080322
2025-12-09 12:08:06.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.009905890884491196 Training loss: 4.326292514801025
2025-12-09 12:08:06.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.00989518155567842 Training loss: 4.436776161193848
2025-12-09 12:08:06.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009883901697039808 Training loss: 3.993624687194824
2025-12-09 12:08:06.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.009872052623234632 Training loss: 4.232292652130127
2025-12-09 12:08:06.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00985963571526376 Training loss: 4.045760154724121
2025-12-09 12:08:07.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.009846652420308728 Training loss: 3.980534315109253
2025-12-09 12:08:07.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.009833104251563056 Training loss: 3.933861494064331
2025-12-09 12:08:07.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.00981899278805589 Training loss: 3.984405994415283
2025-12-09 12:08:07.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.009804319674467968 Training loss: 4.030821323394775
2025-12-09 12:08:07.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009789086620939936 Training loss: 4.160632610321045
2025-12-09 12:08:07.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009773295402873026 Training loss: 4.01830530166626
2025-12-09 12:08:07.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009756947860722143 Training loss: 4.099470615386963
2025-12-09 12:08:07.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009740045899781353 Training loss: 3.988734245300293
2025-12-09 12:08:07.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.009722591489961827 Training loss: 4.058880805969238
2025-12-09 12:08:07.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009704586665562249 Training loss: 4.080212593078613
2025-12-09 12:08:07.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.00968603352503172 Training loss: 3.8112478256225586
2025-12-09 12:08:07.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009666934230725179 Training loss: 3.977064609527588
2025-12-09 12:08:07.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009647291008651398 Training loss: 3.7647006511688232
2025-12-09 12:08:07.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009627106148213521 Training loss: 3.8220467567443848
2025-12-09 12:08:07.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.009606382001942255 Training loss: 3.987565279006958
2025-12-09 12:08:07.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00958512098522167 Training loss: 4.1046013832092285
2025-12-09 12:08:07.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0095633255760077 Training loss: 3.819007158279419
2025-12-09 12:08:07.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009540998314539327 Training loss: 3.8456501960754395
2025-12-09 12:08:07.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009518141803042527 Training loss: 4.14857816696167
2025-12-09 12:08:07.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.009494758705426976 Training loss: 3.924873113632202
2025-12-09 12:08:07.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.009470851746975581 Training loss: 3.9046967029571533
2025-12-09 12:08:07.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009446423714026845 Training loss: 3.9580461978912354
2025-12-09 12:08:07.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.009421477453650118 Training loss: 3.958885908126831
2025-12-09 12:08:07.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.009396015873313781 Training loss: 4.161806106567383
2025-12-09 12:08:07.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00937004194054638 Training loss: 3.9708642959594727
2025-12-09 12:08:07.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009343558682590757 Training loss: 3.9767374992370605
2025-12-09 12:08:07.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.009316569186051234 Training loss: 3.966686964035034
2025-12-09 12:08:07.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009289076596533871 Training loss: 3.950759172439575
2025-12-09 12:08:07.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009261084118279847 Training loss: 3.942756175994873
2025-12-09 12:08:07.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009232595013792002 Training loss: 3.983461618423462
2025-12-09 12:08:07.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009203612603454604 Training loss: 3.7666962146759033
2025-12-09 12:08:07.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.009174140265146355 Training loss: 3.728516101837158
2025-12-09 12:08:07.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009144181433846706 Training loss: 3.991407632827759
2025-12-09 12:08:07.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009113739601235507 Training loss: 3.8842859268188477
2025-12-09 12:08:07.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009082818315286054 Training loss: 3.837867498397827
2025-12-09 12:08:07.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009051421179851587 Training loss: 3.734846353530884
2025-12-09 12:08:07.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.00901955185424525 Training loss: 3.935180187225342
2025-12-09 12:08:07.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.008987214052813604 Training loss: 3.939822196960449
2025-12-09 12:08:07.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00895441154450373 Training loss: 3.8627185821533203
2025-12-09 12:08:07.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.008921148152423945 Training loss: 3.9923183917999268
2025-12-09 12:08:07.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.008887427753398248 Training loss: 3.9145307540893555
2025-12-09 12:08:07.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.008853254277514447 Training loss: 3.8311026096343994
2025-12-09 12:08:07.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.008818631707666134 Training loss: 3.698424816131592
2025-12-09 12:08:07.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.008783564079088476 Training loss: 3.7428462505340576
2025-12-09 12:08:07.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.008748055478887904 Training loss: 3.7172036170959473
2025-12-09 12:08:07.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.008712110045565767 Training loss: 3.8676986694335938
2025-12-09 12:08:07.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.008675731968536002 Training loss: 3.5496506690979004
2025-12-09 12:08:07.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.008638925487636848 Training loss: 3.932173490524292
2025-12-09 12:08:07.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0086016948926367 Training loss: 3.999241828918457
2025-12-09 12:08:07.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.008564044522734147 Training loss: 3.916696548461914
2025-12-09 12:08:07.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.008525978766052229 Training loss: 3.7447242736816406
2025-12-09 12:08:07.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.008487502059127015 Training loss: 3.645946979522705
2025-12-09 12:08:07.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.008448618886390521 Training loss: 3.917273759841919
2025-12-09 12:08:07.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00840933377964806 Training loss: 3.5373523235321045
2025-12-09 12:08:07.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.008369651317550054 Training loss: 4.111636638641357
2025-12-09 12:08:07.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.008329576125058406 Training loss: 4.271346569061279
2025-12-09 12:08:07.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.008289112872907454 Training loss: 3.939560651779175
2025-12-09 12:08:07.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.008248266277059607 Training loss: 3.725374937057495
2025-12-09 12:08:07.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0082070410981557 Training loss: 3.5894367694854736
2025-12-09 12:08:07.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00816544214096015 Training loss: 3.975665330886841
2025-12-09 12:08:07.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.008123474253800956 Training loss: 3.8159921169281006
2025-12-09 12:08:07.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.008081142328004637 Training loss: 3.8124866485595703
2025-12-09 12:08:07.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.008038451297326145 Training loss: 3.867389440536499
2025-12-09 12:08:07.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.007995406137373847 Training loss: 3.8258984088897705
2025-12-09 12:08:07.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.007952011865029614 Training loss: 3.7601795196533203
2025-12-09 12:08:07.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.007908273537864113 Training loss: 3.7580037117004395
2025-12-09 12:08:07.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.007864196253547348 Training loss: 3.7203054428100586
2025-12-09 12:08:07.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.007819785149254532 Training loss: 3.7311806678771973
2025-12-09 12:08:07.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.00777504540106735 Training loss: 3.612208127975464
2025-12-09 12:08:07.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.007729982223370691 Training loss: 3.9780163764953613
2025-12-09 12:08:07.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.00768460086824492 Training loss: 3.7632503509521484
2025-12-09 12:08:07.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.007638906624853743 Training loss: 3.741607904434204
2025-12-09 12:08:07.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.007592904818827774 Training loss: 3.8846967220306396
2025-12-09 12:08:07.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.007546600811643816 Training loss: 3.7135846614837646
2025-12-09 12:08:07.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0075 Training loss: 3.8395676612854004
2025-12-09 12:08:07.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.007453107815186802 Training loss: 3.878628969192505
2025-12-09 12:08:07.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.007405929722454026 Training loss: 3.7882494926452637
2025-12-09 12:08:07.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.007358471220373831 Training loss: 3.477409839630127
2025-12-09 12:08:07.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.007310737840199885 Training loss: 3.7476189136505127
2025-12-09 12:08:07.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.007262735145222695 Training loss: 3.761913776397705
2025-12-09 12:08:07.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.007214468730121208 Training loss: 3.7920243740081787
2025-12-09 12:08:07.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.007165944220310766 Training loss: 3.88323712348938
2025-12-09 12:08:07.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.007117167271287452 Training loss: 3.7832114696502686
2025-12-09 12:08:07.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.007068143567968957 Training loss: 3.604858160018921
2025-12-09 12:08:07.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0070188788240320085 Training loss: 3.6523821353912354
2025-12-09 12:08:07.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.006969378781246436 Training loss: 3.6422693729400635
2025-12-09 12:08:07.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.006919649208805981 Training loss: 3.699244976043701
2025-12-09 12:08:07.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0068696959026558965 Training loss: 3.8413069248199463
2025-12-09 12:08:07.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.006819524684817438 Training loss: 3.6826364994049072
2025-12-09 12:08:07.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0067691414027093045 Training loss: 3.553469657897949
2025-12-09 12:08:07.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.006718551928466133 Training loss: 3.8524248600006104
2025-12-09 12:08:07.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.006667762158254104 Training loss: 3.6983840465545654
2025-12-09 12:08:07.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.006616778011583743 Training loss: 3.748141288757324
2025-12-09 12:08:07.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.006565605430620013 Training loss: 3.765688896179199
2025-12-09 12:08:07.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.006514250379489753 Training loss: 3.7307372093200684
2025-12-09 12:08:07.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.006462718843586571 Training loss: 3.728634834289551
2025-12-09 12:08:07.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0064110168288732386 Training loss: 3.803635358810425
2025-12-09 12:08:07.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.006359150361181715 Training loss: 3.8110415935516357
2025-12-09 12:08:07.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.006307125485510829 Training loss: 3.7982640266418457
2025-12-09 12:08:07.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0062549482653217435 Training loss: 3.618145227432251
2025-12-09 12:08:07.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0062026247818312685 Training loss: 3.51385498046875
2025-12-09 12:08:07.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0061501611333030885 Training loss: 3.5782346725463867
2025-12-09 12:08:07.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.006097563434337026 Training loss: 3.56891131401062
2025-12-09 12:08:07.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.006044837815156376 Training loss: 3.3781228065490723
2025-12-09 12:08:07.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.005991990420893449 Training loss: 3.725404739379883
2025-12-09 12:08:07.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.005939027410873351 Training loss: 3.533642292022705
2025-12-09 12:08:07.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0058859549578961145 Training loss: 3.695100784301758
2025-12-09 12:08:08.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.005832779247517273 Training loss: 3.789862871170044
2025-12-09 12:08:08.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0057795064773269325 Training loss: 3.822359800338745
2025-12-09 12:08:08.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.005726142856227452 Training loss: 3.5691146850585938
2025-12-09 12:08:08.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.005672694603709794 Training loss: 3.606661796569824
2025-12-09 12:08:08.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.005619167949128652 Training loss: 3.651599168777466
2025-12-09 12:08:08.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.005565569130976423 Training loss: 3.8377068042755127
2025-12-09 12:08:08.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.005511904396156113 Training loss: 3.5475616455078125
2025-12-09 12:08:08.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.005458179999253274 Training loss: 3.5323143005371094
2025-12-09 12:08:08.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.005404402201807022 Training loss: 3.519440174102783
2025-12-09 12:08:08.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00535057727158027 Training loss: 3.488731861114502
2025-12-09 12:08:08.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.005296711481829226 Training loss: 3.668745756149292
2025-12-09 12:08:08.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.005242811110572242 Training loss: 3.7320878505706787
2025-12-09 12:08:08.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.005188882439858117 Training loss: 3.4119462966918945
2025-12-09 12:08:08.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.005134931755033936 Training loss: 3.579960346221924
2025-12-09 12:08:08.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.005080965344012508 Training loss: 3.561310291290283
2025-12-09 12:08:08.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.005026989496539522 Training loss: 3.3746659755706787
2025-12-09 12:08:08.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.004973010503460479 Training loss: 3.6181137561798096
2025-12-09 12:08:08.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.004919034655987493 Training loss: 3.461945056915283
2025-12-09 12:08:08.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.004865068244966066 Training loss: 3.566275119781494
2025-12-09 12:08:08.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0048111175601418844 Training loss: 3.5142834186553955
2025-12-09 12:08:08.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0047571888894277605 Training loss: 3.6237871646881104
2025-12-09 12:08:08.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0047032885181707736 Training loss: 3.5322763919830322
2025-12-09 12:08:08.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.004649422728419729 Training loss: 3.66679310798645
2025-12-09 12:08:08.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0045955977981929795 Training loss: 3.54617977142334
2025-12-09 12:08:08.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.004541820000746727 Training loss: 3.5433602333068848
2025-12-09 12:08:08.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.004488095603843887 Training loss: 3.6932992935180664
2025-12-09 12:08:08.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.004434430869023579 Training loss: 3.562283992767334
2025-12-09 12:08:08.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00438083205087135 Training loss: 3.7345168590545654
2025-12-09 12:08:08.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.004327305396290207 Training loss: 3.655421495437622
2025-12-09 12:08:08.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00427385714377255 Training loss: 3.5548651218414307
2025-12-09 12:08:08.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.004220493522673068 Training loss: 3.817049980163574
2025-12-09 12:08:08.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.004167220752482727 Training loss: 3.671358823776245
2025-12-09 12:08:08.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0041140450421038866 Training loss: 3.5207037925720215
2025-12-09 12:08:08.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00406097258912665 Training loss: 3.451143980026245
2025-12-09 12:08:08.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.004008009579106551 Training loss: 3.2644565105438232
2025-12-09 12:08:08.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.003955162184843625 Training loss: 3.3671908378601074
2025-12-09 12:08:08.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.003902436565662977 Training loss: 3.5309669971466064
2025-12-09 12:08:08.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.003849838866696913 Training loss: 3.567234754562378
2025-12-09 12:08:08.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.003797375218168733 Training loss: 3.3663036823272705
2025-12-09 12:08:08.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0037450517346782563 Training loss: 3.3669731616973877
2025-12-09 12:08:08.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0036928745144891727 Training loss: 3.4110286235809326
2025-12-09 12:08:08.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0036408496388182854 Training loss: 3.208122491836548
2025-12-09 12:08:08.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0035889831711267616 Training loss: 3.6078221797943115
2025-12-09 12:08:08.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00353728115641343 Training loss: 3.8069021701812744
2025-12-09 12:08:08.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.003485749620510247 Training loss: 3.5232067108154297
2025-12-09 12:08:08.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0034343945693799884 Training loss: 3.7145049571990967
2025-12-09 12:08:08.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0033832219884162586 Training loss: 3.566326141357422
2025-12-09 12:08:08.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0033322378417458983 Training loss: 3.4138333797454834
2025-12-09 12:08:08.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0032814480715338667 Training loss: 3.2611474990844727
2025-12-09 12:08:08.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0032308585972906966 Training loss: 3.2878410816192627
2025-12-09 12:08:08.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0031804753151825627 Training loss: 3.4729793071746826
2025-12-09 12:08:08.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0031303040973441033 Training loss: 3.533477306365967
2025-12-09 12:08:08.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.003080350791194019 Training loss: 3.637805938720703
2025-12-09 12:08:08.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0030306212187535654 Training loss: 3.6500678062438965
2025-12-09 12:08:08.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029811211759679926 Training loss: 3.5412025451660156
2025-12-09 12:08:08.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029318564320310442 Training loss: 3.464329481124878
2025-12-09 12:08:08.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002882832728712551 Training loss: 3.419491767883301
2025-12-09 12:08:08.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0028340557796892353 Training loss: 3.420792579650879
2025-12-09 12:08:08.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0027855312698787903 Training loss: 3.5235707759857178
2025-12-09 12:08:08.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.002737264854777306 Training loss: 3.4033334255218506
2025-12-09 12:08:08.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0026892621598001154 Training loss: 3.437715530395508
2025-12-09 12:08:08.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0026415287796261707 Training loss: 3.5131239891052246
2025-12-09 12:08:08.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0025940702775459745 Training loss: 3.448634386062622
2025-12-09 12:08:08.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002546892184813198 Training loss: 3.3307416439056396
2025-12-09 12:08:08.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0025000000000000014 Training loss: 3.368478298187256
2025-12-09 12:08:08.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0024533991883561868 Training loss: 3.439700126647949
2025-12-09 12:08:08.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.002407095181172227 Training loss: 3.478945255279541
2025-12-09 12:08:08.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0023610933751462555 Training loss: 3.5967588424682617
2025-12-09 12:08:08.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002315399131755081 Training loss: 3.3542566299438477
2025-12-09 12:08:08.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0022700177766293095 Training loss: 3.4293503761291504
2025-12-09 12:08:08.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.002224954598932651 Training loss: 3.3736634254455566
2025-12-09 12:08:08.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0021802148507454673 Training loss: 3.5010640621185303
2025-12-09 12:08:08.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0021358037464526513 Training loss: 3.4960579872131348
2025-12-09 12:08:08.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0020917264621358876 Training loss: 3.3447768688201904
2025-12-09 12:08:08.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0020479881349703883 Training loss: 3.69101619720459
2025-12-09 12:08:08.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0020045938626261544 Training loss: 3.2765350341796875
2025-12-09 12:08:08.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0019615487026738545 Training loss: 3.3825254440307617
2025-12-09 12:08:08.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0019188576719953632 Training loss: 3.2843170166015625
2025-12-09 12:08:08.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0018765257461990442 Training loss: 3.1289284229278564
2025-12-09 12:08:08.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0018345578590398509 Training loss: 3.283207416534424
2025-12-09 12:08:08.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0017929589018443016 Training loss: 3.408074378967285
2025-12-09 12:08:08.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0017517337229403945 Training loss: 3.2520956993103027
2025-12-09 12:08:08.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.001710887127092548 Training loss: 3.3220791816711426
2025-12-09 12:08:08.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0016704238749415956 Training loss: 3.696371078491211
2025-12-09 12:08:08.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0016303486824499457 Training loss: 3.527463436126709
2025-12-09 12:08:08.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0015906662203519412 Training loss: 3.4340617656707764
2025-12-09 12:08:08.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0015513811136094785 Training loss: 3.2714250087738037
2025-12-09 12:08:08.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.001512497940872986 Training loss: 3.2375686168670654
2025-12-09 12:08:08.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.001474021233947772 Training loss: 3.540729522705078
2025-12-09 12:08:08.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0014359554772658551 Training loss: 3.202730417251587
2025-12-09 12:08:08.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0013983051073632995 Training loss: 3.254734516143799
2025-12-09 12:08:08.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0013610745123631535 Training loss: 3.41575026512146
2025-12-09 12:08:08.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0013242680314639993 Training loss: 3.131587505340576
2025-12-09 12:08:08.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0012878899544342326 Training loss: 3.4599039554595947
2025-12-09 12:08:08.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0012519445211120978 Training loss: 3.314317226409912
2025-12-09 12:08:08.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0012164359209115233 Training loss: 3.2766544818878174
2025-12-09 12:08:08.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0011813682923338654 Training loss: 3.462484836578369
2025-12-09 12:08:08.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0011467457224855543 Training loss: 3.0834298133850098
2025-12-09 12:08:08.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0011125722466017547 Training loss: 3.1120641231536865
2025-12-09 12:08:08.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0010788518475760545 Training loss: 3.288184642791748
2025-12-09 12:08:08.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0010455884554962725 Training loss: 3.4704935550689697
2025-12-09 12:08:08.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.001012785947186397 Training loss: 3.2029101848602295
2025-12-09 12:08:08.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00098044814575475 Training loss: 3.4544358253479004
2025-12-09 12:08:08.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0009485788201484125 Training loss: 3.4737982749938965
2025-12-09 12:08:08.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0009171816847139447 Training loss: 3.327972888946533
2025-12-09 12:08:08.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.000886260398764494 Training loss: 3.427408218383789
2025-12-09 12:08:08.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0008558185661532941 Training loss: 3.339169502258301
2025-12-09 12:08:08.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.000825859734853645 Training loss: 3.190932512283325
2025-12-09 12:08:08.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0007963873965453961 Training loss: 3.4352970123291016
2025-12-09 12:08:09.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.000767404986207999 Training loss: 3.1937711238861084
2025-12-09 12:08:09.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0007389158817201541 Training loss: 3.3900628089904785
2025-12-09 12:08:09.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0007109234034661288 Training loss: 3.3397722244262695
2025-12-09 12:08:09.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0006834308139487672 Training loss: 3.302309274673462
2025-12-09 12:08:09.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0006564413174092443 Training loss: 3.26257061958313
2025-12-09 12:08:09.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0006299580594536214 Training loss: 3.2695116996765137
2025-12-09 12:08:09.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0006039841266862189 Training loss: 3.3314905166625977
2025-12-09 12:08:09.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0005785225463498828 Training loss: 3.293365240097046
2025-12-09 12:08:09.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0005535762859731547 Training loss: 3.1905267238616943
2025-12-09 12:08:09.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0005291482530244179 Training loss: 3.165823221206665
2025-12-09 12:08:09.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0005052412945730239 Training loss: 3.2929370403289795
2025-12-09 12:08:09.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00048185819695747425 Training loss: 3.292713165283203
2025-12-09 12:08:09.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00045900168546067266 Training loss: 3.059622049331665
2025-12-09 12:08:09.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00043667442399229983 Training loss: 3.363856792449951
2025-12-09 12:08:09.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.0004148790147783288 Training loss: 3.1582274436950684
2025-12-09 12:08:09.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00039361799805774536 Training loss: 3.4371519088745117
2025-12-09 12:08:09.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0003728938517864794 Training loss: 3.0630486011505127
2025-12-09 12:08:09.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.00035270899134860366 Training loss: 3.291480541229248
2025-12-09 12:08:09.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00033306576927482126 Training loss: 3.673159599304199
2025-12-09 12:08:09.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0003139664749682825 Training loss: 3.4410462379455566
2025-12-09 12:08:09.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002954133344377524 Training loss: 3.2366089820861816
2025-12-09 12:08:09.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.00027740851003817346 Training loss: 3.4090051651000977
2025-12-09 12:08:09.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00025995410021864785 Training loss: 3.2357420921325684
2025-12-09 12:08:09.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0002430521392778573 Training loss: 3.4421942234039307
2025-12-09 12:08:09.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.00022670459712697378 Training loss: 3.363827705383301
2025-12-09 12:08:09.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0002109133790600648 Training loss: 3.0903944969177246
2025-12-09 12:08:09.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00019568032553203218 Training loss: 3.258955240249634
2025-12-09 12:08:09.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0001810072119441103 Training loss: 3.159600019454956
2025-12-09 12:08:09.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.00016689574843694433 Training loss: 3.1479291915893555
2025-12-09 12:08:09.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.000153347579691272 Training loss: 3.482670545578003
2025-12-09 12:08:09.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0001403642847362402 Training loss: 3.07517671585083
2025-12-09 12:08:09.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00012794737676536993 Training loss: 3.3514034748077393
2025-12-09 12:08:09.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00011609830296019141 Training loss: 3.2653348445892334
2025-12-09 12:08:09.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0001048184443215816 Training loss: 3.3275539875030518
2025-12-09 12:08:09.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880474e-05 Training loss: 3.0299437046051025
2025-12-09 12:08:09.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629208e-05 Training loss: 3.389298439025879
2025-12-09 12:08:09.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.44069733781677e-05 Training loss: 3.301534414291382
2025-12-09 12:08:09.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.54164563305465e-05 Training loss: 3.3059160709381104
2025-12-09 12:08:09.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.7001061381606875e-05 Training loss: 3.2984073162078857
2025-12-09 12:08:09.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946692e-05 Training loss: 3.447613477706909
2025-12-09 12:08:09.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.1899493867874615e-05 Training loss: 3.113941192626953
2025-12-09 12:08:09.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.521508137971807e-05 Training loss: 3.2886950969696045
2025-12-09 12:08:09.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378874e-05 Training loss: 3.530226230621338
2025-12-09 12:08:09.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.358289416693027e-05 Training loss: 3.1239380836486816
2025-12-09 12:08:09.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.863647516520017e-05 Training loss: 3.1706597805023193
2025-12-09 12:08:09.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.4270630434701781e-05 Training loss: 3.431967258453369
2025-12-09 12:08:09.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441756e-05 Training loss: 3.190851926803589
2025-12-09 12:08:09.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.2826314066154475e-06 Training loss: 3.2001609802246094
2025-12-09 12:08:09.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.661291555196345e-06 Training loss: 3.443293809890747
2025-12-09 12:08:09.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253337e-06 Training loss: 3.0633840560913086
2025-12-09 12:08:09.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-06 Training loss: 3.0223968029022217
2025-12-09 12:08:09.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265825e-07 Training loss: 3.330233335494995
2025-12-09 12:08:09.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 3.3531951904296875
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:08:23.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 4.807416915893555
2025-12-09 12:08:23.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 4.884763240814209
2025-12-09 12:08:23.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 4.891476154327393
2025-12-09 12:08:23.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 4.913439750671387
2025-12-09 12:08:23.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 4.7699294090271
2025-12-09 12:08:23.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 4.898793697357178
2025-12-09 12:08:23.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 4.819169521331787
2025-12-09 12:08:23.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 4.967755317687988
2025-12-09 12:08:23.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 4.811676502227783
2025-12-09 12:08:23.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 4.901790142059326
2025-12-09 12:08:23.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 4.706619739532471
2025-12-09 12:08:23.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 4.777202129364014
2025-12-09 12:08:23.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 4.894384384155273
2025-12-09 12:08:23.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 4.827633857727051
2025-12-09 12:08:23.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 4.795583724975586
2025-12-09 12:08:23.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 4.842196941375732
2025-12-09 12:08:23.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 4.92041540145874
2025-12-09 12:08:23.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 4.933459758758545
2025-12-09 12:08:23.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 4.806779384613037
2025-12-09 12:08:23.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 4.900381565093994
2025-12-09 12:08:23.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 4.919936656951904
2025-12-09 12:08:23.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 4.956502914428711
2025-12-09 12:08:23.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 4.77970552444458
2025-12-09 12:08:23.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 4.825094699859619
2025-12-09 12:08:23.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 4.897058010101318
2025-12-09 12:08:23.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 4.80316162109375
2025-12-09 12:08:23.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 4.983546733856201
2025-12-09 12:08:23.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 4.871586322784424
2025-12-09 12:08:23.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 4.913790702819824
2025-12-09 12:08:23.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 4.749074459075928
2025-12-09 12:08:23.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 4.6440348625183105
2025-12-09 12:08:23.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 4.9014973640441895
2025-12-09 12:08:23.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 4.864341735839844
2025-12-09 12:08:23.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 4.7923970222473145
2025-12-09 12:08:23.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 4.745015621185303
2025-12-09 12:08:24.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 4.854106903076172
2025-12-09 12:08:24.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 4.845688819885254
2025-12-09 12:08:24.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 4.8679094314575195
2025-12-09 12:08:24.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 4.877035617828369
2025-12-09 12:08:24.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 4.94230842590332
2025-12-09 12:08:24.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 5.0008063316345215
2025-12-09 12:08:24.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 4.858883857727051
2025-12-09 12:08:24.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 4.813955307006836
2025-12-09 12:08:24.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 4.745957374572754
2025-12-09 12:08:24.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 4.959157943725586
2025-12-09 12:08:24.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 4.75170373916626
2025-12-09 12:08:24.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 4.942490577697754
2025-12-09 12:08:24.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 4.9015583992004395
2025-12-09 12:08:24.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 4.889211177825928
2025-12-09 12:08:24.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 4.773494243621826
2025-12-09 12:08:24.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 4.716901779174805
2025-12-09 12:08:24.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 4.764179706573486
2025-12-09 12:08:24.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 4.733688831329346
2025-12-09 12:08:24.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 4.903561115264893
2025-12-09 12:08:24.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 4.808925151824951
2025-12-09 12:08:24.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 4.716157913208008
2025-12-09 12:08:24.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 4.88922119140625
2025-12-09 12:08:24.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 4.776815414428711
2025-12-09 12:08:24.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 4.769846439361572
2025-12-09 12:08:24.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 4.919809818267822
2025-12-09 12:08:24.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 4.708305835723877
2025-12-09 12:08:24.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 4.734050750732422
2025-12-09 12:08:24.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 4.764756202697754
2025-12-09 12:08:24.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 4.811374664306641
2025-12-09 12:08:24.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 4.700167179107666
2025-12-09 12:08:24.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 4.849527835845947
2025-12-09 12:08:24.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 4.830898284912109
2025-12-09 12:08:24.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 4.87635612487793
2025-12-09 12:08:24.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 4.749935150146484
2025-12-09 12:08:24.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 4.781463623046875
2025-12-09 12:08:24.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 4.916378974914551
2025-12-09 12:08:25.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 4.785304069519043
2025-12-09 12:08:25.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 4.820451736450195
2025-12-09 12:08:25.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 4.859490871429443
2025-12-09 12:08:25.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 4.9258270263671875
2025-12-09 12:08:25.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 4.858382701873779
2025-12-09 12:08:25.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 4.712160587310791
2025-12-09 12:08:25.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 4.798151016235352
2025-12-09 12:08:25.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 4.771055221557617
2025-12-09 12:08:25.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 4.701242446899414
2025-12-09 12:08:25.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 4.6314520835876465
2025-12-09 12:08:25.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 4.759357452392578
2025-12-09 12:08:25.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 4.82227087020874
2025-12-09 12:08:25.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 4.871768951416016
2025-12-09 12:08:25.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 4.796668529510498
2025-12-09 12:08:25.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 4.790532112121582
2025-12-09 12:08:25.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 4.781465530395508
2025-12-09 12:08:25.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 4.8421525955200195
2025-12-09 12:08:25.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 4.805006504058838
2025-12-09 12:08:25.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 4.7216362953186035
2025-12-09 12:08:25.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 4.704891681671143
2025-12-09 12:08:25.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 4.808255195617676
2025-12-09 12:08:25.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 4.7424516677856445
2025-12-09 12:08:25.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 4.705361366271973
2025-12-09 12:08:25.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 4.726911544799805
2025-12-09 12:08:25.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 4.6797661781311035
2025-12-09 12:08:25.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 4.738581657409668
2025-12-09 12:08:25.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 4.670316219329834
2025-12-09 12:08:25.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 4.78372859954834
2025-12-09 12:08:25.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 4.686532497406006
2025-12-09 12:08:25.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.999708626830618e-05 Training loss: 4.746339797973633
2025-12-09 12:08:25.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 9.998834541281798e-05 Training loss: 4.714141845703125
2025-12-09 12:08:25.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 9.997377845227576e-05 Training loss: 4.6724853515625
2025-12-09 12:08:25.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 9.995338708444804e-05 Training loss: 4.709565162658691
2025-12-09 12:08:25.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 9.992717368593385e-05 Training loss: 4.715757369995117
2025-12-09 12:08:25.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 9.989514131188559e-05 Training loss: 4.64192533493042
2025-12-09 12:08:25.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 9.985729369565299e-05 Training loss: 4.679233551025391
2025-12-09 12:08:25.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.9813635248348e-05 Training loss: 4.670106410980225
2025-12-09 12:08:26.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 9.97641710583307e-05 Training loss: 4.658885955810547
2025-12-09 12:08:26.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 9.970890689061622e-05 Training loss: 4.606276988983154
2025-12-09 12:08:26.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 9.964784918620282e-05 Training loss: 4.737740993499756
2025-12-09 12:08:26.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 9.958100506132127e-05 Training loss: 4.566539287567139
2025-12-09 12:08:26.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 9.950838230660534e-05 Training loss: 4.6801652908325195
2025-12-09 12:08:26.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 9.942998938618394e-05 Training loss: 4.560304641723633
2025-12-09 12:08:26.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 9.934583543669453e-05 Training loss: 4.595900535583496
2025-12-09 12:08:26.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 9.925593026621833e-05 Training loss: 4.5907135009765625
2025-12-09 12:08:26.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 9.916028435313708e-05 Training loss: 4.586472988128662
2025-12-09 12:08:26.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 9.905890884491195e-05 Training loss: 4.54141092300415
2025-12-09 12:08:26.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 9.895181555678418e-05 Training loss: 4.518231391906738
2025-12-09 12:08:26.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 9.883901697039808e-05 Training loss: 4.574387550354004
2025-12-09 12:08:26.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 9.872052623234632e-05 Training loss: 4.55807638168335
2025-12-09 12:08:26.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 9.85963571526376e-05 Training loss: 4.612545013427734
2025-12-09 12:08:26.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 9.846652420308728e-05 Training loss: 4.56686544418335
2025-12-09 12:08:26.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 9.833104251563056e-05 Training loss: 4.591782569885254
2025-12-09 12:08:26.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 9.818992788055889e-05 Training loss: 4.505134105682373
2025-12-09 12:08:26.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 9.80431967446797e-05 Training loss: 4.648963451385498
2025-12-09 12:08:26.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 9.789086620939936e-05 Training loss: 4.637988090515137
2025-12-09 12:08:26.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 9.773295402873026e-05 Training loss: 4.534299373626709
2025-12-09 12:08:26.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 9.756947860722143e-05 Training loss: 4.6846160888671875
2025-12-09 12:08:26.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 9.740045899781352e-05 Training loss: 4.59594202041626
2025-12-09 12:08:26.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 9.722591489961827e-05 Training loss: 4.57163143157959
2025-12-09 12:08:26.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 9.70458666556225e-05 Training loss: 4.581573963165283
2025-12-09 12:08:26.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 9.686033525031719e-05 Training loss: 4.5798749923706055
2025-12-09 12:08:26.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 9.66693423072518e-05 Training loss: 4.689960479736328
2025-12-09 12:08:26.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 9.647291008651398e-05 Training loss: 4.535506248474121
2025-12-09 12:08:26.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 9.627106148213522e-05 Training loss: 4.509089469909668
2025-12-09 12:08:26.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 9.606382001942255e-05 Training loss: 4.651177883148193
2025-12-09 12:08:26.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 9.585120985221671e-05 Training loss: 4.665130615234375
2025-12-09 12:08:26.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 9.563325576007701e-05 Training loss: 4.48229455947876
2025-12-09 12:08:26.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 9.540998314539328e-05 Training loss: 4.656438827514648
2025-12-09 12:08:26.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 9.518141803042527e-05 Training loss: 4.657026290893555
2025-12-09 12:08:26.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 9.494758705426978e-05 Training loss: 4.551095485687256
2025-12-09 12:08:26.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 9.470851746975582e-05 Training loss: 4.5218281745910645
2025-12-09 12:08:26.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 9.446423714026846e-05 Training loss: 4.5097975730896
2025-12-09 12:08:26.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 9.421477453650118e-05 Training loss: 4.709960460662842
2025-12-09 12:08:27.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 9.396015873313781e-05 Training loss: 4.47311544418335
2025-12-09 12:08:27.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 9.37004194054638e-05 Training loss: 4.543660640716553
2025-12-09 12:08:27.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 9.343558682590756e-05 Training loss: 4.459718227386475
2025-12-09 12:08:27.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 9.316569186051234e-05 Training loss: 4.596447467803955
2025-12-09 12:08:27.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 9.289076596533872e-05 Training loss: 4.550669193267822
2025-12-09 12:08:27.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 9.261084118279847e-05 Training loss: 4.472076416015625
2025-12-09 12:08:27.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 9.232595013792002e-05 Training loss: 4.4716410636901855
2025-12-09 12:08:27.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 9.203612603454604e-05 Training loss: 4.491650104522705
2025-12-09 12:08:27.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 9.174140265146356e-05 Training loss: 4.59637975692749
2025-12-09 12:08:27.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 9.144181433846707e-05 Training loss: 4.499828815460205
2025-12-09 12:08:27.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 9.113739601235507e-05 Training loss: 4.4388651847839355
2025-12-09 12:08:27.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 9.082818315286055e-05 Training loss: 4.493859767913818
2025-12-09 12:08:27.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 9.051421179851588e-05 Training loss: 4.41414213180542
2025-12-09 12:08:27.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 9.01955185424525e-05 Training loss: 4.602511882781982
2025-12-09 12:08:27.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 8.987214052813604e-05 Training loss: 4.541841506958008
2025-12-09 12:08:27.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 8.954411544503729e-05 Training loss: 4.5359673500061035
2025-12-09 12:08:27.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 8.921148152423946e-05 Training loss: 4.432968616485596
2025-12-09 12:08:27.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 8.887427753398248e-05 Training loss: 4.482220649719238
2025-12-09 12:08:27.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 8.853254277514446e-05 Training loss: 4.388469696044922
2025-12-09 12:08:27.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 8.818631707666135e-05 Training loss: 4.650068283081055
2025-12-09 12:08:27.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 8.783564079088477e-05 Training loss: 4.488872051239014
2025-12-09 12:08:27.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 8.748055478887904e-05 Training loss: 4.524272918701172
2025-12-09 12:08:27.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 8.712110045565768e-05 Training loss: 4.434693336486816
2025-12-09 12:08:27.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 8.675731968536002e-05 Training loss: 4.540933609008789
2025-12-09 12:08:27.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 8.638925487636848e-05 Training loss: 4.466912269592285
2025-12-09 12:08:27.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 8.6016948926367e-05 Training loss: 4.328950881958008
2025-12-09 12:08:27.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 8.564044522734147e-05 Training loss: 4.3315839767456055
2025-12-09 12:08:27.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 8.52597876605223e-05 Training loss: 4.499574661254883
2025-12-09 12:08:27.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 8.487502059127015e-05 Training loss: 4.451755046844482
2025-12-09 12:08:27.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 8.448618886390522e-05 Training loss: 4.643527030944824
2025-12-09 12:08:27.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 8.40933377964806e-05 Training loss: 4.406239986419678
2025-12-09 12:08:27.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 8.369651317550054e-05 Training loss: 4.522305011749268
2025-12-09 12:08:27.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 8.329576125058406e-05 Training loss: 4.469662189483643
2025-12-09 12:08:27.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 8.289112872907454e-05 Training loss: 4.477057456970215
2025-12-09 12:08:27.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 8.248266277059607e-05 Training loss: 4.428930282592773
2025-12-09 12:08:27.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 8.2070410981557e-05 Training loss: 4.585470199584961
2025-12-09 12:08:27.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 8.16544214096015e-05 Training loss: 4.374366283416748
2025-12-09 12:08:28.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 8.123474253800957e-05 Training loss: 4.5215301513671875
2025-12-09 12:08:28.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 8.081142328004637e-05 Training loss: 4.453728199005127
2025-12-09 12:08:28.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 8.038451297326145e-05 Training loss: 4.306363582611084
2025-12-09 12:08:28.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 7.995406137373846e-05 Training loss: 4.580838680267334
2025-12-09 12:08:28.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 7.952011865029614e-05 Training loss: 4.458127498626709
2025-12-09 12:08:28.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 7.908273537864113e-05 Training loss: 4.460531711578369
2025-12-09 12:08:28.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 7.86419625354735e-05 Training loss: 4.401713848114014
2025-12-09 12:08:28.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 7.819785149254532e-05 Training loss: 4.441482067108154
2025-12-09 12:08:28.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 7.77504540106735e-05 Training loss: 4.472036361694336
2025-12-09 12:08:28.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 7.729982223370691e-05 Training loss: 4.37354850769043
2025-12-09 12:08:28.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 7.68460086824492e-05 Training loss: 4.4514241218566895
2025-12-09 12:08:28.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 7.638906624853743e-05 Training loss: 4.333474636077881
2025-12-09 12:08:28.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 7.592904818827775e-05 Training loss: 4.4419732093811035
2025-12-09 12:08:28.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 7.546600811643816e-05 Training loss: 4.469165802001953
2025-12-09 12:08:28.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 7.500000000000001e-05 Training loss: 4.481315612792969
2025-12-09 12:08:28.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 7.453107815186803e-05 Training loss: 4.29518461227417
2025-12-09 12:08:28.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 7.405929722454026e-05 Training loss: 4.473886489868164
2025-12-09 12:08:28.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 7.358471220373832e-05 Training loss: 4.429542064666748
2025-12-09 12:08:28.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 7.310737840199885e-05 Training loss: 4.350740909576416
2025-12-09 12:08:28.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 7.262735145222696e-05 Training loss: 4.34218692779541
2025-12-09 12:08:28.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 7.214468730121208e-05 Training loss: 4.453068733215332
2025-12-09 12:08:28.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 7.165944220310767e-05 Training loss: 4.399875164031982
2025-12-09 12:08:28.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 7.117167271287453e-05 Training loss: 4.334104537963867
2025-12-09 12:08:28.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 7.068143567968957e-05 Training loss: 4.385180473327637
2025-12-09 12:08:28.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 7.018878824032009e-05 Training loss: 4.437130451202393
2025-12-09 12:08:28.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 6.969378781246436e-05 Training loss: 4.483096122741699
2025-12-09 12:08:28.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 6.919649208805981e-05 Training loss: 4.3670244216918945
2025-12-09 12:08:28.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 6.869695902655897e-05 Training loss: 4.26228666305542
2025-12-09 12:08:28.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 6.819524684817438e-05 Training loss: 4.286706447601318
2025-12-09 12:08:28.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 6.769141402709305e-05 Training loss: 4.402121543884277
2025-12-09 12:08:28.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 6.718551928466132e-05 Training loss: 4.350619792938232
2025-12-09 12:08:28.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 6.667762158254104e-05 Training loss: 4.474886417388916
2025-12-09 12:08:28.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 6.616778011583743e-05 Training loss: 4.416744232177734
2025-12-09 12:08:28.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 6.565605430620013e-05 Training loss: 4.247434616088867
2025-12-09 12:08:28.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 6.514250379489753e-05 Training loss: 4.432190895080566
2025-12-09 12:08:28.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 6.462718843586571e-05 Training loss: 4.261624813079834
2025-12-09 12:08:29.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 6.411016828873239e-05 Training loss: 4.41459321975708
2025-12-09 12:08:29.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 6.359150361181715e-05 Training loss: 4.299952507019043
2025-12-09 12:08:29.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 6.307125485510828e-05 Training loss: 4.44692325592041
2025-12-09 12:08:29.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 6.254948265321744e-05 Training loss: 4.428693771362305
2025-12-09 12:08:29.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 6.202624781831268e-05 Training loss: 4.375356197357178
2025-12-09 12:08:29.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 6.150161133303089e-05 Training loss: 4.3486433029174805
2025-12-09 12:08:29.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 6.0975634343370256e-05 Training loss: 4.400364398956299
2025-12-09 12:08:29.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 6.044837815156377e-05 Training loss: 4.418776035308838
2025-12-09 12:08:29.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 5.99199042089345e-05 Training loss: 4.318648338317871
2025-12-09 12:08:29.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 5.939027410873351e-05 Training loss: 4.306934833526611
2025-12-09 12:08:29.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 5.885954957896115e-05 Training loss: 4.432824611663818
2025-12-09 12:08:29.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 5.832779247517273e-05 Training loss: 4.416925430297852
2025-12-09 12:08:29.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 5.779506477326933e-05 Training loss: 4.366994380950928
2025-12-09 12:08:29.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 5.726142856227452e-05 Training loss: 4.374678611755371
2025-12-09 12:08:29.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 5.672694603709794e-05 Training loss: 4.2907280921936035
2025-12-09 12:08:29.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 5.619167949128652e-05 Training loss: 4.450795650482178
2025-12-09 12:08:29.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 5.565569130976422e-05 Training loss: 4.431904315948486
2025-12-09 12:08:29.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 5.5119043961561136e-05 Training loss: 4.277616024017334
2025-12-09 12:08:29.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 5.458179999253275e-05 Training loss: 4.299601078033447
2025-12-09 12:08:29.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 5.4044022018070214e-05 Training loss: 4.381418228149414
2025-12-09 12:08:29.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 5.3505772715802704e-05 Training loss: 4.29187536239624
2025-12-09 12:08:29.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 5.296711481829226e-05 Training loss: 4.458838939666748
2025-12-09 12:08:29.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 5.242811110572242e-05 Training loss: 4.306499004364014
2025-12-09 12:08:29.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 5.188882439858117e-05 Training loss: 4.407374382019043
2025-12-09 12:08:29.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 5.134931755033936e-05 Training loss: 4.243475437164307
2025-12-09 12:08:29.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 5.080965344012508e-05 Training loss: 4.219301700592041
2025-12-09 12:08:29.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 5.0269894965395225e-05 Training loss: 4.247928619384766
2025-12-09 12:08:29.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 4.973010503460479e-05 Training loss: 4.170760631561279
2025-12-09 12:08:29.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 4.919034655987493e-05 Training loss: 4.181494235992432
2025-12-09 12:08:29.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 4.865068244966066e-05 Training loss: 4.244619846343994
2025-12-09 12:08:29.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 4.8111175601418844e-05 Training loss: 4.455161094665527
2025-12-09 12:08:29.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 4.7571888894277604e-05 Training loss: 4.388850688934326
2025-12-09 12:08:29.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 4.703288518170774e-05 Training loss: 4.250951766967773
2025-12-09 12:08:29.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 4.6494227284197294e-05 Training loss: 4.306222438812256
2025-12-09 12:08:29.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 4.59559779819298e-05 Training loss: 4.372572898864746
2025-12-09 12:08:29.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 4.541820000746727e-05 Training loss: 4.270666122436523
2025-12-09 12:08:29.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 4.4880956038438876e-05 Training loss: 4.4165778160095215
2025-12-09 12:08:30.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 4.434430869023579e-05 Training loss: 4.197596549987793
2025-12-09 12:08:30.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 4.38083205087135e-05 Training loss: 4.240001201629639
2025-12-09 12:08:30.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 4.3273053962902076e-05 Training loss: 4.12797737121582
2025-12-09 12:08:30.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 4.27385714377255e-05 Training loss: 4.398800849914551
2025-12-09 12:08:30.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 4.220493522673067e-05 Training loss: 4.210813999176025
2025-12-09 12:08:30.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 4.1672207524827275e-05 Training loss: 4.399049758911133
2025-12-09 12:08:30.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 4.114045042103887e-05 Training loss: 4.282647132873535
2025-12-09 12:08:30.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 4.06097258912665e-05 Training loss: 4.365235805511475
2025-12-09 12:08:30.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 4.0080095791065505e-05 Training loss: 4.251132965087891
2025-12-09 12:08:30.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 3.955162184843625e-05 Training loss: 4.192480087280273
2025-12-09 12:08:30.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 3.902436565662977e-05 Training loss: 4.237423896789551
2025-12-09 12:08:30.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 3.849838866696913e-05 Training loss: 4.458807468414307
2025-12-09 12:08:30.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 3.7973752181687335e-05 Training loss: 4.2835869789123535
2025-12-09 12:08:30.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 3.745051734678256e-05 Training loss: 4.367833614349365
2025-12-09 12:08:30.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 3.692874514489173e-05 Training loss: 4.288788318634033
2025-12-09 12:08:30.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 3.640849638818286e-05 Training loss: 4.369898319244385
2025-12-09 12:08:30.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 3.588983171126762e-05 Training loss: 4.283475875854492
2025-12-09 12:08:30.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 3.53728115641343e-05 Training loss: 4.277584552764893
2025-12-09 12:08:30.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 3.4857496205102474e-05 Training loss: 4.278965473175049
2025-12-09 12:08:30.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 3.434394569379988e-05 Training loss: 4.288785457611084
2025-12-09 12:08:30.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 3.3832219884162585e-05 Training loss: 4.435842514038086
2025-12-09 12:08:30.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 3.332237841745898e-05 Training loss: 4.3075785636901855
2025-12-09 12:08:30.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 3.281448071533867e-05 Training loss: 4.215269565582275
2025-12-09 12:08:30.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 3.2308585972906966e-05 Training loss: 4.170616149902344
2025-12-09 12:08:30.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 3.180475315182563e-05 Training loss: 4.301338195800781
2025-12-09 12:08:30.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 3.130304097344103e-05 Training loss: 4.179366588592529
2025-12-09 12:08:30.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 3.080350791194019e-05 Training loss: 4.391659736633301
2025-12-09 12:08:30.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 3.0306212187535653e-05 Training loss: 4.330071449279785
2025-12-09 12:08:30.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 2.9811211759679924e-05 Training loss: 4.250441074371338
2025-12-09 12:08:30.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 2.9318564320310444e-05 Training loss: 4.315998077392578
2025-12-09 12:08:30.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 2.882832728712551e-05 Training loss: 4.303674697875977
2025-12-09 12:08:30.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 2.8340557796892354e-05 Training loss: 4.170159339904785
2025-12-09 12:08:30.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 2.7855312698787904e-05 Training loss: 4.36315393447876
2025-12-09 12:08:30.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 2.737264854777306e-05 Training loss: 4.362024307250977
2025-12-09 12:08:30.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 2.6892621598001156e-05 Training loss: 4.256870269775391
2025-12-09 12:08:30.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 2.6415287796261706e-05 Training loss: 4.355503559112549
2025-12-09 12:08:31.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 2.5940702775459747e-05 Training loss: 4.371674537658691
2025-12-09 12:08:31.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 2.5468921848131983e-05 Training loss: 4.289384841918945
2025-12-09 12:08:31.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 2.500000000000001e-05 Training loss: 4.291182518005371
2025-12-09 12:08:31.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 2.4533991883561868e-05 Training loss: 4.293264389038086
2025-12-09 12:08:31.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 2.407095181172227e-05 Training loss: 4.11835241317749
2025-12-09 12:08:31.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 2.3610933751462553e-05 Training loss: 4.303590774536133
2025-12-09 12:08:31.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 2.315399131755081e-05 Training loss: 4.277817726135254
2025-12-09 12:08:31.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 2.2700177766293096e-05 Training loss: 4.378958702087402
2025-12-09 12:08:31.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 2.2249545989326514e-05 Training loss: 4.370800018310547
2025-12-09 12:08:31.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 2.180214850745467e-05 Training loss: 4.209883689880371
2025-12-09 12:08:31.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 2.1358037464526515e-05 Training loss: 4.3177595138549805
2025-12-09 12:08:31.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 2.091726462135888e-05 Training loss: 4.307745456695557
2025-12-09 12:08:31.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 2.0479881349703883e-05 Training loss: 4.14977502822876
2025-12-09 12:08:31.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 2.0045938626261546e-05 Training loss: 4.1724371910095215
2025-12-09 12:08:31.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 1.9615487026738543e-05 Training loss: 4.219072341918945
2025-12-09 12:08:31.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 1.9188576719953633e-05 Training loss: 4.310133934020996
2025-12-09 12:08:31.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 1.8765257461990442e-05 Training loss: 4.301385879516602
2025-12-09 12:08:31.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 1.834557859039851e-05 Training loss: 4.276269435882568
2025-12-09 12:08:31.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 1.7929589018443016e-05 Training loss: 4.2066497802734375
2025-12-09 12:08:31.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 1.7517337229403946e-05 Training loss: 4.223169803619385
2025-12-09 12:08:31.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 1.710887127092548e-05 Training loss: 4.204423427581787
2025-12-09 12:08:31.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 1.6704238749415957e-05 Training loss: 4.246867656707764
2025-12-09 12:08:31.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 1.6303486824499458e-05 Training loss: 4.20657205581665
2025-12-09 12:08:31.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 1.5906662203519412e-05 Training loss: 4.356269836425781
2025-12-09 12:08:31.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 1.5513811136094787e-05 Training loss: 4.248010635375977
2025-12-09 12:08:31.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 1.5124979408729861e-05 Training loss: 4.33681058883667
2025-12-09 12:08:31.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 1.4740212339477721e-05 Training loss: 4.208827972412109
2025-12-09 12:08:31.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 1.4359554772658552e-05 Training loss: 4.248037815093994
2025-12-09 12:08:31.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 1.3983051073632997e-05 Training loss: 4.142782211303711
2025-12-09 12:08:31.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 1.3610745123631535e-05 Training loss: 4.133360385894775
2025-12-09 12:08:31.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 1.3242680314639993e-05 Training loss: 4.395676136016846
2025-12-09 12:08:31.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 1.2878899544342327e-05 Training loss: 4.203537940979004
2025-12-09 12:08:31.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 1.2519445211120979e-05 Training loss: 4.255921840667725
2025-12-09 12:08:31.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 1.2164359209115234e-05 Training loss: 4.277631759643555
2025-12-09 12:08:31.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 1.1813682923338653e-05 Training loss: 4.28942346572876
2025-12-09 12:08:32.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 1.1467457224855544e-05 Training loss: 4.322581768035889
2025-12-09 12:08:32.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 1.1125722466017547e-05 Training loss: 4.174400329589844
2025-12-09 12:08:32.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 1.0788518475760545e-05 Training loss: 4.291229248046875
2025-12-09 12:08:32.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 1.0455884554962725e-05 Training loss: 4.175118446350098
2025-12-09 12:08:32.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 1.012785947186397e-05 Training loss: 4.286608695983887
2025-12-09 12:08:32.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.804481457547498e-06 Training loss: 4.324318885803223
2025-12-09 12:08:32.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.485788201484126e-06 Training loss: 4.256133079528809
2025-12-09 12:08:32.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.171816847139448e-06 Training loss: 4.233741760253906
2025-12-09 12:08:32.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 8.86260398764494e-06 Training loss: 4.280683994293213
2025-12-09 12:08:32.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 8.558185661532941e-06 Training loss: 4.337381362915039
2025-12-09 12:08:32.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 8.25859734853645e-06 Training loss: 4.304214954376221
2025-12-09 12:08:32.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 7.96387396545396e-06 Training loss: 4.333521842956543
2025-12-09 12:08:32.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 7.67404986207999e-06 Training loss: 4.1822967529296875
2025-12-09 12:08:32.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 7.389158817201542e-06 Training loss: 4.398521900177002
2025-12-09 12:08:32.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 7.109234034661289e-06 Training loss: 4.171009540557861
2025-12-09 12:08:32.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 6.8343081394876715e-06 Training loss: 4.3329644203186035
2025-12-09 12:08:32.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 6.564413174092443e-06 Training loss: 4.3712286949157715
2025-12-09 12:08:32.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 6.299580594536214e-06 Training loss: 4.166014194488525
2025-12-09 12:08:32.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 6.0398412668621895e-06 Training loss: 4.298115253448486
2025-12-09 12:08:32.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 5.785225463498828e-06 Training loss: 4.302985668182373
2025-12-09 12:08:32.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 5.535762859731547e-06 Training loss: 4.109285354614258
2025-12-09 12:08:32.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 5.291482530244179e-06 Training loss: 4.1128740310668945
2025-12-09 12:08:32.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 5.05241294573024e-06 Training loss: 4.284399032592773
2025-12-09 12:08:32.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 4.818581969574742e-06 Training loss: 4.124522686004639
2025-12-09 12:08:32.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 4.590016854606727e-06 Training loss: 4.21868371963501
2025-12-09 12:08:32.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 4.366744239922998e-06 Training loss: 4.32699728012085
2025-12-09 12:08:32.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 4.148790147783288e-06 Training loss: 4.147897243499756
2025-12-09 12:08:32.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 3.936179980577453e-06 Training loss: 4.269458770751953
2025-12-09 12:08:32.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 3.728938517864794e-06 Training loss: 4.156815528869629
2025-12-09 12:08:32.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 3.527089913486037e-06 Training loss: 4.073634147644043
2025-12-09 12:08:32.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 3.3306576927482126e-06 Training loss: 4.178506374359131
2025-12-09 12:08:32.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 3.1396647496828247e-06 Training loss: 4.256709098815918
2025-12-09 12:08:32.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 2.9541333443775243e-06 Training loss: 4.278628826141357
2025-12-09 12:08:32.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 2.774085100381735e-06 Training loss: 4.175690174102783
2025-12-09 12:08:32.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 2.5995410021864787e-06 Training loss: 4.122504234313965
2025-12-09 12:08:32.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 2.430521392778573e-06 Training loss: 4.1902875900268555
2025-12-09 12:08:33.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 2.2670459712697377e-06 Training loss: 4.17415189743042
2025-12-09 12:08:33.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 2.1091337906006482e-06 Training loss: 4.1553449630737305
2025-12-09 12:08:33.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 1.956803255320322e-06 Training loss: 4.198418140411377
2025-12-09 12:08:33.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 1.810072119441103e-06 Training loss: 4.349740028381348
2025-12-09 12:08:33.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 1.6689574843694433e-06 Training loss: 4.196045875549316
2025-12-09 12:08:33.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 1.53347579691272e-06 Training loss: 4.235924243927002
2025-12-09 12:08:33.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 1.4036428473624019e-06 Training loss: 4.196152210235596
2025-12-09 12:08:33.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 1.2794737676536994e-06 Training loss: 4.136143684387207
2025-12-09 12:08:33.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 1.1609830296019143e-06 Training loss: 4.2739081382751465
2025-12-09 12:08:33.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 1.0481844432158161e-06 Training loss: 4.224211692810059
2025-12-09 12:08:33.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880475e-07 Training loss: 4.104203224182129
2025-12-09 12:08:33.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629208e-07 Training loss: 4.204704761505127
2025-12-09 12:08:33.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.44069733781677e-07 Training loss: 4.219138145446777
2025-12-09 12:08:33.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.54164563305465e-07 Training loss: 4.364266872406006
2025-12-09 12:08:33.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.700106138160688e-07 Training loss: 4.356906414031982
2025-12-09 12:08:33.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946693e-07 Training loss: 4.101128101348877
2025-12-09 12:08:33.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.189949386787462e-07 Training loss: 4.292298793792725
2025-12-09 12:08:33.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.5215081379718074e-07 Training loss: 4.27244758605957
2025-12-09 12:08:33.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378877e-07 Training loss: 4.191623210906982
2025-12-09 12:08:33.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.3582894166930268e-07 Training loss: 4.150076389312744
2025-12-09 12:08:33.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.8636475165200174e-07 Training loss: 4.297043800354004
2025-12-09 12:08:33.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.427063043470178e-07 Training loss: 4.18207311630249
2025-12-09 12:08:33.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441757e-07 Training loss: 4.243820667266846
2025-12-09 12:08:33.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.282631406615447e-08 Training loss: 4.379822254180908
2025-12-09 12:08:33.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.661291555196345e-08 Training loss: 4.247375965118408
2025-12-09 12:08:33.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253337e-08 Training loss: 4.351202011108398
2025-12-09 12:08:33.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-08 Training loss: 4.295895576477051
2025-12-09 12:08:33.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265825e-09 Training loss: 4.30062198638916
2025-12-09 12:08:33.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 4.13051176071167
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:08:47.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 4.867965221405029
2025-12-09 12:08:47.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 4.898160457611084
2025-12-09 12:08:47.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 4.819419860839844
2025-12-09 12:08:47.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 4.770198822021484
2025-12-09 12:08:47.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 4.94188117980957
2025-12-09 12:08:47.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 4.7728166580200195
2025-12-09 12:08:47.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 4.770824432373047
2025-12-09 12:08:47.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 4.942751407623291
2025-12-09 12:08:47.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 4.819450378417969
2025-12-09 12:08:47.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 4.849037170410156
2025-12-09 12:08:47.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 4.857202053070068
2025-12-09 12:08:47.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 4.836520671844482
2025-12-09 12:08:47.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 4.886269569396973
2025-12-09 12:08:47.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 4.820043087005615
2025-12-09 12:08:47.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 4.873227596282959
2025-12-09 12:08:47.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 4.9394354820251465
2025-12-09 12:08:47.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 4.999032020568848
2025-12-09 12:08:47.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 4.817154884338379
2025-12-09 12:08:47.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 4.8733625411987305
2025-12-09 12:08:47.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 4.855020523071289
2025-12-09 12:08:47.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 4.88319206237793
2025-12-09 12:08:47.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 4.966964244842529
2025-12-09 12:08:47.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 4.814157485961914
2025-12-09 12:08:48.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 4.820774078369141
2025-12-09 12:08:48.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 4.9761786460876465
2025-12-09 12:08:48.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 4.900373935699463
2025-12-09 12:08:48.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 4.867234706878662
2025-12-09 12:08:48.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 4.826162338256836
2025-12-09 12:08:48.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 4.789150238037109
2025-12-09 12:08:48.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 4.796202659606934
2025-12-09 12:08:48.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 4.902031421661377
2025-12-09 12:08:48.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 4.8445143699646
2025-12-09 12:08:48.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 4.817885398864746
2025-12-09 12:08:48.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 4.856237888336182
2025-12-09 12:08:48.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 4.817305088043213
2025-12-09 12:08:48.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 4.920900344848633
2025-12-09 12:08:48.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 4.823707103729248
2025-12-09 12:08:48.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 4.732191562652588
2025-12-09 12:08:48.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 4.835465908050537
2025-12-09 12:08:48.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 4.777288913726807
2025-12-09 12:08:48.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 4.827930450439453
2025-12-09 12:08:48.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 4.843242645263672
2025-12-09 12:08:48.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 4.693660259246826
2025-12-09 12:08:48.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 4.831520080566406
2025-12-09 12:08:48.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 4.816177845001221
2025-12-09 12:08:48.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 4.8330159187316895
2025-12-09 12:08:48.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 4.747304916381836
2025-12-09 12:08:48.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 4.7258429527282715
2025-12-09 12:08:48.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 4.752568244934082
2025-12-09 12:08:48.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 4.78287410736084
2025-12-09 12:08:48.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 4.729079246520996
2025-12-09 12:08:48.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 4.729883670806885
2025-12-09 12:08:48.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 4.769861221313477
2025-12-09 12:08:48.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 4.721888542175293
2025-12-09 12:08:48.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 4.702339172363281
2025-12-09 12:08:48.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 4.817317485809326
2025-12-09 12:08:48.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 4.681263446807861
2025-12-09 12:08:48.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 4.691486835479736
2025-12-09 12:08:48.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 4.672207355499268
2025-12-09 12:08:48.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 4.688573360443115
2025-12-09 12:08:49.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 4.69435977935791
2025-12-09 12:08:49.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 4.625432014465332
2025-12-09 12:08:49.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 4.617293834686279
2025-12-09 12:08:49.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 4.6507768630981445
2025-12-09 12:08:49.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 4.625734806060791
2025-12-09 12:08:49.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 4.553564071655273
2025-12-09 12:08:49.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 4.699682235717773
2025-12-09 12:08:49.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 4.682586669921875
2025-12-09 12:08:49.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 4.725435256958008
2025-12-09 12:08:49.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 4.673120021820068
2025-12-09 12:08:49.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 4.686999797821045
2025-12-09 12:08:49.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 4.6582536697387695
2025-12-09 12:08:49.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 4.617325305938721
2025-12-09 12:08:49.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 4.735093593597412
2025-12-09 12:08:49.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 4.675834655761719
2025-12-09 12:08:49.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 4.644320487976074
2025-12-09 12:08:49.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 4.719690322875977
2025-12-09 12:08:49.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 4.598431587219238
2025-12-09 12:08:49.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 4.589298248291016
2025-12-09 12:08:49.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 4.520661354064941
2025-12-09 12:08:49.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 4.525298595428467
2025-12-09 12:08:49.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 4.907539367675781
2025-12-09 12:08:49.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 4.532423973083496
2025-12-09 12:08:49.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 4.523472309112549
2025-12-09 12:08:49.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 4.478995323181152
2025-12-09 12:08:49.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 4.631076335906982
2025-12-09 12:08:49.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 4.579253196716309
2025-12-09 12:08:49.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 4.46012020111084
2025-12-09 12:08:49.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 4.583657264709473
2025-12-09 12:08:49.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 4.422388076782227
2025-12-09 12:08:49.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 4.458600044250488
2025-12-09 12:08:49.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 4.401400089263916
2025-12-09 12:08:49.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 4.549683570861816
2025-12-09 12:08:49.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 4.417386054992676
2025-12-09 12:08:49.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 4.569701194763184
2025-12-09 12:08:49.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 4.405548095703125
2025-12-09 12:08:49.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 4.426734447479248
2025-12-09 12:08:49.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 4.490969181060791
2025-12-09 12:08:50.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 4.5065083503723145
2025-12-09 12:08:50.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 4.369548797607422
2025-12-09 12:08:50.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999125880491846 Training loss: 4.410842418670654
2025-12-09 12:08:50.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029996503623845393 Training loss: 4.374893665313721
2025-12-09 12:08:50.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.00029992133535682725 Training loss: 4.48524284362793
2025-12-09 12:08:50.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00029986016125334406 Training loss: 4.457827568054199
2025-12-09 12:08:50.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0002997815210578015 Training loss: 4.547983646392822
2025-12-09 12:08:50.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002996854239356567 Training loss: 4.495792388916016
2025-12-09 12:08:50.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0002995718810869589 Training loss: 4.401398658752441
2025-12-09 12:08:50.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00029944090574504395 Training loss: 4.422961711883545
2025-12-09 12:08:50.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0002992925131749921 Training loss: 4.320069789886475
2025-12-09 12:08:50.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0002991267206718486 Training loss: 4.414898872375488
2025-12-09 12:08:50.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029894354755860845 Training loss: 4.271417617797852
2025-12-09 12:08:50.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.00029874301518396376 Training loss: 4.37355899810791
2025-12-09 12:08:50.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.000298525146919816 Training loss: 4.200668811798096
2025-12-09 12:08:50.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0002982899681585518 Training loss: 4.173176288604736
2025-12-09 12:08:50.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00029803750631008356 Training loss: 4.3396525382995605
2025-12-09 12:08:50.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00029776779079865496 Training loss: 4.30697774887085
2025-12-09 12:08:50.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029748085305941123 Training loss: 4.372860908508301
2025-12-09 12:08:50.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0002971767265347358 Training loss: 4.249273777008057
2025-12-09 12:08:50.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002968554466703525 Training loss: 4.370296478271484
2025-12-09 12:08:50.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0002965170509111942 Training loss: 4.164098262786865
2025-12-09 12:08:50.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0002961615786970389 Training loss: 4.397573947906494
2025-12-09 12:08:50.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00029578907145791274 Training loss: 4.252185821533203
2025-12-09 12:08:50.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00029539957260926183 Training loss: 4.119600296020508
2025-12-09 12:08:50.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0002949931275468917 Training loss: 4.27135705947876
2025-12-09 12:08:50.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002945697836416767 Training loss: 4.244292259216309
2025-12-09 12:08:50.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.000294129590234039 Training loss: 4.166945457458496
2025-12-09 12:08:50.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.00029367259862819804 Training loss: 4.172090530395508
2025-12-09 12:08:50.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00029319886208619073 Training loss: 4.199826717376709
2025-12-09 12:08:50.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.00029270843582166427 Training loss: 4.25552225112915
2025-12-09 12:08:50.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029220137699344055 Training loss: 4.27305269241333
2025-12-09 12:08:50.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0002916777446988548 Training loss: 4.225600242614746
2025-12-09 12:08:50.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.00029113759996686743 Training loss: 4.215774059295654
2025-12-09 12:08:50.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0002905810057509515 Training loss: 4.094641208648682
2025-12-09 12:08:50.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.00029000802692175537 Training loss: 3.9840378761291504
2025-12-09 12:08:50.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0002894187302595419 Training loss: 4.292548656463623
2025-12-09 12:08:51.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0002888131844464056 Training loss: 4.1360344886779785
2025-12-09 12:08:51.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002881914600582676 Training loss: 4.09790563583374
2025-12-09 12:08:51.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0002875536295566501 Training loss: 4.144312858581543
2025-12-09 12:08:51.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.000286899767280231 Training loss: 4.105487823486328
2025-12-09 12:08:51.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0002862299494361798 Training loss: 4.224993705749512
2025-12-09 12:08:51.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0002855442540912758 Training loss: 4.113079071044922
2025-12-09 12:08:51.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00028484276116280926 Training loss: 4.202744007110596
2025-12-09 12:08:51.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0002841255524092674 Training loss: 4.092536926269531
2025-12-09 12:08:51.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.00028339271142080534 Training loss: 4.119750499725342
2025-12-09 12:08:51.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.00028264432360950353 Training loss: 4.1223273277282715
2025-12-09 12:08:51.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00028188047619941343 Training loss: 4.111131191253662
2025-12-09 12:08:51.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0002811012582163913 Training loss: 4.151861190795898
2025-12-09 12:08:51.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.00028030676047772265 Training loss: 4.171914100646973
2025-12-09 12:08:51.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.000279497075581537 Training loss: 4.109788417816162
2025-12-09 12:08:51.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002786722978960161 Training loss: 3.8926587104797363
2025-12-09 12:08:51.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0002778325235483954 Training loss: 4.072512149810791
2025-12-09 12:08:51.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00027697785041376006 Training loss: 4.060724258422852
2025-12-09 12:08:51.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0002761083781036381 Training loss: 3.9811208248138428
2025-12-09 12:08:51.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00027522420795439065 Training loss: 4.103396415710449
2025-12-09 12:08:51.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0002743254430154012 Training loss: 3.99587082862854
2025-12-09 12:08:51.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0002734121880370652 Training loss: 4.082398891448975
2025-12-09 12:08:51.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0002724845494585816 Training loss: 3.90592622756958
2025-12-09 12:08:51.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002715426353955476 Training loss: 3.9945170879364014
2025-12-09 12:08:51.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0002705865556273575 Training loss: 4.0307536125183105
2025-12-09 12:08:51.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0002696164215844081 Training loss: 4.108791828155518
2025-12-09 12:08:51.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00026863234633511183 Training loss: 4.1514573097229
2025-12-09 12:08:51.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00026763444457271837 Training loss: 3.827557325363159
2025-12-09 12:08:51.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0002666228326019474 Training loss: 4.049561977386475
2025-12-09 12:08:51.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00026559762832543336 Training loss: 3.9449048042297363
2025-12-09 12:08:51.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.000264558951229984 Training loss: 3.9144070148468018
2025-12-09 12:08:51.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.00026350692237265427 Training loss: 4.044183254241943
2025-12-09 12:08:51.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0002624416643666371 Training loss: 3.9638588428497314
2025-12-09 12:08:51.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.000261363301366973 Training loss: 3.911214828491211
2025-12-09 12:08:51.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00026027195905608006 Training loss: 3.9352335929870605
2025-12-09 12:08:51.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0002591677646291054 Training loss: 4.059569835662842
2025-12-09 12:08:51.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00025805084677910095 Training loss: 4.044376373291016
2025-12-09 12:08:51.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0002569213356820244 Training loss: 4.1158647537231445
2025-12-09 12:08:52.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0002557793629815669 Training loss: 3.953948974609375
2025-12-09 12:08:52.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00025462506177381043 Training loss: 3.845747709274292
2025-12-09 12:08:52.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.00025345856659171563 Training loss: 4.081121921539307
2025-12-09 12:08:52.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00025228001338944175 Training loss: 4.082414627075195
2025-12-09 12:08:52.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0002510895395265016 Training loss: 3.9049293994903564
2025-12-09 12:08:52.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00024988728375175214 Training loss: 3.90812349319458
2025-12-09 12:08:52.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00024867338618722357 Training loss: 4.014162063598633
2025-12-09 12:08:52.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0002474479883117882 Training loss: 3.7672557830810547
2025-12-09 12:08:52.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00024621123294467096 Training loss: 3.9199955463409424
2025-12-09 12:08:52.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.0002449632642288045 Training loss: 3.8491930961608887
2025-12-09 12:08:52.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.00024370422761402867 Training loss: 3.971212148666382
2025-12-09 12:08:52.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0002424342698401391 Training loss: 3.8603758811950684
2025-12-09 12:08:52.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00024115353891978431 Training loss: 3.7093896865844727
2025-12-09 12:08:52.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.00023986218412121537 Training loss: 3.7064015865325928
2025-12-09 12:08:52.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.00023856035595088839 Training loss: 3.8943610191345215
2025-12-09 12:08:52.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00023724820613592337 Training loss: 3.9248154163360596
2025-12-09 12:08:52.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.00023592588760642044 Training loss: 3.9032986164093018
2025-12-09 12:08:52.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00023459355447763596 Training loss: 4.048837661743164
2025-12-09 12:08:52.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.00023325136203202049 Training loss: 3.990647792816162
2025-12-09 12:08:52.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.00023189946670112069 Training loss: 4.046004295349121
2025-12-09 12:08:52.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.00023053802604734757 Training loss: 3.8123559951782227
2025-12-09 12:08:52.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.00022916719874561226 Training loss: 3.8111891746520996
2025-12-09 12:08:52.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0002277871445648332 Training loss: 3.9897654056549072
2025-12-09 12:08:52.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.00022639802434931444 Training loss: 3.973400354385376
2025-12-09 12:08:52.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.000225 Training loss: 3.9529812335968018
2025-12-09 12:08:52.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00022359323445560406 Training loss: 4.0087690353393555
2025-12-09 12:08:52.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.00022217789167362073 Training loss: 3.6140732765197754
2025-12-09 12:08:52.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.00022075413661121492 Training loss: 3.9635775089263916
2025-12-09 12:08:52.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.00021932213520599653 Training loss: 3.7959632873535156
2025-12-09 12:08:52.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00021788205435668083 Training loss: 3.8204243183135986
2025-12-09 12:08:52.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00021643406190363624 Training loss: 3.8109827041625977
2025-12-09 12:08:52.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.00021497832660932295 Training loss: 3.72415828704834
2025-12-09 12:08:52.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.00021351501813862356 Training loss: 3.888836622238159
2025-12-09 12:08:52.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0002120443070390687 Training loss: 3.761592149734497
2025-12-09 12:08:52.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00021056636472096025 Training loss: 3.6854629516601562
2025-12-09 12:08:52.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00020908136343739307 Training loss: 3.975198268890381
2025-12-09 12:08:52.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00020758947626417943 Training loss: 3.82783579826355
2025-12-09 12:08:53.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0002060908770796769 Training loss: 3.731381416320801
2025-12-09 12:08:53.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.00020458574054452313 Training loss: 3.8220677375793457
2025-12-09 12:08:53.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00020307424208127912 Training loss: 3.8405394554138184
2025-12-09 12:08:53.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.00020155655785398393 Training loss: 3.6088006496429443
2025-12-09 12:08:53.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0002000328647476231 Training loss: 3.9814722537994385
2025-12-09 12:08:53.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00019850334034751226 Training loss: 3.696209669113159
2025-12-09 12:08:53.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00019696816291860038 Training loss: 3.7267961502075195
2025-12-09 12:08:53.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0001954275113846926 Training loss: 3.7089579105377197
2025-12-09 12:08:53.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00019388156530759712 Training loss: 3.7187206745147705
2025-12-09 12:08:53.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00019233050486619713 Training loss: 3.8088653087615967
2025-12-09 12:08:53.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0001907745108354514 Training loss: 3.808907985687256
2025-12-09 12:08:53.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.00018921376456532482 Training loss: 3.891052722930908
2025-12-09 12:08:53.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.00018764844795965229 Training loss: 3.9029488563537598
2025-12-09 12:08:53.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.00018607874345493805 Training loss: 3.7766575813293457
2025-12-09 12:08:53.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.00018450483399909263 Training loss: 3.769174814224243
2025-12-09 12:08:53.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.00018292690303011076 Training loss: 3.9796109199523926
2025-12-09 12:08:53.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00018134513445469127 Training loss: 3.7520945072174072
2025-12-09 12:08:53.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00017975971262680347 Training loss: 3.9347832202911377
2025-12-09 12:08:53.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00017817082232620052 Training loss: 3.6493537425994873
2025-12-09 12:08:53.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.00017657864873688343 Training loss: 3.787647247314453
2025-12-09 12:08:53.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.00017498337742551817 Training loss: 3.899038791656494
2025-12-09 12:08:53.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00017338519431980796 Training loss: 3.80031681060791
2025-12-09 12:08:53.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.00017178428568682353 Training loss: 3.8898987770080566
2025-12-09 12:08:53.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0001701808381112938 Training loss: 3.5737345218658447
2025-12-09 12:08:53.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00016857503847385953 Training loss: 3.5952367782592773
2025-12-09 12:08:53.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.00016696707392929266 Training loss: 3.8750696182250977
2025-12-09 12:08:53.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0001653571318846834 Training loss: 3.695652723312378
2025-12-09 12:08:53.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00016374539997759821 Training loss: 3.6901159286499023
2025-12-09 12:08:53.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.00016213206605421063 Training loss: 3.901015043258667
2025-12-09 12:08:53.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0001605173181474081 Training loss: 3.7183151245117188
2025-12-09 12:08:53.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.00015890134445487676 Training loss: 3.5177900791168213
2025-12-09 12:08:53.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.00015728433331716724 Training loss: 3.5418455600738525
2025-12-09 12:08:53.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0001556664731957435 Training loss: 3.7205777168273926
2025-12-09 12:08:53.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00015404795265101806 Training loss: 3.699268102645874
2025-12-09 12:08:53.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.00015242896032037522 Training loss: 3.682844638824463
2025-12-09 12:08:53.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.00015080968489618565 Training loss: 3.7383780479431152
2025-12-09 12:08:53.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.00014919031510381435 Training loss: 3.8082966804504395
2025-12-09 12:08:54.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00014757103967962475 Training loss: 3.746485948562622
2025-12-09 12:08:54.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.00014595204734898197 Training loss: 3.649315357208252
2025-12-09 12:08:54.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0001443335268042565 Training loss: 3.9138193130493164
2025-12-09 12:08:54.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0001427156666828328 Training loss: 3.6570122241973877
2025-12-09 12:08:54.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.00014109865554512319 Training loss: 3.819944381713867
2025-12-09 12:08:54.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00013948268185259188 Training loss: 3.9160566329956055
2025-12-09 12:08:54.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00013786793394578937 Training loss: 3.5253140926361084
2025-12-09 12:08:54.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0001362546000224018 Training loss: 3.7032153606414795
2025-12-09 12:08:54.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00013464286811531661 Training loss: 3.768331527709961
2025-12-09 12:08:54.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.00013303292607070737 Training loss: 3.758953332901001
2025-12-09 12:08:54.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.0001314249615261405 Training loss: 3.8681321144104004
2025-12-09 12:08:54.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0001298191618887062 Training loss: 3.625256299972534
2025-12-09 12:08:54.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00012821571431317647 Training loss: 3.617234230041504
2025-12-09 12:08:54.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00012661480568019201 Training loss: 3.6752395629882812
2025-12-09 12:08:54.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0001250166225744818 Training loss: 3.754167318344116
2025-12-09 12:08:54.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0001234213512631166 Training loss: 3.5835728645324707
2025-12-09 12:08:54.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00012182917767379948 Training loss: 3.742845296859741
2025-12-09 12:08:54.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.00012024028737319652 Training loss: 3.814486265182495
2025-12-09 12:08:54.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.00011865486554530873 Training loss: 3.730945348739624
2025-12-09 12:08:54.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0001170730969698893 Training loss: 3.7172296047210693
2025-12-09 12:08:54.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.00011549516600090737 Training loss: 3.6307852268218994
2025-12-09 12:08:54.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00011392125654506198 Training loss: 3.6311168670654297
2025-12-09 12:08:54.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00011235155204034767 Training loss: 3.873595714569092
2025-12-09 12:08:54.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00011078623543467518 Training loss: 3.6402928829193115
2025-12-09 12:08:54.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00010922548916454855 Training loss: 3.6918139457702637
2025-12-09 12:08:54.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00010766949513380284 Training loss: 3.623041868209839
2025-12-09 12:08:54.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00010611843469240288 Training loss: 3.742626905441284
2025-12-09 12:08:54.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00010457248861530741 Training loss: 3.9189038276672363
2025-12-09 12:08:54.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00010303183708139964 Training loss: 3.5674800872802734
2025-12-09 12:08:54.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00010149665965248775 Training loss: 3.7088403701782227
2025-12-09 12:08:54.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 9.996713525237694e-05 Training loss: 3.6441562175750732
2025-12-09 12:08:54.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 9.8443442146016e-05 Training loss: 3.5040292739868164
2025-12-09 12:08:54.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 9.692575791872089e-05 Training loss: 3.630068302154541
2025-12-09 12:08:54.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 9.541425945547687e-05 Training loss: 3.7317349910736084
2025-12-09 12:08:54.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 9.390912292032309e-05 Training loss: 3.641249418258667
2025-12-09 12:08:54.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 9.241052373582057e-05 Training loss: 3.3998048305511475
2025-12-09 12:08:54.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 9.091863656260695e-05 Training loss: 3.566110849380493
2025-12-09 12:08:55.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 8.943363527903976e-05 Training loss: 3.8047854900360107
2025-12-09 12:08:55.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 8.795569296093132e-05 Training loss: 3.805642604827881
2025-12-09 12:08:55.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 8.648498186137653e-05 Training loss: 3.639798879623413
2025-12-09 12:08:55.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 8.502167339067705e-05 Training loss: 3.893200397491455
2025-12-09 12:08:55.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 8.356593809636371e-05 Training loss: 3.482839822769165
2025-12-09 12:08:55.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 8.211794564331917e-05 Training loss: 3.4869208335876465
2025-12-09 12:08:55.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 8.067786479400346e-05 Training loss: 3.675485610961914
2025-12-09 12:08:55.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 7.924586338878511e-05 Training loss: 3.60922908782959
2025-12-09 12:08:55.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 7.782210832637923e-05 Training loss: 3.569741725921631
2025-12-09 12:08:55.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 7.640676554439594e-05 Training loss: 3.568514585494995
2025-12-09 12:08:55.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 7.500000000000002e-05 Training loss: 3.532212018966675
2025-12-09 12:08:55.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 7.36019756506856e-05 Training loss: 3.58622145652771
2025-12-09 12:08:55.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 7.22128554351668e-05 Training loss: 3.6561355590820312
2025-12-09 12:08:55.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 7.083280125438766e-05 Training loss: 3.4425506591796875
2025-12-09 12:08:55.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 6.946197395265242e-05 Training loss: 3.794851064682007
2025-12-09 12:08:55.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 6.810053329887928e-05 Training loss: 3.5727813243865967
2025-12-09 12:08:55.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 6.674863796797953e-05 Training loss: 3.554321050643921
2025-12-09 12:08:55.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 6.540644552236401e-05 Training loss: 3.9078729152679443
2025-12-09 12:08:55.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 6.407411239357953e-05 Training loss: 3.7345848083496094
2025-12-09 12:08:55.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 6.275179386407663e-05 Training loss: 3.5674147605895996
2025-12-09 12:08:55.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 6.143964404911164e-05 Training loss: 3.461974859237671
2025-12-09 12:08:55.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 6.013781587878463e-05 Training loss: 3.751589298248291
2025-12-09 12:08:55.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 5.8846461080215626e-05 Training loss: 3.511281967163086
2025-12-09 12:08:55.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 5.756573015986089e-05 Training loss: 3.40588641166687
2025-12-09 12:08:55.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 5.629577238597132e-05 Training loss: 3.9171407222747803
2025-12-09 12:08:55.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 5.503673577119552e-05 Training loss: 3.4887213706970215
2025-12-09 12:08:55.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 5.378876705532904e-05 Training loss: 3.6512022018432617
2025-12-09 12:08:55.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 5.2552011688211835e-05 Training loss: 3.489225387573242
2025-12-09 12:08:55.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 5.1326613812776434e-05 Training loss: 3.618326187133789
2025-12-09 12:08:55.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 5.011271624824786e-05 Training loss: 3.672733783721924
2025-12-09 12:08:55.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 4.891046047349837e-05 Training loss: 3.7401742935180664
2025-12-09 12:08:55.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 4.7719986610558234e-05 Training loss: 3.768200397491455
2025-12-09 12:08:55.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 4.654143340828435e-05 Training loss: 3.5211539268493652
2025-12-09 12:08:55.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 4.537493822618958e-05 Training loss: 3.4208450317382812
2025-12-09 12:08:55.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 4.422063701843316e-05 Training loss: 3.6275546550750732
2025-12-09 12:08:55.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 4.3078664317975646e-05 Training loss: 3.6325271129608154
2025-12-09 12:08:55.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 4.194915322089898e-05 Training loss: 3.4687957763671875
2025-12-09 12:08:56.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 4.08322353708946e-05 Training loss: 3.689121723175049
2025-12-09 12:08:56.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 3.972804094391998e-05 Training loss: 3.5066654682159424
2025-12-09 12:08:56.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 3.863669863302697e-05 Training loss: 3.7087836265563965
2025-12-09 12:08:56.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 3.755833563336293e-05 Training loss: 3.522789478302002
2025-12-09 12:08:56.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 3.64930776273457e-05 Training loss: 3.6501455307006836
2025-12-09 12:08:56.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 3.5441048770015954e-05 Training loss: 3.798476457595825
2025-12-09 12:08:56.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 3.4402371674566626e-05 Training loss: 3.3202273845672607
2025-12-09 12:08:56.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 3.3377167398052636e-05 Training loss: 3.5103158950805664
2025-12-09 12:08:56.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 3.2365555427281634e-05 Training loss: 3.5280139446258545
2025-12-09 12:08:56.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 3.136765366488817e-05 Training loss: 3.6583926677703857
2025-12-09 12:08:56.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 3.038357841559191e-05 Training loss: 3.506805419921875
2025-12-09 12:08:56.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 2.941344437264249e-05 Training loss: 3.482381820678711
2025-12-09 12:08:56.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 2.8457364604452372e-05 Training loss: 3.457500696182251
2025-12-09 12:08:56.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 2.7515450541418338e-05 Training loss: 3.535734176635742
2025-12-09 12:08:56.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 2.658781196293482e-05 Training loss: 3.4851410388946533
2025-12-09 12:08:56.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 2.5674556984598822e-05 Training loss: 3.601741075515747
2025-12-09 12:08:56.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 2.477579204560935e-05 Training loss: 3.527940273284912
2025-12-09 12:08:56.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 2.389162189636188e-05 Training loss: 3.5864715576171875
2025-12-09 12:08:56.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 2.3022149586239968e-05 Training loss: 3.692070722579956
2025-12-09 12:08:56.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 2.216747645160462e-05 Training loss: 3.6640803813934326
2025-12-09 12:08:56.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 2.1327702103983863e-05 Training loss: 3.859204053878784
2025-12-09 12:08:56.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 2.0502924418463013e-05 Training loss: 3.6007239818573
2025-12-09 12:08:56.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 1.9693239522277327e-05 Training loss: 3.6297383308410645
2025-12-09 12:08:56.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 1.889874178360864e-05 Training loss: 3.3037006855010986
2025-12-09 12:08:56.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 1.8119523800586568e-05 Training loss: 3.8388195037841797
2025-12-09 12:08:56.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 1.735567639049648e-05 Training loss: 3.5700466632843018
2025-12-09 12:08:56.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 1.6607288579194638e-05 Training loss: 3.4702413082122803
2025-12-09 12:08:56.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 1.5874447590732538e-05 Training loss: 3.843100070953369
2025-12-09 12:08:56.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 1.5157238837190716e-05 Training loss: 3.501955986022949
2025-12-09 12:08:56.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 1.4455745908724226e-05 Training loss: 3.671949863433838
2025-12-09 12:08:56.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 1.3770050563820179e-05 Training loss: 3.830010414123535
2025-12-09 12:08:56.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 1.3100232719768994e-05 Training loss: 3.7768912315368652
2025-12-09 12:08:56.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 1.2446370443349863e-05 Training loss: 3.6625545024871826
2025-12-09 12:08:56.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 1.180853994173236e-05 Training loss: 3.7305922508239746
2025-12-09 12:08:56.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 1.118681555359438e-05 Training loss: 3.6586709022521973
2025-12-09 12:08:56.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 1.058126974045811e-05 Training loss: 3.595339775085449
2025-12-09 12:08:56.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.991973078244636e-06 Training loss: 3.5002596378326416
2025-12-09 12:08:56.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.418994249048472e-06 Training loss: 3.6982245445251465
2025-12-09 12:08:57.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 8.862400033132571e-06 Training loss: 3.5952794551849365
2025-12-09 12:08:57.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 8.322255301145204e-06 Training loss: 3.5695807933807373
2025-12-09 12:08:57.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 7.798623006559435e-06 Training loss: 3.590669870376587
2025-12-09 12:08:57.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 7.291564178335718e-06 Training loss: 3.748894453048706
2025-12-09 12:08:57.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 6.801137913809213e-06 Training loss: 3.5678298473358154
2025-12-09 12:08:57.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 6.3274013718019434e-06 Training loss: 3.595111846923828
2025-12-09 12:08:57.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 5.870409765960965e-06 Training loss: 3.6671903133392334
2025-12-09 12:08:57.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 5.430216358323309e-06 Training loss: 3.5021586418151855
2025-12-09 12:08:57.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 5.006872453108329e-06 Training loss: 3.532336473464966
2025-12-09 12:08:57.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 4.600427390738159e-06 Training loss: 3.746706485748291
2025-12-09 12:08:57.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 4.210928542087206e-06 Training loss: 3.6473031044006348
2025-12-09 12:08:57.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 3.838421302961098e-06 Training loss: 3.4567134380340576
2025-12-09 12:08:57.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 3.482949088805742e-06 Training loss: 3.5263922214508057
2025-12-09 12:08:57.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 3.1445533296474478e-06 Training loss: 3.511031150817871
2025-12-09 12:08:57.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 2.823273465264142e-06 Training loss: 3.5679311752319336
2025-12-09 12:08:57.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 2.519146940588762e-06 Training loss: 3.5966646671295166
2025-12-09 12:08:57.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 2.232209201345031e-06 Training loss: 3.651763677597046
2025-12-09 12:08:57.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 1.9624936899163945e-06 Training loss: 3.781480312347412
2025-12-09 12:08:57.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 1.7100318414482061e-06 Training loss: 3.6232712268829346
2025-12-09 12:08:57.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 1.4748530801840074e-06 Training loss: 3.6608541011810303
2025-12-09 12:08:57.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 1.2569848160362384e-06 Training loss: 3.716181516647339
2025-12-09 12:08:57.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 1.056452441391542e-06 Training loss: 3.655507802963257
2025-12-09 12:08:57.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 8.732793281513661e-07 Training loss: 3.4817299842834473
2025-12-09 12:08:57.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 7.07486825007908e-07 Training loss: 3.6791086196899414
2025-12-09 12:08:57.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 5.590942549560051e-07 Training loss: 3.6734519004821777
2025-12-09 12:08:57.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 4.281189130410534e-07 Training loss: 3.8282318115234375
2025-12-09 12:08:57.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 3.1457606434325266e-07 Training loss: 3.701686382293701
2025-12-09 12:08:57.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 2.184789421984634e-07 Training loss: 3.6180248260498047
2025-12-09 12:08:57.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 1.3983874665589035e-07 Training loss: 3.6644790172576904
2025-12-09 12:08:57.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 7.866464317276e-08 Training loss: 3.6003451347351074
2025-12-09 12:08:57.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 3.4963761546041855e-08 Training loss: 3.809035062789917
2025-12-09 12:08:57.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 8.741195081479747e-09 Training loss: 3.630913257598877
2025-12-09 12:08:57.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 3.8141753673553467
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:09:11.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 4.867964267730713
2025-12-09 12:09:11.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 4.752755165100098
2025-12-09 12:09:11.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 4.8450493812561035
2025-12-09 12:09:11.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 4.874756336212158
2025-12-09 12:09:11.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 4.901021957397461
2025-12-09 12:09:11.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 4.947654724121094
2025-12-09 12:09:11.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 4.793710231781006
2025-12-09 12:09:11.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 4.876060485839844
2025-12-09 12:09:11.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 4.891377925872803
2025-12-09 12:09:11.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 4.8655314445495605
2025-12-09 12:09:11.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 4.778134346008301
2025-12-09 12:09:11.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 4.885640621185303
2025-12-09 12:09:11.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 4.768617630004883
2025-12-09 12:09:11.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 4.827330112457275
2025-12-09 12:09:11.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 4.791254043579102
2025-12-09 12:09:11.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 4.897307395935059
2025-12-09 12:09:11.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 4.930380821228027
2025-12-09 12:09:11.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 4.799029350280762
2025-12-09 12:09:11.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 4.868752479553223
2025-12-09 12:09:11.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 4.888643741607666
2025-12-09 12:09:11.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 4.837767124176025
2025-12-09 12:09:11.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 4.970856189727783
2025-12-09 12:09:11.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 4.678867816925049
2025-12-09 12:09:11.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 4.749395847320557
2025-12-09 12:09:12.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 4.803657054901123
2025-12-09 12:09:12.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 4.902401924133301
2025-12-09 12:09:12.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 4.652874946594238
2025-12-09 12:09:12.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 4.672297477722168
2025-12-09 12:09:12.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 4.850939750671387
2025-12-09 12:09:12.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 4.803436756134033
2025-12-09 12:09:12.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 4.736425399780273
2025-12-09 12:09:12.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 4.7653303146362305
2025-12-09 12:09:12.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 4.61149263381958
2025-12-09 12:09:12.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 4.818719387054443
2025-12-09 12:09:12.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 4.726781845092773
2025-12-09 12:09:12.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 4.800220966339111
2025-12-09 12:09:12.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 4.736105918884277
2025-12-09 12:09:12.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 4.626689434051514
2025-12-09 12:09:12.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 4.7491455078125
2025-12-09 12:09:12.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 4.700191020965576
2025-12-09 12:09:12.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 4.736059188842773
2025-12-09 12:09:12.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 4.604253768920898
2025-12-09 12:09:12.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 4.618766784667969
2025-12-09 12:09:12.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 4.60482120513916
2025-12-09 12:09:12.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 4.646894454956055
2025-12-09 12:09:12.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 4.609499931335449
2025-12-09 12:09:12.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 4.574423789978027
2025-12-09 12:09:12.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 4.675196170806885
2025-12-09 12:09:12.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 4.637381553649902
2025-12-09 12:09:12.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 4.676449298858643
2025-12-09 12:09:12.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 4.4951887130737305
2025-12-09 12:09:12.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 4.542452812194824
2025-12-09 12:09:12.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 4.47511100769043
2025-12-09 12:09:12.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 4.495915412902832
2025-12-09 12:09:12.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 4.439041614532471
2025-12-09 12:09:12.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 4.488807201385498
2025-12-09 12:09:12.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 4.340730667114258
2025-12-09 12:09:12.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 4.410270690917969
2025-12-09 12:09:12.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 4.5907368659973145
2025-12-09 12:09:12.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 4.334883213043213
2025-12-09 12:09:12.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 4.4854350090026855
2025-12-09 12:09:12.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 4.444277286529541
2025-12-09 12:09:13.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 4.547708511352539
2025-12-09 12:09:13.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 4.314401626586914
2025-12-09 12:09:13.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 4.402807235717773
2025-12-09 12:09:13.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 4.313190937042236
2025-12-09 12:09:13.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 4.360565662384033
2025-12-09 12:09:13.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 4.2713727951049805
2025-12-09 12:09:13.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 4.226003170013428
2025-12-09 12:09:13.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 4.333756923675537
2025-12-09 12:09:13.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 4.371339797973633
2025-12-09 12:09:13.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 4.237783908843994
2025-12-09 12:09:13.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 4.164427280426025
2025-12-09 12:09:13.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 4.237512588500977
2025-12-09 12:09:13.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 4.228678226470947
2025-12-09 12:09:13.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 4.1300435066223145
2025-12-09 12:09:13.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 4.305498123168945
2025-12-09 12:09:13.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 4.264948844909668
2025-12-09 12:09:13.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 4.150196075439453
2025-12-09 12:09:13.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 4.159801006317139
2025-12-09 12:09:13.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 4.180627346038818
2025-12-09 12:09:13.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 4.136504650115967
2025-12-09 12:09:13.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 4.22734260559082
2025-12-09 12:09:13.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 4.219476222991943
2025-12-09 12:09:13.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 4.114094257354736
2025-12-09 12:09:13.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 4.172778606414795
2025-12-09 12:09:13.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 4.149421691894531
2025-12-09 12:09:13.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 4.070248126983643
2025-12-09 12:09:13.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 4.124423027038574
2025-12-09 12:09:13.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 3.977649688720703
2025-12-09 12:09:13.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 4.269647121429443
2025-12-09 12:09:13.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 3.8925130367279053
2025-12-09 12:09:13.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 4.058652877807617
2025-12-09 12:09:13.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 3.8581485748291016
2025-12-09 12:09:13.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 4.010913848876953
2025-12-09 12:09:13.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 4.0856404304504395
2025-12-09 12:09:13.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 4.011567115783691
2025-12-09 12:09:13.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 3.9758317470550537
2025-12-09 12:09:13.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 3.775001049041748
2025-12-09 12:09:13.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 4.0273213386535645
2025-12-09 12:09:14.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009999708626830617 Training loss: 4.180537223815918
2025-12-09 12:09:14.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0009998834541281797 Training loss: 3.787970781326294
2025-12-09 12:09:14.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0009997377845227576 Training loss: 3.9033966064453125
2025-12-09 12:09:14.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0009995338708444802 Training loss: 3.89699387550354
2025-12-09 12:09:14.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0009992717368593384 Training loss: 4.115595817565918
2025-12-09 12:09:14.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0009989514131188558 Training loss: 3.9052822589874268
2025-12-09 12:09:14.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0009985729369565298 Training loss: 3.989516019821167
2025-12-09 12:09:14.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00099813635248348 Training loss: 3.965050458908081
2025-12-09 12:09:14.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0009976417105833069 Training loss: 3.802431583404541
2025-12-09 12:09:14.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.000997089068906162 Training loss: 3.718979597091675
2025-12-09 12:09:14.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0009964784918620282 Training loss: 3.885645627975464
2025-12-09 12:09:14.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0009958100506132126 Training loss: 3.8712961673736572
2025-12-09 12:09:14.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0009950838230660534 Training loss: 3.807142734527588
2025-12-09 12:09:14.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0009942998938618393 Training loss: 3.8443126678466797
2025-12-09 12:09:14.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0009934583543669453 Training loss: 3.979933738708496
2025-12-09 12:09:14.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0009925593026621834 Training loss: 3.796443223953247
2025-12-09 12:09:14.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.000991602843531371 Training loss: 3.9282989501953125
2025-12-09 12:09:14.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0009905890884491196 Training loss: 3.854079008102417
2025-12-09 12:09:14.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0009895181555678418 Training loss: 3.7231314182281494
2025-12-09 12:09:14.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0009883901697039807 Training loss: 3.866689920425415
2025-12-09 12:09:14.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.000987205262323463 Training loss: 3.820570230484009
2025-12-09 12:09:14.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.000985963571526376 Training loss: 3.6439359188079834
2025-12-09 12:09:14.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0009846652420308728 Training loss: 3.7129693031311035
2025-12-09 12:09:14.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0009833104251563056 Training loss: 3.8171234130859375
2025-12-09 12:09:14.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.000981899278805589 Training loss: 3.6955373287200928
2025-12-09 12:09:14.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0009804319674467969 Training loss: 3.8015472888946533
2025-12-09 12:09:14.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0009789086620939935 Training loss: 3.6446597576141357
2025-12-09 12:09:14.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0009773295402873026 Training loss: 3.5957608222961426
2025-12-09 12:09:14.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0009756947860722143 Training loss: 3.6731133460998535
2025-12-09 12:09:14.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0009740045899781352 Training loss: 3.7929134368896484
2025-12-09 12:09:14.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0009722591489961827 Training loss: 3.3809642791748047
2025-12-09 12:09:14.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0009704586665562249 Training loss: 3.434152603149414
2025-12-09 12:09:14.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0009686033525031719 Training loss: 3.638435125350952
2025-12-09 12:09:14.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0009666934230725179 Training loss: 3.8869941234588623
2025-12-09 12:09:14.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0009647291008651398 Training loss: 3.6348414421081543
2025-12-09 12:09:14.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0009627106148213521 Training loss: 3.551774740219116
2025-12-09 12:09:14.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0009606382001942255 Training loss: 3.788710355758667
2025-12-09 12:09:14.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0009585120985221671 Training loss: 3.8831491470336914
2025-12-09 12:09:15.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0009563325576007701 Training loss: 3.709090232849121
2025-12-09 12:09:15.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0009540998314539327 Training loss: 3.726060152053833
2025-12-09 12:09:15.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0009518141803042527 Training loss: 3.840477466583252
2025-12-09 12:09:15.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0009494758705426977 Training loss: 3.748037815093994
2025-12-09 12:09:15.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0009470851746975581 Training loss: 3.635279655456543
2025-12-09 12:09:15.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0009446423714026846 Training loss: 3.6395668983459473
2025-12-09 12:09:15.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0009421477453650118 Training loss: 3.486353635787964
2025-12-09 12:09:15.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0009396015873313782 Training loss: 3.6370363235473633
2025-12-09 12:09:15.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0009370041940546379 Training loss: 3.4205353260040283
2025-12-09 12:09:15.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0009343558682590756 Training loss: 3.5483145713806152
2025-12-09 12:09:15.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0009316569186051234 Training loss: 3.594210147857666
2025-12-09 12:09:15.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0009289076596533872 Training loss: 3.7317681312561035
2025-12-09 12:09:15.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0009261084118279846 Training loss: 3.5786309242248535
2025-12-09 12:09:15.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0009232595013792003 Training loss: 3.557804822921753
2025-12-09 12:09:15.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0009203612603454604 Training loss: 3.593764066696167
2025-12-09 12:09:15.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0009174140265146356 Training loss: 3.6226603984832764
2025-12-09 12:09:15.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0009144181433846706 Training loss: 3.421926498413086
2025-12-09 12:09:15.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0009113739601235507 Training loss: 3.4176859855651855
2025-12-09 12:09:15.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0009082818315286055 Training loss: 3.6446049213409424
2025-12-09 12:09:15.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0009051421179851588 Training loss: 3.5387372970581055
2025-12-09 12:09:15.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.000901955185424525 Training loss: 3.349642515182495
2025-12-09 12:09:15.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0008987214052813603 Training loss: 3.368021011352539
2025-12-09 12:09:15.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0008954411544503729 Training loss: 3.679715394973755
2025-12-09 12:09:15.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0008921148152423946 Training loss: 3.561262607574463
2025-12-09 12:09:15.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0008887427753398248 Training loss: 3.326637029647827
2025-12-09 12:09:15.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0008853254277514447 Training loss: 3.587017059326172
2025-12-09 12:09:15.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0008818631707666135 Training loss: 3.5592801570892334
2025-12-09 12:09:15.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0008783564079088476 Training loss: 3.488241672515869
2025-12-09 12:09:15.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0008748055478887904 Training loss: 3.7126352787017822
2025-12-09 12:09:15.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0008712110045565768 Training loss: 3.352459192276001
2025-12-09 12:09:15.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0008675731968536002 Training loss: 3.4612462520599365
2025-12-09 12:09:15.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0008638925487636848 Training loss: 3.5426933765411377
2025-12-09 12:09:15.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00086016948926367 Training loss: 3.6005120277404785
2025-12-09 12:09:15.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0008564044522734146 Training loss: 3.3567726612091064
2025-12-09 12:09:15.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.000852597876605223 Training loss: 3.432292938232422
2025-12-09 12:09:15.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0008487502059127015 Training loss: 3.327268362045288
2025-12-09 12:09:15.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0008448618886390522 Training loss: 3.5398447513580322
2025-12-09 12:09:15.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.0008409333779648059 Training loss: 3.359544515609741
2025-12-09 12:09:16.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0008369651317550054 Training loss: 3.697922468185425
2025-12-09 12:09:16.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0008329576125058406 Training loss: 3.7162063121795654
2025-12-09 12:09:16.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0008289112872907454 Training loss: 3.3821260929107666
2025-12-09 12:09:16.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0008248266277059606 Training loss: 3.446566343307495
2025-12-09 12:09:16.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00082070410981557 Training loss: 3.6524410247802734
2025-12-09 12:09:16.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.000816544214096015 Training loss: 3.364715814590454
2025-12-09 12:09:16.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0008123474253800957 Training loss: 3.4935379028320312
2025-12-09 12:09:16.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0008081142328004637 Training loss: 3.512636423110962
2025-12-09 12:09:16.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0008038451297326145 Training loss: 3.313667058944702
2025-12-09 12:09:16.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0007995406137373846 Training loss: 3.563220977783203
2025-12-09 12:09:16.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0007952011865029613 Training loss: 3.3646886348724365
2025-12-09 12:09:16.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0007908273537864113 Training loss: 3.0694727897644043
2025-12-09 12:09:16.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0007864196253547349 Training loss: 3.779280424118042
2025-12-09 12:09:16.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0007819785149254532 Training loss: 3.049384355545044
2025-12-09 12:09:16.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.000777504540106735 Training loss: 3.3079028129577637
2025-12-09 12:09:16.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0007729982223370691 Training loss: 3.210268259048462
2025-12-09 12:09:16.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0007684600868244919 Training loss: 3.4543564319610596
2025-12-09 12:09:16.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0007638906624853743 Training loss: 3.6774373054504395
2025-12-09 12:09:16.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0007592904818827774 Training loss: 3.3189916610717773
2025-12-09 12:09:16.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0007546600811643815 Training loss: 3.3172881603240967
2025-12-09 12:09:16.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.00075 Training loss: 3.4331533908843994
2025-12-09 12:09:16.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0007453107815186803 Training loss: 3.4237582683563232
2025-12-09 12:09:16.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0007405929722454026 Training loss: 3.3491461277008057
2025-12-09 12:09:16.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0007358471220373831 Training loss: 3.3383586406707764
2025-12-09 12:09:16.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0007310737840199885 Training loss: 3.5037946701049805
2025-12-09 12:09:16.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0007262735145222696 Training loss: 3.2202351093292236
2025-12-09 12:09:16.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0007214468730121209 Training loss: 3.2543163299560547
2025-12-09 12:09:16.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0007165944220310766 Training loss: 3.260937213897705
2025-12-09 12:09:16.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0007117167271287453 Training loss: 3.3913707733154297
2025-12-09 12:09:16.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0007068143567968958 Training loss: 3.774512767791748
2025-12-09 12:09:16.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0007018878824032009 Training loss: 3.643768072128296
2025-12-09 12:09:16.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.0006969378781246436 Training loss: 3.0878400802612305
2025-12-09 12:09:16.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0006919649208805981 Training loss: 3.3893356323242188
2025-12-09 12:09:16.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0006869695902655897 Training loss: 3.559068202972412
2025-12-09 12:09:16.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0006819524684817438 Training loss: 3.668868064880371
2025-12-09 12:09:16.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0006769141402709304 Training loss: 3.2111477851867676
2025-12-09 12:09:16.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0006718551928466132 Training loss: 3.589773178100586
2025-12-09 12:09:16.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0006667762158254104 Training loss: 3.327845573425293
2025-12-09 12:09:17.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.0006616778011583743 Training loss: 3.1906793117523193
2025-12-09 12:09:17.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0006565605430620013 Training loss: 3.4275479316711426
2025-12-09 12:09:17.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0006514250379489753 Training loss: 3.2560572624206543
2025-12-09 12:09:17.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0006462718843586572 Training loss: 3.3565499782562256
2025-12-09 12:09:17.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0006411016828873239 Training loss: 3.3947300910949707
2025-12-09 12:09:17.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0006359150361181715 Training loss: 3.4092400074005127
2025-12-09 12:09:17.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0006307125485510829 Training loss: 3.3818376064300537
2025-12-09 12:09:17.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0006254948265321744 Training loss: 3.2490251064300537
2025-12-09 12:09:17.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0006202624781831269 Training loss: 3.2614152431488037
2025-12-09 12:09:17.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0006150161133303088 Training loss: 3.339339017868042
2025-12-09 12:09:17.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0006097563434337025 Training loss: 3.4167776107788086
2025-12-09 12:09:17.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0006044837815156376 Training loss: 3.392859697341919
2025-12-09 12:09:17.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0005991990420893449 Training loss: 3.251948356628418
2025-12-09 12:09:17.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0005939027410873352 Training loss: 3.304143190383911
2025-12-09 12:09:17.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0005885954957896114 Training loss: 3.42814040184021
2025-12-09 12:09:17.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0005832779247517272 Training loss: 3.179335117340088
2025-12-09 12:09:17.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0005779506477326933 Training loss: 3.226577043533325
2025-12-09 12:09:17.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0005726142856227452 Training loss: 3.1027841567993164
2025-12-09 12:09:17.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0005672694603709794 Training loss: 3.1106293201446533
2025-12-09 12:09:17.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0005619167949128652 Training loss: 3.312873363494873
2025-12-09 12:09:17.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0005565569130976422 Training loss: 3.3613219261169434
2025-12-09 12:09:17.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0005511904396156113 Training loss: 3.112075090408325
2025-12-09 12:09:17.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0005458179999253274 Training loss: 3.2250542640686035
2025-12-09 12:09:17.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0005404402201807021 Training loss: 3.215672731399536
2025-12-09 12:09:17.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0005350577271580271 Training loss: 3.3876941204071045
2025-12-09 12:09:17.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0005296711481829226 Training loss: 3.1856329441070557
2025-12-09 12:09:17.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0005242811110572242 Training loss: 3.3778741359710693
2025-12-09 12:09:17.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0005188882439858117 Training loss: 3.3060076236724854
2025-12-09 12:09:17.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0005134931755033936 Training loss: 3.210923671722412
2025-12-09 12:09:17.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0005080965344012508 Training loss: 3.31127667427063
2025-12-09 12:09:17.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0005026989496539523 Training loss: 3.325978994369507
2025-12-09 12:09:17.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0004973010503460479 Training loss: 3.1976287364959717
2025-12-09 12:09:17.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0004919034655987492 Training loss: 3.406550645828247
2025-12-09 12:09:17.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0004865068244966066 Training loss: 3.2292537689208984
2025-12-09 12:09:17.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0004811117560141884 Training loss: 3.3332509994506836
2025-12-09 12:09:17.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.000475718888942776 Training loss: 3.4058451652526855
2025-12-09 12:09:17.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0004703288518170774 Training loss: 3.2924201488494873
2025-12-09 12:09:17.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00046494227284197295 Training loss: 2.954603433609009
2025-12-09 12:09:18.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00045955977981929796 Training loss: 3.3928146362304688
2025-12-09 12:09:18.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0004541820000746727 Training loss: 3.3866190910339355
2025-12-09 12:09:18.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00044880956038438873 Training loss: 3.2957961559295654
2025-12-09 12:09:18.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0004434430869023579 Training loss: 3.234299898147583
2025-12-09 12:09:18.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.000438083205087135 Training loss: 3.335815906524658
2025-12-09 12:09:18.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.00043273053962902076 Training loss: 3.1053507328033447
2025-12-09 12:09:18.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.000427385714377255 Training loss: 3.3851606845855713
2025-12-09 12:09:18.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0004220493522673067 Training loss: 2.9796383380889893
2025-12-09 12:09:18.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0004167220752482728 Training loss: 2.942444324493408
2025-12-09 12:09:18.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.00041140450421038864 Training loss: 3.5223910808563232
2025-12-09 12:09:18.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.000406097258912665 Training loss: 3.3376994132995605
2025-12-09 12:09:18.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0004008009579106551 Training loss: 3.1782853603363037
2025-12-09 12:09:18.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0003955162184843625 Training loss: 3.122279405593872
2025-12-09 12:09:18.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.00039024365656629766 Training loss: 3.0369763374328613
2025-12-09 12:09:18.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0003849838866696913 Training loss: 3.155331611633301
2025-12-09 12:09:18.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00037973752181687335 Training loss: 3.4066600799560547
2025-12-09 12:09:18.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00037450517346782563 Training loss: 3.288241386413574
2025-12-09 12:09:18.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0003692874514489173 Training loss: 3.0149359703063965
2025-12-09 12:09:18.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00036408496388182855 Training loss: 3.3236467838287354
2025-12-09 12:09:18.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0003588983171126762 Training loss: 3.1830170154571533
2025-12-09 12:09:18.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.000353728115641343 Training loss: 3.1620383262634277
2025-12-09 12:09:18.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0003485749620510247 Training loss: 3.1074817180633545
2025-12-09 12:09:18.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00034343945693799885 Training loss: 3.076838731765747
2025-12-09 12:09:18.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00033832219884162584 Training loss: 3.3078041076660156
2025-12-09 12:09:18.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0003332237841745898 Training loss: 3.1956605911254883
2025-12-09 12:09:18.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00032814480715338666 Training loss: 3.0986809730529785
2025-12-09 12:09:18.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0003230858597290697 Training loss: 3.1220059394836426
2025-12-09 12:09:18.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0003180475315182563 Training loss: 3.090812921524048
2025-12-09 12:09:18.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0003130304097344103 Training loss: 3.081214189529419
2025-12-09 12:09:18.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0003080350791194019 Training loss: 2.9027063846588135
2025-12-09 12:09:18.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.00030306212187535653 Training loss: 2.985888957977295
2025-12-09 12:09:18.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002981121175967992 Training loss: 3.2362136840820312
2025-12-09 12:09:18.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.00029318564320310444 Training loss: 3.2363686561584473
2025-12-09 12:09:18.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0002882832728712551 Training loss: 3.2665421962738037
2025-12-09 12:09:18.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0002834055779689235 Training loss: 3.1541523933410645
2025-12-09 12:09:18.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.00027855312698787905 Training loss: 2.9155044555664062
2025-12-09 12:09:18.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0002737264854777306 Training loss: 3.240668296813965
2025-12-09 12:09:19.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.00026892621598001155 Training loss: 3.5081472396850586
2025-12-09 12:09:19.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0002641528779626171 Training loss: 3.1982667446136475
2025-12-09 12:09:19.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.00025940702775459747 Training loss: 2.840261220932007
2025-12-09 12:09:19.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.00025468921848131984 Training loss: 2.9993646144866943
2025-12-09 12:09:19.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0002500000000000001 Training loss: 3.1183297634124756
2025-12-09 12:09:19.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.00024533991883561866 Training loss: 3.2474708557128906
2025-12-09 12:09:19.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.00024070951811722268 Training loss: 3.288486957550049
2025-12-09 12:09:19.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00023610933751462554 Training loss: 3.0099332332611084
2025-12-09 12:09:19.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0002315399131755081 Training loss: 2.972219944000244
2025-12-09 12:09:19.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00022700177766293096 Training loss: 3.353273391723633
2025-12-09 12:09:19.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.00022249545989326514 Training loss: 3.380249261856079
2025-12-09 12:09:19.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002180214850745467 Training loss: 3.0167882442474365
2025-12-09 12:09:19.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.00021358037464526514 Training loss: 3.0888328552246094
2025-12-09 12:09:19.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.00020917264621358878 Training loss: 3.207172393798828
2025-12-09 12:09:19.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.00020479881349703882 Training loss: 3.397608518600464
2025-12-09 12:09:19.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.00020045938626261545 Training loss: 3.1436336040496826
2025-12-09 12:09:19.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.00019615487026738542 Training loss: 3.043191432952881
2025-12-09 12:09:19.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.00019188576719953633 Training loss: 2.864285945892334
2025-12-09 12:09:19.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.00018765257461990443 Training loss: 3.0910744667053223
2025-12-09 12:09:19.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0001834557859039851 Training loss: 2.942129135131836
2025-12-09 12:09:19.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.00017929589018443015 Training loss: 3.0002989768981934
2025-12-09 12:09:19.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.00017517337229403947 Training loss: 3.127915382385254
2025-12-09 12:09:19.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0001710887127092548 Training loss: 3.4705264568328857
2025-12-09 12:09:19.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.00016704238749415957 Training loss: 3.08196759223938
2025-12-09 12:09:19.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0001630348682449946 Training loss: 3.0320169925689697
2025-12-09 12:09:19.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00015906662203519413 Training loss: 3.2321743965148926
2025-12-09 12:09:19.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00015513811136094787 Training loss: 3.196728467941284
2025-12-09 12:09:19.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.0001512497940872986 Training loss: 3.046377420425415
2025-12-09 12:09:19.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0001474021233947772 Training loss: 3.0347023010253906
2025-12-09 12:09:19.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00014359554772658552 Training loss: 3.114652633666992
2025-12-09 12:09:19.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.00013983051073632996 Training loss: 3.070955514907837
2025-12-09 12:09:19.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00013610745123631535 Training loss: 3.055513381958008
2025-12-09 12:09:19.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00013242680314639994 Training loss: 3.1437549591064453
2025-12-09 12:09:19.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00012878899544342326 Training loss: 3.2042665481567383
2025-12-09 12:09:19.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00012519445211120977 Training loss: 3.1323628425598145
2025-12-09 12:09:19.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00012164359209115234 Training loss: 3.010483741760254
2025-12-09 12:09:19.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00011813682923338653 Training loss: 3.22316837310791
2025-12-09 12:09:19.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.00011467457224855543 Training loss: 3.1557717323303223
2025-12-09 12:09:20.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.00011125722466017545 Training loss: 3.1257622241973877
2025-12-09 12:09:20.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00010788518475760544 Training loss: 2.9180965423583984
2025-12-09 12:09:20.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00010455884554962725 Training loss: 3.282376527786255
2025-12-09 12:09:20.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0001012785947186397 Training loss: 3.137205123901367
2025-12-09 12:09:20.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.804481457547498e-05 Training loss: 3.153491258621216
2025-12-09 12:09:20.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.485788201484124e-05 Training loss: 3.3390543460845947
2025-12-09 12:09:20.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.171816847139447e-05 Training loss: 3.071697950363159
2025-12-09 12:09:20.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 8.862603987644941e-05 Training loss: 3.0913050174713135
2025-12-09 12:09:20.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 8.558185661532942e-05 Training loss: 3.1263949871063232
2025-12-09 12:09:20.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 8.258597348536451e-05 Training loss: 3.1573565006256104
2025-12-09 12:09:20.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 7.96387396545396e-05 Training loss: 3.186473846435547
2025-12-09 12:09:20.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 7.674049862079991e-05 Training loss: 3.0633368492126465
2025-12-09 12:09:20.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 7.38915881720154e-05 Training loss: 3.1564888954162598
2025-12-09 12:09:20.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 7.109234034661289e-05 Training loss: 3.165513515472412
2025-12-09 12:09:20.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 6.834308139487671e-05 Training loss: 3.0134124755859375
2025-12-09 12:09:20.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 6.564413174092443e-05 Training loss: 3.1487486362457275
2025-12-09 12:09:20.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 6.299580594536214e-05 Training loss: 3.1813035011291504
2025-12-09 12:09:20.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 6.0398412668621897e-05 Training loss: 3.18255877494812
2025-12-09 12:09:20.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 5.785225463498828e-05 Training loss: 3.085847854614258
2025-12-09 12:09:20.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 5.535762859731547e-05 Training loss: 2.901620626449585
2025-12-09 12:09:20.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 5.291482530244179e-05 Training loss: 3.255058765411377
2025-12-09 12:09:20.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 5.0524129457302394e-05 Training loss: 2.8630542755126953
2025-12-09 12:09:20.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 4.818581969574742e-05 Training loss: 2.9862208366394043
2025-12-09 12:09:20.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 4.5900168546067264e-05 Training loss: 3.1815543174743652
2025-12-09 12:09:20.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 4.366744239922998e-05 Training loss: 3.1705729961395264
2025-12-09 12:09:20.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 4.148790147783288e-05 Training loss: 3.065471887588501
2025-12-09 12:09:20.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 3.936179980577453e-05 Training loss: 2.9494898319244385
2025-12-09 12:09:20.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 3.728938517864794e-05 Training loss: 2.9353296756744385
2025-12-09 12:09:20.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 3.527089913486037e-05 Training loss: 3.0202202796936035
2025-12-09 12:09:20.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 3.330657692748212e-05 Training loss: 3.011760950088501
2025-12-09 12:09:20.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 3.1396647496828245e-05 Training loss: 2.9748878479003906
2025-12-09 12:09:20.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 2.9541333443775243e-05 Training loss: 3.024543046951294
2025-12-09 12:09:20.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 2.7740851003817347e-05 Training loss: 3.062976598739624
2025-12-09 12:09:20.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 2.5995410021864786e-05 Training loss: 3.212524652481079
2025-12-09 12:09:20.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 2.430521392778573e-05 Training loss: 2.9893665313720703
2025-12-09 12:09:20.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 2.2670459712697378e-05 Training loss: 3.1297597885131836
2025-12-09 12:09:20.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 2.109133790600648e-05 Training loss: 3.0053257942199707
2025-12-09 12:09:20.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 1.956803255320322e-05 Training loss: 2.936614513397217
2025-12-09 12:09:21.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 1.810072119441103e-05 Training loss: 3.2976956367492676
2025-12-09 12:09:21.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 1.6689574843694434e-05 Training loss: 2.990933418273926
2025-12-09 12:09:21.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 1.53347579691272e-05 Training loss: 3.1919407844543457
2025-12-09 12:09:21.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 1.4036428473624019e-05 Training loss: 2.8590407371520996
2025-12-09 12:09:21.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 1.2794737676536993e-05 Training loss: 2.9946448802948
2025-12-09 12:09:21.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 1.1609830296019142e-05 Training loss: 3.0953004360198975
2025-12-09 12:09:21.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 1.048184443215816e-05 Training loss: 3.100318431854248
2025-12-09 12:09:21.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880474e-06 Training loss: 3.006666898727417
2025-12-09 12:09:21.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629207e-06 Training loss: 3.1773955821990967
2025-12-09 12:09:21.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.440697337816771e-06 Training loss: 3.008298873901367
2025-12-09 12:09:21.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.541645633054649e-06 Training loss: 3.1008260250091553
2025-12-09 12:09:21.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.700106138160688e-06 Training loss: 3.2917561531066895
2025-12-09 12:09:21.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946692e-06 Training loss: 3.083470582962036
2025-12-09 12:09:21.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.189949386787462e-06 Training loss: 2.9920847415924072
2025-12-09 12:09:21.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.521508137971807e-06 Training loss: 2.914149284362793
2025-12-09 12:09:21.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378875e-06 Training loss: 3.1162850856781006
2025-12-09 12:09:21.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.3582894166930268e-06 Training loss: 3.0260279178619385
2025-12-09 12:09:21.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.8636475165200173e-06 Training loss: 2.8754515647888184
2025-12-09 12:09:21.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.4270630434701782e-06 Training loss: 3.2164785861968994
2025-12-09 12:09:21.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441758e-06 Training loss: 3.0323026180267334
2025-12-09 12:09:21.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.282631406615448e-07 Training loss: 2.944396495819092
2025-12-09 12:09:21.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.6612915551963455e-07 Training loss: 3.0973634719848633
2025-12-09 12:09:21.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253336e-07 Training loss: 3.1554181575775146
2025-12-09 12:09:21.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-07 Training loss: 3.2025370597839355
2025-12-09 12:09:21.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265823e-08 Training loss: 2.9282970428466797
2025-12-09 12:09:21.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 2.993347644805908
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:09:35.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 4.923840522766113
2025-12-09 12:09:35.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 4.878653049468994
2025-12-09 12:09:35.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 4.7015910148620605
2025-12-09 12:09:35.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 4.927030563354492
2025-12-09 12:09:35.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 4.923947334289551
2025-12-09 12:09:35.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 4.854702949523926
2025-12-09 12:09:35.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 4.95744514465332
2025-12-09 12:09:35.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 4.833139419555664
2025-12-09 12:09:35.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 4.952014923095703
2025-12-09 12:09:35.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 4.889257907867432
2025-12-09 12:09:35.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 4.946124076843262
2025-12-09 12:09:35.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 4.855247497558594
2025-12-09 12:09:35.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 4.914652347564697
2025-12-09 12:09:35.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 4.802492141723633
2025-12-09 12:09:35.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 4.796213626861572
2025-12-09 12:09:35.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 4.863239288330078
2025-12-09 12:09:35.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 4.81233549118042
2025-12-09 12:09:35.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 4.912334442138672
2025-12-09 12:09:35.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 4.825279712677002
2025-12-09 12:09:35.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 4.568806171417236
2025-12-09 12:09:35.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 4.9300150871276855
2025-12-09 12:09:35.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 4.780421257019043
2025-12-09 12:09:35.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 4.755265712738037
2025-12-09 12:09:35.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 4.698812961578369
2025-12-09 12:09:35.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 4.6635003089904785
2025-12-09 12:09:35.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 4.659378528594971
2025-12-09 12:09:35.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 4.577142715454102
2025-12-09 12:09:35.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 4.613121509552002
2025-12-09 12:09:35.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 4.575951099395752
2025-12-09 12:09:35.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 4.554028511047363
2025-12-09 12:09:35.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 4.5075764656066895
2025-12-09 12:09:35.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 4.574678897857666
2025-12-09 12:09:36.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 4.42556619644165
2025-12-09 12:09:36.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 4.521207809448242
2025-12-09 12:09:36.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 4.466638088226318
2025-12-09 12:09:36.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 4.458279132843018
2025-12-09 12:09:36.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 4.35275936126709
2025-12-09 12:09:36.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 4.492370128631592
2025-12-09 12:09:36.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 4.332821369171143
2025-12-09 12:09:36.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 4.394385814666748
2025-12-09 12:09:36.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 4.366175174713135
2025-12-09 12:09:36.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 4.370734214782715
2025-12-09 12:09:36.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 4.3082709312438965
2025-12-09 12:09:36.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 4.108285903930664
2025-12-09 12:09:36.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 4.264075756072998
2025-12-09 12:09:36.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 4.266147136688232
2025-12-09 12:09:36.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 4.236865997314453
2025-12-09 12:09:36.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 4.240080833435059
2025-12-09 12:09:36.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 4.2355451583862305
2025-12-09 12:09:36.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 4.126309871673584
2025-12-09 12:09:36.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 4.146917819976807
2025-12-09 12:09:36.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 4.329170227050781
2025-12-09 12:09:36.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 4.186344623565674
2025-12-09 12:09:36.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 4.135413646697998
2025-12-09 12:09:36.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 4.23691463470459
2025-12-09 12:09:36.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 4.124946594238281
2025-12-09 12:09:36.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 3.950423240661621
2025-12-09 12:09:36.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 4.20054292678833
2025-12-09 12:09:36.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 3.9687798023223877
2025-12-09 12:09:36.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 4.117016792297363
2025-12-09 12:09:36.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 3.996483564376831
2025-12-09 12:09:36.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 4.181003570556641
2025-12-09 12:09:36.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 4.0851240158081055
2025-12-09 12:09:36.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 4.0237226486206055
2025-12-09 12:09:36.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 3.953470230102539
2025-12-09 12:09:36.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 4.149335861206055
2025-12-09 12:09:36.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 4.200077056884766
2025-12-09 12:09:36.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 3.975933313369751
2025-12-09 12:09:36.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 3.794203996658325
2025-12-09 12:09:36.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 3.926321268081665
2025-12-09 12:09:36.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 3.693568706512451
2025-12-09 12:09:37.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 3.9956607818603516
2025-12-09 12:09:37.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 4.07109260559082
2025-12-09 12:09:37.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 3.886039972305298
2025-12-09 12:09:37.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 3.9603614807128906
2025-12-09 12:09:37.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 4.161355495452881
2025-12-09 12:09:37.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 3.8795723915100098
2025-12-09 12:09:37.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 3.90464448928833
2025-12-09 12:09:37.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 3.9734816551208496
2025-12-09 12:09:37.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 3.7363264560699463
2025-12-09 12:09:37.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 3.871445894241333
2025-12-09 12:09:37.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 3.7236404418945312
2025-12-09 12:09:37.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 3.727752208709717
2025-12-09 12:09:37.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 3.959074020385742
2025-12-09 12:09:37.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 3.5647802352905273
2025-12-09 12:09:37.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 3.711250066757202
2025-12-09 12:09:37.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 3.9737517833709717
2025-12-09 12:09:37.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 3.4828789234161377
2025-12-09 12:09:37.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 3.649932622909546
2025-12-09 12:09:37.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 3.865278959274292
2025-12-09 12:09:37.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 3.5720324516296387
2025-12-09 12:09:37.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 3.845717430114746
2025-12-09 12:09:37.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 3.5411970615386963
2025-12-09 12:09:37.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 3.540647506713867
2025-12-09 12:09:37.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 3.8607163429260254
2025-12-09 12:09:37.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 3.700941562652588
2025-12-09 12:09:37.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 3.6051793098449707
2025-12-09 12:09:37.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 3.859027147293091
2025-12-09 12:09:37.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 3.6391780376434326
2025-12-09 12:09:37.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 3.509237051010132
2025-12-09 12:09:37.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999125880491853 Training loss: 3.5537185668945312
2025-12-09 12:09:37.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0029996503623845394 Training loss: 3.847893238067627
2025-12-09 12:09:37.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029992133535682725 Training loss: 3.6919569969177246
2025-12-09 12:09:37.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.002998601612533441 Training loss: 3.5895020961761475
2025-12-09 12:09:37.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0029978152105780156 Training loss: 3.860852003097534
2025-12-09 12:09:37.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0029968542393565677 Training loss: 3.9038877487182617
2025-12-09 12:09:37.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029957188108695894 Training loss: 3.6476645469665527
2025-12-09 12:09:37.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00299440905745044 Training loss: 3.6206631660461426
2025-12-09 12:09:37.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.002992925131749921 Training loss: 3.64192533493042
2025-12-09 12:09:38.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0029912672067184862 Training loss: 3.4459705352783203
2025-12-09 12:09:38.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0029894354755860848 Training loss: 3.703874349594116
2025-12-09 12:09:38.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0029874301518396378 Training loss: 3.8924853801727295
2025-12-09 12:09:38.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029852514691981603 Training loss: 3.6055502891540527
2025-12-09 12:09:38.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029828996815855183 Training loss: 3.5867345333099365
2025-12-09 12:09:38.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.002980375063100836 Training loss: 3.6495914459228516
2025-12-09 12:09:38.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00297767790798655 Training loss: 3.6307291984558105
2025-12-09 12:09:38.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.0029748085305941127 Training loss: 3.8905577659606934
2025-12-09 12:09:38.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.002971767265347359 Training loss: 3.6857242584228516
2025-12-09 12:09:38.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029685544667035256 Training loss: 3.7084338665008545
2025-12-09 12:09:38.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0029651705091119423 Training loss: 3.7946105003356934
2025-12-09 12:09:38.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0029616157869703894 Training loss: 3.6394834518432617
2025-12-09 12:09:38.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002957890714579128 Training loss: 3.7623848915100098
2025-12-09 12:09:38.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0029539957260926184 Training loss: 3.7587320804595947
2025-12-09 12:09:38.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.002949931275468917 Training loss: 3.7453744411468506
2025-12-09 12:09:38.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029456978364167667 Training loss: 3.4409561157226562
2025-12-09 12:09:38.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0029412959023403904 Training loss: 3.5212302207946777
2025-12-09 12:09:38.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.002936725986281981 Training loss: 3.722450017929077
2025-12-09 12:09:38.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.002931988620861908 Training loss: 3.809114933013916
2025-12-09 12:09:38.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.002927084358216643 Training loss: 3.5855696201324463
2025-12-09 12:09:38.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0029220137699344055 Training loss: 3.543316125869751
2025-12-09 12:09:38.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0029167774469885483 Training loss: 3.7855305671691895
2025-12-09 12:09:38.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002911375999668675 Training loss: 3.7140486240386963
2025-12-09 12:09:38.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.002905810057509516 Training loss: 3.502523899078369
2025-12-09 12:09:38.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.002900080269217554 Training loss: 3.765103816986084
2025-12-09 12:09:38.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.002894187302595419 Training loss: 3.731943130493164
2025-12-09 12:09:38.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0028881318444640564 Training loss: 3.5025429725646973
2025-12-09 12:09:38.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0028819146005826766 Training loss: 3.7730798721313477
2025-12-09 12:09:38.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0028755362955665015 Training loss: 3.647186279296875
2025-12-09 12:09:38.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0028689976728023103 Training loss: 3.678227424621582
2025-12-09 12:09:38.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0028622994943617985 Training loss: 3.3890206813812256
2025-12-09 12:09:38.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0028554425409127583 Training loss: 3.470898151397705
2025-12-09 12:09:38.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.002848427611628093 Training loss: 3.7437257766723633
2025-12-09 12:09:38.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0028412555240926746 Training loss: 3.8191263675689697
2025-12-09 12:09:38.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.002833927114208054 Training loss: 3.4686038494110107
2025-12-09 12:09:38.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0028264432360950355 Training loss: 3.5452489852905273
2025-12-09 12:09:38.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0028188047619941343 Training loss: 3.2753593921661377
2025-12-09 12:09:38.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0028110125821639137 Training loss: 3.5916976928710938
2025-12-09 12:09:38.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.002803067604777227 Training loss: 3.8012876510620117
2025-12-09 12:09:39.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0027949707558153703 Training loss: 3.4969658851623535
2025-12-09 12:09:39.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.002786722978960162 Training loss: 3.4961588382720947
2025-12-09 12:09:39.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.002778325235483954 Training loss: 3.3785288333892822
2025-12-09 12:09:39.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0027697785041376007 Training loss: 3.9486608505249023
2025-12-09 12:09:39.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.002761083781036381 Training loss: 3.5863149166107178
2025-12-09 12:09:39.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.002752242079543907 Training loss: 3.365023612976074
2025-12-09 12:09:39.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002743254430154012 Training loss: 3.4632790088653564
2025-12-09 12:09:39.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.002734121880370652 Training loss: 3.4236648082733154
2025-12-09 12:09:39.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0027248454945858164 Training loss: 3.416590452194214
2025-12-09 12:09:39.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0027154263539554764 Training loss: 3.7196686267852783
2025-12-09 12:09:39.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.002705865556273575 Training loss: 3.489264726638794
2025-12-09 12:09:39.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.002696164215844081 Training loss: 3.413600444793701
2025-12-09 12:09:39.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0026863234633511188 Training loss: 3.3347995281219482
2025-12-09 12:09:39.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0026763444457271837 Training loss: 3.628021240234375
2025-12-09 12:09:39.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.002666228326019474 Training loss: 3.448188543319702
2025-12-09 12:09:39.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.002655976283254334 Training loss: 3.3065388202667236
2025-12-09 12:09:39.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0026455895122998404 Training loss: 3.484581708908081
2025-12-09 12:09:39.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002635069223726543 Training loss: 3.4177489280700684
2025-12-09 12:09:39.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.002624416643666371 Training loss: 3.5092813968658447
2025-12-09 12:09:39.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0026136330136697305 Training loss: 3.2869908809661865
2025-12-09 12:09:39.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0026027195905608006 Training loss: 3.8249239921569824
2025-12-09 12:09:39.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0025916776462910542 Training loss: 3.1798930168151855
2025-12-09 12:09:39.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00258050846779101 Training loss: 3.364429235458374
2025-12-09 12:09:39.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0025692133568202442 Training loss: 3.620304584503174
2025-12-09 12:09:39.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.002557793629815669 Training loss: 3.4330501556396484
2025-12-09 12:09:39.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0025462506177381045 Training loss: 3.668865203857422
2025-12-09 12:09:39.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0025345856659171567 Training loss: 3.2132742404937744
2025-12-09 12:09:39.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002522800133894418 Training loss: 3.2419652938842773
2025-12-09 12:09:39.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0025108953952650164 Training loss: 3.3809213638305664
2025-12-09 12:09:39.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0024988728375175216 Training loss: 3.2413041591644287
2025-12-09 12:09:39.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.002486733861872236 Training loss: 3.4544644355773926
2025-12-09 12:09:39.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0024744798831178817 Training loss: 3.1036782264709473
2025-12-09 12:09:39.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0024621123294467097 Training loss: 3.536487579345703
2025-12-09 12:09:39.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.002449632642288045 Training loss: 3.636887550354004
2025-12-09 12:09:39.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002437042276140287 Training loss: 3.1078455448150635
2025-12-09 12:09:39.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0024243426984013913 Training loss: 3.04699444770813
2025-12-09 12:09:39.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0024115353891978435 Training loss: 3.402841567993164
2025-12-09 12:09:39.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.002398621841212154 Training loss: 3.4338207244873047
2025-12-09 12:09:40.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.002385603559508884 Training loss: 3.3752570152282715
2025-12-09 12:09:40.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.002372482061359234 Training loss: 3.2614855766296387
2025-12-09 12:09:40.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0023592588760642046 Training loss: 3.4980874061584473
2025-12-09 12:09:40.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00234593554477636 Training loss: 3.5336155891418457
2025-12-09 12:09:40.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.002332513620320205 Training loss: 3.3822426795959473
2025-12-09 12:09:40.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.002318994667011207 Training loss: 3.1870131492614746
2025-12-09 12:09:40.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.002305380260473476 Training loss: 3.200725793838501
2025-12-09 12:09:40.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002291671987456123 Training loss: 3.3044021129608154
2025-12-09 12:09:40.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0022778714456483324 Training loss: 3.345193386077881
2025-12-09 12:09:40.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0022639802434931446 Training loss: 3.3241498470306396
2025-12-09 12:09:40.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0022500000000000003 Training loss: 3.3823232650756836
2025-12-09 12:09:40.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0022359323445560408 Training loss: 3.3028464317321777
2025-12-09 12:09:40.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0022217789167362076 Training loss: 3.515592336654663
2025-12-09 12:09:40.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.002207541366112149 Training loss: 3.417271375656128
2025-12-09 12:09:40.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0021932213520599654 Training loss: 3.2743959426879883
2025-12-09 12:09:40.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0021788205435668085 Training loss: 3.1796696186065674
2025-12-09 12:09:40.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0021643406190363624 Training loss: 3.2600293159484863
2025-12-09 12:09:40.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0021497832660932296 Training loss: 3.125781536102295
2025-12-09 12:09:40.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0021351501813862356 Training loss: 3.228538990020752
2025-12-09 12:09:40.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0021204430703906873 Training loss: 3.20013689994812
2025-12-09 12:09:40.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0021056636472096026 Training loss: 3.152590036392212
2025-12-09 12:09:40.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002090813634373931 Training loss: 2.9720633029937744
2025-12-09 12:09:40.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0020758947626417943 Training loss: 2.984464168548584
2025-12-09 12:09:40.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.002060908770796769 Training loss: 3.4554343223571777
2025-12-09 12:09:40.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0020458574054452315 Training loss: 3.1232829093933105
2025-12-09 12:09:40.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.002030742420812791 Training loss: 3.102217435836792
2025-12-09 12:09:40.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0020155655785398397 Training loss: 3.429144859313965
2025-12-09 12:09:40.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.002000328647476231 Training loss: 3.153921604156494
2025-12-09 12:09:40.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.001985033403475123 Training loss: 3.1851861476898193
2025-12-09 12:09:40.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.001969681629186004 Training loss: 3.13974928855896
2025-12-09 12:09:40.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.001954275113846926 Training loss: 3.4642107486724854
2025-12-09 12:09:40.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0019388156530759713 Training loss: 3.1835014820098877
2025-12-09 12:09:40.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0019233050486619715 Training loss: 3.1920459270477295
2025-12-09 12:09:40.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0019077451083545144 Training loss: 3.2410824298858643
2025-12-09 12:09:40.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0018921376456532484 Training loss: 2.822206974029541
2025-12-09 12:09:40.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0018764844795965232 Training loss: 3.207975387573242
2025-12-09 12:09:40.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0018607874345493807 Training loss: 3.120133638381958
2025-12-09 12:09:40.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0018450483399909264 Training loss: 3.1213669776916504
2025-12-09 12:09:41.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0018292690303011077 Training loss: 2.9919774532318115
2025-12-09 12:09:41.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.001813451344546913 Training loss: 3.042710781097412
2025-12-09 12:09:41.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0017975971262680348 Training loss: 3.188671588897705
2025-12-09 12:09:41.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0017817082232620054 Training loss: 3.414771556854248
2025-12-09 12:09:41.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0017657864873688344 Training loss: 3.1789650917053223
2025-12-09 12:09:41.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0017498337742551818 Training loss: 3.0368082523345947
2025-12-09 12:09:41.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0017338519431980798 Training loss: 3.10540771484375
2025-12-09 12:09:41.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0017178428568682357 Training loss: 3.0279343128204346
2025-12-09 12:09:41.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.001701808381112938 Training loss: 3.3605332374572754
2025-12-09 12:09:41.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0016857503847385955 Training loss: 3.252774238586426
2025-12-09 12:09:41.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0016696707392929266 Training loss: 2.998605489730835
2025-12-09 12:09:41.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.001653571318846834 Training loss: 3.0774686336517334
2025-12-09 12:09:41.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0016374539997759824 Training loss: 2.9950289726257324
2025-12-09 12:09:41.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0016213206605421066 Training loss: 3.2886803150177
2025-12-09 12:09:41.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.001605173181474081 Training loss: 2.9908483028411865
2025-12-09 12:09:41.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0015890134445487678 Training loss: 3.0213844776153564
2025-12-09 12:09:41.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0015728433331716725 Training loss: 3.1614584922790527
2025-12-09 12:09:41.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0015566647319574351 Training loss: 3.1150412559509277
2025-12-09 12:09:41.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0015404795265101807 Training loss: 3.173187732696533
2025-12-09 12:09:41.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0015242896032037524 Training loss: 3.043447256088257
2025-12-09 12:09:41.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0015080968489618568 Training loss: 3.0792157649993896
2025-12-09 12:09:41.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0014919031510381437 Training loss: 3.073824405670166
2025-12-09 12:09:41.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0014757103967962479 Training loss: 3.292358875274658
2025-12-09 12:09:41.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0014595204734898198 Training loss: 3.0019800662994385
2025-12-09 12:09:41.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0014433352680425654 Training loss: 3.0233535766601562
2025-12-09 12:09:41.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0014271566668283282 Training loss: 3.0847256183624268
2025-12-09 12:09:41.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.001410986555451232 Training loss: 3.2810590267181396
2025-12-09 12:09:41.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0013948268185259188 Training loss: 3.0732996463775635
2025-12-09 12:09:41.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.001378679339457894 Training loss: 3.182689666748047
2025-12-09 12:09:41.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0013625460002240181 Training loss: 2.8567392826080322
2025-12-09 12:09:41.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0013464286811531662 Training loss: 2.973587989807129
2025-12-09 12:09:41.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0013303292607070737 Training loss: 3.2135090827941895
2025-12-09 12:09:41.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.001314249615261405 Training loss: 2.8326823711395264
2025-12-09 12:09:41.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0012981916188870622 Training loss: 2.710895538330078
2025-12-09 12:09:41.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.001282157143131765 Training loss: 2.925565004348755
2025-12-09 12:09:41.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00126614805680192 Training loss: 2.607431650161743
2025-12-09 12:09:41.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0012501662257448183 Training loss: 3.1204662322998047
2025-12-09 12:09:41.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.001234213512631166 Training loss: 2.7175049781799316
2025-12-09 12:09:41.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.001218291776737995 Training loss: 3.1607909202575684
2025-12-09 12:09:42.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0012024028737319652 Training loss: 2.860107898712158
2025-12-09 12:09:42.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0011865486554530874 Training loss: 2.763814687728882
2025-12-09 12:09:42.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.001170730969698893 Training loss: 2.888667583465576
2025-12-09 12:09:42.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0011549516600090739 Training loss: 2.954587459564209
2025-12-09 12:09:42.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00113921256545062 Training loss: 2.8766860961914062
2025-12-09 12:09:42.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0011235155204034769 Training loss: 3.0659937858581543
2025-12-09 12:09:42.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0011078623543467519 Training loss: 3.0512685775756836
2025-12-09 12:09:42.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0010922548916454857 Training loss: 2.800318479537964
2025-12-09 12:09:42.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0010766949513380285 Training loss: 3.0067853927612305
2025-12-09 12:09:42.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.001061184346924029 Training loss: 2.931057929992676
2025-12-09 12:09:42.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0010457248861530741 Training loss: 2.928330659866333
2025-12-09 12:09:42.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0010303183708139964 Training loss: 2.8434395790100098
2025-12-09 12:09:42.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0010149665965248776 Training loss: 3.141514539718628
2025-12-09 12:09:42.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009996713525237694 Training loss: 3.131199598312378
2025-12-09 12:09:42.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0009844344214601601 Training loss: 2.8903961181640625
2025-12-09 12:09:42.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.000969257579187209 Training loss: 2.937819004058838
2025-12-09 12:09:42.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009541425945547689 Training loss: 3.0185904502868652
2025-12-09 12:09:42.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.000939091229203231 Training loss: 3.0160324573516846
2025-12-09 12:09:42.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0009241052373582058 Training loss: 2.904252290725708
2025-12-09 12:09:42.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009091863656260695 Training loss: 2.8807168006896973
2025-12-09 12:09:42.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0008943363527903977 Training loss: 2.7725448608398438
2025-12-09 12:09:42.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0008795569296093132 Training loss: 2.9344642162323
2025-12-09 12:09:42.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0008648498186137653 Training loss: 2.99066424369812
2025-12-09 12:09:42.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0008502167339067705 Training loss: 2.977346420288086
2025-12-09 12:09:42.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0008356593809636371 Training loss: 3.027559518814087
2025-12-09 12:09:42.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0008211794564331918 Training loss: 2.665647268295288
2025-12-09 12:09:42.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0008067786479400346 Training loss: 3.0259604454040527
2025-12-09 12:09:42.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0007924586338878512 Training loss: 2.9461793899536133
2025-12-09 12:09:42.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0007782210832637924 Training loss: 2.8269307613372803
2025-12-09 12:09:42.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0007640676554439594 Training loss: 2.840070962905884
2025-12-09 12:09:42.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0007500000000000003 Training loss: 2.647841215133667
2025-12-09 12:09:42.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.000736019756506856 Training loss: 2.7590041160583496
2025-12-09 12:09:42.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.000722128554351668 Training loss: 2.64890718460083
2025-12-09 12:09:42.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0007083280125438767 Training loss: 2.6339595317840576
2025-12-09 12:09:42.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0006946197395265243 Training loss: 2.974273920059204
2025-12-09 12:09:42.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0006810053329887928 Training loss: 2.742279052734375
2025-12-09 12:09:42.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0006674863796797954 Training loss: 2.8418939113616943
2025-12-09 12:09:42.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0006540644552236401 Training loss: 2.981637716293335
2025-12-09 12:09:43.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0006407411239357954 Training loss: 2.623563289642334
2025-12-09 12:09:43.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0006275179386407663 Training loss: 2.869741916656494
2025-12-09 12:09:43.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0006143964404911165 Training loss: 3.1564383506774902
2025-12-09 12:09:43.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0006013781587878463 Training loss: 2.8441834449768066
2025-12-09 12:09:43.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0005884646108021563 Training loss: 2.6580240726470947
2025-12-09 12:09:43.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.000575657301598609 Training loss: 2.642543315887451
2025-12-09 12:09:43.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0005629577238597133 Training loss: 3.1259357929229736
2025-12-09 12:09:43.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0005503673577119553 Training loss: 3.0628111362457275
2025-12-09 12:09:43.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0005378876705532904 Training loss: 2.458674669265747
2025-12-09 12:09:43.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0005255201168821184 Training loss: 3.0853006839752197
2025-12-09 12:09:43.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0005132661381277644 Training loss: 2.776233673095703
2025-12-09 12:09:43.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0005011271624824787 Training loss: 2.744112491607666
2025-12-09 12:09:43.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0004891046047349837 Training loss: 3.1098179817199707
2025-12-09 12:09:43.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00047719986610558236 Training loss: 3.028801441192627
2025-12-09 12:09:43.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00046541433408284357 Training loss: 3.0743722915649414
2025-12-09 12:09:43.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00045374938226189584 Training loss: 2.767472982406616
2025-12-09 12:09:43.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.00044220637018433165 Training loss: 2.678732395172119
2025-12-09 12:09:43.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00043078664317975653 Training loss: 2.7579309940338135
2025-12-09 12:09:43.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0004194915322089899 Training loss: 3.149806022644043
2025-12-09 12:09:43.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00040832235370894606 Training loss: 2.5932884216308594
2025-12-09 12:09:43.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0003972804094391998 Training loss: 2.7277560234069824
2025-12-09 12:09:43.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0003863669863302698 Training loss: 2.9827568531036377
2025-12-09 12:09:43.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00037558335633362935 Training loss: 2.8395884037017822
2025-12-09 12:09:43.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.000364930776273457 Training loss: 2.4770050048828125
2025-12-09 12:09:43.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0003544104877001596 Training loss: 2.831569194793701
2025-12-09 12:09:43.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0003440237167456663 Training loss: 2.719374895095825
2025-12-09 12:09:43.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0003337716739805264 Training loss: 2.5892388820648193
2025-12-09 12:09:43.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00032365555427281634 Training loss: 2.967301845550537
2025-12-09 12:09:43.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00031367653664888173 Training loss: 2.714531898498535
2025-12-09 12:09:43.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0003038357841559191 Training loss: 2.727421283721924
2025-12-09 12:09:43.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00029413444372642495 Training loss: 2.5843088626861572
2025-12-09 12:09:43.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.00028457364604452375 Training loss: 2.9751064777374268
2025-12-09 12:09:43.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.00027515450541418343 Training loss: 2.78802227973938
2025-12-09 12:09:43.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.0002658781196293482 Training loss: 2.8227946758270264
2025-12-09 12:09:43.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0002567455698459882 Training loss: 2.6167922019958496
2025-12-09 12:09:43.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00024775792045609354 Training loss: 3.013664722442627
2025-12-09 12:09:43.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.00023891621896361882 Training loss: 2.6747748851776123
2025-12-09 12:09:43.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.00023022149586239971 Training loss: 2.893693208694458
2025-12-09 12:09:43.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.00022167476451604625 Training loss: 2.748284339904785
2025-12-09 12:09:44.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.00021327702103983865 Training loss: 2.8010518550872803
2025-12-09 12:09:44.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.00020502924418463014 Training loss: 2.6890406608581543
2025-12-09 12:09:44.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0001969323952227733 Training loss: 2.7097318172454834
2025-12-09 12:09:44.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.00018898741783608642 Training loss: 2.5405819416046143
2025-12-09 12:09:44.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00018119523800586568 Training loss: 2.7766294479370117
2025-12-09 12:09:44.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.00017355676390496484 Training loss: 2.421110153198242
2025-12-09 12:09:44.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0001660728857919464 Training loss: 2.5985095500946045
2025-12-09 12:09:44.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00015874447590732537 Training loss: 2.8296396732330322
2025-12-09 12:09:44.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0001515723883719072 Training loss: 2.730746269226074
2025-12-09 12:09:44.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00014455745908724228 Training loss: 2.748819351196289
2025-12-09 12:09:44.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0001377005056382018 Training loss: 2.8608081340789795
2025-12-09 12:09:44.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00013100232719768994 Training loss: 2.705702781677246
2025-12-09 12:09:44.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00012446370443349862 Training loss: 2.9217963218688965
2025-12-09 12:09:44.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0001180853994173236 Training loss: 2.811530113220215
2025-12-09 12:09:44.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.00011186815553594382 Training loss: 2.6527035236358643
2025-12-09 12:09:44.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0001058126974045811 Training loss: 2.6284353733062744
2025-12-09 12:09:44.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.991973078244637e-05 Training loss: 2.8730053901672363
2025-12-09 12:09:44.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.418994249048473e-05 Training loss: 2.7395427227020264
2025-12-09 12:09:44.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 8.862400033132573e-05 Training loss: 2.6825106143951416
2025-12-09 12:09:44.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 8.322255301145204e-05 Training loss: 2.8855841159820557
2025-12-09 12:09:44.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 7.798623006559435e-05 Training loss: 2.8155734539031982
2025-12-09 12:09:44.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 7.291564178335719e-05 Training loss: 2.767254114151001
2025-12-09 12:09:44.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 6.801137913809213e-05 Training loss: 2.715932846069336
2025-12-09 12:09:44.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 6.327401371801944e-05 Training loss: 2.461841106414795
2025-12-09 12:09:44.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 5.870409765960966e-05 Training loss: 2.8565237522125244
2025-12-09 12:09:44.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 5.430216358323309e-05 Training loss: 2.935590982437134
2025-12-09 12:09:44.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 5.00687245310833e-05 Training loss: 2.7078590393066406
2025-12-09 12:09:44.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 4.60042739073816e-05 Training loss: 2.673067092895508
2025-12-09 12:09:44.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 4.2109285420872056e-05 Training loss: 2.7648892402648926
2025-12-09 12:09:44.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 3.8384213029610984e-05 Training loss: 2.7153637409210205
2025-12-09 12:09:44.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 3.4829490888057425e-05 Training loss: 2.694042921066284
2025-12-09 12:09:44.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 3.144553329647448e-05 Training loss: 2.7035810947418213
2025-12-09 12:09:44.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 2.823273465264142e-05 Training loss: 2.5590834617614746
2025-12-09 12:09:44.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 2.5191469405887624e-05 Training loss: 2.6136252880096436
2025-12-09 12:09:44.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 2.2322092013450313e-05 Training loss: 2.654212236404419
2025-12-09 12:09:44.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 1.9624936899163947e-05 Training loss: 2.8757925033569336
2025-12-09 12:09:44.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 1.7100318414482063e-05 Training loss: 2.614724636077881
2025-12-09 12:09:44.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 1.4748530801840077e-05 Training loss: 2.640151023864746
2025-12-09 12:09:44.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 1.2569848160362384e-05 Training loss: 2.645998954772949
2025-12-09 12:09:45.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 1.0564524413915422e-05 Training loss: 2.5440399646759033
2025-12-09 12:09:45.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 8.732793281513663e-06 Training loss: 2.737574338912964
2025-12-09 12:09:45.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 7.07486825007908e-06 Training loss: 2.497730255126953
2025-12-09 12:09:45.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 5.590942549560052e-06 Training loss: 2.7085866928100586
2025-12-09 12:09:45.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 4.281189130410535e-06 Training loss: 2.410207748413086
2025-12-09 12:09:45.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 3.145760643432527e-06 Training loss: 2.667987823486328
2025-12-09 12:09:45.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 2.184789421984634e-06 Training loss: 2.9026124477386475
2025-12-09 12:09:45.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 1.3983874665589036e-06 Training loss: 2.516084909439087
2025-12-09 12:09:45.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 7.866464317276001e-07 Training loss: 2.5713179111480713
2025-12-09 12:09:45.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 3.496376154604186e-07 Training loss: 2.573791742324829
2025-12-09 12:09:45.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 8.741195081479747e-08 Training loss: 2.395939350128174
2025-12-09 12:09:45.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 2.7812044620513916
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:09:58.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 4.873716831207275
2025-12-09 12:09:58.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 4.773757457733154
2025-12-09 12:09:58.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 4.9079270362854
2025-12-09 12:09:58.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 4.854887008666992
2025-12-09 12:09:58.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 4.803802013397217
2025-12-09 12:09:59.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 4.9105634689331055
2025-12-09 12:09:59.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 4.870787143707275
2025-12-09 12:09:59.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 4.858786582946777
2025-12-09 12:09:59.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 4.7845845222473145
2025-12-09 12:09:59.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 4.837215900421143
2025-12-09 12:09:59.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 4.847665309906006
2025-12-09 12:09:59.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 4.6451334953308105
2025-12-09 12:09:59.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 4.744940757751465
2025-12-09 12:09:59.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 4.736783981323242
2025-12-09 12:09:59.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 4.602291584014893
2025-12-09 12:09:59.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 4.604862213134766
2025-12-09 12:09:59.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 4.655924320220947
2025-12-09 12:09:59.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 4.648123264312744
2025-12-09 12:09:59.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 4.591699600219727
2025-12-09 12:09:59.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 4.505617618560791
2025-12-09 12:09:59.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 4.40597677230835
2025-12-09 12:09:59.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 4.416983604431152
2025-12-09 12:09:59.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 4.510196208953857
2025-12-09 12:09:59.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 4.353265762329102
2025-12-09 12:09:59.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 4.153914928436279
2025-12-09 12:09:59.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 4.336745738983154
2025-12-09 12:09:59.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 4.364870548248291
2025-12-09 12:09:59.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 4.117680549621582
2025-12-09 12:09:59.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 4.274554252624512
2025-12-09 12:09:59.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 4.366634368896484
2025-12-09 12:09:59.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 4.182000160217285
2025-12-09 12:09:59.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 4.408579349517822
2025-12-09 12:09:59.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 4.032167434692383
2025-12-09 12:09:59.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 4.248020648956299
2025-12-09 12:09:59.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 4.072471618652344
2025-12-09 12:09:59.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 4.001396179199219
2025-12-09 12:09:59.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 4.07572078704834
2025-12-09 12:09:59.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 4.140365123748779
2025-12-09 12:09:59.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 4.011847019195557
2025-12-09 12:10:00.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 4.103078842163086
2025-12-09 12:10:00.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 4.285792827606201
2025-12-09 12:10:00.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 4.221397399902344
2025-12-09 12:10:00.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 4.053434371948242
2025-12-09 12:10:00.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 4.085414409637451
2025-12-09 12:10:00.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 4.207278251647949
2025-12-09 12:10:00.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 3.927719831466675
2025-12-09 12:10:00.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 4.125540733337402
2025-12-09 12:10:00.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 4.0250043869018555
2025-12-09 12:10:00.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 4.056691646575928
2025-12-09 12:10:00.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 3.982571601867676
2025-12-09 12:10:00.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 4.143777847290039
2025-12-09 12:10:00.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 3.949434280395508
2025-12-09 12:10:00.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 3.8406498432159424
2025-12-09 12:10:00.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 4.206794261932373
2025-12-09 12:10:00.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 4.07515811920166
2025-12-09 12:10:00.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 4.182270526885986
2025-12-09 12:10:00.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 3.7981722354888916
2025-12-09 12:10:00.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 3.7260775566101074
2025-12-09 12:10:00.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 4.056958198547363
2025-12-09 12:10:00.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 3.8204147815704346
2025-12-09 12:10:00.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 3.9547832012176514
2025-12-09 12:10:00.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 4.202723503112793
2025-12-09 12:10:00.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 4.067487716674805
2025-12-09 12:10:00.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 4.028541564941406
2025-12-09 12:10:00.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 3.9488630294799805
2025-12-09 12:10:00.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 3.7986466884613037
2025-12-09 12:10:00.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 4.162458419799805
2025-12-09 12:10:00.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 3.991375207901001
2025-12-09 12:10:00.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 3.973848581314087
2025-12-09 12:10:00.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 3.8944191932678223
2025-12-09 12:10:00.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 3.804138660430908
2025-12-09 12:10:00.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 4.11467981338501
2025-12-09 12:10:00.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 4.043365001678467
2025-12-09 12:10:01.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 3.923389434814453
2025-12-09 12:10:01.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 3.933879852294922
2025-12-09 12:10:01.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 3.9889347553253174
2025-12-09 12:10:01.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 4.203179836273193
2025-12-09 12:10:01.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 3.6650052070617676
2025-12-09 12:10:01.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 3.8747429847717285
2025-12-09 12:10:01.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 3.5727553367614746
2025-12-09 12:10:01.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 3.842240571975708
2025-12-09 12:10:01.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 4.047804355621338
2025-12-09 12:10:01.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 3.823007106781006
2025-12-09 12:10:01.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 3.709409475326538
2025-12-09 12:10:01.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 3.9068946838378906
2025-12-09 12:10:01.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 3.9430644512176514
2025-12-09 12:10:01.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 3.9717516899108887
2025-12-09 12:10:01.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 3.929222583770752
2025-12-09 12:10:01.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 3.860926389694214
2025-12-09 12:10:01.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 3.763129472732544
2025-12-09 12:10:01.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 3.8656086921691895
2025-12-09 12:10:01.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 4.064054012298584
2025-12-09 12:10:01.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 4.39816427230835
2025-12-09 12:10:01.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 3.9446256160736084
2025-12-09 12:10:01.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 3.9572463035583496
2025-12-09 12:10:01.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 3.7304437160491943
2025-12-09 12:10:01.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 3.8506176471710205
2025-12-09 12:10:01.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 3.6202476024627686
2025-12-09 12:10:01.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 3.6710100173950195
2025-12-09 12:10:01.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 4.18840217590332
2025-12-09 12:10:01.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999708626830616 Training loss: 3.741240978240967
2025-12-09 12:10:01.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.009998834541281799 Training loss: 4.211655139923096
2025-12-09 12:10:01.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009997377845227575 Training loss: 3.9232752323150635
2025-12-09 12:10:01.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009995338708444804 Training loss: 3.7332465648651123
2025-12-09 12:10:01.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009992717368593385 Training loss: 3.88285231590271
2025-12-09 12:10:01.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009989514131188558 Training loss: 3.73427152633667
2025-12-09 12:10:02.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009985729369565299 Training loss: 3.6996657848358154
2025-12-09 12:10:02.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0099813635248348 Training loss: 3.9719433784484863
2025-12-09 12:10:02.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.00997641710583307 Training loss: 3.6444039344787598
2025-12-09 12:10:02.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.009970890689061622 Training loss: 3.884121894836426
2025-12-09 12:10:02.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009964784918620281 Training loss: 3.80865740776062
2025-12-09 12:10:02.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.009958100506132127 Training loss: 4.090987205505371
2025-12-09 12:10:02.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009950838230660534 Training loss: 3.837148666381836
2025-12-09 12:10:02.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009942998938618395 Training loss: 3.891568660736084
2025-12-09 12:10:02.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.009934583543669454 Training loss: 3.622014045715332
2025-12-09 12:10:02.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009925593026621833 Training loss: 3.79780912399292
2025-12-09 12:10:02.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.009916028435313709 Training loss: 4.090449810028076
2025-12-09 12:10:02.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.009905890884491196 Training loss: 3.794830560684204
2025-12-09 12:10:02.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.00989518155567842 Training loss: 3.6722934246063232
2025-12-09 12:10:02.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009883901697039808 Training loss: 3.5664925575256348
2025-12-09 12:10:02.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.009872052623234632 Training loss: 3.89508056640625
2025-12-09 12:10:02.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00985963571526376 Training loss: 3.7873010635375977
2025-12-09 12:10:02.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.009846652420308728 Training loss: 3.8984107971191406
2025-12-09 12:10:02.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.009833104251563056 Training loss: 3.9405548572540283
2025-12-09 12:10:02.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.00981899278805589 Training loss: 3.7393641471862793
2025-12-09 12:10:02.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.009804319674467968 Training loss: 3.671917676925659
2025-12-09 12:10:02.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009789086620939936 Training loss: 3.7335472106933594
2025-12-09 12:10:02.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009773295402873026 Training loss: 3.711927652359009
2025-12-09 12:10:02.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009756947860722143 Training loss: 3.7056658267974854
2025-12-09 12:10:02.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009740045899781353 Training loss: 3.806854248046875
2025-12-09 12:10:02.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.009722591489961827 Training loss: 3.5709996223449707
2025-12-09 12:10:02.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009704586665562249 Training loss: 3.701220750808716
2025-12-09 12:10:02.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.00968603352503172 Training loss: 3.676114082336426
2025-12-09 12:10:02.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009666934230725179 Training loss: 3.5405843257904053
2025-12-09 12:10:02.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009647291008651398 Training loss: 3.820183515548706
2025-12-09 12:10:02.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009627106148213521 Training loss: 3.644517183303833
2025-12-09 12:10:02.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.009606382001942255 Training loss: 3.9635019302368164
2025-12-09 12:10:02.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00958512098522167 Training loss: 3.6055748462677
2025-12-09 12:10:02.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0095633255760077 Training loss: 3.327970504760742
2025-12-09 12:10:02.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009540998314539327 Training loss: 3.442556381225586
2025-12-09 12:10:03.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009518141803042527 Training loss: 3.578702926635742
2025-12-09 12:10:03.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.009494758705426976 Training loss: 3.5906271934509277
2025-12-09 12:10:03.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.009470851746975581 Training loss: 3.310724973678589
2025-12-09 12:10:03.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009446423714026845 Training loss: 4.270821571350098
2025-12-09 12:10:03.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.009421477453650118 Training loss: 3.430147409439087
2025-12-09 12:10:03.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.009396015873313781 Training loss: 3.47463059425354
2025-12-09 12:10:03.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00937004194054638 Training loss: 3.8948771953582764
2025-12-09 12:10:03.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009343558682590757 Training loss: 3.7046470642089844
2025-12-09 12:10:03.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.009316569186051234 Training loss: 3.802342414855957
2025-12-09 12:10:03.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009289076596533871 Training loss: 3.4040064811706543
2025-12-09 12:10:03.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009261084118279847 Training loss: 3.2948176860809326
2025-12-09 12:10:03.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009232595013792002 Training loss: 3.6563923358917236
2025-12-09 12:10:03.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009203612603454604 Training loss: 3.568525791168213
2025-12-09 12:10:03.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.009174140265146355 Training loss: 3.343338966369629
2025-12-09 12:10:03.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009144181433846706 Training loss: 3.3981316089630127
2025-12-09 12:10:03.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009113739601235507 Training loss: 3.5411157608032227
2025-12-09 12:10:03.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009082818315286054 Training loss: 3.3964405059814453
2025-12-09 12:10:03.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009051421179851587 Training loss: 3.556246757507324
2025-12-09 12:10:03.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.00901955185424525 Training loss: 3.6515491008758545
2025-12-09 12:10:03.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.008987214052813604 Training loss: 3.363617420196533
2025-12-09 12:10:03.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00895441154450373 Training loss: 3.6612894535064697
2025-12-09 12:10:03.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.008921148152423945 Training loss: 3.5216197967529297
2025-12-09 12:10:03.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.008887427753398248 Training loss: 3.5571680068969727
2025-12-09 12:10:03.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.008853254277514447 Training loss: 3.0415561199188232
2025-12-09 12:10:03.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.008818631707666134 Training loss: 3.5958752632141113
2025-12-09 12:10:03.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.008783564079088476 Training loss: 3.5985286235809326
2025-12-09 12:10:03.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.008748055478887904 Training loss: 3.4127068519592285
2025-12-09 12:10:03.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.008712110045565767 Training loss: 3.5214405059814453
2025-12-09 12:10:03.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.008675731968536002 Training loss: 3.3203277587890625
2025-12-09 12:10:03.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.008638925487636848 Training loss: 3.327277660369873
2025-12-09 12:10:03.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0086016948926367 Training loss: 3.4919097423553467
2025-12-09 12:10:03.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.008564044522734147 Training loss: 3.608690023422241
2025-12-09 12:10:03.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.008525978766052229 Training loss: 3.646653413772583
2025-12-09 12:10:03.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.008487502059127015 Training loss: 3.4083175659179688
2025-12-09 12:10:04.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.008448618886390521 Training loss: 3.5550172328948975
2025-12-09 12:10:04.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00840933377964806 Training loss: 3.4982292652130127
2025-12-09 12:10:04.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.008369651317550054 Training loss: 3.242204427719116
2025-12-09 12:10:04.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.008329576125058406 Training loss: 3.4513959884643555
2025-12-09 12:10:04.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.008289112872907454 Training loss: 3.460871458053589
2025-12-09 12:10:04.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.008248266277059607 Training loss: 3.1848456859588623
2025-12-09 12:10:04.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0082070410981557 Training loss: 3.4442296028137207
2025-12-09 12:10:04.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00816544214096015 Training loss: 3.103973150253296
2025-12-09 12:10:04.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.008123474253800956 Training loss: 3.6491663455963135
2025-12-09 12:10:04.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.008081142328004637 Training loss: 3.4527194499969482
2025-12-09 12:10:04.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.008038451297326145 Training loss: 3.554202079772949
2025-12-09 12:10:04.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.007995406137373847 Training loss: 3.3031489849090576
2025-12-09 12:10:04.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.007952011865029614 Training loss: 3.3193883895874023
2025-12-09 12:10:04.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.007908273537864113 Training loss: 3.1939351558685303
2025-12-09 12:10:04.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.007864196253547348 Training loss: 3.5612661838531494
2025-12-09 12:10:04.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.007819785149254532 Training loss: 3.079118251800537
2025-12-09 12:10:04.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.00777504540106735 Training loss: 3.232170343399048
2025-12-09 12:10:04.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.007729982223370691 Training loss: 3.146273612976074
2025-12-09 12:10:04.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.00768460086824492 Training loss: 3.433720350265503
2025-12-09 12:10:04.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.007638906624853743 Training loss: 3.435047149658203
2025-12-09 12:10:04.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.007592904818827774 Training loss: 3.2310047149658203
2025-12-09 12:10:04.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.007546600811643816 Training loss: 3.209449052810669
2025-12-09 12:10:04.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0075 Training loss: 3.5194687843322754
2025-12-09 12:10:04.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.007453107815186802 Training loss: 2.9904723167419434
2025-12-09 12:10:04.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.007405929722454026 Training loss: 3.191807508468628
2025-12-09 12:10:04.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.007358471220373831 Training loss: 3.486999750137329
2025-12-09 12:10:04.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.007310737840199885 Training loss: 3.3835055828094482
2025-12-09 12:10:04.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.007262735145222695 Training loss: 3.7291574478149414
2025-12-09 12:10:04.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.007214468730121208 Training loss: 3.3953256607055664
2025-12-09 12:10:04.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.007165944220310766 Training loss: 3.2815346717834473
2025-12-09 12:10:04.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.007117167271287452 Training loss: 3.4599180221557617
2025-12-09 12:10:04.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.007068143567968957 Training loss: 3.3672451972961426
2025-12-09 12:10:04.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0070188788240320085 Training loss: 3.068624973297119
2025-12-09 12:10:05.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.006969378781246436 Training loss: 3.400958776473999
2025-12-09 12:10:05.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.006919649208805981 Training loss: 3.19297456741333
2025-12-09 12:10:05.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0068696959026558965 Training loss: 3.085484743118286
2025-12-09 12:10:05.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.006819524684817438 Training loss: 3.3314692974090576
2025-12-09 12:10:05.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0067691414027093045 Training loss: 3.274589776992798
2025-12-09 12:10:05.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.006718551928466133 Training loss: 2.895446538925171
2025-12-09 12:10:05.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.006667762158254104 Training loss: 2.8968822956085205
2025-12-09 12:10:05.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.006616778011583743 Training loss: 3.090118408203125
2025-12-09 12:10:05.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.006565605430620013 Training loss: 3.1561453342437744
2025-12-09 12:10:05.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.006514250379489753 Training loss: 3.3745100498199463
2025-12-09 12:10:05.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.006462718843586571 Training loss: 3.494124174118042
2025-12-09 12:10:05.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0064110168288732386 Training loss: 3.271164655685425
2025-12-09 12:10:05.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.006359150361181715 Training loss: 3.1986684799194336
2025-12-09 12:10:05.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.006307125485510829 Training loss: 3.166670799255371
2025-12-09 12:10:05.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0062549482653217435 Training loss: 3.2698147296905518
2025-12-09 12:10:05.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0062026247818312685 Training loss: 3.451256036758423
2025-12-09 12:10:05.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0061501611333030885 Training loss: 3.049833059310913
2025-12-09 12:10:05.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.006097563434337026 Training loss: 3.2973926067352295
2025-12-09 12:10:05.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.006044837815156376 Training loss: 3.2220420837402344
2025-12-09 12:10:05.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.005991990420893449 Training loss: 3.2206661701202393
2025-12-09 12:10:05.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.005939027410873351 Training loss: 3.1045167446136475
2025-12-09 12:10:05.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0058859549578961145 Training loss: 3.3213627338409424
2025-12-09 12:10:05.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.005832779247517273 Training loss: 3.206376791000366
2025-12-09 12:10:05.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0057795064773269325 Training loss: 3.2721996307373047
2025-12-09 12:10:05.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.005726142856227452 Training loss: 3.1008057594299316
2025-12-09 12:10:05.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.005672694603709794 Training loss: 3.2923452854156494
2025-12-09 12:10:05.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.005619167949128652 Training loss: 3.090176820755005
2025-12-09 12:10:05.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.005565569130976423 Training loss: 3.2260541915893555
2025-12-09 12:10:05.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.005511904396156113 Training loss: 3.1251041889190674
2025-12-09 12:10:05.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.005458179999253274 Training loss: 3.1518666744232178
2025-12-09 12:10:05.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.005404402201807022 Training loss: 3.0710225105285645
2025-12-09 12:10:05.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00535057727158027 Training loss: 3.417673110961914
2025-12-09 12:10:05.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.005296711481829226 Training loss: 3.0498390197753906
2025-12-09 12:10:05.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.005242811110572242 Training loss: 2.9844443798065186
2025-12-09 12:10:06.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.005188882439858117 Training loss: 3.166271924972534
2025-12-09 12:10:06.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.005134931755033936 Training loss: 3.0119173526763916
2025-12-09 12:10:06.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.005080965344012508 Training loss: 2.982626438140869
2025-12-09 12:10:06.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.005026989496539522 Training loss: 3.171799659729004
2025-12-09 12:10:06.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.004973010503460479 Training loss: 2.9850282669067383
2025-12-09 12:10:06.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.004919034655987493 Training loss: 2.804065227508545
2025-12-09 12:10:06.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.004865068244966066 Training loss: 3.0164666175842285
2025-12-09 12:10:06.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0048111175601418844 Training loss: 3.2515177726745605
2025-12-09 12:10:06.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0047571888894277605 Training loss: 3.0706331729888916
2025-12-09 12:10:06.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0047032885181707736 Training loss: 3.0027694702148438
2025-12-09 12:10:06.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.004649422728419729 Training loss: 3.093916654586792
2025-12-09 12:10:06.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0045955977981929795 Training loss: 3.167428493499756
2025-12-09 12:10:06.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.004541820000746727 Training loss: 2.8998639583587646
2025-12-09 12:10:06.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.004488095603843887 Training loss: 2.935512065887451
2025-12-09 12:10:06.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.004434430869023579 Training loss: 2.914829969406128
2025-12-09 12:10:06.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00438083205087135 Training loss: 2.8172199726104736
2025-12-09 12:10:06.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.004327305396290207 Training loss: 2.82548451423645
2025-12-09 12:10:06.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00427385714377255 Training loss: 2.911376953125
2025-12-09 12:10:06.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.004220493522673068 Training loss: 2.8177642822265625
2025-12-09 12:10:06.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.004167220752482727 Training loss: 2.9105498790740967
2025-12-09 12:10:06.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0041140450421038866 Training loss: 2.6710002422332764
2025-12-09 12:10:06.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00406097258912665 Training loss: 3.0625553131103516
2025-12-09 12:10:06.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.004008009579106551 Training loss: 3.324012279510498
2025-12-09 12:10:06.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.003955162184843625 Training loss: 2.9133903980255127
2025-12-09 12:10:06.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.003902436565662977 Training loss: 2.9875333309173584
2025-12-09 12:10:06.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.003849838866696913 Training loss: 3.170703172683716
2025-12-09 12:10:06.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.003797375218168733 Training loss: 3.0430169105529785
2025-12-09 12:10:06.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0037450517346782563 Training loss: 2.997544765472412
2025-12-09 12:10:06.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0036928745144891727 Training loss: 3.11118745803833
2025-12-09 12:10:06.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0036408496388182854 Training loss: 2.863250732421875
2025-12-09 12:10:06.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0035889831711267616 Training loss: 2.8659539222717285
2025-12-09 12:10:06.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00353728115641343 Training loss: 3.206066370010376
2025-12-09 12:10:06.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.003485749620510247 Training loss: 2.6818695068359375
2025-12-09 12:10:07.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0034343945693799884 Training loss: 2.8646082878112793
2025-12-09 12:10:07.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0033832219884162586 Training loss: 2.6950888633728027
2025-12-09 12:10:07.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0033322378417458983 Training loss: 2.945742607116699
2025-12-09 12:10:07.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0032814480715338667 Training loss: 2.928621530532837
2025-12-09 12:10:07.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0032308585972906966 Training loss: 2.7707128524780273
2025-12-09 12:10:07.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0031804753151825627 Training loss: 3.169172525405884
2025-12-09 12:10:07.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0031303040973441033 Training loss: 3.0200772285461426
2025-12-09 12:10:07.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.003080350791194019 Training loss: 2.980520009994507
2025-12-09 12:10:07.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0030306212187535654 Training loss: 2.9899098873138428
2025-12-09 12:10:07.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029811211759679926 Training loss: 2.749290704727173
2025-12-09 12:10:07.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029318564320310442 Training loss: 2.879971504211426
2025-12-09 12:10:07.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002882832728712551 Training loss: 2.585149049758911
2025-12-09 12:10:07.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0028340557796892353 Training loss: 3.0066049098968506
2025-12-09 12:10:07.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0027855312698787903 Training loss: 2.8694992065429688
2025-12-09 12:10:07.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.002737264854777306 Training loss: 2.798083782196045
2025-12-09 12:10:07.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0026892621598001154 Training loss: 2.820956230163574
2025-12-09 12:10:07.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0026415287796261707 Training loss: 2.9036147594451904
2025-12-09 12:10:07.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0025940702775459745 Training loss: 2.7314376831054688
2025-12-09 12:10:07.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002546892184813198 Training loss: 2.537431001663208
2025-12-09 12:10:07.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0025000000000000014 Training loss: 2.701753616333008
2025-12-09 12:10:07.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0024533991883561868 Training loss: 3.0303149223327637
2025-12-09 12:10:07.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.002407095181172227 Training loss: 2.893786668777466
2025-12-09 12:10:07.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0023610933751462555 Training loss: 2.754795551300049
2025-12-09 12:10:07.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002315399131755081 Training loss: 2.890143394470215
2025-12-09 12:10:07.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0022700177766293095 Training loss: 2.8813395500183105
2025-12-09 12:10:07.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.002224954598932651 Training loss: 2.8800082206726074
2025-12-09 12:10:07.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0021802148507454673 Training loss: 2.8097918033599854
2025-12-09 12:10:07.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0021358037464526513 Training loss: 2.731369733810425
2025-12-09 12:10:07.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0020917264621358876 Training loss: 2.759659767150879
2025-12-09 12:10:07.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0020479881349703883 Training loss: 2.839604616165161
2025-12-09 12:10:07.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0020045938626261544 Training loss: 2.702310562133789
2025-12-09 12:10:07.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0019615487026738545 Training loss: 2.9167792797088623
2025-12-09 12:10:07.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0019188576719953632 Training loss: 2.8397274017333984
2025-12-09 12:10:07.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0018765257461990442 Training loss: 2.853736400604248
2025-12-09 12:10:08.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0018345578590398509 Training loss: 2.746478319168091
2025-12-09 12:10:08.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0017929589018443016 Training loss: 2.9488909244537354
2025-12-09 12:10:08.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0017517337229403945 Training loss: 2.678009271621704
2025-12-09 12:10:08.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.001710887127092548 Training loss: 2.924659013748169
2025-12-09 12:10:08.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0016704238749415956 Training loss: 2.763211965560913
2025-12-09 12:10:08.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0016303486824499457 Training loss: 2.807802677154541
2025-12-09 12:10:08.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0015906662203519412 Training loss: 2.9860126972198486
2025-12-09 12:10:08.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0015513811136094785 Training loss: 2.7866318225860596
2025-12-09 12:10:08.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.001512497940872986 Training loss: 2.5023550987243652
2025-12-09 12:10:08.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.001474021233947772 Training loss: 2.7475526332855225
2025-12-09 12:10:08.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0014359554772658551 Training loss: 2.775404453277588
2025-12-09 12:10:08.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0013983051073632995 Training loss: 2.6705129146575928
2025-12-09 12:10:08.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0013610745123631535 Training loss: 2.4461781978607178
2025-12-09 12:10:08.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0013242680314639993 Training loss: 2.654829740524292
2025-12-09 12:10:08.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0012878899544342326 Training loss: 2.651118040084839
2025-12-09 12:10:08.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0012519445211120978 Training loss: 2.286097288131714
2025-12-09 12:10:08.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0012164359209115233 Training loss: 2.4876368045806885
2025-12-09 12:10:08.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0011813682923338654 Training loss: 2.577550172805786
2025-12-09 12:10:08.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0011467457224855543 Training loss: 2.66644287109375
2025-12-09 12:10:08.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0011125722466017547 Training loss: 2.685816764831543
2025-12-09 12:10:08.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0010788518475760545 Training loss: 2.364914655685425
2025-12-09 12:10:08.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0010455884554962725 Training loss: 2.487959623336792
2025-12-09 12:10:08.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.001012785947186397 Training loss: 2.406618118286133
2025-12-09 12:10:08.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00098044814575475 Training loss: 2.426147699356079
2025-12-09 12:10:08.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0009485788201484125 Training loss: 2.661572217941284
2025-12-09 12:10:08.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0009171816847139447 Training loss: 2.466866970062256
2025-12-09 12:10:08.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.000886260398764494 Training loss: 2.5715768337249756
2025-12-09 12:10:08.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0008558185661532941 Training loss: 2.6854443550109863
2025-12-09 12:10:08.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.000825859734853645 Training loss: 2.5296905040740967
2025-12-09 12:10:08.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0007963873965453961 Training loss: 2.595163345336914
2025-12-09 12:10:08.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.000767404986207999 Training loss: 2.4914751052856445
2025-12-09 12:10:08.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0007389158817201541 Training loss: 2.542816162109375
2025-12-09 12:10:08.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0007109234034661288 Training loss: 2.297574996948242
2025-12-09 12:10:09.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0006834308139487672 Training loss: 2.4554057121276855
2025-12-09 12:10:09.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0006564413174092443 Training loss: 2.517836332321167
2025-12-09 12:10:09.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0006299580594536214 Training loss: 2.476773738861084
2025-12-09 12:10:09.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0006039841266862189 Training loss: 2.6756651401519775
2025-12-09 12:10:09.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0005785225463498828 Training loss: 2.7046191692352295
2025-12-09 12:10:09.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0005535762859731547 Training loss: 2.682389736175537
2025-12-09 12:10:09.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0005291482530244179 Training loss: 2.313730001449585
2025-12-09 12:10:09.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0005052412945730239 Training loss: 2.6381802558898926
2025-12-09 12:10:09.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00048185819695747425 Training loss: 3.046520233154297
2025-12-09 12:10:09.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00045900168546067266 Training loss: 2.66825008392334
2025-12-09 12:10:09.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00043667442399229983 Training loss: 2.5858383178710938
2025-12-09 12:10:09.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.0004148790147783288 Training loss: 2.6120762825012207
2025-12-09 12:10:09.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00039361799805774536 Training loss: 2.5570764541625977
2025-12-09 12:10:09.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0003728938517864794 Training loss: 2.7587404251098633
2025-12-09 12:10:09.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.00035270899134860366 Training loss: 2.3748087882995605
2025-12-09 12:10:09.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00033306576927482126 Training loss: 2.333617687225342
2025-12-09 12:10:09.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0003139664749682825 Training loss: 2.830509662628174
2025-12-09 12:10:09.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002954133344377524 Training loss: 2.4305169582366943
2025-12-09 12:10:09.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.00027740851003817346 Training loss: 2.509709596633911
2025-12-09 12:10:09.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00025995410021864785 Training loss: 2.5977673530578613
2025-12-09 12:10:09.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0002430521392778573 Training loss: 2.565417528152466
2025-12-09 12:10:09.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.00022670459712697378 Training loss: 2.5876688957214355
2025-12-09 12:10:09.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0002109133790600648 Training loss: 2.2121081352233887
2025-12-09 12:10:09.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00019568032553203218 Training loss: 2.428361415863037
2025-12-09 12:10:09.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0001810072119441103 Training loss: 2.8054234981536865
2025-12-09 12:10:09.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.00016689574843694433 Training loss: 2.3395142555236816
2025-12-09 12:10:09.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.000153347579691272 Training loss: 2.76367449760437
2025-12-09 12:10:09.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0001403642847362402 Training loss: 2.7854692935943604
2025-12-09 12:10:09.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00012794737676536993 Training loss: 2.6557304859161377
2025-12-09 12:10:09.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00011609830296019141 Training loss: 2.3366920948028564
2025-12-09 12:10:09.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0001048184443215816 Training loss: 2.4964990615844727
2025-12-09 12:10:09.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880474e-05 Training loss: 2.3588688373565674
2025-12-09 12:10:09.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629208e-05 Training loss: 2.579059600830078
2025-12-09 12:10:09.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.44069733781677e-05 Training loss: 2.614393711090088
2025-12-09 12:10:10.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.54164563305465e-05 Training loss: 2.548208475112915
2025-12-09 12:10:10.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.7001061381606875e-05 Training loss: 2.483823537826538
2025-12-09 12:10:10.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946692e-05 Training loss: 2.7388455867767334
2025-12-09 12:10:10.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.1899493867874615e-05 Training loss: 2.7264091968536377
2025-12-09 12:10:10.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.521508137971807e-05 Training loss: 2.303849220275879
2025-12-09 12:10:10.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378874e-05 Training loss: 2.7042133808135986
2025-12-09 12:10:10.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.358289416693027e-05 Training loss: 2.736959934234619
2025-12-09 12:10:10.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.863647516520017e-05 Training loss: 2.490659475326538
2025-12-09 12:10:10.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.4270630434701781e-05 Training loss: 2.360684394836426
2025-12-09 12:10:10.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441756e-05 Training loss: 2.4220926761627197
2025-12-09 12:10:10.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.2826314066154475e-06 Training loss: 2.611109495162964
2025-12-09 12:10:10.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.661291555196345e-06 Training loss: 2.7542381286621094
2025-12-09 12:10:10.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253337e-06 Training loss: 2.6955223083496094
2025-12-09 12:10:10.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-06 Training loss: 2.4426469802856445
2025-12-09 12:10:10.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265825e-07 Training loss: 2.48650860786438
2025-12-09 12:10:10.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 2.494539976119995
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:10:23.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 4.907971382141113
2025-12-09 12:10:24.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 4.957444190979004
2025-12-09 12:10:24.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 4.860047817230225
2025-12-09 12:10:24.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 4.812797546386719
2025-12-09 12:10:24.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 4.787594795227051
2025-12-09 12:10:24.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 4.766610145568848
2025-12-09 12:10:24.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 4.905178070068359
2025-12-09 12:10:24.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 4.8737993240356445
2025-12-09 12:10:24.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 4.873974800109863
2025-12-09 12:10:24.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 4.973836898803711
2025-12-09 12:10:24.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 4.882420063018799
2025-12-09 12:10:24.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 4.875424385070801
2025-12-09 12:10:24.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 4.886445999145508
2025-12-09 12:10:24.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 4.794862270355225
2025-12-09 12:10:24.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 4.873569965362549
2025-12-09 12:10:24.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 4.9359331130981445
2025-12-09 12:10:24.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 4.936360836029053
2025-12-09 12:10:24.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 4.909849643707275
2025-12-09 12:10:24.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 4.868920803070068
2025-12-09 12:10:24.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 4.84190034866333
2025-12-09 12:10:24.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 4.921121597290039
2025-12-09 12:10:24.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 4.852879047393799
2025-12-09 12:10:24.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 4.8064866065979
2025-12-09 12:10:24.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 4.8459272384643555
2025-12-09 12:10:24.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 4.865230083465576
2025-12-09 12:10:24.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 4.869145393371582
2025-12-09 12:10:24.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 4.963872909545898
2025-12-09 12:10:24.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 4.909871578216553
2025-12-09 12:10:24.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 4.998039245605469
2025-12-09 12:10:24.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 4.832668304443359
2025-12-09 12:10:24.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 4.767815113067627
2025-12-09 12:10:24.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 4.8122477531433105
2025-12-09 12:10:24.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 4.96865701675415
2025-12-09 12:10:24.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 4.926826477050781
2025-12-09 12:10:24.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 4.875787258148193
2025-12-09 12:10:24.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 4.922014236450195
2025-12-09 12:10:24.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 4.908091068267822
2025-12-09 12:10:24.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 4.743923187255859
2025-12-09 12:10:24.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 4.972306251525879
2025-12-09 12:10:24.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 4.8860931396484375
2025-12-09 12:10:24.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 4.810646057128906
2025-12-09 12:10:24.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 4.908370494842529
2025-12-09 12:10:24.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 4.816158294677734
2025-12-09 12:10:24.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 4.862439155578613
2025-12-09 12:10:24.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 4.916985034942627
2025-12-09 12:10:24.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 4.819779872894287
2025-12-09 12:10:24.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 4.842116832733154
2025-12-09 12:10:24.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 4.789724826812744
2025-12-09 12:10:24.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 4.933648109436035
2025-12-09 12:10:24.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 4.885436534881592
2025-12-09 12:10:24.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 4.8044915199279785
2025-12-09 12:10:24.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 4.865243911743164
2025-12-09 12:10:24.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 4.76973819732666
2025-12-09 12:10:24.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 4.860470294952393
2025-12-09 12:10:24.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 4.819348335266113
2025-12-09 12:10:24.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 4.7875189781188965
2025-12-09 12:10:24.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 4.818707466125488
2025-12-09 12:10:24.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 4.819823741912842
2025-12-09 12:10:24.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 4.7914509773254395
2025-12-09 12:10:24.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 4.771718502044678
2025-12-09 12:10:24.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 4.8953680992126465
2025-12-09 12:10:24.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 4.774362564086914
2025-12-09 12:10:24.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 4.898242950439453
2025-12-09 12:10:24.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 4.780333518981934
2025-12-09 12:10:24.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 4.8127899169921875
2025-12-09 12:10:24.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 4.782648086547852
2025-12-09 12:10:24.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 4.788565158843994
2025-12-09 12:10:24.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 4.861630439758301
2025-12-09 12:10:24.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 4.780557155609131
2025-12-09 12:10:24.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 4.912302017211914
2025-12-09 12:10:24.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 4.89421272277832
2025-12-09 12:10:24.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 4.800652503967285
2025-12-09 12:10:24.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 4.7626519203186035
2025-12-09 12:10:24.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 4.815915107727051
2025-12-09 12:10:24.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 4.8003644943237305
2025-12-09 12:10:24.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 4.97343635559082
2025-12-09 12:10:24.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 4.841297149658203
2025-12-09 12:10:24.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 4.858682632446289
2025-12-09 12:10:24.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 4.8419718742370605
2025-12-09 12:10:24.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 4.8026604652404785
2025-12-09 12:10:24.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 4.761198997497559
2025-12-09 12:10:24.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 4.759632110595703
2025-12-09 12:10:24.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 4.846512794494629
2025-12-09 12:10:24.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 4.883699417114258
2025-12-09 12:10:24.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 4.714009761810303
2025-12-09 12:10:24.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 4.820606708526611
2025-12-09 12:10:24.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 4.845696449279785
2025-12-09 12:10:24.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 4.815609455108643
2025-12-09 12:10:24.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 4.821203231811523
2025-12-09 12:10:24.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 4.8105058670043945
2025-12-09 12:10:24.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 4.743127346038818
2025-12-09 12:10:24.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 4.82726526260376
2025-12-09 12:10:24.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 4.778238773345947
2025-12-09 12:10:24.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 4.829257965087891
2025-12-09 12:10:24.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 4.665696144104004
2025-12-09 12:10:24.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 4.808218002319336
2025-12-09 12:10:24.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 4.847850799560547
2025-12-09 12:10:24.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 4.774447917938232
2025-12-09 12:10:24.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 4.919154644012451
2025-12-09 12:10:24.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 4.86950159072876
2025-12-09 12:10:24.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.999708626830618e-05 Training loss: 4.836139678955078
2025-12-09 12:10:24.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 9.998834541281798e-05 Training loss: 4.779212951660156
2025-12-09 12:10:24.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 9.997377845227576e-05 Training loss: 4.680291652679443
2025-12-09 12:10:24.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 9.995338708444804e-05 Training loss: 4.763490200042725
2025-12-09 12:10:24.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 9.992717368593385e-05 Training loss: 4.742900848388672
2025-12-09 12:10:24.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 9.989514131188559e-05 Training loss: 4.715579986572266
2025-12-09 12:10:24.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 9.985729369565299e-05 Training loss: 4.853318214416504
2025-12-09 12:10:24.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.9813635248348e-05 Training loss: 4.80556583404541
2025-12-09 12:10:24.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 9.97641710583307e-05 Training loss: 4.703510761260986
2025-12-09 12:10:24.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 9.970890689061622e-05 Training loss: 4.742842197418213
2025-12-09 12:10:24.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 9.964784918620282e-05 Training loss: 4.698842525482178
2025-12-09 12:10:25.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 9.958100506132127e-05 Training loss: 4.804790019989014
2025-12-09 12:10:25.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 9.950838230660534e-05 Training loss: 4.837248802185059
2025-12-09 12:10:25.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 9.942998938618394e-05 Training loss: 4.775973796844482
2025-12-09 12:10:25.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 9.934583543669453e-05 Training loss: 4.633072853088379
2025-12-09 12:10:25.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 9.925593026621833e-05 Training loss: 4.870121479034424
2025-12-09 12:10:25.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 9.916028435313708e-05 Training loss: 4.6728997230529785
2025-12-09 12:10:25.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 9.905890884491195e-05 Training loss: 4.803297519683838
2025-12-09 12:10:25.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 9.895181555678418e-05 Training loss: 4.78734827041626
2025-12-09 12:10:25.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 9.883901697039808e-05 Training loss: 4.7867817878723145
2025-12-09 12:10:25.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 9.872052623234632e-05 Training loss: 4.791261196136475
2025-12-09 12:10:25.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 9.85963571526376e-05 Training loss: 4.752967834472656
2025-12-09 12:10:25.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 9.846652420308728e-05 Training loss: 4.685964584350586
2025-12-09 12:10:25.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 9.833104251563056e-05 Training loss: 4.652489185333252
2025-12-09 12:10:25.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 9.818992788055889e-05 Training loss: 4.677829265594482
2025-12-09 12:10:25.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 9.80431967446797e-05 Training loss: 4.766981601715088
2025-12-09 12:10:25.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 9.789086620939936e-05 Training loss: 4.801562786102295
2025-12-09 12:10:25.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 9.773295402873026e-05 Training loss: 4.734707832336426
2025-12-09 12:10:25.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 9.756947860722143e-05 Training loss: 4.710258483886719
2025-12-09 12:10:25.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 9.740045899781352e-05 Training loss: 4.816636085510254
2025-12-09 12:10:25.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 9.722591489961827e-05 Training loss: 4.819004058837891
2025-12-09 12:10:25.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 9.70458666556225e-05 Training loss: 4.784666538238525
2025-12-09 12:10:25.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 9.686033525031719e-05 Training loss: 4.77259635925293
2025-12-09 12:10:25.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 9.66693423072518e-05 Training loss: 4.678206443786621
2025-12-09 12:10:25.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 9.647291008651398e-05 Training loss: 4.707163333892822
2025-12-09 12:10:25.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 9.627106148213522e-05 Training loss: 4.866433143615723
2025-12-09 12:10:25.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 9.606382001942255e-05 Training loss: 4.712533473968506
2025-12-09 12:10:25.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 9.585120985221671e-05 Training loss: 4.6754560470581055
2025-12-09 12:10:25.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 9.563325576007701e-05 Training loss: 4.6743059158325195
2025-12-09 12:10:25.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 9.540998314539328e-05 Training loss: 4.748734951019287
2025-12-09 12:10:25.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 9.518141803042527e-05 Training loss: 4.7048258781433105
2025-12-09 12:10:25.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 9.494758705426978e-05 Training loss: 4.583733558654785
2025-12-09 12:10:25.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 9.470851746975582e-05 Training loss: 4.688715934753418
2025-12-09 12:10:25.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 9.446423714026846e-05 Training loss: 4.661032676696777
2025-12-09 12:10:25.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 9.421477453650118e-05 Training loss: 4.718573570251465
2025-12-09 12:10:25.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 9.396015873313781e-05 Training loss: 4.734680652618408
2025-12-09 12:10:25.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 9.37004194054638e-05 Training loss: 4.615506649017334
2025-12-09 12:10:25.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 9.343558682590756e-05 Training loss: 4.74954891204834
2025-12-09 12:10:25.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 9.316569186051234e-05 Training loss: 4.6720075607299805
2025-12-09 12:10:25.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 9.289076596533872e-05 Training loss: 4.754241943359375
2025-12-09 12:10:25.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 9.261084118279847e-05 Training loss: 4.637587547302246
2025-12-09 12:10:25.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 9.232595013792002e-05 Training loss: 4.7117791175842285
2025-12-09 12:10:25.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 9.203612603454604e-05 Training loss: 4.792900085449219
2025-12-09 12:10:25.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 9.174140265146356e-05 Training loss: 4.7995195388793945
2025-12-09 12:10:25.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 9.144181433846707e-05 Training loss: 4.756005764007568
2025-12-09 12:10:25.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 9.113739601235507e-05 Training loss: 4.661434650421143
2025-12-09 12:10:25.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 9.082818315286055e-05 Training loss: 4.6773905754089355
2025-12-09 12:10:25.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 9.051421179851588e-05 Training loss: 4.694996356964111
2025-12-09 12:10:25.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 9.01955185424525e-05 Training loss: 4.65143346786499
2025-12-09 12:10:25.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 8.987214052813604e-05 Training loss: 4.758235931396484
2025-12-09 12:10:25.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 8.954411544503729e-05 Training loss: 4.612920761108398
2025-12-09 12:10:25.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 8.921148152423946e-05 Training loss: 4.688267230987549
2025-12-09 12:10:25.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 8.887427753398248e-05 Training loss: 4.7402729988098145
2025-12-09 12:10:25.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 8.853254277514446e-05 Training loss: 4.722153663635254
2025-12-09 12:10:25.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 8.818631707666135e-05 Training loss: 4.748049736022949
2025-12-09 12:10:25.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 8.783564079088477e-05 Training loss: 4.675985336303711
2025-12-09 12:10:25.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 8.748055478887904e-05 Training loss: 4.7215962409973145
2025-12-09 12:10:25.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 8.712110045565768e-05 Training loss: 4.645313739776611
2025-12-09 12:10:25.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 8.675731968536002e-05 Training loss: 4.783406734466553
2025-12-09 12:10:25.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 8.638925487636848e-05 Training loss: 4.615962982177734
2025-12-09 12:10:25.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 8.6016948926367e-05 Training loss: 4.695751190185547
2025-12-09 12:10:25.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 8.564044522734147e-05 Training loss: 4.677157878875732
2025-12-09 12:10:25.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 8.52597876605223e-05 Training loss: 4.736030101776123
2025-12-09 12:10:25.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 8.487502059127015e-05 Training loss: 4.676167964935303
2025-12-09 12:10:25.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 8.448618886390522e-05 Training loss: 4.761224746704102
2025-12-09 12:10:25.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 8.40933377964806e-05 Training loss: 4.604290962219238
2025-12-09 12:10:25.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 8.369651317550054e-05 Training loss: 4.641727924346924
2025-12-09 12:10:25.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 8.329576125058406e-05 Training loss: 4.6582112312316895
2025-12-09 12:10:25.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 8.289112872907454e-05 Training loss: 4.688159942626953
2025-12-09 12:10:25.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 8.248266277059607e-05 Training loss: 4.746052265167236
2025-12-09 12:10:25.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 8.2070410981557e-05 Training loss: 4.784171104431152
2025-12-09 12:10:25.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 8.16544214096015e-05 Training loss: 4.748006343841553
2025-12-09 12:10:25.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 8.123474253800957e-05 Training loss: 4.7680888175964355
2025-12-09 12:10:25.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 8.081142328004637e-05 Training loss: 4.617072582244873
2025-12-09 12:10:25.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 8.038451297326145e-05 Training loss: 4.602840423583984
2025-12-09 12:10:25.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 7.995406137373846e-05 Training loss: 4.603789329528809
2025-12-09 12:10:25.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 7.952011865029614e-05 Training loss: 4.646778106689453
2025-12-09 12:10:25.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 7.908273537864113e-05 Training loss: 4.648396968841553
2025-12-09 12:10:25.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 7.86419625354735e-05 Training loss: 4.671241760253906
2025-12-09 12:10:25.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 7.819785149254532e-05 Training loss: 4.669809341430664
2025-12-09 12:10:25.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 7.77504540106735e-05 Training loss: 4.6280107498168945
2025-12-09 12:10:25.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 7.729982223370691e-05 Training loss: 4.659243106842041
2025-12-09 12:10:25.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 7.68460086824492e-05 Training loss: 4.730655193328857
2025-12-09 12:10:25.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 7.638906624853743e-05 Training loss: 4.659505844116211
2025-12-09 12:10:25.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 7.592904818827775e-05 Training loss: 4.689550399780273
2025-12-09 12:10:25.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 7.546600811643816e-05 Training loss: 4.658249855041504
2025-12-09 12:10:25.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 7.500000000000001e-05 Training loss: 4.814933776855469
2025-12-09 12:10:25.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 7.453107815186803e-05 Training loss: 4.679050445556641
2025-12-09 12:10:25.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 7.405929722454026e-05 Training loss: 4.591284275054932
2025-12-09 12:10:25.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 7.358471220373832e-05 Training loss: 4.5713582038879395
2025-12-09 12:10:25.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 7.310737840199885e-05 Training loss: 4.732172966003418
2025-12-09 12:10:25.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 7.262735145222696e-05 Training loss: 4.599201679229736
2025-12-09 12:10:25.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 7.214468730121208e-05 Training loss: 4.688650131225586
2025-12-09 12:10:25.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 7.165944220310767e-05 Training loss: 4.648667335510254
2025-12-09 12:10:25.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 7.117167271287453e-05 Training loss: 4.586517810821533
2025-12-09 12:10:25.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 7.068143567968957e-05 Training loss: 4.62443208694458
2025-12-09 12:10:25.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 7.018878824032009e-05 Training loss: 4.59727144241333
2025-12-09 12:10:25.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 6.969378781246436e-05 Training loss: 4.623604774475098
2025-12-09 12:10:25.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 6.919649208805981e-05 Training loss: 4.6200432777404785
2025-12-09 12:10:25.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 6.869695902655897e-05 Training loss: 4.647836208343506
2025-12-09 12:10:25.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 6.819524684817438e-05 Training loss: 4.652505874633789
2025-12-09 12:10:25.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 6.769141402709305e-05 Training loss: 4.689259052276611
2025-12-09 12:10:25.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 6.718551928466132e-05 Training loss: 4.654417991638184
2025-12-09 12:10:25.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 6.667762158254104e-05 Training loss: 4.692049980163574
2025-12-09 12:10:25.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 6.616778011583743e-05 Training loss: 4.641163349151611
2025-12-09 12:10:25.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 6.565605430620013e-05 Training loss: 4.6899847984313965
2025-12-09 12:10:25.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 6.514250379489753e-05 Training loss: 4.682065486907959
2025-12-09 12:10:25.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 6.462718843586571e-05 Training loss: 4.6492018699646
2025-12-09 12:10:25.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 6.411016828873239e-05 Training loss: 4.649108409881592
2025-12-09 12:10:25.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 6.359150361181715e-05 Training loss: 4.617119312286377
2025-12-09 12:10:25.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 6.307125485510828e-05 Training loss: 4.6674394607543945
2025-12-09 12:10:26.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 6.254948265321744e-05 Training loss: 4.628439426422119
2025-12-09 12:10:26.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 6.202624781831268e-05 Training loss: 4.673192977905273
2025-12-09 12:10:26.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 6.150161133303089e-05 Training loss: 4.620997428894043
2025-12-09 12:10:26.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 6.0975634343370256e-05 Training loss: 4.73552942276001
2025-12-09 12:10:26.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 6.044837815156377e-05 Training loss: 4.614482402801514
2025-12-09 12:10:26.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 5.99199042089345e-05 Training loss: 4.7403998374938965
2025-12-09 12:10:26.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 5.939027410873351e-05 Training loss: 4.59008264541626
2025-12-09 12:10:26.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 5.885954957896115e-05 Training loss: 4.627496719360352
2025-12-09 12:10:26.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 5.832779247517273e-05 Training loss: 4.60443639755249
2025-12-09 12:10:26.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 5.779506477326933e-05 Training loss: 4.622897624969482
2025-12-09 12:10:26.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 5.726142856227452e-05 Training loss: 4.680037021636963
2025-12-09 12:10:26.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 5.672694603709794e-05 Training loss: 4.591458320617676
2025-12-09 12:10:26.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 5.619167949128652e-05 Training loss: 4.5417656898498535
2025-12-09 12:10:26.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 5.565569130976422e-05 Training loss: 4.714883327484131
2025-12-09 12:10:26.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 5.5119043961561136e-05 Training loss: 4.604859352111816
2025-12-09 12:10:26.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 5.458179999253275e-05 Training loss: 4.662763595581055
2025-12-09 12:10:26.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 5.4044022018070214e-05 Training loss: 4.610616683959961
2025-12-09 12:10:26.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 5.3505772715802704e-05 Training loss: 4.750649452209473
2025-12-09 12:10:26.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 5.296711481829226e-05 Training loss: 4.608469486236572
2025-12-09 12:10:26.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 5.242811110572242e-05 Training loss: 4.633726596832275
2025-12-09 12:10:26.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 5.188882439858117e-05 Training loss: 4.669454574584961
2025-12-09 12:10:26.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 5.134931755033936e-05 Training loss: 4.705146312713623
2025-12-09 12:10:26.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 5.080965344012508e-05 Training loss: 4.589592456817627
2025-12-09 12:10:26.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 5.0269894965395225e-05 Training loss: 4.667049407958984
2025-12-09 12:10:26.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 4.973010503460479e-05 Training loss: 4.696052551269531
2025-12-09 12:10:26.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 4.919034655987493e-05 Training loss: 4.647568702697754
2025-12-09 12:10:26.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 4.865068244966066e-05 Training loss: 4.681477069854736
2025-12-09 12:10:26.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 4.8111175601418844e-05 Training loss: 4.618325710296631
2025-12-09 12:10:26.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 4.7571888894277604e-05 Training loss: 4.627194404602051
2025-12-09 12:10:26.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 4.703288518170774e-05 Training loss: 4.585501194000244
2025-12-09 12:10:26.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 4.6494227284197294e-05 Training loss: 4.612973213195801
2025-12-09 12:10:26.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 4.59559779819298e-05 Training loss: 4.610203266143799
2025-12-09 12:10:26.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 4.541820000746727e-05 Training loss: 4.603785037994385
2025-12-09 12:10:26.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 4.4880956038438876e-05 Training loss: 4.627879619598389
2025-12-09 12:10:26.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 4.434430869023579e-05 Training loss: 4.599803924560547
2025-12-09 12:10:26.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 4.38083205087135e-05 Training loss: 4.544999599456787
2025-12-09 12:10:26.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 4.3273053962902076e-05 Training loss: 4.562901020050049
2025-12-09 12:10:26.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 4.27385714377255e-05 Training loss: 4.708462715148926
2025-12-09 12:10:26.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 4.220493522673067e-05 Training loss: 4.721587181091309
2025-12-09 12:10:26.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 4.1672207524827275e-05 Training loss: 4.671030521392822
2025-12-09 12:10:26.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 4.114045042103887e-05 Training loss: 4.648709774017334
2025-12-09 12:10:26.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 4.06097258912665e-05 Training loss: 4.767886638641357
2025-12-09 12:10:26.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 4.0080095791065505e-05 Training loss: 4.655065059661865
2025-12-09 12:10:26.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 3.955162184843625e-05 Training loss: 4.7029290199279785
2025-12-09 12:10:26.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 3.902436565662977e-05 Training loss: 4.620434761047363
2025-12-09 12:10:26.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 3.849838866696913e-05 Training loss: 4.644344806671143
2025-12-09 12:10:26.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 3.7973752181687335e-05 Training loss: 4.581216812133789
2025-12-09 12:10:26.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 3.745051734678256e-05 Training loss: 4.595482349395752
2025-12-09 12:10:26.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 3.692874514489173e-05 Training loss: 4.55275821685791
2025-12-09 12:10:26.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 3.640849638818286e-05 Training loss: 4.641238689422607
2025-12-09 12:10:26.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 3.588983171126762e-05 Training loss: 4.717303276062012
2025-12-09 12:10:26.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 3.53728115641343e-05 Training loss: 4.582077980041504
2025-12-09 12:10:26.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 3.4857496205102474e-05 Training loss: 4.605231761932373
2025-12-09 12:10:26.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 3.434394569379988e-05 Training loss: 4.609257698059082
2025-12-09 12:10:26.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 3.3832219884162585e-05 Training loss: 4.606118202209473
2025-12-09 12:10:26.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 3.332237841745898e-05 Training loss: 4.614644527435303
2025-12-09 12:10:26.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 3.281448071533867e-05 Training loss: 4.670120716094971
2025-12-09 12:10:26.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 3.2308585972906966e-05 Training loss: 4.637135982513428
2025-12-09 12:10:26.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 3.180475315182563e-05 Training loss: 4.616158485412598
2025-12-09 12:10:26.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 3.130304097344103e-05 Training loss: 4.685121059417725
2025-12-09 12:10:26.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 3.080350791194019e-05 Training loss: 4.731886386871338
2025-12-09 12:10:26.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 3.0306212187535653e-05 Training loss: 4.5429253578186035
2025-12-09 12:10:26.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 2.9811211759679924e-05 Training loss: 4.6024651527404785
2025-12-09 12:10:26.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 2.9318564320310444e-05 Training loss: 4.540170192718506
2025-12-09 12:10:26.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 2.882832728712551e-05 Training loss: 4.618969917297363
2025-12-09 12:10:26.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 2.8340557796892354e-05 Training loss: 4.617508888244629
2025-12-09 12:10:26.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 2.7855312698787904e-05 Training loss: 4.699535846710205
2025-12-09 12:10:26.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 2.737264854777306e-05 Training loss: 4.618800163269043
2025-12-09 12:10:26.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 2.6892621598001156e-05 Training loss: 4.586756706237793
2025-12-09 12:10:26.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 2.6415287796261706e-05 Training loss: 4.5034284591674805
2025-12-09 12:10:26.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 2.5940702775459747e-05 Training loss: 4.695474624633789
2025-12-09 12:10:26.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 2.5468921848131983e-05 Training loss: 4.563948631286621
2025-12-09 12:10:26.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 2.500000000000001e-05 Training loss: 4.628469944000244
2025-12-09 12:10:26.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 2.4533991883561868e-05 Training loss: 4.701230049133301
2025-12-09 12:10:26.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 2.407095181172227e-05 Training loss: 4.503653049468994
2025-12-09 12:10:26.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 2.3610933751462553e-05 Training loss: 4.602452754974365
2025-12-09 12:10:26.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 2.315399131755081e-05 Training loss: 4.569428443908691
2025-12-09 12:10:26.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 2.2700177766293096e-05 Training loss: 4.643949031829834
2025-12-09 12:10:26.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 2.2249545989326514e-05 Training loss: 4.5848870277404785
2025-12-09 12:10:26.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 2.180214850745467e-05 Training loss: 4.583914756774902
2025-12-09 12:10:26.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 2.1358037464526515e-05 Training loss: 4.646420955657959
2025-12-09 12:10:26.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 2.091726462135888e-05 Training loss: 4.602053642272949
2025-12-09 12:10:26.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 2.0479881349703883e-05 Training loss: 4.509942531585693
2025-12-09 12:10:26.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 2.0045938626261546e-05 Training loss: 4.668471336364746
2025-12-09 12:10:26.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 1.9615487026738543e-05 Training loss: 4.669211387634277
2025-12-09 12:10:26.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 1.9188576719953633e-05 Training loss: 4.755727291107178
2025-12-09 12:10:26.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 1.8765257461990442e-05 Training loss: 4.592434883117676
2025-12-09 12:10:26.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 1.834557859039851e-05 Training loss: 4.642806053161621
2025-12-09 12:10:26.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 1.7929589018443016e-05 Training loss: 4.635798931121826
2025-12-09 12:10:26.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 1.7517337229403946e-05 Training loss: 4.597878456115723
2025-12-09 12:10:26.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 1.710887127092548e-05 Training loss: 4.6483259201049805
2025-12-09 12:10:26.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 1.6704238749415957e-05 Training loss: 4.577254295349121
2025-12-09 12:10:26.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 1.6303486824499458e-05 Training loss: 4.620059490203857
2025-12-09 12:10:26.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 1.5906662203519412e-05 Training loss: 4.671082019805908
2025-12-09 12:10:26.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 1.5513811136094787e-05 Training loss: 4.605955600738525
2025-12-09 12:10:26.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 1.5124979408729861e-05 Training loss: 4.6059794425964355
2025-12-09 12:10:26.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 1.4740212339477721e-05 Training loss: 4.576930046081543
2025-12-09 12:10:26.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 1.4359554772658552e-05 Training loss: 4.648825645446777
2025-12-09 12:10:26.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 1.3983051073632997e-05 Training loss: 4.65449333190918
2025-12-09 12:10:26.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 1.3610745123631535e-05 Training loss: 4.529854774475098
2025-12-09 12:10:26.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 1.3242680314639993e-05 Training loss: 4.583434581756592
2025-12-09 12:10:26.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 1.2878899544342327e-05 Training loss: 4.615452289581299
2025-12-09 12:10:26.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 1.2519445211120979e-05 Training loss: 4.6322407722473145
2025-12-09 12:10:26.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 1.2164359209115234e-05 Training loss: 4.610702037811279
2025-12-09 12:10:26.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 1.1813682923338653e-05 Training loss: 4.517981052398682
2025-12-09 12:10:26.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 1.1467457224855544e-05 Training loss: 4.627261161804199
2025-12-09 12:10:26.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 1.1125722466017547e-05 Training loss: 4.587721347808838
2025-12-09 12:10:26.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 1.0788518475760545e-05 Training loss: 4.629973888397217
2025-12-09 12:10:26.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 1.0455884554962725e-05 Training loss: 4.560359954833984
2025-12-09 12:10:26.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 1.012785947186397e-05 Training loss: 4.530886650085449
2025-12-09 12:10:26.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.804481457547498e-06 Training loss: 4.648863315582275
2025-12-09 12:10:26.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.485788201484126e-06 Training loss: 4.7101030349731445
2025-12-09 12:10:27.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.171816847139448e-06 Training loss: 4.626212120056152
2025-12-09 12:10:27.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 8.86260398764494e-06 Training loss: 4.713550090789795
2025-12-09 12:10:27.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 8.558185661532941e-06 Training loss: 4.639843463897705
2025-12-09 12:10:27.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 8.25859734853645e-06 Training loss: 4.659512042999268
2025-12-09 12:10:27.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 7.96387396545396e-06 Training loss: 4.585109710693359
2025-12-09 12:10:27.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 7.67404986207999e-06 Training loss: 4.500757217407227
2025-12-09 12:10:27.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 7.389158817201542e-06 Training loss: 4.655235290527344
2025-12-09 12:10:27.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 7.109234034661289e-06 Training loss: 4.594937324523926
2025-12-09 12:10:27.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 6.8343081394876715e-06 Training loss: 4.579573631286621
2025-12-09 12:10:27.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 6.564413174092443e-06 Training loss: 4.648648262023926
2025-12-09 12:10:27.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 6.299580594536214e-06 Training loss: 4.7948384284973145
2025-12-09 12:10:27.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 6.0398412668621895e-06 Training loss: 4.654236793518066
2025-12-09 12:10:27.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 5.785225463498828e-06 Training loss: 4.674225807189941
2025-12-09 12:10:27.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 5.535762859731547e-06 Training loss: 4.661069393157959
2025-12-09 12:10:27.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 5.291482530244179e-06 Training loss: 4.595643043518066
2025-12-09 12:10:27.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 5.05241294573024e-06 Training loss: 4.613334655761719
2025-12-09 12:10:27.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 4.818581969574742e-06 Training loss: 4.636871814727783
2025-12-09 12:10:27.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 4.590016854606727e-06 Training loss: 4.503238201141357
2025-12-09 12:10:27.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 4.366744239922998e-06 Training loss: 4.564212799072266
2025-12-09 12:10:27.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 4.148790147783288e-06 Training loss: 4.568807601928711
2025-12-09 12:10:27.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 3.936179980577453e-06 Training loss: 4.500736713409424
2025-12-09 12:10:27.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 3.728938517864794e-06 Training loss: 4.598309516906738
2025-12-09 12:10:27.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 3.527089913486037e-06 Training loss: 4.732204437255859
2025-12-09 12:10:27.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 3.3306576927482126e-06 Training loss: 4.569018840789795
2025-12-09 12:10:27.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 3.1396647496828247e-06 Training loss: 4.59356164932251
2025-12-09 12:10:27.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 2.9541333443775243e-06 Training loss: 4.637939453125
2025-12-09 12:10:27.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 2.774085100381735e-06 Training loss: 4.660507678985596
2025-12-09 12:10:27.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 2.5995410021864787e-06 Training loss: 4.59423303604126
2025-12-09 12:10:27.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 2.430521392778573e-06 Training loss: 4.625412940979004
2025-12-09 12:10:27.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 2.2670459712697377e-06 Training loss: 4.6414570808410645
2025-12-09 12:10:27.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 2.1091337906006482e-06 Training loss: 4.566089153289795
2025-12-09 12:10:27.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 1.956803255320322e-06 Training loss: 4.572930812835693
2025-12-09 12:10:27.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 1.810072119441103e-06 Training loss: 4.632882118225098
2025-12-09 12:10:27.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 1.6689574843694433e-06 Training loss: 4.558849334716797
2025-12-09 12:10:27.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 1.53347579691272e-06 Training loss: 4.64373254776001
2025-12-09 12:10:27.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 1.4036428473624019e-06 Training loss: 4.705921649932861
2025-12-09 12:10:27.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 1.2794737676536994e-06 Training loss: 4.617335319519043
2025-12-09 12:10:27.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 1.1609830296019143e-06 Training loss: 4.634812355041504
2025-12-09 12:10:27.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 1.0481844432158161e-06 Training loss: 4.667198181152344
2025-12-09 12:10:27.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880475e-07 Training loss: 4.675706386566162
2025-12-09 12:10:27.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629208e-07 Training loss: 4.683103084564209
2025-12-09 12:10:27.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.44069733781677e-07 Training loss: 4.584011077880859
2025-12-09 12:10:27.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.54164563305465e-07 Training loss: 4.677125930786133
2025-12-09 12:10:27.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.700106138160688e-07 Training loss: 4.628673553466797
2025-12-09 12:10:27.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946693e-07 Training loss: 4.544559955596924
2025-12-09 12:10:27.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.189949386787462e-07 Training loss: 4.642754554748535
2025-12-09 12:10:27.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.5215081379718074e-07 Training loss: 4.682863235473633
2025-12-09 12:10:27.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378877e-07 Training loss: 4.6006388664245605
2025-12-09 12:10:27.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.3582894166930268e-07 Training loss: 4.572535514831543
2025-12-09 12:10:27.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.8636475165200174e-07 Training loss: 4.512269496917725
2025-12-09 12:10:27.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.427063043470178e-07 Training loss: 4.5580267906188965
2025-12-09 12:10:27.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441757e-07 Training loss: 4.680985927581787
2025-12-09 12:10:27.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.282631406615447e-08 Training loss: 4.583568096160889
2025-12-09 12:10:27.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.661291555196345e-08 Training loss: 4.613864421844482
2025-12-09 12:10:27.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253337e-08 Training loss: 4.658463478088379
2025-12-09 12:10:27.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-08 Training loss: 4.633259296417236
2025-12-09 12:10:27.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265825e-09 Training loss: 4.598299980163574
2025-12-09 12:10:27.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 4.527580738067627
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:10:40.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 4.923524856567383
2025-12-09 12:10:40.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 4.917691707611084
2025-12-09 12:10:40.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 4.883355617523193
2025-12-09 12:10:41.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 4.857679843902588
2025-12-09 12:10:41.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 4.911452293395996
2025-12-09 12:10:41.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 4.790577411651611
2025-12-09 12:10:41.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 4.81107234954834
2025-12-09 12:10:41.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 5.0420613288879395
2025-12-09 12:10:41.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 4.823736190795898
2025-12-09 12:10:41.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 4.7508745193481445
2025-12-09 12:10:41.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 4.774764060974121
2025-12-09 12:10:41.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 4.824504852294922
2025-12-09 12:10:41.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 4.872519493103027
2025-12-09 12:10:41.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 4.871403217315674
2025-12-09 12:10:41.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 4.855477333068848
2025-12-09 12:10:41.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 4.762186527252197
2025-12-09 12:10:41.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 4.952995777130127
2025-12-09 12:10:41.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 4.851293087005615
2025-12-09 12:10:41.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 4.763354778289795
2025-12-09 12:10:41.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 4.8883771896362305
2025-12-09 12:10:41.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 4.834596633911133
2025-12-09 12:10:41.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 4.870547771453857
2025-12-09 12:10:41.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 4.844584941864014
2025-12-09 12:10:41.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 4.758405685424805
2025-12-09 12:10:41.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 4.775813579559326
2025-12-09 12:10:41.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 4.818179130554199
2025-12-09 12:10:41.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 4.897711277008057
2025-12-09 12:10:41.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 4.882034778594971
2025-12-09 12:10:41.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 4.85836124420166
2025-12-09 12:10:41.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 4.828201770782471
2025-12-09 12:10:41.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 4.7470784187316895
2025-12-09 12:10:41.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 4.815918922424316
2025-12-09 12:10:41.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 4.790370941162109
2025-12-09 12:10:41.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 4.888246059417725
2025-12-09 12:10:41.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 4.84777307510376
2025-12-09 12:10:41.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 4.85235071182251
2025-12-09 12:10:41.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 4.885766506195068
2025-12-09 12:10:41.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 4.980050086975098
2025-12-09 12:10:41.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 4.86566686630249
2025-12-09 12:10:41.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 4.730164051055908
2025-12-09 12:10:41.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 4.853756904602051
2025-12-09 12:10:41.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 4.852707386016846
2025-12-09 12:10:41.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 4.8217267990112305
2025-12-09 12:10:41.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 4.895575523376465
2025-12-09 12:10:41.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 4.766790866851807
2025-12-09 12:10:41.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 4.7584662437438965
2025-12-09 12:10:41.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 4.738488674163818
2025-12-09 12:10:41.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 4.805789947509766
2025-12-09 12:10:41.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 4.748051643371582
2025-12-09 12:10:41.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 4.765286922454834
2025-12-09 12:10:41.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 4.817471027374268
2025-12-09 12:10:41.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 4.886542797088623
2025-12-09 12:10:41.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 4.85871696472168
2025-12-09 12:10:41.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 4.670317649841309
2025-12-09 12:10:41.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 4.754739761352539
2025-12-09 12:10:41.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 4.794753551483154
2025-12-09 12:10:41.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 4.906783580780029
2025-12-09 12:10:41.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 4.767303943634033
2025-12-09 12:10:41.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 4.711618900299072
2025-12-09 12:10:41.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 4.751558780670166
2025-12-09 12:10:41.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 4.811636447906494
2025-12-09 12:10:41.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 4.859556674957275
2025-12-09 12:10:41.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 4.723453521728516
2025-12-09 12:10:41.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 4.827639579772949
2025-12-09 12:10:41.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 4.851753234863281
2025-12-09 12:10:41.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 4.779086589813232
2025-12-09 12:10:41.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 4.829927921295166
2025-12-09 12:10:41.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 4.795703887939453
2025-12-09 12:10:41.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 4.726868152618408
2025-12-09 12:10:41.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 4.7036237716674805
2025-12-09 12:10:41.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 4.649196624755859
2025-12-09 12:10:41.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 4.625326156616211
2025-12-09 12:10:41.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 4.670447826385498
2025-12-09 12:10:41.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 4.804738521575928
2025-12-09 12:10:41.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 4.772231101989746
2025-12-09 12:10:41.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 4.610632419586182
2025-12-09 12:10:41.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 4.751290321350098
2025-12-09 12:10:41.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 4.67846155166626
2025-12-09 12:10:41.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 4.8711652755737305
2025-12-09 12:10:41.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 4.733456134796143
2025-12-09 12:10:41.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 4.693757057189941
2025-12-09 12:10:41.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 4.723306179046631
2025-12-09 12:10:41.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 4.7204694747924805
2025-12-09 12:10:41.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 4.695163249969482
2025-12-09 12:10:41.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 4.82147741317749
2025-12-09 12:10:41.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 4.670745849609375
2025-12-09 12:10:41.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 4.7808451652526855
2025-12-09 12:10:41.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 4.671902179718018
2025-12-09 12:10:41.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 4.7642436027526855
2025-12-09 12:10:41.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 4.626420021057129
2025-12-09 12:10:41.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 4.681816101074219
2025-12-09 12:10:41.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 4.702230453491211
2025-12-09 12:10:41.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 4.631865501403809
2025-12-09 12:10:41.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 4.755547523498535
2025-12-09 12:10:41.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 4.656495094299316
2025-12-09 12:10:41.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 4.749840259552002
2025-12-09 12:10:41.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 4.605946063995361
2025-12-09 12:10:41.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 4.658005714416504
2025-12-09 12:10:41.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 4.7712788581848145
2025-12-09 12:10:41.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 4.699216365814209
2025-12-09 12:10:41.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999125880491846 Training loss: 4.65709924697876
2025-12-09 12:10:41.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029996503623845393 Training loss: 4.543164253234863
2025-12-09 12:10:41.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.00029992133535682725 Training loss: 4.676092147827148
2025-12-09 12:10:41.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00029986016125334406 Training loss: 4.612221717834473
2025-12-09 12:10:41.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0002997815210578015 Training loss: 4.691955089569092
2025-12-09 12:10:41.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002996854239356567 Training loss: 4.712009906768799
2025-12-09 12:10:41.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0002995718810869589 Training loss: 4.698428630828857
2025-12-09 12:10:41.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00029944090574504395 Training loss: 4.656438827514648
2025-12-09 12:10:41.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0002992925131749921 Training loss: 4.595066070556641
2025-12-09 12:10:41.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0002991267206718486 Training loss: 4.693675518035889
2025-12-09 12:10:41.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029894354755860845 Training loss: 4.60822868347168
2025-12-09 12:10:41.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.00029874301518396376 Training loss: 4.679912567138672
2025-12-09 12:10:41.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.000298525146919816 Training loss: 4.679566383361816
2025-12-09 12:10:41.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0002982899681585518 Training loss: 4.676482677459717
2025-12-09 12:10:41.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00029803750631008356 Training loss: 4.647018909454346
2025-12-09 12:10:41.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00029776779079865496 Training loss: 4.600032329559326
2025-12-09 12:10:41.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029748085305941123 Training loss: 4.511446952819824
2025-12-09 12:10:41.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0002971767265347358 Training loss: 4.558138847351074
2025-12-09 12:10:42.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002968554466703525 Training loss: 4.559233665466309
2025-12-09 12:10:42.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0002965170509111942 Training loss: 4.747154235839844
2025-12-09 12:10:42.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0002961615786970389 Training loss: 4.57178258895874
2025-12-09 12:10:42.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00029578907145791274 Training loss: 4.743042469024658
2025-12-09 12:10:42.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00029539957260926183 Training loss: 4.698686122894287
2025-12-09 12:10:42.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0002949931275468917 Training loss: 4.597690105438232
2025-12-09 12:10:42.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002945697836416767 Training loss: 4.625166416168213
2025-12-09 12:10:42.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.000294129590234039 Training loss: 4.707520961761475
2025-12-09 12:10:42.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.00029367259862819804 Training loss: 4.579916477203369
2025-12-09 12:10:42.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00029319886208619073 Training loss: 4.645374298095703
2025-12-09 12:10:42.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.00029270843582166427 Training loss: 4.609116077423096
2025-12-09 12:10:42.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029220137699344055 Training loss: 4.698586940765381
2025-12-09 12:10:42.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0002916777446988548 Training loss: 4.6325788497924805
2025-12-09 12:10:42.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.00029113759996686743 Training loss: 4.669450759887695
2025-12-09 12:10:42.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0002905810057509515 Training loss: 4.634336948394775
2025-12-09 12:10:42.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.00029000802692175537 Training loss: 4.66238260269165
2025-12-09 12:10:42.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0002894187302595419 Training loss: 4.6053385734558105
2025-12-09 12:10:42.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0002888131844464056 Training loss: 4.650503158569336
2025-12-09 12:10:42.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002881914600582676 Training loss: 4.568224906921387
2025-12-09 12:10:42.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0002875536295566501 Training loss: 4.56351375579834
2025-12-09 12:10:42.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.000286899767280231 Training loss: 4.588467597961426
2025-12-09 12:10:42.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0002862299494361798 Training loss: 4.618247985839844
2025-12-09 12:10:42.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0002855442540912758 Training loss: 4.605144500732422
2025-12-09 12:10:42.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00028484276116280926 Training loss: 4.669950485229492
2025-12-09 12:10:42.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0002841255524092674 Training loss: 4.644477367401123
2025-12-09 12:10:42.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.00028339271142080534 Training loss: 4.496387958526611
2025-12-09 12:10:42.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.00028264432360950353 Training loss: 4.571413993835449
2025-12-09 12:10:42.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00028188047619941343 Training loss: 4.673977375030518
2025-12-09 12:10:42.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0002811012582163913 Training loss: 4.56592321395874
2025-12-09 12:10:42.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.00028030676047772265 Training loss: 4.540207386016846
2025-12-09 12:10:42.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.000279497075581537 Training loss: 4.625739097595215
2025-12-09 12:10:42.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002786722978960161 Training loss: 4.549921989440918
2025-12-09 12:10:42.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0002778325235483954 Training loss: 4.581111907958984
2025-12-09 12:10:42.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00027697785041376006 Training loss: 4.565978527069092
2025-12-09 12:10:42.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0002761083781036381 Training loss: 4.467986106872559
2025-12-09 12:10:42.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00027522420795439065 Training loss: 4.567814350128174
2025-12-09 12:10:42.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0002743254430154012 Training loss: 4.55072546005249
2025-12-09 12:10:42.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0002734121880370652 Training loss: 4.585017204284668
2025-12-09 12:10:42.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0002724845494585816 Training loss: 4.527657508850098
2025-12-09 12:10:42.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002715426353955476 Training loss: 4.517692565917969
2025-12-09 12:10:42.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0002705865556273575 Training loss: 4.502403259277344
2025-12-09 12:10:42.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0002696164215844081 Training loss: 4.613414764404297
2025-12-09 12:10:42.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00026863234633511183 Training loss: 4.544559001922607
2025-12-09 12:10:42.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00026763444457271837 Training loss: 4.620053291320801
2025-12-09 12:10:42.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0002666228326019474 Training loss: 4.521151542663574
2025-12-09 12:10:42.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00026559762832543336 Training loss: 4.4811906814575195
2025-12-09 12:10:42.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.000264558951229984 Training loss: 4.556140422821045
2025-12-09 12:10:42.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.00026350692237265427 Training loss: 4.542926788330078
2025-12-09 12:10:42.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0002624416643666371 Training loss: 4.50507926940918
2025-12-09 12:10:42.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.000261363301366973 Training loss: 4.481649875640869
2025-12-09 12:10:42.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00026027195905608006 Training loss: 4.499714374542236
2025-12-09 12:10:42.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0002591677646291054 Training loss: 4.508194923400879
2025-12-09 12:10:42.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00025805084677910095 Training loss: 4.594491481781006
2025-12-09 12:10:42.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0002569213356820244 Training loss: 4.588169097900391
2025-12-09 12:10:42.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0002557793629815669 Training loss: 4.567795276641846
2025-12-09 12:10:42.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00025462506177381043 Training loss: 4.6202497482299805
2025-12-09 12:10:42.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.00025345856659171563 Training loss: 4.545119285583496
2025-12-09 12:10:42.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00025228001338944175 Training loss: 4.515810966491699
2025-12-09 12:10:42.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0002510895395265016 Training loss: 4.463412284851074
2025-12-09 12:10:42.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00024988728375175214 Training loss: 4.452515125274658
2025-12-09 12:10:42.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00024867338618722357 Training loss: 4.579396724700928
2025-12-09 12:10:42.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0002474479883117882 Training loss: 4.448605537414551
2025-12-09 12:10:42.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00024621123294467096 Training loss: 4.436147212982178
2025-12-09 12:10:42.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.0002449632642288045 Training loss: 4.571305274963379
2025-12-09 12:10:42.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.00024370422761402867 Training loss: 4.533444881439209
2025-12-09 12:10:42.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0002424342698401391 Training loss: 4.3738555908203125
2025-12-09 12:10:42.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00024115353891978431 Training loss: 4.448312759399414
2025-12-09 12:10:42.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.00023986218412121537 Training loss: 4.508676528930664
2025-12-09 12:10:42.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.00023856035595088839 Training loss: 4.529903411865234
2025-12-09 12:10:42.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00023724820613592337 Training loss: 4.503085613250732
2025-12-09 12:10:42.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.00023592588760642044 Training loss: 4.510905742645264
2025-12-09 12:10:42.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00023459355447763596 Training loss: 4.561859130859375
2025-12-09 12:10:42.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.00023325136203202049 Training loss: 4.501755714416504
2025-12-09 12:10:42.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.00023189946670112069 Training loss: 4.531229019165039
2025-12-09 12:10:42.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.00023053802604734757 Training loss: 4.494935035705566
2025-12-09 12:10:42.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.00022916719874561226 Training loss: 4.470911979675293
2025-12-09 12:10:42.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0002277871445648332 Training loss: 4.505495071411133
2025-12-09 12:10:42.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.00022639802434931444 Training loss: 4.528920650482178
2025-12-09 12:10:42.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.000225 Training loss: 4.508203506469727
2025-12-09 12:10:42.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00022359323445560406 Training loss: 4.472737789154053
2025-12-09 12:10:42.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.00022217789167362073 Training loss: 4.481135845184326
2025-12-09 12:10:42.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.00022075413661121492 Training loss: 4.441999912261963
2025-12-09 12:10:42.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.00021932213520599653 Training loss: 4.468811511993408
2025-12-09 12:10:42.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00021788205435668083 Training loss: 4.431142807006836
2025-12-09 12:10:42.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00021643406190363624 Training loss: 4.442102909088135
2025-12-09 12:10:42.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.00021497832660932295 Training loss: 4.512395858764648
2025-12-09 12:10:42.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.00021351501813862356 Training loss: 4.4003520011901855
2025-12-09 12:10:42.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0002120443070390687 Training loss: 4.525990009307861
2025-12-09 12:10:42.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00021056636472096025 Training loss: 4.477585315704346
2025-12-09 12:10:42.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00020908136343739307 Training loss: 4.386143207550049
2025-12-09 12:10:42.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00020758947626417943 Training loss: 4.458678722381592
2025-12-09 12:10:42.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0002060908770796769 Training loss: 4.482021808624268
2025-12-09 12:10:42.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.00020458574054452313 Training loss: 4.44083309173584
2025-12-09 12:10:42.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00020307424208127912 Training loss: 4.411304950714111
2025-12-09 12:10:42.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.00020155655785398393 Training loss: 4.491649627685547
2025-12-09 12:10:42.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0002000328647476231 Training loss: 4.469689846038818
2025-12-09 12:10:42.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00019850334034751226 Training loss: 4.426296234130859
2025-12-09 12:10:42.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00019696816291860038 Training loss: 4.507365703582764
2025-12-09 12:10:42.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0001954275113846926 Training loss: 4.44722843170166
2025-12-09 12:10:42.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00019388156530759712 Training loss: 4.488461494445801
2025-12-09 12:10:42.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00019233050486619713 Training loss: 4.451944828033447
2025-12-09 12:10:42.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0001907745108354514 Training loss: 4.515422344207764
2025-12-09 12:10:42.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.00018921376456532482 Training loss: 4.3406476974487305
2025-12-09 12:10:42.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.00018764844795965229 Training loss: 4.486544132232666
2025-12-09 12:10:42.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.00018607874345493805 Training loss: 4.43675422668457
2025-12-09 12:10:42.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.00018450483399909263 Training loss: 4.412952899932861
2025-12-09 12:10:42.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.00018292690303011076 Training loss: 4.4161376953125
2025-12-09 12:10:42.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00018134513445469127 Training loss: 4.516952991485596
2025-12-09 12:10:42.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00017975971262680347 Training loss: 4.378630638122559
2025-12-09 12:10:42.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00017817082232620052 Training loss: 4.397191524505615
2025-12-09 12:10:42.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.00017657864873688343 Training loss: 4.473413467407227
2025-12-09 12:10:43.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.00017498337742551817 Training loss: 4.391659736633301
2025-12-09 12:10:43.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00017338519431980796 Training loss: 4.412095069885254
2025-12-09 12:10:43.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.00017178428568682353 Training loss: 4.5068535804748535
2025-12-09 12:10:43.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0001701808381112938 Training loss: 4.461086273193359
2025-12-09 12:10:43.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00016857503847385953 Training loss: 4.3985114097595215
2025-12-09 12:10:43.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.00016696707392929266 Training loss: 4.481683731079102
2025-12-09 12:10:43.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0001653571318846834 Training loss: 4.4583330154418945
2025-12-09 12:10:43.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00016374539997759821 Training loss: 4.507311820983887
2025-12-09 12:10:43.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.00016213206605421063 Training loss: 4.361344814300537
2025-12-09 12:10:43.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0001605173181474081 Training loss: 4.430853843688965
2025-12-09 12:10:43.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.00015890134445487676 Training loss: 4.4405198097229
2025-12-09 12:10:43.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.00015728433331716724 Training loss: 4.396716117858887
2025-12-09 12:10:43.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0001556664731957435 Training loss: 4.4192585945129395
2025-12-09 12:10:43.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00015404795265101806 Training loss: 4.4077277183532715
2025-12-09 12:10:43.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.00015242896032037522 Training loss: 4.424525737762451
2025-12-09 12:10:43.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.00015080968489618565 Training loss: 4.437948226928711
2025-12-09 12:10:43.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.00014919031510381435 Training loss: 4.3537516593933105
2025-12-09 12:10:43.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00014757103967962475 Training loss: 4.395471096038818
2025-12-09 12:10:43.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.00014595204734898197 Training loss: 4.4128947257995605
2025-12-09 12:10:43.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0001443335268042565 Training loss: 4.4473795890808105
2025-12-09 12:10:43.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0001427156666828328 Training loss: 4.428282260894775
2025-12-09 12:10:43.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.00014109865554512319 Training loss: 4.504930019378662
2025-12-09 12:10:43.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00013948268185259188 Training loss: 4.410021781921387
2025-12-09 12:10:43.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00013786793394578937 Training loss: 4.525735855102539
2025-12-09 12:10:43.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0001362546000224018 Training loss: 4.423225402832031
2025-12-09 12:10:43.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00013464286811531661 Training loss: 4.382453441619873
2025-12-09 12:10:43.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.00013303292607070737 Training loss: 4.326246738433838
2025-12-09 12:10:43.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.0001314249615261405 Training loss: 4.265264511108398
2025-12-09 12:10:43.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0001298191618887062 Training loss: 4.4400482177734375
2025-12-09 12:10:43.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00012821571431317647 Training loss: 4.339352130889893
2025-12-09 12:10:43.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00012661480568019201 Training loss: 4.296823978424072
2025-12-09 12:10:43.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0001250166225744818 Training loss: 4.436720848083496
2025-12-09 12:10:43.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0001234213512631166 Training loss: 4.37613582611084
2025-12-09 12:10:43.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00012182917767379948 Training loss: 4.504053115844727
2025-12-09 12:10:43.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.00012024028737319652 Training loss: 4.484712600708008
2025-12-09 12:10:43.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.00011865486554530873 Training loss: 4.370748996734619
2025-12-09 12:10:43.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0001170730969698893 Training loss: 4.289870738983154
2025-12-09 12:10:43.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.00011549516600090737 Training loss: 4.439903259277344
2025-12-09 12:10:43.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00011392125654506198 Training loss: 4.37606143951416
2025-12-09 12:10:43.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00011235155204034767 Training loss: 4.392282962799072
2025-12-09 12:10:43.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00011078623543467518 Training loss: 4.3624396324157715
2025-12-09 12:10:43.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00010922548916454855 Training loss: 4.349993705749512
2025-12-09 12:10:43.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00010766949513380284 Training loss: 4.3792924880981445
2025-12-09 12:10:43.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00010611843469240288 Training loss: 4.284769535064697
2025-12-09 12:10:43.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00010457248861530741 Training loss: 4.402535915374756
2025-12-09 12:10:43.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00010303183708139964 Training loss: 4.425441265106201
2025-12-09 12:10:43.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00010149665965248775 Training loss: 4.5248026847839355
2025-12-09 12:10:43.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 9.996713525237694e-05 Training loss: 4.31233024597168
2025-12-09 12:10:43.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 9.8443442146016e-05 Training loss: 4.292062759399414
2025-12-09 12:10:43.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 9.692575791872089e-05 Training loss: 4.376317024230957
2025-12-09 12:10:43.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 9.541425945547687e-05 Training loss: 4.434989929199219
2025-12-09 12:10:43.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 9.390912292032309e-05 Training loss: 4.319254398345947
2025-12-09 12:10:43.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 9.241052373582057e-05 Training loss: 4.362839221954346
2025-12-09 12:10:43.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 9.091863656260695e-05 Training loss: 4.306515216827393
2025-12-09 12:10:43.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 8.943363527903976e-05 Training loss: 4.4382643699646
2025-12-09 12:10:43.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 8.795569296093132e-05 Training loss: 4.368865966796875
2025-12-09 12:10:43.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 8.648498186137653e-05 Training loss: 4.490551471710205
2025-12-09 12:10:43.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 8.502167339067705e-05 Training loss: 4.319126605987549
2025-12-09 12:10:43.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 8.356593809636371e-05 Training loss: 4.418765544891357
2025-12-09 12:10:43.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 8.211794564331917e-05 Training loss: 4.236790180206299
2025-12-09 12:10:43.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 8.067786479400346e-05 Training loss: 4.3231658935546875
2025-12-09 12:10:43.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 7.924586338878511e-05 Training loss: 4.29986047744751
2025-12-09 12:10:43.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 7.782210832637923e-05 Training loss: 4.345158100128174
2025-12-09 12:10:43.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 7.640676554439594e-05 Training loss: 4.4509358406066895
2025-12-09 12:10:43.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 7.500000000000002e-05 Training loss: 4.381167411804199
2025-12-09 12:10:43.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 7.36019756506856e-05 Training loss: 4.4018120765686035
2025-12-09 12:10:43.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 7.22128554351668e-05 Training loss: 4.3513641357421875
2025-12-09 12:10:43.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 7.083280125438766e-05 Training loss: 4.339053153991699
2025-12-09 12:10:43.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 6.946197395265242e-05 Training loss: 4.286625862121582
2025-12-09 12:10:43.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 6.810053329887928e-05 Training loss: 4.3333916664123535
2025-12-09 12:10:43.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 6.674863796797953e-05 Training loss: 4.397094249725342
2025-12-09 12:10:43.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 6.540644552236401e-05 Training loss: 4.3857221603393555
2025-12-09 12:10:43.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 6.407411239357953e-05 Training loss: 4.352280616760254
2025-12-09 12:10:43.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 6.275179386407663e-05 Training loss: 4.3557844161987305
2025-12-09 12:10:43.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 6.143964404911164e-05 Training loss: 4.403833389282227
2025-12-09 12:10:43.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 6.013781587878463e-05 Training loss: 4.431765079498291
2025-12-09 12:10:43.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 5.8846461080215626e-05 Training loss: 4.383589744567871
2025-12-09 12:10:43.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 5.756573015986089e-05 Training loss: 4.48802375793457
2025-12-09 12:10:43.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 5.629577238597132e-05 Training loss: 4.369038105010986
2025-12-09 12:10:43.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 5.503673577119552e-05 Training loss: 4.340268135070801
2025-12-09 12:10:43.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 5.378876705532904e-05 Training loss: 4.591423511505127
2025-12-09 12:10:43.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 5.2552011688211835e-05 Training loss: 4.243477821350098
2025-12-09 12:10:43.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 5.1326613812776434e-05 Training loss: 4.407210826873779
2025-12-09 12:10:43.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 5.011271624824786e-05 Training loss: 4.2985382080078125
2025-12-09 12:10:43.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 4.891046047349837e-05 Training loss: 4.285995960235596
2025-12-09 12:10:43.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 4.7719986610558234e-05 Training loss: 4.451419353485107
2025-12-09 12:10:43.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 4.654143340828435e-05 Training loss: 4.285309791564941
2025-12-09 12:10:43.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 4.537493822618958e-05 Training loss: 4.35732889175415
2025-12-09 12:10:43.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 4.422063701843316e-05 Training loss: 4.292530536651611
2025-12-09 12:10:43.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 4.3078664317975646e-05 Training loss: 4.44524621963501
2025-12-09 12:10:43.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 4.194915322089898e-05 Training loss: 4.33014440536499
2025-12-09 12:10:43.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 4.08322353708946e-05 Training loss: 4.253078937530518
2025-12-09 12:10:43.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 3.972804094391998e-05 Training loss: 4.34231424331665
2025-12-09 12:10:43.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 3.863669863302697e-05 Training loss: 4.349165916442871
2025-12-09 12:10:43.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 3.755833563336293e-05 Training loss: 4.423608303070068
2025-12-09 12:10:43.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 3.64930776273457e-05 Training loss: 4.290409088134766
2025-12-09 12:10:43.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 3.5441048770015954e-05 Training loss: 4.424646854400635
2025-12-09 12:10:43.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 3.4402371674566626e-05 Training loss: 4.365022659301758
2025-12-09 12:10:43.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 3.3377167398052636e-05 Training loss: 4.268333911895752
2025-12-09 12:10:43.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 3.2365555427281634e-05 Training loss: 4.38218879699707
2025-12-09 12:10:43.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 3.136765366488817e-05 Training loss: 4.39894962310791
2025-12-09 12:10:43.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 3.038357841559191e-05 Training loss: 4.281968116760254
2025-12-09 12:10:43.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 2.941344437264249e-05 Training loss: 4.356578350067139
2025-12-09 12:10:43.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 2.8457364604452372e-05 Training loss: 4.382942199707031
2025-12-09 12:10:43.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 2.7515450541418338e-05 Training loss: 4.271102428436279
2025-12-09 12:10:43.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 2.658781196293482e-05 Training loss: 4.335931777954102
2025-12-09 12:10:43.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 2.5674556984598822e-05 Training loss: 4.503103256225586
2025-12-09 12:10:43.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 2.477579204560935e-05 Training loss: 4.476944446563721
2025-12-09 12:10:43.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 2.389162189636188e-05 Training loss: 4.375973701477051
2025-12-09 12:10:43.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 2.3022149586239968e-05 Training loss: 4.389944553375244
2025-12-09 12:10:43.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 2.216747645160462e-05 Training loss: 4.3619890213012695
2025-12-09 12:10:44.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 2.1327702103983863e-05 Training loss: 4.312648296356201
2025-12-09 12:10:44.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 2.0502924418463013e-05 Training loss: 4.409222602844238
2025-12-09 12:10:44.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 1.9693239522277327e-05 Training loss: 4.274643898010254
2025-12-09 12:10:44.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 1.889874178360864e-05 Training loss: 4.308535575866699
2025-12-09 12:10:44.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 1.8119523800586568e-05 Training loss: 4.27377986907959
2025-12-09 12:10:44.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 1.735567639049648e-05 Training loss: 4.496062278747559
2025-12-09 12:10:44.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 1.6607288579194638e-05 Training loss: 4.381468772888184
2025-12-09 12:10:44.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 1.5874447590732538e-05 Training loss: 4.358310699462891
2025-12-09 12:10:44.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 1.5157238837190716e-05 Training loss: 4.428773403167725
2025-12-09 12:10:44.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 1.4455745908724226e-05 Training loss: 4.382340908050537
2025-12-09 12:10:44.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 1.3770050563820179e-05 Training loss: 4.461502552032471
2025-12-09 12:10:44.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 1.3100232719768994e-05 Training loss: 4.375328063964844
2025-12-09 12:10:44.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 1.2446370443349863e-05 Training loss: 4.435407638549805
2025-12-09 12:10:44.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 1.180853994173236e-05 Training loss: 4.3436713218688965
2025-12-09 12:10:44.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 1.118681555359438e-05 Training loss: 4.387261390686035
2025-12-09 12:10:44.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 1.058126974045811e-05 Training loss: 4.276790142059326
2025-12-09 12:10:44.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.991973078244636e-06 Training loss: 4.340399265289307
2025-12-09 12:10:44.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.418994249048472e-06 Training loss: 4.474699974060059
2025-12-09 12:10:44.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 8.862400033132571e-06 Training loss: 4.345830917358398
2025-12-09 12:10:44.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 8.322255301145204e-06 Training loss: 4.362090110778809
2025-12-09 12:10:44.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 7.798623006559435e-06 Training loss: 4.269545078277588
2025-12-09 12:10:44.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 7.291564178335718e-06 Training loss: 4.4197540283203125
2025-12-09 12:10:44.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 6.801137913809213e-06 Training loss: 4.41564416885376
2025-12-09 12:10:44.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 6.3274013718019434e-06 Training loss: 4.319411754608154
2025-12-09 12:10:44.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 5.870409765960965e-06 Training loss: 4.409239292144775
2025-12-09 12:10:44.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 5.430216358323309e-06 Training loss: 4.253771781921387
2025-12-09 12:10:44.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 5.006872453108329e-06 Training loss: 4.309874057769775
2025-12-09 12:10:44.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 4.600427390738159e-06 Training loss: 4.3620829582214355
2025-12-09 12:10:44.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 4.210928542087206e-06 Training loss: 4.460825443267822
2025-12-09 12:10:44.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 3.838421302961098e-06 Training loss: 4.398655891418457
2025-12-09 12:10:44.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 3.482949088805742e-06 Training loss: 4.256054401397705
2025-12-09 12:10:44.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 3.1445533296474478e-06 Training loss: 4.2476725578308105
2025-12-09 12:10:44.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 2.823273465264142e-06 Training loss: 4.445261001586914
2025-12-09 12:10:44.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 2.519146940588762e-06 Training loss: 4.373728275299072
2025-12-09 12:10:44.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 2.232209201345031e-06 Training loss: 4.261410236358643
2025-12-09 12:10:44.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 1.9624936899163945e-06 Training loss: 4.422285079956055
2025-12-09 12:10:44.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 1.7100318414482061e-06 Training loss: 4.418106555938721
2025-12-09 12:10:44.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 1.4748530801840074e-06 Training loss: 4.244777202606201
2025-12-09 12:10:44.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 1.2569848160362384e-06 Training loss: 4.402139186859131
2025-12-09 12:10:44.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 1.056452441391542e-06 Training loss: 4.525832653045654
2025-12-09 12:10:44.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 8.732793281513661e-07 Training loss: 4.3658857345581055
2025-12-09 12:10:44.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 7.07486825007908e-07 Training loss: 4.384942054748535
2025-12-09 12:10:44.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 5.590942549560051e-07 Training loss: 4.42264461517334
2025-12-09 12:10:44.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 4.281189130410534e-07 Training loss: 4.332993030548096
2025-12-09 12:10:44.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 3.1457606434325266e-07 Training loss: 4.341168403625488
2025-12-09 12:10:44.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 2.184789421984634e-07 Training loss: 4.306196689605713
2025-12-09 12:10:44.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 1.3983874665589035e-07 Training loss: 4.295928001403809
2025-12-09 12:10:44.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 7.866464317276e-08 Training loss: 4.375732898712158
2025-12-09 12:10:44.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 3.4963761546041855e-08 Training loss: 4.468233585357666
2025-12-09 12:10:44.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 8.741195081479747e-09 Training loss: 4.486572265625
2025-12-09 12:10:44.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 4.39247989654541
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:10:57.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 4.799441337585449
2025-12-09 12:10:57.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 4.914732456207275
2025-12-09 12:10:57.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 4.891353130340576
2025-12-09 12:10:57.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 4.865036964416504
2025-12-09 12:10:57.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 4.966080665588379
2025-12-09 12:10:57.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 4.945106506347656
2025-12-09 12:10:57.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 4.817596435546875
2025-12-09 12:10:57.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 4.902554512023926
2025-12-09 12:10:57.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 4.842945575714111
2025-12-09 12:10:57.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 4.916752815246582
2025-12-09 12:10:57.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 4.848223686218262
2025-12-09 12:10:57.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 4.947699069976807
2025-12-09 12:10:57.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 4.8872857093811035
2025-12-09 12:10:57.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 4.905829429626465
2025-12-09 12:10:58.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 4.965372085571289
2025-12-09 12:10:58.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 4.784575462341309
2025-12-09 12:10:58.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 4.911920547485352
2025-12-09 12:10:58.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 4.87745475769043
2025-12-09 12:10:58.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 4.898805141448975
2025-12-09 12:10:58.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 4.765896320343018
2025-12-09 12:10:58.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 4.852935314178467
2025-12-09 12:10:58.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 4.85922908782959
2025-12-09 12:10:58.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 4.905550479888916
2025-12-09 12:10:58.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 4.90382719039917
2025-12-09 12:10:58.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 4.733087062835693
2025-12-09 12:10:58.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 4.8201398849487305
2025-12-09 12:10:58.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 4.8505330085754395
2025-12-09 12:10:58.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 4.709349632263184
2025-12-09 12:10:58.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 4.891254425048828
2025-12-09 12:10:58.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 4.824747562408447
2025-12-09 12:10:58.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 4.839806079864502
2025-12-09 12:10:58.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 4.733992576599121
2025-12-09 12:10:58.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 4.72875452041626
2025-12-09 12:10:58.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 4.731082439422607
2025-12-09 12:10:58.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 4.885717391967773
2025-12-09 12:10:58.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 4.676303863525391
2025-12-09 12:10:58.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 4.874965667724609
2025-12-09 12:10:58.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 4.784118175506592
2025-12-09 12:10:58.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 4.8006792068481445
2025-12-09 12:10:58.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 4.670970916748047
2025-12-09 12:10:58.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 4.731918811798096
2025-12-09 12:10:58.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 4.671916484832764
2025-12-09 12:10:58.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 4.697631359100342
2025-12-09 12:10:58.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 4.761440277099609
2025-12-09 12:10:58.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 4.670803546905518
2025-12-09 12:10:58.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 4.7290120124816895
2025-12-09 12:10:58.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 4.812119483947754
2025-12-09 12:10:58.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 4.690625190734863
2025-12-09 12:10:58.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 4.779584884643555
2025-12-09 12:10:58.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 4.75649881362915
2025-12-09 12:10:58.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 4.7104902267456055
2025-12-09 12:10:58.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 4.645386695861816
2025-12-09 12:10:58.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 4.716014862060547
2025-12-09 12:10:58.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 4.770779609680176
2025-12-09 12:10:58.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 4.645096302032471
2025-12-09 12:10:58.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 4.705102443695068
2025-12-09 12:10:58.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 4.677120685577393
2025-12-09 12:10:58.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 4.831351280212402
2025-12-09 12:10:58.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 4.665558338165283
2025-12-09 12:10:58.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 4.641007423400879
2025-12-09 12:10:58.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 4.7563371658325195
2025-12-09 12:10:58.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 4.721767425537109
2025-12-09 12:10:58.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 4.599218845367432
2025-12-09 12:10:58.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 4.65836238861084
2025-12-09 12:10:58.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 4.611093997955322
2025-12-09 12:10:58.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 4.645829677581787
2025-12-09 12:10:58.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 4.724404811859131
2025-12-09 12:10:58.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 4.5223870277404785
2025-12-09 12:10:58.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 4.68642520904541
2025-12-09 12:10:58.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 4.598046779632568
2025-12-09 12:10:58.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 4.533565521240234
2025-12-09 12:10:58.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 4.702326774597168
2025-12-09 12:10:58.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 4.564001560211182
2025-12-09 12:10:58.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 4.638516426086426
2025-12-09 12:10:58.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 4.6475019454956055
2025-12-09 12:10:58.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 4.530306339263916
2025-12-09 12:10:58.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 4.639609336853027
2025-12-09 12:10:58.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 4.526675224304199
2025-12-09 12:10:58.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 4.656404972076416
2025-12-09 12:10:58.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 4.529786109924316
2025-12-09 12:10:58.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 4.59048318862915
2025-12-09 12:10:58.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 4.594250679016113
2025-12-09 12:10:58.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 4.608389854431152
2025-12-09 12:10:58.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 4.531171798706055
2025-12-09 12:10:58.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 4.602690696716309
2025-12-09 12:10:58.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 4.382874488830566
2025-12-09 12:10:58.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 4.460888862609863
2025-12-09 12:10:58.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 4.669951915740967
2025-12-09 12:10:58.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 4.61004638671875
2025-12-09 12:10:58.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 4.471206188201904
2025-12-09 12:10:58.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 4.494771957397461
2025-12-09 12:10:58.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 4.570964336395264
2025-12-09 12:10:58.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 4.492762088775635
2025-12-09 12:10:58.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 4.447566986083984
2025-12-09 12:10:58.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 4.508766174316406
2025-12-09 12:10:58.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 4.57028341293335
2025-12-09 12:10:58.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 4.517399787902832
2025-12-09 12:10:58.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 4.563647747039795
2025-12-09 12:10:58.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 4.535506725311279
2025-12-09 12:10:58.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 4.4569830894470215
2025-12-09 12:10:58.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009999708626830617 Training loss: 4.5072922706604
2025-12-09 12:10:58.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0009998834541281797 Training loss: 4.443086624145508
2025-12-09 12:10:58.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0009997377845227576 Training loss: 4.506800174713135
2025-12-09 12:10:58.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0009995338708444802 Training loss: 4.409257411956787
2025-12-09 12:10:58.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0009992717368593384 Training loss: 4.378263473510742
2025-12-09 12:10:58.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0009989514131188558 Training loss: 4.400128364562988
2025-12-09 12:10:58.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0009985729369565298 Training loss: 4.457643985748291
2025-12-09 12:10:58.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00099813635248348 Training loss: 4.398209095001221
2025-12-09 12:10:58.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0009976417105833069 Training loss: 4.372634410858154
2025-12-09 12:10:58.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.000997089068906162 Training loss: 4.533471584320068
2025-12-09 12:10:58.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0009964784918620282 Training loss: 4.449352264404297
2025-12-09 12:10:58.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0009958100506132126 Training loss: 4.565271854400635
2025-12-09 12:10:58.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0009950838230660534 Training loss: 4.4074506759643555
2025-12-09 12:10:58.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0009942998938618393 Training loss: 4.333776950836182
2025-12-09 12:10:58.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0009934583543669453 Training loss: 4.430954933166504
2025-12-09 12:10:58.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0009925593026621834 Training loss: 4.283018589019775
2025-12-09 12:10:58.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.000991602843531371 Training loss: 4.37205696105957
2025-12-09 12:10:58.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0009905890884491196 Training loss: 4.296481609344482
2025-12-09 12:10:58.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0009895181555678418 Training loss: 4.314758777618408
2025-12-09 12:10:58.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0009883901697039807 Training loss: 4.466207981109619
2025-12-09 12:10:58.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.000987205262323463 Training loss: 4.402377605438232
2025-12-09 12:10:58.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.000985963571526376 Training loss: 4.550894260406494
2025-12-09 12:10:58.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0009846652420308728 Training loss: 4.307478427886963
2025-12-09 12:10:58.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0009833104251563056 Training loss: 4.28005313873291
2025-12-09 12:10:58.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.000981899278805589 Training loss: 4.39719295501709
2025-12-09 12:10:58.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0009804319674467969 Training loss: 4.432000160217285
2025-12-09 12:10:59.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0009789086620939935 Training loss: 4.358069896697998
2025-12-09 12:10:59.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0009773295402873026 Training loss: 4.323906421661377
2025-12-09 12:10:59.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0009756947860722143 Training loss: 4.434451580047607
2025-12-09 12:10:59.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0009740045899781352 Training loss: 4.360098838806152
2025-12-09 12:10:59.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0009722591489961827 Training loss: 4.302737712860107
2025-12-09 12:10:59.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0009704586665562249 Training loss: 4.248969078063965
2025-12-09 12:10:59.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0009686033525031719 Training loss: 4.377053260803223
2025-12-09 12:10:59.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0009666934230725179 Training loss: 4.404607772827148
2025-12-09 12:10:59.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0009647291008651398 Training loss: 4.345753192901611
2025-12-09 12:10:59.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0009627106148213521 Training loss: 4.3221540451049805
2025-12-09 12:10:59.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0009606382001942255 Training loss: 4.378196716308594
2025-12-09 12:10:59.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0009585120985221671 Training loss: 4.316645622253418
2025-12-09 12:10:59.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0009563325576007701 Training loss: 4.26944637298584
2025-12-09 12:10:59.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0009540998314539327 Training loss: 4.252004146575928
2025-12-09 12:10:59.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0009518141803042527 Training loss: 4.318114280700684
2025-12-09 12:10:59.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0009494758705426977 Training loss: 4.267062187194824
2025-12-09 12:10:59.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0009470851746975581 Training loss: 4.1707048416137695
2025-12-09 12:10:59.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0009446423714026846 Training loss: 4.296070575714111
2025-12-09 12:10:59.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0009421477453650118 Training loss: 4.292147159576416
2025-12-09 12:10:59.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0009396015873313782 Training loss: 4.202832221984863
2025-12-09 12:10:59.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0009370041940546379 Training loss: 4.243066310882568
2025-12-09 12:10:59.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0009343558682590756 Training loss: 4.212497711181641
2025-12-09 12:10:59.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0009316569186051234 Training loss: 4.295650482177734
2025-12-09 12:10:59.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0009289076596533872 Training loss: 4.21821403503418
2025-12-09 12:10:59.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0009261084118279846 Training loss: 4.138847351074219
2025-12-09 12:10:59.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0009232595013792003 Training loss: 4.1599040031433105
2025-12-09 12:10:59.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0009203612603454604 Training loss: 4.366397380828857
2025-12-09 12:10:59.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0009174140265146356 Training loss: 4.248627185821533
2025-12-09 12:10:59.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0009144181433846706 Training loss: 4.091280937194824
2025-12-09 12:10:59.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0009113739601235507 Training loss: 4.181821346282959
2025-12-09 12:10:59.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0009082818315286055 Training loss: 4.328579902648926
2025-12-09 12:10:59.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0009051421179851588 Training loss: 4.226744651794434
2025-12-09 12:10:59.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.000901955185424525 Training loss: 4.195913314819336
2025-12-09 12:10:59.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0008987214052813603 Training loss: 4.113466262817383
2025-12-09 12:10:59.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0008954411544503729 Training loss: 4.178589820861816
2025-12-09 12:10:59.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0008921148152423946 Training loss: 4.270861625671387
2025-12-09 12:10:59.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0008887427753398248 Training loss: 4.247288227081299
2025-12-09 12:10:59.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0008853254277514447 Training loss: 4.104770660400391
2025-12-09 12:10:59.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0008818631707666135 Training loss: 4.263167381286621
2025-12-09 12:10:59.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0008783564079088476 Training loss: 4.142199993133545
2025-12-09 12:10:59.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0008748055478887904 Training loss: 4.256121635437012
2025-12-09 12:10:59.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0008712110045565768 Training loss: 4.2281599044799805
2025-12-09 12:10:59.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0008675731968536002 Training loss: 4.182939529418945
2025-12-09 12:10:59.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0008638925487636848 Training loss: 4.235040187835693
2025-12-09 12:10:59.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00086016948926367 Training loss: 4.321477890014648
2025-12-09 12:10:59.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0008564044522734146 Training loss: 4.130992889404297
2025-12-09 12:10:59.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.000852597876605223 Training loss: 4.164215087890625
2025-12-09 12:10:59.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0008487502059127015 Training loss: 4.249163627624512
2025-12-09 12:10:59.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0008448618886390522 Training loss: 4.154848098754883
2025-12-09 12:10:59.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.0008409333779648059 Training loss: 4.214036464691162
2025-12-09 12:10:59.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0008369651317550054 Training loss: 4.210058689117432
2025-12-09 12:10:59.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0008329576125058406 Training loss: 4.157115936279297
2025-12-09 12:10:59.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0008289112872907454 Training loss: 4.16034460067749
2025-12-09 12:10:59.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0008248266277059606 Training loss: 4.112102508544922
2025-12-09 12:10:59.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00082070410981557 Training loss: 4.2143235206604
2025-12-09 12:10:59.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.000816544214096015 Training loss: 4.110337734222412
2025-12-09 12:10:59.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0008123474253800957 Training loss: 4.264427661895752
2025-12-09 12:10:59.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0008081142328004637 Training loss: 4.170443058013916
2025-12-09 12:10:59.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0008038451297326145 Training loss: 4.180099010467529
2025-12-09 12:10:59.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0007995406137373846 Training loss: 4.102377414703369
2025-12-09 12:10:59.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0007952011865029613 Training loss: 4.141979694366455
2025-12-09 12:10:59.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0007908273537864113 Training loss: 4.136616230010986
2025-12-09 12:10:59.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0007864196253547349 Training loss: 4.15324068069458
2025-12-09 12:10:59.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0007819785149254532 Training loss: 4.195643901824951
2025-12-09 12:10:59.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.000777504540106735 Training loss: 4.175057411193848
2025-12-09 12:10:59.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0007729982223370691 Training loss: 4.2640581130981445
2025-12-09 12:10:59.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0007684600868244919 Training loss: 4.115002155303955
2025-12-09 12:10:59.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0007638906624853743 Training loss: 4.221356391906738
2025-12-09 12:10:59.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0007592904818827774 Training loss: 4.103404998779297
2025-12-09 12:10:59.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0007546600811643815 Training loss: 4.222961902618408
2025-12-09 12:10:59.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.00075 Training loss: 4.095706462860107
2025-12-09 12:10:59.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0007453107815186803 Training loss: 4.086813926696777
2025-12-09 12:10:59.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0007405929722454026 Training loss: 4.140241622924805
2025-12-09 12:10:59.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0007358471220373831 Training loss: 4.103396892547607
2025-12-09 12:10:59.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0007310737840199885 Training loss: 4.107895374298096
2025-12-09 12:10:59.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0007262735145222696 Training loss: 4.135643482208252
2025-12-09 12:10:59.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0007214468730121209 Training loss: 4.007576942443848
2025-12-09 12:10:59.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0007165944220310766 Training loss: 4.075497150421143
2025-12-09 12:10:59.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0007117167271287453 Training loss: 4.185122489929199
2025-12-09 12:10:59.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0007068143567968958 Training loss: 3.9969980716705322
2025-12-09 12:10:59.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0007018878824032009 Training loss: 3.9714527130126953
2025-12-09 12:10:59.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.0006969378781246436 Training loss: 4.1660237312316895
2025-12-09 12:10:59.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0006919649208805981 Training loss: 4.022801876068115
2025-12-09 12:10:59.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0006869695902655897 Training loss: 4.175904273986816
2025-12-09 12:10:59.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0006819524684817438 Training loss: 3.969622850418091
2025-12-09 12:10:59.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0006769141402709304 Training loss: 4.152525901794434
2025-12-09 12:10:59.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0006718551928466132 Training loss: 4.11409854888916
2025-12-09 12:10:59.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0006667762158254104 Training loss: 4.156352519989014
2025-12-09 12:10:59.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.0006616778011583743 Training loss: 4.035251617431641
2025-12-09 12:10:59.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0006565605430620013 Training loss: 3.96345853805542
2025-12-09 12:10:59.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0006514250379489753 Training loss: 4.070532321929932
2025-12-09 12:10:59.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0006462718843586572 Training loss: 4.103879451751709
2025-12-09 12:10:59.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0006411016828873239 Training loss: 4.115255355834961
2025-12-09 12:10:59.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0006359150361181715 Training loss: 3.9708425998687744
2025-12-09 12:10:59.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0006307125485510829 Training loss: 4.204823017120361
2025-12-09 12:10:59.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0006254948265321744 Training loss: 4.071065425872803
2025-12-09 12:10:59.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0006202624781831269 Training loss: 4.105712890625
2025-12-09 12:10:59.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0006150161133303088 Training loss: 4.037594795227051
2025-12-09 12:10:59.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0006097563434337025 Training loss: 4.124294281005859
2025-12-09 12:10:59.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0006044837815156376 Training loss: 3.9828479290008545
2025-12-09 12:10:59.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0005991990420893449 Training loss: 3.850792646408081
2025-12-09 12:10:59.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0005939027410873352 Training loss: 4.072061538696289
2025-12-09 12:10:59.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0005885954957896114 Training loss: 4.131982803344727
2025-12-09 12:10:59.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0005832779247517272 Training loss: 4.079115867614746
2025-12-09 12:10:59.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0005779506477326933 Training loss: 4.145873546600342
2025-12-09 12:10:59.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0005726142856227452 Training loss: 4.119042873382568
2025-12-09 12:10:59.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0005672694603709794 Training loss: 4.084249019622803
2025-12-09 12:10:59.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0005619167949128652 Training loss: 4.053470611572266
2025-12-09 12:10:59.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0005565569130976422 Training loss: 3.9960246086120605
2025-12-09 12:10:59.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0005511904396156113 Training loss: 4.053301811218262
2025-12-09 12:10:59.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0005458179999253274 Training loss: 4.046913146972656
2025-12-09 12:10:59.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0005404402201807021 Training loss: 4.022291660308838
2025-12-09 12:10:59.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0005350577271580271 Training loss: 3.9857563972473145
2025-12-09 12:11:00.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0005296711481829226 Training loss: 4.026637554168701
2025-12-09 12:11:00.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0005242811110572242 Training loss: 4.214059829711914
2025-12-09 12:11:00.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0005188882439858117 Training loss: 4.041836261749268
2025-12-09 12:11:00.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0005134931755033936 Training loss: 4.172823429107666
2025-12-09 12:11:00.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0005080965344012508 Training loss: 4.0854411125183105
2025-12-09 12:11:00.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0005026989496539523 Training loss: 3.9445738792419434
2025-12-09 12:11:00.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0004973010503460479 Training loss: 4.0500969886779785
2025-12-09 12:11:00.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0004919034655987492 Training loss: 4.054111957550049
2025-12-09 12:11:00.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0004865068244966066 Training loss: 3.956305742263794
2025-12-09 12:11:00.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0004811117560141884 Training loss: 4.046626091003418
2025-12-09 12:11:00.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.000475718888942776 Training loss: 4.017827033996582
2025-12-09 12:11:00.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0004703288518170774 Training loss: 3.959071397781372
2025-12-09 12:11:00.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00046494227284197295 Training loss: 4.012273788452148
2025-12-09 12:11:00.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00045955977981929796 Training loss: 3.9625165462493896
2025-12-09 12:11:00.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0004541820000746727 Training loss: 3.969285488128662
2025-12-09 12:11:00.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00044880956038438873 Training loss: 3.9756476879119873
2025-12-09 12:11:00.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0004434430869023579 Training loss: 3.9838552474975586
2025-12-09 12:11:00.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.000438083205087135 Training loss: 3.9578793048858643
2025-12-09 12:11:00.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.00043273053962902076 Training loss: 4.0431742668151855
2025-12-09 12:11:00.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.000427385714377255 Training loss: 3.985825777053833
2025-12-09 12:11:00.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0004220493522673067 Training loss: 4.057399749755859
2025-12-09 12:11:00.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0004167220752482728 Training loss: 4.003139495849609
2025-12-09 12:11:00.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.00041140450421038864 Training loss: 4.144470691680908
2025-12-09 12:11:00.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.000406097258912665 Training loss: 3.9806137084960938
2025-12-09 12:11:00.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0004008009579106551 Training loss: 4.08868932723999
2025-12-09 12:11:00.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0003955162184843625 Training loss: 3.9950296878814697
2025-12-09 12:11:00.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.00039024365656629766 Training loss: 3.905130386352539
2025-12-09 12:11:00.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0003849838866696913 Training loss: 4.0577073097229
2025-12-09 12:11:00.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00037973752181687335 Training loss: 4.003559112548828
2025-12-09 12:11:00.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00037450517346782563 Training loss: 3.909708261489868
2025-12-09 12:11:00.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0003692874514489173 Training loss: 4.014383316040039
2025-12-09 12:11:00.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00036408496388182855 Training loss: 3.905919075012207
2025-12-09 12:11:00.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0003588983171126762 Training loss: 3.988826036453247
2025-12-09 12:11:00.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.000353728115641343 Training loss: 4.131052017211914
2025-12-09 12:11:00.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0003485749620510247 Training loss: 3.996157169342041
2025-12-09 12:11:00.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00034343945693799885 Training loss: 4.049779415130615
2025-12-09 12:11:00.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00033832219884162584 Training loss: 4.026370048522949
2025-12-09 12:11:00.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0003332237841745898 Training loss: 3.9378440380096436
2025-12-09 12:11:00.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00032814480715338666 Training loss: 3.923036575317383
2025-12-09 12:11:00.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0003230858597290697 Training loss: 3.939849853515625
2025-12-09 12:11:00.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0003180475315182563 Training loss: 4.0539231300354
2025-12-09 12:11:00.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0003130304097344103 Training loss: 3.9717302322387695
2025-12-09 12:11:00.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0003080350791194019 Training loss: 4.104416847229004
2025-12-09 12:11:00.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.00030306212187535653 Training loss: 4.050414085388184
2025-12-09 12:11:00.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002981121175967992 Training loss: 3.9696836471557617
2025-12-09 12:11:00.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.00029318564320310444 Training loss: 4.005795955657959
2025-12-09 12:11:00.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0002882832728712551 Training loss: 4.035501956939697
2025-12-09 12:11:00.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0002834055779689235 Training loss: 3.854215145111084
2025-12-09 12:11:00.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.00027855312698787905 Training loss: 4.0165696144104
2025-12-09 12:11:00.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0002737264854777306 Training loss: 3.895721912384033
2025-12-09 12:11:00.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.00026892621598001155 Training loss: 3.8889527320861816
2025-12-09 12:11:00.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0002641528779626171 Training loss: 3.9777448177337646
2025-12-09 12:11:00.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.00025940702775459747 Training loss: 4.054665565490723
2025-12-09 12:11:00.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.00025468921848131984 Training loss: 4.020269393920898
2025-12-09 12:11:00.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0002500000000000001 Training loss: 4.057692527770996
2025-12-09 12:11:00.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.00024533991883561866 Training loss: 3.9382665157318115
2025-12-09 12:11:00.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.00024070951811722268 Training loss: 3.90563702583313
2025-12-09 12:11:00.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00023610933751462554 Training loss: 3.9053924083709717
2025-12-09 12:11:00.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0002315399131755081 Training loss: 3.9340152740478516
2025-12-09 12:11:00.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00022700177766293096 Training loss: 4.039727210998535
2025-12-09 12:11:00.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.00022249545989326514 Training loss: 3.923875331878662
2025-12-09 12:11:00.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002180214850745467 Training loss: 4.007476329803467
2025-12-09 12:11:00.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.00021358037464526514 Training loss: 4.010979175567627
2025-12-09 12:11:00.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.00020917264621358878 Training loss: 3.964362382888794
2025-12-09 12:11:00.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.00020479881349703882 Training loss: 3.9101133346557617
2025-12-09 12:11:00.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.00020045938626261545 Training loss: 4.0129194259643555
2025-12-09 12:11:00.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.00019615487026738542 Training loss: 3.8719305992126465
2025-12-09 12:11:00.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.00019188576719953633 Training loss: 4.071925640106201
2025-12-09 12:11:00.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.00018765257461990443 Training loss: 4.029876232147217
2025-12-09 12:11:00.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0001834557859039851 Training loss: 3.9904634952545166
2025-12-09 12:11:00.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.00017929589018443015 Training loss: 3.8587987422943115
2025-12-09 12:11:00.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.00017517337229403947 Training loss: 3.964186429977417
2025-12-09 12:11:00.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0001710887127092548 Training loss: 3.9836769104003906
2025-12-09 12:11:00.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.00016704238749415957 Training loss: 3.8867475986480713
2025-12-09 12:11:00.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0001630348682449946 Training loss: 3.9748880863189697
2025-12-09 12:11:00.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00015906662203519413 Training loss: 3.984677314758301
2025-12-09 12:11:00.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00015513811136094787 Training loss: 3.9850597381591797
2025-12-09 12:11:00.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.0001512497940872986 Training loss: 3.880098342895508
2025-12-09 12:11:00.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0001474021233947772 Training loss: 4.004222869873047
2025-12-09 12:11:00.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00014359554772658552 Training loss: 4.0716352462768555
2025-12-09 12:11:00.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.00013983051073632996 Training loss: 4.017656326293945
2025-12-09 12:11:00.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00013610745123631535 Training loss: 4.071552276611328
2025-12-09 12:11:00.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00013242680314639994 Training loss: 3.977905750274658
2025-12-09 12:11:00.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00012878899544342326 Training loss: 3.9414143562316895
2025-12-09 12:11:00.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00012519445211120977 Training loss: 4.06050443649292
2025-12-09 12:11:00.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00012164359209115234 Training loss: 3.9364817142486572
2025-12-09 12:11:00.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00011813682923338653 Training loss: 3.9198567867279053
2025-12-09 12:11:00.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.00011467457224855543 Training loss: 3.9349732398986816
2025-12-09 12:11:00.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.00011125722466017545 Training loss: 3.84588360786438
2025-12-09 12:11:00.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00010788518475760544 Training loss: 4.153438091278076
2025-12-09 12:11:00.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00010455884554962725 Training loss: 3.834334373474121
2025-12-09 12:11:00.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0001012785947186397 Training loss: 3.8706326484680176
2025-12-09 12:11:00.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.804481457547498e-05 Training loss: 4.024033546447754
2025-12-09 12:11:00.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.485788201484124e-05 Training loss: 3.9042603969573975
2025-12-09 12:11:00.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.171816847139447e-05 Training loss: 3.856119155883789
2025-12-09 12:11:00.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 8.862603987644941e-05 Training loss: 4.077032089233398
2025-12-09 12:11:00.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 8.558185661532942e-05 Training loss: 3.999555826187134
2025-12-09 12:11:00.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 8.258597348536451e-05 Training loss: 4.034861087799072
2025-12-09 12:11:00.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 7.96387396545396e-05 Training loss: 3.921759843826294
2025-12-09 12:11:00.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 7.674049862079991e-05 Training loss: 3.8678698539733887
2025-12-09 12:11:00.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 7.38915881720154e-05 Training loss: 3.8970444202423096
2025-12-09 12:11:00.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 7.109234034661289e-05 Training loss: 3.9616539478302
2025-12-09 12:11:00.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 6.834308139487671e-05 Training loss: 3.976067304611206
2025-12-09 12:11:00.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 6.564413174092443e-05 Training loss: 3.9682583808898926
2025-12-09 12:11:00.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 6.299580594536214e-05 Training loss: 3.9430692195892334
2025-12-09 12:11:00.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 6.0398412668621897e-05 Training loss: 3.970104455947876
2025-12-09 12:11:00.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 5.785225463498828e-05 Training loss: 4.018916606903076
2025-12-09 12:11:00.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 5.535762859731547e-05 Training loss: 3.9702999591827393
2025-12-09 12:11:00.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 5.291482530244179e-05 Training loss: 3.860018014907837
2025-12-09 12:11:00.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 5.0524129457302394e-05 Training loss: 3.978304386138916
2025-12-09 12:11:00.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 4.818581969574742e-05 Training loss: 4.141230583190918
2025-12-09 12:11:00.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 4.5900168546067264e-05 Training loss: 3.8605892658233643
2025-12-09 12:11:01.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 4.366744239922998e-05 Training loss: 3.8326497077941895
2025-12-09 12:11:01.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 4.148790147783288e-05 Training loss: 3.9543297290802
2025-12-09 12:11:01.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 3.936179980577453e-05 Training loss: 3.992661952972412
2025-12-09 12:11:01.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 3.728938517864794e-05 Training loss: 3.9472217559814453
2025-12-09 12:11:01.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 3.527089913486037e-05 Training loss: 3.952045440673828
2025-12-09 12:11:01.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 3.330657692748212e-05 Training loss: 3.891641616821289
2025-12-09 12:11:01.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 3.1396647496828245e-05 Training loss: 3.937959909439087
2025-12-09 12:11:01.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 2.9541333443775243e-05 Training loss: 3.9515931606292725
2025-12-09 12:11:01.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 2.7740851003817347e-05 Training loss: 3.8872945308685303
2025-12-09 12:11:01.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 2.5995410021864786e-05 Training loss: 3.8063881397247314
2025-12-09 12:11:01.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 2.430521392778573e-05 Training loss: 3.8825290203094482
2025-12-09 12:11:01.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 2.2670459712697378e-05 Training loss: 4.033377170562744
2025-12-09 12:11:01.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 2.109133790600648e-05 Training loss: 3.871217727661133
2025-12-09 12:11:01.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 1.956803255320322e-05 Training loss: 4.056816577911377
2025-12-09 12:11:01.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 1.810072119441103e-05 Training loss: 4.023941516876221
2025-12-09 12:11:01.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 1.6689574843694434e-05 Training loss: 4.102121353149414
2025-12-09 12:11:01.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 1.53347579691272e-05 Training loss: 3.9463396072387695
2025-12-09 12:11:01.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 1.4036428473624019e-05 Training loss: 3.9654016494750977
2025-12-09 12:11:01.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 1.2794737676536993e-05 Training loss: 3.9176292419433594
2025-12-09 12:11:01.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 1.1609830296019142e-05 Training loss: 3.8405961990356445
2025-12-09 12:11:01.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 1.048184443215816e-05 Training loss: 3.908345937728882
2025-12-09 12:11:01.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880474e-06 Training loss: 3.9508004188537598
2025-12-09 12:11:01.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629207e-06 Training loss: 3.9891726970672607
2025-12-09 12:11:01.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.440697337816771e-06 Training loss: 3.88519549369812
2025-12-09 12:11:01.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.541645633054649e-06 Training loss: 3.9554338455200195
2025-12-09 12:11:01.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.700106138160688e-06 Training loss: 3.9769952297210693
2025-12-09 12:11:01.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946692e-06 Training loss: 3.954976797103882
2025-12-09 12:11:01.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.189949386787462e-06 Training loss: 3.890406608581543
2025-12-09 12:11:01.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.521508137971807e-06 Training loss: 4.132169246673584
2025-12-09 12:11:01.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378875e-06 Training loss: 3.873798370361328
2025-12-09 12:11:01.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.3582894166930268e-06 Training loss: 3.9857869148254395
2025-12-09 12:11:01.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.8636475165200173e-06 Training loss: 3.919203519821167
2025-12-09 12:11:01.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.4270630434701782e-06 Training loss: 3.8824429512023926
2025-12-09 12:11:01.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441758e-06 Training loss: 3.936258316040039
2025-12-09 12:11:01.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.282631406615448e-07 Training loss: 4.045574188232422
2025-12-09 12:11:01.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.6612915551963455e-07 Training loss: 3.922720193862915
2025-12-09 12:11:01.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253336e-07 Training loss: 3.9822399616241455
2025-12-09 12:11:01.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-07 Training loss: 4.124643802642822
2025-12-09 12:11:01.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265823e-08 Training loss: 3.9650192260742188
2025-12-09 12:11:01.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 4.065388202667236
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:11:14.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 4.865200519561768
2025-12-09 12:11:14.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 5.003516674041748
2025-12-09 12:11:14.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 4.846035003662109
2025-12-09 12:11:14.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 4.847034931182861
2025-12-09 12:11:14.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 4.87977409362793
2025-12-09 12:11:14.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 4.918314456939697
2025-12-09 12:11:14.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 4.871994495391846
2025-12-09 12:11:14.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 4.767423152923584
2025-12-09 12:11:14.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 4.933836936950684
2025-12-09 12:11:14.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 4.993866443634033
2025-12-09 12:11:14.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 4.648959636688232
2025-12-09 12:11:14.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 4.798664093017578
2025-12-09 12:11:14.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 4.826470851898193
2025-12-09 12:11:14.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 4.7629594802856445
2025-12-09 12:11:14.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 4.8664984703063965
2025-12-09 12:11:14.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 4.754019737243652
2025-12-09 12:11:14.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 4.848529815673828
2025-12-09 12:11:14.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 4.854944705963135
2025-12-09 12:11:14.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 4.897099494934082
2025-12-09 12:11:14.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 4.737712383270264
2025-12-09 12:11:14.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 4.889869689941406
2025-12-09 12:11:14.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 4.704140663146973
2025-12-09 12:11:14.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 4.784346580505371
2025-12-09 12:11:14.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 4.749177932739258
2025-12-09 12:11:14.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 4.758347034454346
2025-12-09 12:11:14.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 4.822022438049316
2025-12-09 12:11:14.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 4.749704360961914
2025-12-09 12:11:14.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 4.730852127075195
2025-12-09 12:11:14.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 4.797062397003174
2025-12-09 12:11:14.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 4.803155899047852
2025-12-09 12:11:14.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 4.616823673248291
2025-12-09 12:11:14.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 4.6409759521484375
2025-12-09 12:11:14.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 4.668203830718994
2025-12-09 12:11:14.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 4.668692588806152
2025-12-09 12:11:14.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 4.736132621765137
2025-12-09 12:11:14.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 4.804342269897461
2025-12-09 12:11:14.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 4.73070764541626
2025-12-09 12:11:14.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 4.654799461364746
2025-12-09 12:11:14.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 4.728949546813965
2025-12-09 12:11:14.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 4.748133182525635
2025-12-09 12:11:14.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 4.694742679595947
2025-12-09 12:11:14.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 4.600666046142578
2025-12-09 12:11:14.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 4.584481716156006
2025-12-09 12:11:15.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 4.516683101654053
2025-12-09 12:11:15.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 4.638516902923584
2025-12-09 12:11:15.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 4.659780025482178
2025-12-09 12:11:15.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 4.642886161804199
2025-12-09 12:11:15.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 4.544114589691162
2025-12-09 12:11:15.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 4.6020050048828125
2025-12-09 12:11:15.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 4.529401779174805
2025-12-09 12:11:15.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 4.645579814910889
2025-12-09 12:11:15.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 4.6602630615234375
2025-12-09 12:11:15.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 4.58096981048584
2025-12-09 12:11:15.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 4.6105756759643555
2025-12-09 12:11:15.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 4.595900058746338
2025-12-09 12:11:15.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 4.649106979370117
2025-12-09 12:11:15.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 4.602089881896973
2025-12-09 12:11:15.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 4.37040901184082
2025-12-09 12:11:15.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 4.514552116394043
2025-12-09 12:11:15.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 4.531003952026367
2025-12-09 12:11:15.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 4.473671913146973
2025-12-09 12:11:15.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 4.445855617523193
2025-12-09 12:11:15.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 4.4859137535095215
2025-12-09 12:11:15.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 4.536176681518555
2025-12-09 12:11:15.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 4.478714942932129
2025-12-09 12:11:15.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 4.435723304748535
2025-12-09 12:11:15.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 4.3065690994262695
2025-12-09 12:11:15.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 4.457512855529785
2025-12-09 12:11:15.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 4.362163543701172
2025-12-09 12:11:15.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 4.317525863647461
2025-12-09 12:11:15.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 4.300910472869873
2025-12-09 12:11:15.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 4.395002365112305
2025-12-09 12:11:15.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 4.4087934494018555
2025-12-09 12:11:15.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 4.368921279907227
2025-12-09 12:11:15.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 4.45265007019043
2025-12-09 12:11:15.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 4.320850372314453
2025-12-09 12:11:15.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 4.295417785644531
2025-12-09 12:11:15.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 4.3266801834106445
2025-12-09 12:11:15.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 4.34787130355835
2025-12-09 12:11:15.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 4.155981063842773
2025-12-09 12:11:15.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 4.458536624908447
2025-12-09 12:11:15.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 4.2238054275512695
2025-12-09 12:11:15.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 4.312494277954102
2025-12-09 12:11:15.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 4.198537826538086
2025-12-09 12:11:15.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 4.259096145629883
2025-12-09 12:11:15.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 4.2543816566467285
2025-12-09 12:11:15.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 4.313222885131836
2025-12-09 12:11:15.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 4.205333709716797
2025-12-09 12:11:15.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 4.241311550140381
2025-12-09 12:11:15.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 4.193734169006348
2025-12-09 12:11:15.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 4.348112106323242
2025-12-09 12:11:15.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 4.337839603424072
2025-12-09 12:11:15.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 4.043024063110352
2025-12-09 12:11:15.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 4.3376312255859375
2025-12-09 12:11:15.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 4.194945335388184
2025-12-09 12:11:15.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 4.206786632537842
2025-12-09 12:11:15.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 4.2800421714782715
2025-12-09 12:11:15.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 4.248509407043457
2025-12-09 12:11:15.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 4.269098281860352
2025-12-09 12:11:15.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 4.175448894500732
2025-12-09 12:11:15.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999125880491853 Training loss: 4.2689714431762695
2025-12-09 12:11:15.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0029996503623845394 Training loss: 4.195382595062256
2025-12-09 12:11:15.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029992133535682725 Training loss: 4.118780136108398
2025-12-09 12:11:15.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.002998601612533441 Training loss: 4.197338581085205
2025-12-09 12:11:15.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0029978152105780156 Training loss: 4.184132099151611
2025-12-09 12:11:15.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0029968542393565677 Training loss: 4.133236885070801
2025-12-09 12:11:15.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029957188108695894 Training loss: 4.0734944343566895
2025-12-09 12:11:15.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00299440905745044 Training loss: 4.209383964538574
2025-12-09 12:11:15.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.002992925131749921 Training loss: 4.0752763748168945
2025-12-09 12:11:15.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0029912672067184862 Training loss: 4.134471416473389
2025-12-09 12:11:15.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0029894354755860848 Training loss: 4.1162109375
2025-12-09 12:11:15.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0029874301518396378 Training loss: 4.009818077087402
2025-12-09 12:11:15.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029852514691981603 Training loss: 4.120541572570801
2025-12-09 12:11:15.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029828996815855183 Training loss: 4.05603551864624
2025-12-09 12:11:15.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.002980375063100836 Training loss: 4.105778217315674
2025-12-09 12:11:15.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00297767790798655 Training loss: 4.0528459548950195
2025-12-09 12:11:15.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.0029748085305941127 Training loss: 4.065720081329346
2025-12-09 12:11:15.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.002971767265347359 Training loss: 4.209921836853027
2025-12-09 12:11:15.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029685544667035256 Training loss: 4.048572063446045
2025-12-09 12:11:15.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0029651705091119423 Training loss: 4.16221284866333
2025-12-09 12:11:15.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0029616157869703894 Training loss: 4.069677829742432
2025-12-09 12:11:15.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002957890714579128 Training loss: 4.054923057556152
2025-12-09 12:11:15.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0029539957260926184 Training loss: 4.101183891296387
2025-12-09 12:11:15.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.002949931275468917 Training loss: 4.033324718475342
2025-12-09 12:11:15.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029456978364167667 Training loss: 4.004396915435791
2025-12-09 12:11:15.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0029412959023403904 Training loss: 4.0785932540893555
2025-12-09 12:11:15.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.002936725986281981 Training loss: 4.013464450836182
2025-12-09 12:11:15.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.002931988620861908 Training loss: 3.966420888900757
2025-12-09 12:11:15.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.002927084358216643 Training loss: 4.008319854736328
2025-12-09 12:11:15.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0029220137699344055 Training loss: 4.122683525085449
2025-12-09 12:11:15.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0029167774469885483 Training loss: 4.081767559051514
2025-12-09 12:11:15.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002911375999668675 Training loss: 4.107147693634033
2025-12-09 12:11:15.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.002905810057509516 Training loss: 4.025094032287598
2025-12-09 12:11:15.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.002900080269217554 Training loss: 4.082528114318848
2025-12-09 12:11:15.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.002894187302595419 Training loss: 4.086021900177002
2025-12-09 12:11:15.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0028881318444640564 Training loss: 3.972580909729004
2025-12-09 12:11:15.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0028819146005826766 Training loss: 4.136672496795654
2025-12-09 12:11:15.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0028755362955665015 Training loss: 4.092700958251953
2025-12-09 12:11:15.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0028689976728023103 Training loss: 3.9524998664855957
2025-12-09 12:11:15.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0028622994943617985 Training loss: 3.885427474975586
2025-12-09 12:11:15.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0028554425409127583 Training loss: 3.862367630004883
2025-12-09 12:11:15.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.002848427611628093 Training loss: 3.920224189758301
2025-12-09 12:11:15.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0028412555240926746 Training loss: 3.878828763961792
2025-12-09 12:11:15.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.002833927114208054 Training loss: 3.876875400543213
2025-12-09 12:11:15.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0028264432360950355 Training loss: 4.020354747772217
2025-12-09 12:11:15.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0028188047619941343 Training loss: 3.9788331985473633
2025-12-09 12:11:15.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0028110125821639137 Training loss: 3.9261364936828613
2025-12-09 12:11:15.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.002803067604777227 Training loss: 3.8259778022766113
2025-12-09 12:11:15.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0027949707558153703 Training loss: 3.9359354972839355
2025-12-09 12:11:15.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.002786722978960162 Training loss: 3.9635086059570312
2025-12-09 12:11:15.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.002778325235483954 Training loss: 3.9571926593780518
2025-12-09 12:11:15.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0027697785041376007 Training loss: 4.031590938568115
2025-12-09 12:11:15.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.002761083781036381 Training loss: 3.9734225273132324
2025-12-09 12:11:15.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.002752242079543907 Training loss: 3.964884042739868
2025-12-09 12:11:15.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002743254430154012 Training loss: 3.9235057830810547
2025-12-09 12:11:15.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.002734121880370652 Training loss: 4.012024879455566
2025-12-09 12:11:15.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0027248454945858164 Training loss: 3.9194774627685547
2025-12-09 12:11:15.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0027154263539554764 Training loss: 3.8432459831237793
2025-12-09 12:11:15.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.002705865556273575 Training loss: 3.890326738357544
2025-12-09 12:11:16.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.002696164215844081 Training loss: 3.9908926486968994
2025-12-09 12:11:16.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0026863234633511188 Training loss: 3.8958077430725098
2025-12-09 12:11:16.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0026763444457271837 Training loss: 4.127495765686035
2025-12-09 12:11:16.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.002666228326019474 Training loss: 4.046571254730225
2025-12-09 12:11:16.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.002655976283254334 Training loss: 3.907547950744629
2025-12-09 12:11:16.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0026455895122998404 Training loss: 4.044897079467773
2025-12-09 12:11:16.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002635069223726543 Training loss: 3.9700100421905518
2025-12-09 12:11:16.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.002624416643666371 Training loss: 3.879242420196533
2025-12-09 12:11:16.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0026136330136697305 Training loss: 3.931032180786133
2025-12-09 12:11:16.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0026027195905608006 Training loss: 3.809215545654297
2025-12-09 12:11:16.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0025916776462910542 Training loss: 3.9676735401153564
2025-12-09 12:11:16.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00258050846779101 Training loss: 3.881274700164795
2025-12-09 12:11:16.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0025692133568202442 Training loss: 3.8829851150512695
2025-12-09 12:11:16.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.002557793629815669 Training loss: 3.7896764278411865
2025-12-09 12:11:16.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0025462506177381045 Training loss: 3.9349992275238037
2025-12-09 12:11:16.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0025345856659171567 Training loss: 3.940248966217041
2025-12-09 12:11:16.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002522800133894418 Training loss: 3.936354160308838
2025-12-09 12:11:16.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0025108953952650164 Training loss: 3.9037184715270996
2025-12-09 12:11:16.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0024988728375175216 Training loss: 3.960911989212036
2025-12-09 12:11:16.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.002486733861872236 Training loss: 3.950523853302002
2025-12-09 12:11:16.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0024744798831178817 Training loss: 3.793215274810791
2025-12-09 12:11:16.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0024621123294467097 Training loss: 3.8099656105041504
2025-12-09 12:11:16.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.002449632642288045 Training loss: 3.988910675048828
2025-12-09 12:11:16.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002437042276140287 Training loss: 4.0135931968688965
2025-12-09 12:11:16.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0024243426984013913 Training loss: 3.8893444538116455
2025-12-09 12:11:16.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0024115353891978435 Training loss: 3.8520305156707764
2025-12-09 12:11:16.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.002398621841212154 Training loss: 3.970724582672119
2025-12-09 12:11:16.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.002385603559508884 Training loss: 3.7975127696990967
2025-12-09 12:11:16.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.002372482061359234 Training loss: 3.899089813232422
2025-12-09 12:11:16.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0023592588760642046 Training loss: 3.773437261581421
2025-12-09 12:11:16.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00234593554477636 Training loss: 3.7943735122680664
2025-12-09 12:11:16.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.002332513620320205 Training loss: 3.7504444122314453
2025-12-09 12:11:16.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.002318994667011207 Training loss: 3.8254213333129883
2025-12-09 12:11:16.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.002305380260473476 Training loss: 3.8248326778411865
2025-12-09 12:11:16.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002291671987456123 Training loss: 3.908243417739868
2025-12-09 12:11:16.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0022778714456483324 Training loss: 3.812270402908325
2025-12-09 12:11:16.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0022639802434931446 Training loss: 3.933288335800171
2025-12-09 12:11:16.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0022500000000000003 Training loss: 3.696514368057251
2025-12-09 12:11:16.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0022359323445560408 Training loss: 3.818056106567383
2025-12-09 12:11:16.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0022217789167362076 Training loss: 3.905412435531616
2025-12-09 12:11:16.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.002207541366112149 Training loss: 3.8531978130340576
2025-12-09 12:11:16.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0021932213520599654 Training loss: 3.880239963531494
2025-12-09 12:11:16.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0021788205435668085 Training loss: 3.7087652683258057
2025-12-09 12:11:16.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0021643406190363624 Training loss: 3.870549440383911
2025-12-09 12:11:16.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0021497832660932296 Training loss: 3.850036382675171
2025-12-09 12:11:16.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0021351501813862356 Training loss: 3.7770473957061768
2025-12-09 12:11:16.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0021204430703906873 Training loss: 3.900420904159546
2025-12-09 12:11:16.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0021056636472096026 Training loss: 3.7797224521636963
2025-12-09 12:11:16.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002090813634373931 Training loss: 3.928713798522949
2025-12-09 12:11:16.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0020758947626417943 Training loss: 3.7515947818756104
2025-12-09 12:11:16.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.002060908770796769 Training loss: 3.878295660018921
2025-12-09 12:11:16.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0020458574054452315 Training loss: 3.772263526916504
2025-12-09 12:11:16.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.002030742420812791 Training loss: 3.77453875541687
2025-12-09 12:11:16.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0020155655785398397 Training loss: 3.713388204574585
2025-12-09 12:11:16.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.002000328647476231 Training loss: 3.7492496967315674
2025-12-09 12:11:16.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.001985033403475123 Training loss: 3.6796765327453613
2025-12-09 12:11:16.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.001969681629186004 Training loss: 3.948488473892212
2025-12-09 12:11:16.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.001954275113846926 Training loss: 3.8402230739593506
2025-12-09 12:11:16.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0019388156530759713 Training loss: 3.7646565437316895
2025-12-09 12:11:16.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0019233050486619715 Training loss: 3.8859989643096924
2025-12-09 12:11:16.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0019077451083545144 Training loss: 3.8468658924102783
2025-12-09 12:11:16.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0018921376456532484 Training loss: 3.909980058670044
2025-12-09 12:11:16.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0018764844795965232 Training loss: 3.8529136180877686
2025-12-09 12:11:16.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0018607874345493807 Training loss: 3.9437146186828613
2025-12-09 12:11:16.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0018450483399909264 Training loss: 3.833292007446289
2025-12-09 12:11:16.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0018292690303011077 Training loss: 3.975245952606201
2025-12-09 12:11:16.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.001813451344546913 Training loss: 3.8426599502563477
2025-12-09 12:11:16.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0017975971262680348 Training loss: 3.7686123847961426
2025-12-09 12:11:16.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0017817082232620054 Training loss: 3.6021029949188232
2025-12-09 12:11:16.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0017657864873688344 Training loss: 3.68123197555542
2025-12-09 12:11:16.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0017498337742551818 Training loss: 3.728268623352051
2025-12-09 12:11:16.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0017338519431980798 Training loss: 3.7125933170318604
2025-12-09 12:11:16.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0017178428568682357 Training loss: 3.6876635551452637
2025-12-09 12:11:16.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.001701808381112938 Training loss: 3.839083671569824
2025-12-09 12:11:16.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0016857503847385955 Training loss: 3.774883508682251
2025-12-09 12:11:16.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0016696707392929266 Training loss: 3.9227681159973145
2025-12-09 12:11:16.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.001653571318846834 Training loss: 3.741102457046509
2025-12-09 12:11:16.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0016374539997759824 Training loss: 3.611140727996826
2025-12-09 12:11:16.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0016213206605421066 Training loss: 3.7666587829589844
2025-12-09 12:11:16.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.001605173181474081 Training loss: 3.7247698307037354
2025-12-09 12:11:16.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0015890134445487678 Training loss: 3.746859550476074
2025-12-09 12:11:16.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0015728433331716725 Training loss: 3.7799718379974365
2025-12-09 12:11:16.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0015566647319574351 Training loss: 3.8245632648468018
2025-12-09 12:11:16.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0015404795265101807 Training loss: 3.7405476570129395
2025-12-09 12:11:16.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0015242896032037524 Training loss: 3.732273578643799
2025-12-09 12:11:16.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0015080968489618568 Training loss: 3.7700366973876953
2025-12-09 12:11:16.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0014919031510381437 Training loss: 3.65207839012146
2025-12-09 12:11:16.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0014757103967962479 Training loss: 3.6237385272979736
2025-12-09 12:11:16.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0014595204734898198 Training loss: 3.655108690261841
2025-12-09 12:11:16.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0014433352680425654 Training loss: 3.5953290462493896
2025-12-09 12:11:16.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0014271566668283282 Training loss: 3.7952239513397217
2025-12-09 12:11:16.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.001410986555451232 Training loss: 3.734088659286499
2025-12-09 12:11:16.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0013948268185259188 Training loss: 3.70878267288208
2025-12-09 12:11:16.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.001378679339457894 Training loss: 3.6708717346191406
2025-12-09 12:11:16.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0013625460002240181 Training loss: 3.6540708541870117
2025-12-09 12:11:16.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0013464286811531662 Training loss: 3.78922438621521
2025-12-09 12:11:16.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0013303292607070737 Training loss: 3.756970167160034
2025-12-09 12:11:16.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.001314249615261405 Training loss: 3.8160412311553955
2025-12-09 12:11:16.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0012981916188870622 Training loss: 3.6546638011932373
2025-12-09 12:11:16.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.001282157143131765 Training loss: 3.7586829662323
2025-12-09 12:11:16.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00126614805680192 Training loss: 3.564440965652466
2025-12-09 12:11:16.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0012501662257448183 Training loss: 3.6702194213867188
2025-12-09 12:11:16.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.001234213512631166 Training loss: 3.7378716468811035
2025-12-09 12:11:16.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.001218291776737995 Training loss: 3.6862990856170654
2025-12-09 12:11:16.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0012024028737319652 Training loss: 3.8665690422058105
2025-12-09 12:11:16.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0011865486554530874 Training loss: 3.6094510555267334
2025-12-09 12:11:16.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.001170730969698893 Training loss: 3.7939352989196777
2025-12-09 12:11:16.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0011549516600090739 Training loss: 3.7086575031280518
2025-12-09 12:11:16.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00113921256545062 Training loss: 3.6620185375213623
2025-12-09 12:11:16.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0011235155204034769 Training loss: 3.6164095401763916
2025-12-09 12:11:16.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0011078623543467519 Training loss: 3.662523031234741
2025-12-09 12:11:16.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0010922548916454857 Training loss: 3.832707405090332
2025-12-09 12:11:16.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0010766949513380285 Training loss: 3.7165815830230713
2025-12-09 12:11:16.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.001061184346924029 Training loss: 3.8282582759857178
2025-12-09 12:11:16.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0010457248861530741 Training loss: 3.6399621963500977
2025-12-09 12:11:16.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0010303183708139964 Training loss: 3.779167413711548
2025-12-09 12:11:16.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0010149665965248776 Training loss: 3.731172561645508
2025-12-09 12:11:16.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009996713525237694 Training loss: 3.5289666652679443
2025-12-09 12:11:17.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0009844344214601601 Training loss: 3.6931207180023193
2025-12-09 12:11:17.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.000969257579187209 Training loss: 3.632047176361084
2025-12-09 12:11:17.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009541425945547689 Training loss: 3.632629156112671
2025-12-09 12:11:17.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.000939091229203231 Training loss: 3.6704788208007812
2025-12-09 12:11:17.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0009241052373582058 Training loss: 3.807861089706421
2025-12-09 12:11:17.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009091863656260695 Training loss: 3.61881947517395
2025-12-09 12:11:17.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0008943363527903977 Training loss: 3.614280939102173
2025-12-09 12:11:17.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0008795569296093132 Training loss: 3.6600985527038574
2025-12-09 12:11:17.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0008648498186137653 Training loss: 3.725537061691284
2025-12-09 12:11:17.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0008502167339067705 Training loss: 3.5643680095672607
2025-12-09 12:11:17.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0008356593809636371 Training loss: 3.706554412841797
2025-12-09 12:11:17.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0008211794564331918 Training loss: 3.715320348739624
2025-12-09 12:11:17.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0008067786479400346 Training loss: 3.7779767513275146
2025-12-09 12:11:17.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0007924586338878512 Training loss: 3.6570370197296143
2025-12-09 12:11:17.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0007782210832637924 Training loss: 3.639760971069336
2025-12-09 12:11:17.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0007640676554439594 Training loss: 3.590991497039795
2025-12-09 12:11:17.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0007500000000000003 Training loss: 3.603179454803467
2025-12-09 12:11:17.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.000736019756506856 Training loss: 3.6473493576049805
2025-12-09 12:11:17.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.000722128554351668 Training loss: 3.675332546234131
2025-12-09 12:11:17.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0007083280125438767 Training loss: 3.605457305908203
2025-12-09 12:11:17.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0006946197395265243 Training loss: 3.629565715789795
2025-12-09 12:11:17.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0006810053329887928 Training loss: 3.686429023742676
2025-12-09 12:11:17.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0006674863796797954 Training loss: 3.531639814376831
2025-12-09 12:11:17.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0006540644552236401 Training loss: 3.6169493198394775
2025-12-09 12:11:17.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0006407411239357954 Training loss: 3.478607416152954
2025-12-09 12:11:17.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0006275179386407663 Training loss: 3.7347211837768555
2025-12-09 12:11:17.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0006143964404911165 Training loss: 3.551790714263916
2025-12-09 12:11:17.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0006013781587878463 Training loss: 3.6628663539886475
2025-12-09 12:11:17.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0005884646108021563 Training loss: 3.7082033157348633
2025-12-09 12:11:17.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.000575657301598609 Training loss: 3.6681227684020996
2025-12-09 12:11:17.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0005629577238597133 Training loss: 3.7171988487243652
2025-12-09 12:11:17.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0005503673577119553 Training loss: 3.631030797958374
2025-12-09 12:11:17.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0005378876705532904 Training loss: 3.7543561458587646
2025-12-09 12:11:17.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0005255201168821184 Training loss: 3.7378268241882324
2025-12-09 12:11:17.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0005132661381277644 Training loss: 3.526942491531372
2025-12-09 12:11:17.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0005011271624824787 Training loss: 3.7059240341186523
2025-12-09 12:11:17.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0004891046047349837 Training loss: 3.670167922973633
2025-12-09 12:11:17.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00047719986610558236 Training loss: 3.70457124710083
2025-12-09 12:11:17.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00046541433408284357 Training loss: 3.7292397022247314
2025-12-09 12:11:17.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00045374938226189584 Training loss: 3.6711528301239014
2025-12-09 12:11:17.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.00044220637018433165 Training loss: 3.720659017562866
2025-12-09 12:11:17.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00043078664317975653 Training loss: 3.769650936126709
2025-12-09 12:11:17.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0004194915322089899 Training loss: 3.691012144088745
2025-12-09 12:11:17.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00040832235370894606 Training loss: 3.6157045364379883
2025-12-09 12:11:17.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0003972804094391998 Training loss: 3.721829891204834
2025-12-09 12:11:17.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0003863669863302698 Training loss: 3.626802682876587
2025-12-09 12:11:17.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00037558335633362935 Training loss: 3.6349053382873535
2025-12-09 12:11:17.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.000364930776273457 Training loss: 3.6700546741485596
2025-12-09 12:11:17.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0003544104877001596 Training loss: 3.6728365421295166
2025-12-09 12:11:17.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0003440237167456663 Training loss: 3.7096567153930664
2025-12-09 12:11:17.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0003337716739805264 Training loss: 3.6453793048858643
2025-12-09 12:11:17.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00032365555427281634 Training loss: 3.65512752532959
2025-12-09 12:11:17.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00031367653664888173 Training loss: 3.6861917972564697
2025-12-09 12:11:17.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0003038357841559191 Training loss: 3.7061915397644043
2025-12-09 12:11:17.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00029413444372642495 Training loss: 3.5000641345977783
2025-12-09 12:11:17.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.00028457364604452375 Training loss: 3.5359504222869873
2025-12-09 12:11:17.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.00027515450541418343 Training loss: 3.5243031978607178
2025-12-09 12:11:17.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.0002658781196293482 Training loss: 3.695631742477417
2025-12-09 12:11:17.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0002567455698459882 Training loss: 3.811934232711792
2025-12-09 12:11:17.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00024775792045609354 Training loss: 3.5547783374786377
2025-12-09 12:11:17.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.00023891621896361882 Training loss: 3.650632858276367
2025-12-09 12:11:17.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.00023022149586239971 Training loss: 3.685685634613037
2025-12-09 12:11:17.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.00022167476451604625 Training loss: 3.565363883972168
2025-12-09 12:11:17.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.00021327702103983865 Training loss: 3.6627588272094727
2025-12-09 12:11:17.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.00020502924418463014 Training loss: 3.623643159866333
2025-12-09 12:11:17.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0001969323952227733 Training loss: 3.7307283878326416
2025-12-09 12:11:17.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.00018898741783608642 Training loss: 3.743192434310913
2025-12-09 12:11:17.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00018119523800586568 Training loss: 3.584869384765625
2025-12-09 12:11:17.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.00017355676390496484 Training loss: 3.6303141117095947
2025-12-09 12:11:17.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0001660728857919464 Training loss: 3.6914708614349365
2025-12-09 12:11:17.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00015874447590732537 Training loss: 3.6648669242858887
2025-12-09 12:11:17.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0001515723883719072 Training loss: 3.5884857177734375
2025-12-09 12:11:17.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00014455745908724228 Training loss: 3.716402530670166
2025-12-09 12:11:17.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0001377005056382018 Training loss: 3.6422224044799805
2025-12-09 12:11:17.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00013100232719768994 Training loss: 3.4899890422821045
2025-12-09 12:11:17.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00012446370443349862 Training loss: 3.7910196781158447
2025-12-09 12:11:17.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0001180853994173236 Training loss: 3.553525447845459
2025-12-09 12:11:17.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.00011186815553594382 Training loss: 3.667499303817749
2025-12-09 12:11:17.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0001058126974045811 Training loss: 3.6726555824279785
2025-12-09 12:11:17.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.991973078244637e-05 Training loss: 3.667052984237671
2025-12-09 12:11:17.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.418994249048473e-05 Training loss: 3.6212940216064453
2025-12-09 12:11:17.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 8.862400033132573e-05 Training loss: 3.525625467300415
2025-12-09 12:11:17.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 8.322255301145204e-05 Training loss: 3.6613752841949463
2025-12-09 12:11:17.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 7.798623006559435e-05 Training loss: 3.64807391166687
2025-12-09 12:11:17.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 7.291564178335719e-05 Training loss: 3.6626946926116943
2025-12-09 12:11:17.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 6.801137913809213e-05 Training loss: 3.759507417678833
2025-12-09 12:11:17.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 6.327401371801944e-05 Training loss: 3.6763875484466553
2025-12-09 12:11:17.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 5.870409765960966e-05 Training loss: 3.517735004425049
2025-12-09 12:11:17.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 5.430216358323309e-05 Training loss: 3.740917444229126
2025-12-09 12:11:17.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 5.00687245310833e-05 Training loss: 3.691002368927002
2025-12-09 12:11:17.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 4.60042739073816e-05 Training loss: 3.753091812133789
2025-12-09 12:11:17.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 4.2109285420872056e-05 Training loss: 3.7970335483551025
2025-12-09 12:11:17.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 3.8384213029610984e-05 Training loss: 3.6770973205566406
2025-12-09 12:11:17.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 3.4829490888057425e-05 Training loss: 3.786064624786377
2025-12-09 12:11:17.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 3.144553329647448e-05 Training loss: 3.731086254119873
2025-12-09 12:11:17.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 2.823273465264142e-05 Training loss: 3.7673516273498535
2025-12-09 12:11:17.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 2.5191469405887624e-05 Training loss: 3.8123297691345215
2025-12-09 12:11:17.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 2.2322092013450313e-05 Training loss: 3.526984214782715
2025-12-09 12:11:17.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 1.9624936899163947e-05 Training loss: 3.717155694961548
2025-12-09 12:11:17.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 1.7100318414482063e-05 Training loss: 3.8006625175476074
2025-12-09 12:11:17.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 1.4748530801840077e-05 Training loss: 3.5948002338409424
2025-12-09 12:11:17.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 1.2569848160362384e-05 Training loss: 3.5763020515441895
2025-12-09 12:11:17.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 1.0564524413915422e-05 Training loss: 3.5917298793792725
2025-12-09 12:11:17.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 8.732793281513663e-06 Training loss: 3.6813905239105225
2025-12-09 12:11:17.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 7.07486825007908e-06 Training loss: 3.664419651031494
2025-12-09 12:11:17.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 5.590942549560052e-06 Training loss: 3.6167490482330322
2025-12-09 12:11:17.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 4.281189130410535e-06 Training loss: 3.5333473682403564
2025-12-09 12:11:17.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 3.145760643432527e-06 Training loss: 3.5475738048553467
2025-12-09 12:11:17.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 2.184789421984634e-06 Training loss: 3.6816697120666504
2025-12-09 12:11:17.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 1.3983874665589036e-06 Training loss: 3.5150558948516846
2025-12-09 12:11:17.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 7.866464317276001e-07 Training loss: 3.660283088684082
2025-12-09 12:11:17.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 3.496376154604186e-07 Training loss: 3.6694021224975586
2025-12-09 12:11:17.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 8.741195081479747e-08 Training loss: 3.647266387939453
2025-12-09 12:11:18.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 3.6059932708740234
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-12-09 12:11:31.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 4.862654209136963
2025-12-09 12:11:31.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 4.721698760986328
2025-12-09 12:11:31.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 4.969914436340332
2025-12-09 12:11:31.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 4.924443244934082
2025-12-09 12:11:31.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 4.905307292938232
2025-12-09 12:11:31.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 4.857205390930176
2025-12-09 12:11:31.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 4.911686897277832
2025-12-09 12:11:31.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 4.9668049812316895
2025-12-09 12:11:31.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 4.875638484954834
2025-12-09 12:11:31.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 4.87880802154541
2025-12-09 12:11:31.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 4.728329658508301
2025-12-09 12:11:31.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 4.811370372772217
2025-12-09 12:11:31.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 4.927544593811035
2025-12-09 12:11:31.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 4.795581340789795
2025-12-09 12:11:31.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 4.891586780548096
2025-12-09 12:11:31.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 4.785671234130859
2025-12-09 12:11:31.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 4.739665985107422
2025-12-09 12:11:31.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 4.747305870056152
2025-12-09 12:11:31.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 4.695624828338623
2025-12-09 12:11:31.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 4.755557060241699
2025-12-09 12:11:31.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 4.767401218414307
2025-12-09 12:11:31.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 4.668264865875244
2025-12-09 12:11:31.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 4.6385416984558105
2025-12-09 12:11:31.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 4.745790004730225
2025-12-09 12:11:31.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 4.727835178375244
2025-12-09 12:11:31.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 4.595595359802246
2025-12-09 12:11:31.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 4.644693374633789
2025-12-09 12:11:31.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 4.682067394256592
2025-12-09 12:11:31.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 4.639716625213623
2025-12-09 12:11:31.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 4.526797294616699
2025-12-09 12:11:31.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 4.484350204467773
2025-12-09 12:11:31.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 4.586214542388916
2025-12-09 12:11:31.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 4.644262313842773
2025-12-09 12:11:31.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 4.515822410583496
2025-12-09 12:11:31.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 4.501793384552002
2025-12-09 12:11:31.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 4.590963840484619
2025-12-09 12:11:31.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 4.5852580070495605
2025-12-09 12:11:31.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 4.629395008087158
2025-12-09 12:11:31.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 4.412075042724609
2025-12-09 12:11:31.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 4.4919538497924805
2025-12-09 12:11:31.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 4.5680131912231445
2025-12-09 12:11:31.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 4.437747478485107
2025-12-09 12:11:31.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 4.5107316970825195
2025-12-09 12:11:31.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 4.375293731689453
2025-12-09 12:11:31.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 4.487929821014404
2025-12-09 12:11:31.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 4.5067620277404785
2025-12-09 12:11:31.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 4.519905090332031
2025-12-09 12:11:31.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 4.478378772735596
2025-12-09 12:11:31.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 4.287261486053467
2025-12-09 12:11:31.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 4.419903755187988
2025-12-09 12:11:31.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 4.412005424499512
2025-12-09 12:11:31.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 4.422255992889404
2025-12-09 12:11:31.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 4.322958469390869
2025-12-09 12:11:31.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 4.327676773071289
2025-12-09 12:11:31.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 4.196472644805908
2025-12-09 12:11:31.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 4.43166971206665
2025-12-09 12:11:31.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 4.2722272872924805
2025-12-09 12:11:31.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 4.35259485244751
2025-12-09 12:11:31.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 4.324082374572754
2025-12-09 12:11:31.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 4.236082553863525
2025-12-09 12:11:31.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 4.299930572509766
2025-12-09 12:11:31.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 4.306300640106201
2025-12-09 12:11:31.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 4.182985305786133
2025-12-09 12:11:31.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 4.296854019165039
2025-12-09 12:11:31.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 4.183943271636963
2025-12-09 12:11:31.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 4.17283296585083
2025-12-09 12:11:31.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 4.201562881469727
2025-12-09 12:11:31.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 4.273834228515625
2025-12-09 12:11:31.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 4.310874938964844
2025-12-09 12:11:31.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 4.190938949584961
2025-12-09 12:11:31.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 4.231650352478027
2025-12-09 12:11:31.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 4.04066276550293
2025-12-09 12:11:31.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 4.151917457580566
2025-12-09 12:11:31.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 4.239709377288818
2025-12-09 12:11:31.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 4.177313804626465
2025-12-09 12:11:32.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 4.175087928771973
2025-12-09 12:11:32.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 4.090084552764893
2025-12-09 12:11:32.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 4.188939094543457
2025-12-09 12:11:32.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 4.159921169281006
2025-12-09 12:11:32.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 4.05527400970459
2025-12-09 12:11:32.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 4.191657543182373
2025-12-09 12:11:32.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 4.026464939117432
2025-12-09 12:11:32.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 4.099954128265381
2025-12-09 12:11:32.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 4.021544456481934
2025-12-09 12:11:32.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 4.079756736755371
2025-12-09 12:11:32.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 3.988640785217285
2025-12-09 12:11:32.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 4.019314289093018
2025-12-09 12:11:32.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 3.9952590465545654
2025-12-09 12:11:32.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 4.053541660308838
2025-12-09 12:11:32.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 4.08223295211792
2025-12-09 12:11:32.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 4.098720550537109
2025-12-09 12:11:32.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 4.05739688873291
2025-12-09 12:11:32.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 4.086058616638184
2025-12-09 12:11:32.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 3.9477956295013428
2025-12-09 12:11:32.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 4.193170070648193
2025-12-09 12:11:32.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 4.015628814697266
2025-12-09 12:11:32.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 3.8913424015045166
2025-12-09 12:11:32.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 4.031365871429443
2025-12-09 12:11:32.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 3.8871357440948486
2025-12-09 12:11:32.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 4.076841831207275
2025-12-09 12:11:32.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999708626830616 Training loss: 4.098217487335205
2025-12-09 12:11:32.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.009998834541281799 Training loss: 4.044252872467041
2025-12-09 12:11:32.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009997377845227575 Training loss: 4.103930950164795
2025-12-09 12:11:32.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009995338708444804 Training loss: 4.0360283851623535
2025-12-09 12:11:32.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009992717368593385 Training loss: 3.9324235916137695
2025-12-09 12:11:32.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009989514131188558 Training loss: 3.8838694095611572
2025-12-09 12:11:32.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009985729369565299 Training loss: 3.9518728256225586
2025-12-09 12:11:32.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0099813635248348 Training loss: 4.035652160644531
2025-12-09 12:11:32.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.00997641710583307 Training loss: 3.983050584793091
2025-12-09 12:11:32.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.009970890689061622 Training loss: 3.9074456691741943
2025-12-09 12:11:32.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009964784918620281 Training loss: 3.833235740661621
2025-12-09 12:11:32.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.009958100506132127 Training loss: 3.9820804595947266
2025-12-09 12:11:32.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009950838230660534 Training loss: 3.9679923057556152
2025-12-09 12:11:32.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009942998938618395 Training loss: 3.8568732738494873
2025-12-09 12:11:32.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.009934583543669454 Training loss: 3.898693084716797
2025-12-09 12:11:32.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009925593026621833 Training loss: 3.773484468460083
2025-12-09 12:11:32.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.009916028435313709 Training loss: 3.80088210105896
2025-12-09 12:11:32.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.009905890884491196 Training loss: 3.8615643978118896
2025-12-09 12:11:32.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.00989518155567842 Training loss: 3.8784496784210205
2025-12-09 12:11:32.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009883901697039808 Training loss: 3.7996814250946045
2025-12-09 12:11:32.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.009872052623234632 Training loss: 3.9008889198303223
2025-12-09 12:11:32.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00985963571526376 Training loss: 3.992875576019287
2025-12-09 12:11:32.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.009846652420308728 Training loss: 3.972672939300537
2025-12-09 12:11:32.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.009833104251563056 Training loss: 4.089195251464844
2025-12-09 12:11:32.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.00981899278805589 Training loss: 3.973763942718506
2025-12-09 12:11:32.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.009804319674467968 Training loss: 3.9818267822265625
2025-12-09 12:11:32.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009789086620939936 Training loss: 3.9627599716186523
2025-12-09 12:11:32.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009773295402873026 Training loss: 3.9199695587158203
2025-12-09 12:11:32.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009756947860722143 Training loss: 3.7752280235290527
2025-12-09 12:11:32.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009740045899781353 Training loss: 3.8157455921173096
2025-12-09 12:11:32.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.009722591489961827 Training loss: 3.9317028522491455
2025-12-09 12:11:32.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009704586665562249 Training loss: 3.832310914993286
2025-12-09 12:11:32.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.00968603352503172 Training loss: 3.838536500930786
2025-12-09 12:11:32.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009666934230725179 Training loss: 3.8432419300079346
2025-12-09 12:11:32.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009647291008651398 Training loss: 3.946479558944702
2025-12-09 12:11:32.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009627106148213521 Training loss: 3.8278565406799316
2025-12-09 12:11:32.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.009606382001942255 Training loss: 3.837176561355591
2025-12-09 12:11:32.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00958512098522167 Training loss: 3.7752645015716553
2025-12-09 12:11:32.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0095633255760077 Training loss: 3.876997232437134
2025-12-09 12:11:32.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009540998314539327 Training loss: 3.799747943878174
2025-12-09 12:11:32.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009518141803042527 Training loss: 3.902280569076538
2025-12-09 12:11:32.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.009494758705426976 Training loss: 3.7510015964508057
2025-12-09 12:11:32.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.009470851746975581 Training loss: 3.851285457611084
2025-12-09 12:11:32.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009446423714026845 Training loss: 3.93859601020813
2025-12-09 12:11:32.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.009421477453650118 Training loss: 3.791714906692505
2025-12-09 12:11:32.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.009396015873313781 Training loss: 3.931658983230591
2025-12-09 12:11:32.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00937004194054638 Training loss: 3.971956968307495
2025-12-09 12:11:32.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009343558682590757 Training loss: 3.920332193374634
2025-12-09 12:11:32.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.009316569186051234 Training loss: 4.01720666885376
2025-12-09 12:11:32.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009289076596533871 Training loss: 3.8473055362701416
2025-12-09 12:11:32.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009261084118279847 Training loss: 3.701213836669922
2025-12-09 12:11:32.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009232595013792002 Training loss: 3.8348889350891113
2025-12-09 12:11:32.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009203612603454604 Training loss: 3.7391135692596436
2025-12-09 12:11:32.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.009174140265146355 Training loss: 3.8918018341064453
2025-12-09 12:11:32.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009144181433846706 Training loss: 3.833735942840576
2025-12-09 12:11:32.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009113739601235507 Training loss: 3.9575021266937256
2025-12-09 12:11:32.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009082818315286054 Training loss: 3.9917266368865967
2025-12-09 12:11:32.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009051421179851587 Training loss: 3.9493062496185303
2025-12-09 12:11:32.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.00901955185424525 Training loss: 3.891906499862671
2025-12-09 12:11:32.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.008987214052813604 Training loss: 3.801133155822754
2025-12-09 12:11:32.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00895441154450373 Training loss: 3.788966417312622
2025-12-09 12:11:32.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.008921148152423945 Training loss: 3.9665262699127197
2025-12-09 12:11:32.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.008887427753398248 Training loss: 3.7710320949554443
2025-12-09 12:11:32.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.008853254277514447 Training loss: 3.7717249393463135
2025-12-09 12:11:32.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.008818631707666134 Training loss: 3.8564321994781494
2025-12-09 12:11:32.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.008783564079088476 Training loss: 3.8475704193115234
2025-12-09 12:11:32.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.008748055478887904 Training loss: 3.802943229675293
2025-12-09 12:11:32.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.008712110045565767 Training loss: 3.893688917160034
2025-12-09 12:11:32.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.008675731968536002 Training loss: 4.012231349945068
2025-12-09 12:11:32.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.008638925487636848 Training loss: 3.7744903564453125
2025-12-09 12:11:32.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0086016948926367 Training loss: 3.740689754486084
2025-12-09 12:11:32.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.008564044522734147 Training loss: 3.7097160816192627
2025-12-09 12:11:32.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.008525978766052229 Training loss: 3.7699060440063477
2025-12-09 12:11:32.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.008487502059127015 Training loss: 3.8364624977111816
2025-12-09 12:11:32.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.008448618886390521 Training loss: 3.77376651763916
2025-12-09 12:11:32.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00840933377964806 Training loss: 3.9062561988830566
2025-12-09 12:11:32.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.008369651317550054 Training loss: 3.8237380981445312
2025-12-09 12:11:32.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.008329576125058406 Training loss: 3.937168598175049
2025-12-09 12:11:32.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.008289112872907454 Training loss: 3.7132492065429688
2025-12-09 12:11:32.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.008248266277059607 Training loss: 3.85089373588562
2025-12-09 12:11:32.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0082070410981557 Training loss: 3.9408817291259766
2025-12-09 12:11:32.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00816544214096015 Training loss: 3.9082353115081787
2025-12-09 12:11:32.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.008123474253800956 Training loss: 3.9722323417663574
2025-12-09 12:11:32.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.008081142328004637 Training loss: 3.9065396785736084
2025-12-09 12:11:32.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.008038451297326145 Training loss: 3.952177047729492
2025-12-09 12:11:32.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.007995406137373847 Training loss: 3.7601816654205322
2025-12-09 12:11:32.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.007952011865029614 Training loss: 3.7707784175872803
2025-12-09 12:11:32.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.007908273537864113 Training loss: 3.8210134506225586
2025-12-09 12:11:33.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.007864196253547348 Training loss: 3.9829063415527344
2025-12-09 12:11:33.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.007819785149254532 Training loss: 3.908796787261963
2025-12-09 12:11:33.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.00777504540106735 Training loss: 3.9035048484802246
2025-12-09 12:11:33.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.007729982223370691 Training loss: 3.909440755844116
2025-12-09 12:11:33.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.00768460086824492 Training loss: 3.7268006801605225
2025-12-09 12:11:33.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.007638906624853743 Training loss: 3.982117176055908
2025-12-09 12:11:33.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.007592904818827774 Training loss: 3.8920631408691406
2025-12-09 12:11:33.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.007546600811643816 Training loss: 3.99932861328125
2025-12-09 12:11:33.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0075 Training loss: 3.8728384971618652
2025-12-09 12:11:33.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.007453107815186802 Training loss: 3.882960557937622
2025-12-09 12:11:33.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.007405929722454026 Training loss: 3.858671188354492
2025-12-09 12:11:33.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.007358471220373831 Training loss: 3.8291189670562744
2025-12-09 12:11:33.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.007310737840199885 Training loss: 3.8449020385742188
2025-12-09 12:11:33.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.007262735145222695 Training loss: 3.9609317779541016
2025-12-09 12:11:33.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.007214468730121208 Training loss: 3.9249985218048096
2025-12-09 12:11:33.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.007165944220310766 Training loss: 3.8906047344207764
2025-12-09 12:11:33.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.007117167271287452 Training loss: 3.9052555561065674
2025-12-09 12:11:33.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.007068143567968957 Training loss: 3.8469834327697754
2025-12-09 12:11:33.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0070188788240320085 Training loss: 3.8811252117156982
2025-12-09 12:11:33.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.006969378781246436 Training loss: 3.8949191570281982
2025-12-09 12:11:33.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.006919649208805981 Training loss: 3.917062759399414
2025-12-09 12:11:33.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0068696959026558965 Training loss: 3.9057724475860596
2025-12-09 12:11:33.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.006819524684817438 Training loss: 3.928098440170288
2025-12-09 12:11:33.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0067691414027093045 Training loss: 3.9533424377441406
2025-12-09 12:11:33.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.006718551928466133 Training loss: 3.960625171661377
2025-12-09 12:11:33.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.006667762158254104 Training loss: 3.9195752143859863
2025-12-09 12:11:33.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.006616778011583743 Training loss: 3.945659637451172
2025-12-09 12:11:33.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.006565605430620013 Training loss: 3.888101100921631
2025-12-09 12:11:33.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.006514250379489753 Training loss: 3.8216516971588135
2025-12-09 12:11:33.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.006462718843586571 Training loss: 3.922191619873047
2025-12-09 12:11:33.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0064110168288732386 Training loss: 3.828227996826172
2025-12-09 12:11:33.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.006359150361181715 Training loss: 3.7989819049835205
2025-12-09 12:11:33.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.006307125485510829 Training loss: 3.8461391925811768
2025-12-09 12:11:33.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0062549482653217435 Training loss: 3.9375855922698975
2025-12-09 12:11:33.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0062026247818312685 Training loss: 3.9012951850891113
2025-12-09 12:11:33.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0061501611333030885 Training loss: 3.7938294410705566
2025-12-09 12:11:33.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.006097563434337026 Training loss: 3.860767364501953
2025-12-09 12:11:33.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.006044837815156376 Training loss: 3.8050098419189453
2025-12-09 12:11:33.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.005991990420893449 Training loss: 3.9085919857025146
2025-12-09 12:11:33.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.005939027410873351 Training loss: 3.9429352283477783
2025-12-09 12:11:33.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0058859549578961145 Training loss: 3.878110885620117
2025-12-09 12:11:33.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.005832779247517273 Training loss: 3.7897520065307617
2025-12-09 12:11:33.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0057795064773269325 Training loss: 3.9077813625335693
2025-12-09 12:11:33.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.005726142856227452 Training loss: 3.849391460418701
2025-12-09 12:11:33.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.005672694603709794 Training loss: 3.919929265975952
2025-12-09 12:11:33.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.005619167949128652 Training loss: 3.9092259407043457
2025-12-09 12:11:33.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.005565569130976423 Training loss: 3.8333356380462646
2025-12-09 12:11:33.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.005511904396156113 Training loss: 3.8947973251342773
2025-12-09 12:11:33.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.005458179999253274 Training loss: 3.7952513694763184
2025-12-09 12:11:33.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.005404402201807022 Training loss: 3.902158498764038
2025-12-09 12:11:33.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00535057727158027 Training loss: 3.963897228240967
2025-12-09 12:11:33.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.005296711481829226 Training loss: 3.9625351428985596
2025-12-09 12:11:33.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.005242811110572242 Training loss: 3.986726760864258
2025-12-09 12:11:33.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.005188882439858117 Training loss: 3.782170534133911
2025-12-09 12:11:33.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.005134931755033936 Training loss: 3.913926601409912
2025-12-09 12:11:33.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.005080965344012508 Training loss: 3.8284809589385986
2025-12-09 12:11:33.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.005026989496539522 Training loss: 3.9078915119171143
2025-12-09 12:11:33.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.004973010503460479 Training loss: 3.890385150909424
2025-12-09 12:11:33.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.004919034655987493 Training loss: 3.9408814907073975
2025-12-09 12:11:33.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.004865068244966066 Training loss: 3.933562755584717
2025-12-09 12:11:33.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0048111175601418844 Training loss: 3.893522262573242
2025-12-09 12:11:33.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0047571888894277605 Training loss: 3.865877866744995
2025-12-09 12:11:33.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0047032885181707736 Training loss: 3.9430980682373047
2025-12-09 12:11:33.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.004649422728419729 Training loss: 3.903740406036377
2025-12-09 12:11:33.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0045955977981929795 Training loss: 3.7563443183898926
2025-12-09 12:11:33.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.004541820000746727 Training loss: 3.9440009593963623
2025-12-09 12:11:33.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.004488095603843887 Training loss: 3.9523515701293945
2025-12-09 12:11:33.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.004434430869023579 Training loss: 3.9414052963256836
2025-12-09 12:11:33.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00438083205087135 Training loss: 3.9192235469818115
2025-12-09 12:11:33.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.004327305396290207 Training loss: 3.916205883026123
2025-12-09 12:11:33.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00427385714377255 Training loss: 3.912727117538452
2025-12-09 12:11:33.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.004220493522673068 Training loss: 3.9411745071411133
2025-12-09 12:11:33.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.004167220752482727 Training loss: 3.916149854660034
2025-12-09 12:11:33.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0041140450421038866 Training loss: 3.986769437789917
2025-12-09 12:11:33.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00406097258912665 Training loss: 3.9621918201446533
2025-12-09 12:11:33.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.004008009579106551 Training loss: 3.941664934158325
2025-12-09 12:11:33.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.003955162184843625 Training loss: 4.070698261260986
2025-12-09 12:11:33.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.003902436565662977 Training loss: 3.9918408393859863
2025-12-09 12:11:33.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.003849838866696913 Training loss: 3.9952499866485596
2025-12-09 12:11:33.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.003797375218168733 Training loss: 3.938755512237549
2025-12-09 12:11:33.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0037450517346782563 Training loss: 3.990736722946167
2025-12-09 12:11:33.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0036928745144891727 Training loss: 3.9722604751586914
2025-12-09 12:11:33.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0036408496388182854 Training loss: 3.8684351444244385
2025-12-09 12:11:33.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0035889831711267616 Training loss: 3.8545961380004883
2025-12-09 12:11:33.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00353728115641343 Training loss: 4.054102420806885
2025-12-09 12:11:33.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.003485749620510247 Training loss: 3.942277669906616
2025-12-09 12:11:33.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0034343945693799884 Training loss: 4.013033866882324
2025-12-09 12:11:33.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0033832219884162586 Training loss: 3.9333646297454834
2025-12-09 12:11:33.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0033322378417458983 Training loss: 3.873056173324585
2025-12-09 12:11:33.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0032814480715338667 Training loss: 3.894305944442749
2025-12-09 12:11:33.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0032308585972906966 Training loss: 3.9368066787719727
2025-12-09 12:11:33.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0031804753151825627 Training loss: 3.870872974395752
2025-12-09 12:11:33.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0031303040973441033 Training loss: 3.9965274333953857
2025-12-09 12:11:33.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.003080350791194019 Training loss: 3.879369020462036
2025-12-09 12:11:33.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0030306212187535654 Training loss: 4.049580097198486
2025-12-09 12:11:33.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029811211759679926 Training loss: 3.919034719467163
2025-12-09 12:11:33.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029318564320310442 Training loss: 3.973761558532715
2025-12-09 12:11:33.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002882832728712551 Training loss: 4.031489849090576
2025-12-09 12:11:33.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0028340557796892353 Training loss: 3.980501651763916
2025-12-09 12:11:33.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0027855312698787903 Training loss: 3.8006439208984375
2025-12-09 12:11:33.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.002737264854777306 Training loss: 3.944746732711792
2025-12-09 12:11:33.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0026892621598001154 Training loss: 3.9156999588012695
2025-12-09 12:11:33.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0026415287796261707 Training loss: 3.902442693710327
2025-12-09 12:11:33.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0025940702775459745 Training loss: 4.028079986572266
2025-12-09 12:11:33.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002546892184813198 Training loss: 3.9284791946411133
2025-12-09 12:11:33.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0025000000000000014 Training loss: 3.90989351272583
2025-12-09 12:11:33.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0024533991883561868 Training loss: 3.9583487510681152
2025-12-09 12:11:33.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.002407095181172227 Training loss: 3.9241085052490234
2025-12-09 12:11:33.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0023610933751462555 Training loss: 3.830796718597412
2025-12-09 12:11:33.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002315399131755081 Training loss: 4.122804641723633
2025-12-09 12:11:33.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0022700177766293095 Training loss: 3.9318158626556396
2025-12-09 12:11:34.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.002224954598932651 Training loss: 3.9008328914642334
2025-12-09 12:11:34.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0021802148507454673 Training loss: 3.932372570037842
2025-12-09 12:11:34.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0021358037464526513 Training loss: 3.9924678802490234
2025-12-09 12:11:34.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0020917264621358876 Training loss: 3.7409842014312744
2025-12-09 12:11:34.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0020479881349703883 Training loss: 3.8990533351898193
2025-12-09 12:11:34.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0020045938626261544 Training loss: 3.9349300861358643
2025-12-09 12:11:34.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0019615487026738545 Training loss: 3.800612688064575
2025-12-09 12:11:34.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0019188576719953632 Training loss: 3.9569833278656006
2025-12-09 12:11:34.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0018765257461990442 Training loss: 3.9225854873657227
2025-12-09 12:11:34.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0018345578590398509 Training loss: 3.91028094291687
2025-12-09 12:11:34.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0017929589018443016 Training loss: 3.9152793884277344
2025-12-09 12:11:34.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0017517337229403945 Training loss: 3.902869701385498
2025-12-09 12:11:34.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.001710887127092548 Training loss: 3.9113223552703857
2025-12-09 12:11:34.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0016704238749415956 Training loss: 3.9429147243499756
2025-12-09 12:11:34.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0016303486824499457 Training loss: 3.9565155506134033
2025-12-09 12:11:34.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0015906662203519412 Training loss: 3.9254329204559326
2025-12-09 12:11:34.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0015513811136094785 Training loss: 3.939687490463257
2025-12-09 12:11:34.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.001512497940872986 Training loss: 3.9328417778015137
2025-12-09 12:11:34.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.001474021233947772 Training loss: 4.05983304977417
2025-12-09 12:11:34.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0014359554772658551 Training loss: 3.8749053478240967
2025-12-09 12:11:34.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0013983051073632995 Training loss: 3.938941717147827
2025-12-09 12:11:34.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0013610745123631535 Training loss: 3.8913333415985107
2025-12-09 12:11:34.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0013242680314639993 Training loss: 3.917250394821167
2025-12-09 12:11:34.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0012878899544342326 Training loss: 3.9330332279205322
2025-12-09 12:11:34.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0012519445211120978 Training loss: 3.917512893676758
2025-12-09 12:11:34.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0012164359209115233 Training loss: 3.9962706565856934
2025-12-09 12:11:34.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0011813682923338654 Training loss: 4.017280578613281
2025-12-09 12:11:34.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0011467457224855543 Training loss: 3.9850916862487793
2025-12-09 12:11:34.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0011125722466017547 Training loss: 3.916255474090576
2025-12-09 12:11:34.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0010788518475760545 Training loss: 3.990952968597412
2025-12-09 12:11:34.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0010455884554962725 Training loss: 3.8530120849609375
2025-12-09 12:11:34.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.001012785947186397 Training loss: 3.9510042667388916
2025-12-09 12:11:34.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00098044814575475 Training loss: 3.885038375854492
2025-12-09 12:11:34.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0009485788201484125 Training loss: 3.8682806491851807
2025-12-09 12:11:34.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0009171816847139447 Training loss: 3.881326675415039
2025-12-09 12:11:34.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.000886260398764494 Training loss: 3.9488165378570557
2025-12-09 12:11:34.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0008558185661532941 Training loss: 3.9613871574401855
2025-12-09 12:11:34.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.000825859734853645 Training loss: 3.905569553375244
2025-12-09 12:11:34.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0007963873965453961 Training loss: 3.942383289337158
2025-12-09 12:11:34.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.000767404986207999 Training loss: 3.8871045112609863
2025-12-09 12:11:34.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0007389158817201541 Training loss: 3.868868827819824
2025-12-09 12:11:34.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0007109234034661288 Training loss: 3.8949484825134277
2025-12-09 12:11:34.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0006834308139487672 Training loss: 3.8463385105133057
2025-12-09 12:11:34.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0006564413174092443 Training loss: 3.992729902267456
2025-12-09 12:11:34.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0006299580594536214 Training loss: 3.9357361793518066
2025-12-09 12:11:34.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0006039841266862189 Training loss: 3.9334776401519775
2025-12-09 12:11:34.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0005785225463498828 Training loss: 3.7592787742614746
2025-12-09 12:11:34.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0005535762859731547 Training loss: 3.9143030643463135
2025-12-09 12:11:34.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0005291482530244179 Training loss: 3.927861213684082
2025-12-09 12:11:34.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0005052412945730239 Training loss: 3.942962884902954
2025-12-09 12:11:34.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00048185819695747425 Training loss: 3.9304752349853516
2025-12-09 12:11:34.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00045900168546067266 Training loss: 3.964097023010254
2025-12-09 12:11:34.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00043667442399229983 Training loss: 3.8872013092041016
2025-12-09 12:11:34.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.0004148790147783288 Training loss: 3.9013235569000244
2025-12-09 12:11:34.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00039361799805774536 Training loss: 3.797542095184326
2025-12-09 12:11:34.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0003728938517864794 Training loss: 3.9449353218078613
2025-12-09 12:11:34.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.00035270899134860366 Training loss: 3.894970178604126
2025-12-09 12:11:34.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00033306576927482126 Training loss: 3.9375550746917725
2025-12-09 12:11:34.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0003139664749682825 Training loss: 3.915729284286499
2025-12-09 12:11:34.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002954133344377524 Training loss: 4.051356315612793
2025-12-09 12:11:34.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.00027740851003817346 Training loss: 3.8921754360198975
2025-12-09 12:11:34.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00025995410021864785 Training loss: 4.056573390960693
2025-12-09 12:11:34.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0002430521392778573 Training loss: 4.006616592407227
2025-12-09 12:11:34.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.00022670459712697378 Training loss: 3.9521312713623047
2025-12-09 12:11:34.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0002109133790600648 Training loss: 3.9455838203430176
2025-12-09 12:11:34.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00019568032553203218 Training loss: 3.8730010986328125
2025-12-09 12:11:34.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0001810072119441103 Training loss: 4.015859603881836
2025-12-09 12:11:34.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.00016689574843694433 Training loss: 3.9601869583129883
2025-12-09 12:11:34.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.000153347579691272 Training loss: 3.9517340660095215
2025-12-09 12:11:34.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0001403642847362402 Training loss: 3.7733254432678223
2025-12-09 12:11:34.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00012794737676536993 Training loss: 3.9681990146636963
2025-12-09 12:11:34.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00011609830296019141 Training loss: 3.886141777038574
2025-12-09 12:11:34.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0001048184443215816 Training loss: 4.091668128967285
2025-12-09 12:11:34.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880474e-05 Training loss: 3.928171157836914
2025-12-09 12:11:34.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629208e-05 Training loss: 3.9110515117645264
2025-12-09 12:11:34.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.44069733781677e-05 Training loss: 3.8733232021331787
2025-12-09 12:11:34.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.54164563305465e-05 Training loss: 3.89414644241333
2025-12-09 12:11:34.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.7001061381606875e-05 Training loss: 3.958062171936035
2025-12-09 12:11:34.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946692e-05 Training loss: 3.9129767417907715
2025-12-09 12:11:34.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.1899493867874615e-05 Training loss: 3.9523539543151855
2025-12-09 12:11:34.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.521508137971807e-05 Training loss: 3.8005762100219727
2025-12-09 12:11:34.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378874e-05 Training loss: 3.9646689891815186
2025-12-09 12:11:34.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.358289416693027e-05 Training loss: 3.847015142440796
2025-12-09 12:11:34.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.863647516520017e-05 Training loss: 3.8909761905670166
2025-12-09 12:11:34.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.4270630434701781e-05 Training loss: 3.9198834896087646
2025-12-09 12:11:34.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441756e-05 Training loss: 3.9823365211486816
2025-12-09 12:11:34.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.2826314066154475e-06 Training loss: 3.8539812564849854
2025-12-09 12:11:34.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.661291555196345e-06 Training loss: 3.9627573490142822
2025-12-09 12:11:34.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253337e-06 Training loss: 3.8935186862945557
2025-12-09 12:11:34.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-06 Training loss: 3.9624314308166504
2025-12-09 12:11:34.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265825e-07 Training loss: 3.9579694271087646
2025-12-09 12:11:34.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 3.942992687225342
