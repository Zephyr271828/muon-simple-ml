2025-12-09 12:10:57.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 4.799441337585449
2025-12-09 12:10:57.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 4.914732456207275
2025-12-09 12:10:57.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 4.891353130340576
2025-12-09 12:10:57.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 4.865036964416504
2025-12-09 12:10:57.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 4.966080665588379
2025-12-09 12:10:57.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 4.945106506347656
2025-12-09 12:10:57.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 4.817596435546875
2025-12-09 12:10:57.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 4.902554512023926
2025-12-09 12:10:57.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 4.842945575714111
2025-12-09 12:10:57.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 4.916752815246582
2025-12-09 12:10:57.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 4.848223686218262
2025-12-09 12:10:57.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 4.947699069976807
2025-12-09 12:10:57.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 4.8872857093811035
2025-12-09 12:10:57.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 4.905829429626465
2025-12-09 12:10:58.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 4.965372085571289
2025-12-09 12:10:58.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 4.784575462341309
2025-12-09 12:10:58.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 4.911920547485352
2025-12-09 12:10:58.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 4.87745475769043
2025-12-09 12:10:58.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 4.898805141448975
2025-12-09 12:10:58.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 4.765896320343018
2025-12-09 12:10:58.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 4.852935314178467
2025-12-09 12:10:58.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 4.85922908782959
2025-12-09 12:10:58.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 4.905550479888916
2025-12-09 12:10:58.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 4.90382719039917
2025-12-09 12:10:58.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 4.733087062835693
2025-12-09 12:10:58.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 4.8201398849487305
2025-12-09 12:10:58.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 4.8505330085754395
2025-12-09 12:10:58.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 4.709349632263184
2025-12-09 12:10:58.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 4.891254425048828
2025-12-09 12:10:58.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 4.824747562408447
2025-12-09 12:10:58.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 4.839806079864502
2025-12-09 12:10:58.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 4.733992576599121
2025-12-09 12:10:58.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 4.72875452041626
2025-12-09 12:10:58.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 4.731082439422607
2025-12-09 12:10:58.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 4.885717391967773
2025-12-09 12:10:58.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 4.676303863525391
2025-12-09 12:10:58.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 4.874965667724609
2025-12-09 12:10:58.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 4.784118175506592
2025-12-09 12:10:58.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 4.8006792068481445
2025-12-09 12:10:58.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 4.670970916748047
2025-12-09 12:10:58.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 4.731918811798096
2025-12-09 12:10:58.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 4.671916484832764
2025-12-09 12:10:58.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 4.697631359100342
2025-12-09 12:10:58.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 4.761440277099609
2025-12-09 12:10:58.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 4.670803546905518
2025-12-09 12:10:58.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 4.7290120124816895
2025-12-09 12:10:58.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 4.812119483947754
2025-12-09 12:10:58.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 4.690625190734863
2025-12-09 12:10:58.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 4.779584884643555
2025-12-09 12:10:58.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 4.75649881362915
2025-12-09 12:10:58.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 4.7104902267456055
2025-12-09 12:10:58.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 4.645386695861816
2025-12-09 12:10:58.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 4.716014862060547
2025-12-09 12:10:58.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 4.770779609680176
2025-12-09 12:10:58.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 4.645096302032471
2025-12-09 12:10:58.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 4.705102443695068
2025-12-09 12:10:58.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 4.677120685577393
2025-12-09 12:10:58.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 4.831351280212402
2025-12-09 12:10:58.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 4.665558338165283
2025-12-09 12:10:58.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 4.641007423400879
2025-12-09 12:10:58.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 4.7563371658325195
2025-12-09 12:10:58.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 4.721767425537109
2025-12-09 12:10:58.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 4.599218845367432
2025-12-09 12:10:58.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 4.65836238861084
2025-12-09 12:10:58.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 4.611093997955322
2025-12-09 12:10:58.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 4.645829677581787
2025-12-09 12:10:58.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 4.724404811859131
2025-12-09 12:10:58.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 4.5223870277404785
2025-12-09 12:10:58.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 4.68642520904541
2025-12-09 12:10:58.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 4.598046779632568
2025-12-09 12:10:58.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 4.533565521240234
2025-12-09 12:10:58.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 4.702326774597168
2025-12-09 12:10:58.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 4.564001560211182
2025-12-09 12:10:58.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 4.638516426086426
2025-12-09 12:10:58.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 4.6475019454956055
2025-12-09 12:10:58.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 4.530306339263916
2025-12-09 12:10:58.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 4.639609336853027
2025-12-09 12:10:58.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 4.526675224304199
2025-12-09 12:10:58.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 4.656404972076416
2025-12-09 12:10:58.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 4.529786109924316
2025-12-09 12:10:58.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 4.59048318862915
2025-12-09 12:10:58.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 4.594250679016113
2025-12-09 12:10:58.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 4.608389854431152
2025-12-09 12:10:58.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 4.531171798706055
2025-12-09 12:10:58.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 4.602690696716309
2025-12-09 12:10:58.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 4.382874488830566
2025-12-09 12:10:58.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 4.460888862609863
2025-12-09 12:10:58.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 4.669951915740967
2025-12-09 12:10:58.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 4.61004638671875
2025-12-09 12:10:58.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 4.471206188201904
2025-12-09 12:10:58.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 4.494771957397461
2025-12-09 12:10:58.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 4.570964336395264
2025-12-09 12:10:58.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 4.492762088775635
2025-12-09 12:10:58.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 4.447566986083984
2025-12-09 12:10:58.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 4.508766174316406
2025-12-09 12:10:58.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 4.57028341293335
2025-12-09 12:10:58.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 4.517399787902832
2025-12-09 12:10:58.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 4.563647747039795
2025-12-09 12:10:58.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 4.535506725311279
2025-12-09 12:10:58.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 4.4569830894470215
2025-12-09 12:10:58.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009999708626830617 Training loss: 4.5072922706604
2025-12-09 12:10:58.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0009998834541281797 Training loss: 4.443086624145508
2025-12-09 12:10:58.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0009997377845227576 Training loss: 4.506800174713135
2025-12-09 12:10:58.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0009995338708444802 Training loss: 4.409257411956787
2025-12-09 12:10:58.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0009992717368593384 Training loss: 4.378263473510742
2025-12-09 12:10:58.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0009989514131188558 Training loss: 4.400128364562988
2025-12-09 12:10:58.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0009985729369565298 Training loss: 4.457643985748291
2025-12-09 12:10:58.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00099813635248348 Training loss: 4.398209095001221
2025-12-09 12:10:58.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0009976417105833069 Training loss: 4.372634410858154
2025-12-09 12:10:58.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.000997089068906162 Training loss: 4.533471584320068
2025-12-09 12:10:58.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0009964784918620282 Training loss: 4.449352264404297
2025-12-09 12:10:58.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0009958100506132126 Training loss: 4.565271854400635
2025-12-09 12:10:58.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0009950838230660534 Training loss: 4.4074506759643555
2025-12-09 12:10:58.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0009942998938618393 Training loss: 4.333776950836182
2025-12-09 12:10:58.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0009934583543669453 Training loss: 4.430954933166504
2025-12-09 12:10:58.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0009925593026621834 Training loss: 4.283018589019775
2025-12-09 12:10:58.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.000991602843531371 Training loss: 4.37205696105957
2025-12-09 12:10:58.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0009905890884491196 Training loss: 4.296481609344482
2025-12-09 12:10:58.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0009895181555678418 Training loss: 4.314758777618408
2025-12-09 12:10:58.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0009883901697039807 Training loss: 4.466207981109619
2025-12-09 12:10:58.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.000987205262323463 Training loss: 4.402377605438232
2025-12-09 12:10:58.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.000985963571526376 Training loss: 4.550894260406494
2025-12-09 12:10:58.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0009846652420308728 Training loss: 4.307478427886963
2025-12-09 12:10:58.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0009833104251563056 Training loss: 4.28005313873291
2025-12-09 12:10:58.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.000981899278805589 Training loss: 4.39719295501709
2025-12-09 12:10:58.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0009804319674467969 Training loss: 4.432000160217285
2025-12-09 12:10:59.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0009789086620939935 Training loss: 4.358069896697998
2025-12-09 12:10:59.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0009773295402873026 Training loss: 4.323906421661377
2025-12-09 12:10:59.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0009756947860722143 Training loss: 4.434451580047607
2025-12-09 12:10:59.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0009740045899781352 Training loss: 4.360098838806152
2025-12-09 12:10:59.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0009722591489961827 Training loss: 4.302737712860107
2025-12-09 12:10:59.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0009704586665562249 Training loss: 4.248969078063965
2025-12-09 12:10:59.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0009686033525031719 Training loss: 4.377053260803223
2025-12-09 12:10:59.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0009666934230725179 Training loss: 4.404607772827148
2025-12-09 12:10:59.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0009647291008651398 Training loss: 4.345753192901611
2025-12-09 12:10:59.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0009627106148213521 Training loss: 4.3221540451049805
2025-12-09 12:10:59.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0009606382001942255 Training loss: 4.378196716308594
2025-12-09 12:10:59.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0009585120985221671 Training loss: 4.316645622253418
2025-12-09 12:10:59.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0009563325576007701 Training loss: 4.26944637298584
2025-12-09 12:10:59.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0009540998314539327 Training loss: 4.252004146575928
2025-12-09 12:10:59.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0009518141803042527 Training loss: 4.318114280700684
2025-12-09 12:10:59.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0009494758705426977 Training loss: 4.267062187194824
2025-12-09 12:10:59.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0009470851746975581 Training loss: 4.1707048416137695
2025-12-09 12:10:59.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0009446423714026846 Training loss: 4.296070575714111
2025-12-09 12:10:59.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0009421477453650118 Training loss: 4.292147159576416
2025-12-09 12:10:59.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0009396015873313782 Training loss: 4.202832221984863
2025-12-09 12:10:59.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0009370041940546379 Training loss: 4.243066310882568
2025-12-09 12:10:59.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0009343558682590756 Training loss: 4.212497711181641
2025-12-09 12:10:59.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0009316569186051234 Training loss: 4.295650482177734
2025-12-09 12:10:59.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0009289076596533872 Training loss: 4.21821403503418
2025-12-09 12:10:59.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0009261084118279846 Training loss: 4.138847351074219
2025-12-09 12:10:59.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0009232595013792003 Training loss: 4.1599040031433105
2025-12-09 12:10:59.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0009203612603454604 Training loss: 4.366397380828857
2025-12-09 12:10:59.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0009174140265146356 Training loss: 4.248627185821533
2025-12-09 12:10:59.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0009144181433846706 Training loss: 4.091280937194824
2025-12-09 12:10:59.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0009113739601235507 Training loss: 4.181821346282959
2025-12-09 12:10:59.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0009082818315286055 Training loss: 4.328579902648926
2025-12-09 12:10:59.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0009051421179851588 Training loss: 4.226744651794434
2025-12-09 12:10:59.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.000901955185424525 Training loss: 4.195913314819336
2025-12-09 12:10:59.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0008987214052813603 Training loss: 4.113466262817383
2025-12-09 12:10:59.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0008954411544503729 Training loss: 4.178589820861816
2025-12-09 12:10:59.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0008921148152423946 Training loss: 4.270861625671387
2025-12-09 12:10:59.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0008887427753398248 Training loss: 4.247288227081299
2025-12-09 12:10:59.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0008853254277514447 Training loss: 4.104770660400391
2025-12-09 12:10:59.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0008818631707666135 Training loss: 4.263167381286621
2025-12-09 12:10:59.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0008783564079088476 Training loss: 4.142199993133545
2025-12-09 12:10:59.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0008748055478887904 Training loss: 4.256121635437012
2025-12-09 12:10:59.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0008712110045565768 Training loss: 4.2281599044799805
2025-12-09 12:10:59.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0008675731968536002 Training loss: 4.182939529418945
2025-12-09 12:10:59.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0008638925487636848 Training loss: 4.235040187835693
2025-12-09 12:10:59.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00086016948926367 Training loss: 4.321477890014648
2025-12-09 12:10:59.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0008564044522734146 Training loss: 4.130992889404297
2025-12-09 12:10:59.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.000852597876605223 Training loss: 4.164215087890625
2025-12-09 12:10:59.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0008487502059127015 Training loss: 4.249163627624512
2025-12-09 12:10:59.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0008448618886390522 Training loss: 4.154848098754883
2025-12-09 12:10:59.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.0008409333779648059 Training loss: 4.214036464691162
2025-12-09 12:10:59.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0008369651317550054 Training loss: 4.210058689117432
2025-12-09 12:10:59.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0008329576125058406 Training loss: 4.157115936279297
2025-12-09 12:10:59.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0008289112872907454 Training loss: 4.16034460067749
2025-12-09 12:10:59.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0008248266277059606 Training loss: 4.112102508544922
2025-12-09 12:10:59.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00082070410981557 Training loss: 4.2143235206604
2025-12-09 12:10:59.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.000816544214096015 Training loss: 4.110337734222412
2025-12-09 12:10:59.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0008123474253800957 Training loss: 4.264427661895752
2025-12-09 12:10:59.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0008081142328004637 Training loss: 4.170443058013916
2025-12-09 12:10:59.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0008038451297326145 Training loss: 4.180099010467529
2025-12-09 12:10:59.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0007995406137373846 Training loss: 4.102377414703369
2025-12-09 12:10:59.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0007952011865029613 Training loss: 4.141979694366455
2025-12-09 12:10:59.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0007908273537864113 Training loss: 4.136616230010986
2025-12-09 12:10:59.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0007864196253547349 Training loss: 4.15324068069458
2025-12-09 12:10:59.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0007819785149254532 Training loss: 4.195643901824951
2025-12-09 12:10:59.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.000777504540106735 Training loss: 4.175057411193848
2025-12-09 12:10:59.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0007729982223370691 Training loss: 4.2640581130981445
2025-12-09 12:10:59.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0007684600868244919 Training loss: 4.115002155303955
2025-12-09 12:10:59.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0007638906624853743 Training loss: 4.221356391906738
2025-12-09 12:10:59.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0007592904818827774 Training loss: 4.103404998779297
2025-12-09 12:10:59.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0007546600811643815 Training loss: 4.222961902618408
2025-12-09 12:10:59.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.00075 Training loss: 4.095706462860107
2025-12-09 12:10:59.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0007453107815186803 Training loss: 4.086813926696777
2025-12-09 12:10:59.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0007405929722454026 Training loss: 4.140241622924805
2025-12-09 12:10:59.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0007358471220373831 Training loss: 4.103396892547607
2025-12-09 12:10:59.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0007310737840199885 Training loss: 4.107895374298096
2025-12-09 12:10:59.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0007262735145222696 Training loss: 4.135643482208252
2025-12-09 12:10:59.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0007214468730121209 Training loss: 4.007576942443848
2025-12-09 12:10:59.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0007165944220310766 Training loss: 4.075497150421143
2025-12-09 12:10:59.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0007117167271287453 Training loss: 4.185122489929199
2025-12-09 12:10:59.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0007068143567968958 Training loss: 3.9969980716705322
2025-12-09 12:10:59.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0007018878824032009 Training loss: 3.9714527130126953
2025-12-09 12:10:59.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.0006969378781246436 Training loss: 4.1660237312316895
2025-12-09 12:10:59.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0006919649208805981 Training loss: 4.022801876068115
2025-12-09 12:10:59.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0006869695902655897 Training loss: 4.175904273986816
2025-12-09 12:10:59.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0006819524684817438 Training loss: 3.969622850418091
2025-12-09 12:10:59.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0006769141402709304 Training loss: 4.152525901794434
2025-12-09 12:10:59.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0006718551928466132 Training loss: 4.11409854888916
2025-12-09 12:10:59.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0006667762158254104 Training loss: 4.156352519989014
2025-12-09 12:10:59.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.0006616778011583743 Training loss: 4.035251617431641
2025-12-09 12:10:59.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0006565605430620013 Training loss: 3.96345853805542
2025-12-09 12:10:59.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0006514250379489753 Training loss: 4.070532321929932
2025-12-09 12:10:59.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0006462718843586572 Training loss: 4.103879451751709
2025-12-09 12:10:59.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0006411016828873239 Training loss: 4.115255355834961
2025-12-09 12:10:59.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0006359150361181715 Training loss: 3.9708425998687744
2025-12-09 12:10:59.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0006307125485510829 Training loss: 4.204823017120361
2025-12-09 12:10:59.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0006254948265321744 Training loss: 4.071065425872803
2025-12-09 12:10:59.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0006202624781831269 Training loss: 4.105712890625
2025-12-09 12:10:59.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0006150161133303088 Training loss: 4.037594795227051
2025-12-09 12:10:59.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0006097563434337025 Training loss: 4.124294281005859
2025-12-09 12:10:59.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0006044837815156376 Training loss: 3.9828479290008545
2025-12-09 12:10:59.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0005991990420893449 Training loss: 3.850792646408081
2025-12-09 12:10:59.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0005939027410873352 Training loss: 4.072061538696289
2025-12-09 12:10:59.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0005885954957896114 Training loss: 4.131982803344727
2025-12-09 12:10:59.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0005832779247517272 Training loss: 4.079115867614746
2025-12-09 12:10:59.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0005779506477326933 Training loss: 4.145873546600342
2025-12-09 12:10:59.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0005726142856227452 Training loss: 4.119042873382568
2025-12-09 12:10:59.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0005672694603709794 Training loss: 4.084249019622803
2025-12-09 12:10:59.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0005619167949128652 Training loss: 4.053470611572266
2025-12-09 12:10:59.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0005565569130976422 Training loss: 3.9960246086120605
2025-12-09 12:10:59.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0005511904396156113 Training loss: 4.053301811218262
2025-12-09 12:10:59.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0005458179999253274 Training loss: 4.046913146972656
2025-12-09 12:10:59.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0005404402201807021 Training loss: 4.022291660308838
2025-12-09 12:10:59.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0005350577271580271 Training loss: 3.9857563972473145
2025-12-09 12:11:00.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0005296711481829226 Training loss: 4.026637554168701
2025-12-09 12:11:00.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0005242811110572242 Training loss: 4.214059829711914
2025-12-09 12:11:00.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0005188882439858117 Training loss: 4.041836261749268
2025-12-09 12:11:00.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0005134931755033936 Training loss: 4.172823429107666
2025-12-09 12:11:00.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0005080965344012508 Training loss: 4.0854411125183105
2025-12-09 12:11:00.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0005026989496539523 Training loss: 3.9445738792419434
2025-12-09 12:11:00.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0004973010503460479 Training loss: 4.0500969886779785
2025-12-09 12:11:00.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0004919034655987492 Training loss: 4.054111957550049
2025-12-09 12:11:00.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0004865068244966066 Training loss: 3.956305742263794
2025-12-09 12:11:00.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0004811117560141884 Training loss: 4.046626091003418
2025-12-09 12:11:00.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.000475718888942776 Training loss: 4.017827033996582
2025-12-09 12:11:00.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0004703288518170774 Training loss: 3.959071397781372
2025-12-09 12:11:00.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00046494227284197295 Training loss: 4.012273788452148
2025-12-09 12:11:00.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00045955977981929796 Training loss: 3.9625165462493896
2025-12-09 12:11:00.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0004541820000746727 Training loss: 3.969285488128662
2025-12-09 12:11:00.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00044880956038438873 Training loss: 3.9756476879119873
2025-12-09 12:11:00.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0004434430869023579 Training loss: 3.9838552474975586
2025-12-09 12:11:00.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.000438083205087135 Training loss: 3.9578793048858643
2025-12-09 12:11:00.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.00043273053962902076 Training loss: 4.0431742668151855
2025-12-09 12:11:00.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.000427385714377255 Training loss: 3.985825777053833
2025-12-09 12:11:00.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0004220493522673067 Training loss: 4.057399749755859
2025-12-09 12:11:00.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0004167220752482728 Training loss: 4.003139495849609
2025-12-09 12:11:00.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.00041140450421038864 Training loss: 4.144470691680908
2025-12-09 12:11:00.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.000406097258912665 Training loss: 3.9806137084960938
2025-12-09 12:11:00.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0004008009579106551 Training loss: 4.08868932723999
2025-12-09 12:11:00.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0003955162184843625 Training loss: 3.9950296878814697
2025-12-09 12:11:00.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.00039024365656629766 Training loss: 3.905130386352539
2025-12-09 12:11:00.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0003849838866696913 Training loss: 4.0577073097229
2025-12-09 12:11:00.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00037973752181687335 Training loss: 4.003559112548828
2025-12-09 12:11:00.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00037450517346782563 Training loss: 3.909708261489868
2025-12-09 12:11:00.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0003692874514489173 Training loss: 4.014383316040039
2025-12-09 12:11:00.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00036408496388182855 Training loss: 3.905919075012207
2025-12-09 12:11:00.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0003588983171126762 Training loss: 3.988826036453247
2025-12-09 12:11:00.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.000353728115641343 Training loss: 4.131052017211914
2025-12-09 12:11:00.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0003485749620510247 Training loss: 3.996157169342041
2025-12-09 12:11:00.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00034343945693799885 Training loss: 4.049779415130615
2025-12-09 12:11:00.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00033832219884162584 Training loss: 4.026370048522949
2025-12-09 12:11:00.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0003332237841745898 Training loss: 3.9378440380096436
2025-12-09 12:11:00.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00032814480715338666 Training loss: 3.923036575317383
2025-12-09 12:11:00.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0003230858597290697 Training loss: 3.939849853515625
2025-12-09 12:11:00.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0003180475315182563 Training loss: 4.0539231300354
2025-12-09 12:11:00.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0003130304097344103 Training loss: 3.9717302322387695
2025-12-09 12:11:00.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0003080350791194019 Training loss: 4.104416847229004
2025-12-09 12:11:00.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.00030306212187535653 Training loss: 4.050414085388184
2025-12-09 12:11:00.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002981121175967992 Training loss: 3.9696836471557617
2025-12-09 12:11:00.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.00029318564320310444 Training loss: 4.005795955657959
2025-12-09 12:11:00.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0002882832728712551 Training loss: 4.035501956939697
2025-12-09 12:11:00.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0002834055779689235 Training loss: 3.854215145111084
2025-12-09 12:11:00.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.00027855312698787905 Training loss: 4.0165696144104
2025-12-09 12:11:00.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0002737264854777306 Training loss: 3.895721912384033
2025-12-09 12:11:00.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.00026892621598001155 Training loss: 3.8889527320861816
2025-12-09 12:11:00.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0002641528779626171 Training loss: 3.9777448177337646
2025-12-09 12:11:00.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.00025940702775459747 Training loss: 4.054665565490723
2025-12-09 12:11:00.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.00025468921848131984 Training loss: 4.020269393920898
2025-12-09 12:11:00.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0002500000000000001 Training loss: 4.057692527770996
2025-12-09 12:11:00.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.00024533991883561866 Training loss: 3.9382665157318115
2025-12-09 12:11:00.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.00024070951811722268 Training loss: 3.90563702583313
2025-12-09 12:11:00.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00023610933751462554 Training loss: 3.9053924083709717
2025-12-09 12:11:00.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0002315399131755081 Training loss: 3.9340152740478516
2025-12-09 12:11:00.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00022700177766293096 Training loss: 4.039727210998535
2025-12-09 12:11:00.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.00022249545989326514 Training loss: 3.923875331878662
2025-12-09 12:11:00.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002180214850745467 Training loss: 4.007476329803467
2025-12-09 12:11:00.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.00021358037464526514 Training loss: 4.010979175567627
2025-12-09 12:11:00.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.00020917264621358878 Training loss: 3.964362382888794
2025-12-09 12:11:00.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.00020479881349703882 Training loss: 3.9101133346557617
2025-12-09 12:11:00.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.00020045938626261545 Training loss: 4.0129194259643555
2025-12-09 12:11:00.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.00019615487026738542 Training loss: 3.8719305992126465
2025-12-09 12:11:00.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.00019188576719953633 Training loss: 4.071925640106201
2025-12-09 12:11:00.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.00018765257461990443 Training loss: 4.029876232147217
2025-12-09 12:11:00.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0001834557859039851 Training loss: 3.9904634952545166
2025-12-09 12:11:00.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.00017929589018443015 Training loss: 3.8587987422943115
2025-12-09 12:11:00.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.00017517337229403947 Training loss: 3.964186429977417
2025-12-09 12:11:00.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0001710887127092548 Training loss: 3.9836769104003906
2025-12-09 12:11:00.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.00016704238749415957 Training loss: 3.8867475986480713
2025-12-09 12:11:00.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0001630348682449946 Training loss: 3.9748880863189697
2025-12-09 12:11:00.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00015906662203519413 Training loss: 3.984677314758301
2025-12-09 12:11:00.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00015513811136094787 Training loss: 3.9850597381591797
2025-12-09 12:11:00.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.0001512497940872986 Training loss: 3.880098342895508
2025-12-09 12:11:00.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0001474021233947772 Training loss: 4.004222869873047
2025-12-09 12:11:00.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00014359554772658552 Training loss: 4.0716352462768555
2025-12-09 12:11:00.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.00013983051073632996 Training loss: 4.017656326293945
2025-12-09 12:11:00.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00013610745123631535 Training loss: 4.071552276611328
2025-12-09 12:11:00.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00013242680314639994 Training loss: 3.977905750274658
2025-12-09 12:11:00.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00012878899544342326 Training loss: 3.9414143562316895
2025-12-09 12:11:00.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00012519445211120977 Training loss: 4.06050443649292
2025-12-09 12:11:00.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00012164359209115234 Training loss: 3.9364817142486572
2025-12-09 12:11:00.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00011813682923338653 Training loss: 3.9198567867279053
2025-12-09 12:11:00.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.00011467457224855543 Training loss: 3.9349732398986816
2025-12-09 12:11:00.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.00011125722466017545 Training loss: 3.84588360786438
2025-12-09 12:11:00.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00010788518475760544 Training loss: 4.153438091278076
2025-12-09 12:11:00.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00010455884554962725 Training loss: 3.834334373474121
2025-12-09 12:11:00.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0001012785947186397 Training loss: 3.8706326484680176
2025-12-09 12:11:00.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.804481457547498e-05 Training loss: 4.024033546447754
2025-12-09 12:11:00.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.485788201484124e-05 Training loss: 3.9042603969573975
2025-12-09 12:11:00.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.171816847139447e-05 Training loss: 3.856119155883789
2025-12-09 12:11:00.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 8.862603987644941e-05 Training loss: 4.077032089233398
2025-12-09 12:11:00.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 8.558185661532942e-05 Training loss: 3.999555826187134
2025-12-09 12:11:00.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 8.258597348536451e-05 Training loss: 4.034861087799072
2025-12-09 12:11:00.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 7.96387396545396e-05 Training loss: 3.921759843826294
2025-12-09 12:11:00.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 7.674049862079991e-05 Training loss: 3.8678698539733887
2025-12-09 12:11:00.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 7.38915881720154e-05 Training loss: 3.8970444202423096
2025-12-09 12:11:00.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 7.109234034661289e-05 Training loss: 3.9616539478302
2025-12-09 12:11:00.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 6.834308139487671e-05 Training loss: 3.976067304611206
2025-12-09 12:11:00.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 6.564413174092443e-05 Training loss: 3.9682583808898926
2025-12-09 12:11:00.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 6.299580594536214e-05 Training loss: 3.9430692195892334
2025-12-09 12:11:00.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 6.0398412668621897e-05 Training loss: 3.970104455947876
2025-12-09 12:11:00.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 5.785225463498828e-05 Training loss: 4.018916606903076
2025-12-09 12:11:00.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 5.535762859731547e-05 Training loss: 3.9702999591827393
2025-12-09 12:11:00.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 5.291482530244179e-05 Training loss: 3.860018014907837
2025-12-09 12:11:00.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 5.0524129457302394e-05 Training loss: 3.978304386138916
2025-12-09 12:11:00.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 4.818581969574742e-05 Training loss: 4.141230583190918
2025-12-09 12:11:00.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 4.5900168546067264e-05 Training loss: 3.8605892658233643
2025-12-09 12:11:01.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 4.366744239922998e-05 Training loss: 3.8326497077941895
2025-12-09 12:11:01.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 4.148790147783288e-05 Training loss: 3.9543297290802
2025-12-09 12:11:01.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 3.936179980577453e-05 Training loss: 3.992661952972412
2025-12-09 12:11:01.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 3.728938517864794e-05 Training loss: 3.9472217559814453
2025-12-09 12:11:01.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 3.527089913486037e-05 Training loss: 3.952045440673828
2025-12-09 12:11:01.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 3.330657692748212e-05 Training loss: 3.891641616821289
2025-12-09 12:11:01.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 3.1396647496828245e-05 Training loss: 3.937959909439087
2025-12-09 12:11:01.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 2.9541333443775243e-05 Training loss: 3.9515931606292725
2025-12-09 12:11:01.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 2.7740851003817347e-05 Training loss: 3.8872945308685303
2025-12-09 12:11:01.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 2.5995410021864786e-05 Training loss: 3.8063881397247314
2025-12-09 12:11:01.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 2.430521392778573e-05 Training loss: 3.8825290203094482
2025-12-09 12:11:01.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 2.2670459712697378e-05 Training loss: 4.033377170562744
2025-12-09 12:11:01.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 2.109133790600648e-05 Training loss: 3.871217727661133
2025-12-09 12:11:01.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 1.956803255320322e-05 Training loss: 4.056816577911377
2025-12-09 12:11:01.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 1.810072119441103e-05 Training loss: 4.023941516876221
2025-12-09 12:11:01.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 1.6689574843694434e-05 Training loss: 4.102121353149414
2025-12-09 12:11:01.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 1.53347579691272e-05 Training loss: 3.9463396072387695
2025-12-09 12:11:01.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 1.4036428473624019e-05 Training loss: 3.9654016494750977
2025-12-09 12:11:01.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 1.2794737676536993e-05 Training loss: 3.9176292419433594
2025-12-09 12:11:01.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 1.1609830296019142e-05 Training loss: 3.8405961990356445
2025-12-09 12:11:01.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 1.048184443215816e-05 Training loss: 3.908345937728882
2025-12-09 12:11:01.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880474e-06 Training loss: 3.9508004188537598
2025-12-09 12:11:01.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629207e-06 Training loss: 3.9891726970672607
2025-12-09 12:11:01.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.440697337816771e-06 Training loss: 3.88519549369812
2025-12-09 12:11:01.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.541645633054649e-06 Training loss: 3.9554338455200195
2025-12-09 12:11:01.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.700106138160688e-06 Training loss: 3.9769952297210693
2025-12-09 12:11:01.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946692e-06 Training loss: 3.954976797103882
2025-12-09 12:11:01.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.189949386787462e-06 Training loss: 3.890406608581543
2025-12-09 12:11:01.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.521508137971807e-06 Training loss: 4.132169246673584
2025-12-09 12:11:01.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378875e-06 Training loss: 3.873798370361328
2025-12-09 12:11:01.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.3582894166930268e-06 Training loss: 3.9857869148254395
2025-12-09 12:11:01.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.8636475165200173e-06 Training loss: 3.919203519821167
2025-12-09 12:11:01.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.4270630434701782e-06 Training loss: 3.8824429512023926
2025-12-09 12:11:01.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441758e-06 Training loss: 3.936258316040039
2025-12-09 12:11:01.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.282631406615448e-07 Training loss: 4.045574188232422
2025-12-09 12:11:01.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.6612915551963455e-07 Training loss: 3.922720193862915
2025-12-09 12:11:01.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253336e-07 Training loss: 3.9822399616241455
2025-12-09 12:11:01.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-07 Training loss: 4.124643802642822
2025-12-09 12:11:01.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265823e-08 Training loss: 3.9650192260742188
2025-12-09 12:11:01.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 4.065388202667236
