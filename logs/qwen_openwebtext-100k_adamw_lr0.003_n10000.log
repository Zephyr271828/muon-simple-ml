2025-12-09 11:53:20.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 12.009098052978516
2025-12-09 11:53:20.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 12.017152786254883
2025-12-09 11:53:20.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 12.02890396118164
2025-12-09 11:53:20.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 11.899643898010254
2025-12-09 11:53:20.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 11.789310455322266
2025-12-09 11:53:21.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 11.64932632446289
2025-12-09 11:53:21.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 11.285983085632324
2025-12-09 11:53:21.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 11.192520141601562
2025-12-09 11:53:21.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 11.120076179504395
2025-12-09 11:53:21.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 11.165185928344727
2025-12-09 11:53:21.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 10.959159851074219
2025-12-09 11:53:21.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 10.831549644470215
2025-12-09 11:53:21.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 10.561138153076172
2025-12-09 11:53:21.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 10.507753372192383
2025-12-09 11:53:21.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 10.31694507598877
2025-12-09 11:53:21.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 10.122363090515137
2025-12-09 11:53:21.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 9.893074035644531
2025-12-09 11:53:21.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 9.617085456848145
2025-12-09 11:53:22.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 9.508902549743652
2025-12-09 11:53:22.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 9.309374809265137
2025-12-09 11:53:22.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 9.269715309143066
2025-12-09 11:53:22.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 9.283414840698242
2025-12-09 11:53:22.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 8.830001831054688
2025-12-09 11:53:22.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 8.615086555480957
2025-12-09 11:53:22.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 8.491084098815918
2025-12-09 11:53:22.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 8.850181579589844
2025-12-09 11:53:22.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 8.423858642578125
2025-12-09 11:53:22.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 8.208181381225586
2025-12-09 11:53:22.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 7.875988960266113
2025-12-09 11:53:22.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 8.150135040283203
2025-12-09 11:53:22.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 8.160780906677246
2025-12-09 11:53:23.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 7.88335657119751
2025-12-09 11:53:23.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 8.132401466369629
2025-12-09 11:53:23.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 7.835785865783691
2025-12-09 11:53:23.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 8.217789649963379
2025-12-09 11:53:23.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 8.464860916137695
2025-12-09 11:53:23.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 8.110757827758789
2025-12-09 11:53:23.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 7.812037467956543
2025-12-09 11:53:23.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 8.245192527770996
2025-12-09 11:53:23.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 7.824046611785889
2025-12-09 11:53:23.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 8.083730697631836
2025-12-09 11:53:23.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 8.547029495239258
2025-12-09 11:53:23.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 8.400196075439453
2025-12-09 11:53:23.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 7.985250473022461
2025-12-09 11:53:24.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 8.210016250610352
2025-12-09 11:53:24.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 7.951878547668457
2025-12-09 11:53:24.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 8.11397933959961
2025-12-09 11:53:24.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 7.9811787605285645
2025-12-09 11:53:24.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 8.835654258728027
2025-12-09 11:53:24.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 7.843908309936523
2025-12-09 11:53:24.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 8.321881294250488
2025-12-09 11:53:24.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 8.168910026550293
2025-12-09 11:53:24.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 7.992438316345215
2025-12-09 11:53:24.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 8.299551963806152
2025-12-09 11:53:24.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 7.468395709991455
2025-12-09 11:53:24.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 8.207155227661133
2025-12-09 11:53:25.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 8.211689949035645
2025-12-09 11:53:25.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 8.211209297180176
2025-12-09 11:53:25.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 7.989394664764404
2025-12-09 11:53:25.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 8.028884887695312
2025-12-09 11:53:25.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 8.304654121398926
2025-12-09 11:53:25.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 8.128897666931152
2025-12-09 11:53:25.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 8.261049270629883
2025-12-09 11:53:25.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 8.015047073364258
2025-12-09 11:53:25.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 8.42009162902832
2025-12-09 11:53:25.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 7.938027381896973
2025-12-09 11:53:25.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 8.51538372039795
2025-12-09 11:53:25.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 8.185559272766113
2025-12-09 11:53:25.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 8.19789981842041
2025-12-09 11:53:26.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 8.125591278076172
2025-12-09 11:53:26.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 7.9817795753479
2025-12-09 11:53:26.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 8.069066047668457
2025-12-09 11:53:26.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 7.981679916381836
2025-12-09 11:53:26.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 8.059549331665039
2025-12-09 11:53:26.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 8.007674217224121
2025-12-09 11:53:26.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 8.281046867370605
2025-12-09 11:53:26.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 8.637401580810547
2025-12-09 11:53:26.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 8.535751342773438
2025-12-09 11:53:26.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 7.803657531738281
2025-12-09 11:53:26.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 8.067160606384277
2025-12-09 11:53:26.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 7.963364601135254
2025-12-09 11:53:26.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 8.189638137817383
2025-12-09 11:53:27.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 8.086050987243652
2025-12-09 11:53:27.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 8.381065368652344
2025-12-09 11:53:27.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 7.651259422302246
2025-12-09 11:53:27.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 8.059021949768066
2025-12-09 11:53:27.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 7.75242805480957
2025-12-09 11:53:27.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 7.982885837554932
2025-12-09 11:53:27.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 7.874251365661621
2025-12-09 11:53:27.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 8.271088600158691
2025-12-09 11:53:27.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 8.147571563720703
2025-12-09 11:53:27.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 8.166125297546387
2025-12-09 11:53:27.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 7.8411712646484375
2025-12-09 11:53:27.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 7.877776622772217
2025-12-09 11:53:27.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 8.384093284606934
2025-12-09 11:53:28.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 7.893198013305664
2025-12-09 11:53:28.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 8.022703170776367
2025-12-09 11:53:28.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 8.025874137878418
2025-12-09 11:53:28.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 8.067768096923828
2025-12-09 11:53:28.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 7.969457149505615
2025-12-09 11:53:28.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999997217736107 Training loss: 7.951020240783691
2025-12-09 11:53:28.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002999998887094546 Training loss: 8.122194290161133
2025-12-09 11:53:28.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029999974959631155 Training loss: 7.956821918487549
2025-12-09 11:53:28.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0029999955483798347 Training loss: 8.117585182189941
2025-12-09 11:53:28.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0029999930443454273 Training loss: 7.584741115570068
2025-12-09 11:53:28.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.002999989983860821 Training loss: 8.389920234680176
2025-12-09 11:53:28.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029999863669271528 Training loss: 7.868276119232178
2025-12-09 11:53:29.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0029999821935457623 Training loss: 8.09997844696045
2025-12-09 11:53:29.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0029999774637181993 Training loss: 7.852794170379639
2025-12-09 11:53:29.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.002999972177446218 Training loss: 7.834774017333984
2025-12-09 11:53:29.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.002999966334731779 Training loss: 7.692677974700928
2025-12-09 11:53:29.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0029999599355770503 Training loss: 9.47362995147705
2025-12-09 11:53:29.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029999529799844054 Training loss: 7.911147117614746
2025-12-09 11:53:29.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029999454679564244 Training loss: 7.814628601074219
2025-12-09 11:53:29.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.002999937399495895 Training loss: 7.387720584869385
2025-12-09 11:53:29.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0029999287746058094 Training loss: 7.881276607513428
2025-12-09 11:53:29.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.002999919593289368 Training loss: 8.133663177490234
2025-12-09 11:53:29.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.002999909855549976 Training loss: 7.780333042144775
2025-12-09 11:53:29.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029998995613912463 Training loss: 7.948533058166504
2025-12-09 11:53:29.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.002999888710816997 Training loss: 7.710259914398193
2025-12-09 11:53:30.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.002999877303831254 Training loss: 8.070045471191406
2025-12-09 11:53:30.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002999865340438249 Training loss: 6.937975883483887
2025-12-09 11:53:30.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0029998528206424202 Training loss: 7.961223602294922
2025-12-09 11:53:30.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0029998397444484107 Training loss: 8.367616653442383
2025-12-09 11:53:30.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029998261118610726 Training loss: 7.753511905670166
2025-12-09 11:53:30.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.002999811922885463 Training loss: 8.192743301391602
2025-12-09 11:53:30.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0029997971775268454 Training loss: 7.8334760665893555
2025-12-09 11:53:30.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00299978187579069 Training loss: 7.707699775695801
2025-12-09 11:53:30.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0029997660176826735 Training loss: 7.714071750640869
2025-12-09 11:53:30.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.002999749603208678 Training loss: 7.931206703186035
2025-12-09 11:53:30.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.002999732632374793 Training loss: 7.651483535766602
2025-12-09 11:53:30.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002999715105187314 Training loss: 7.616082668304443
2025-12-09 11:53:30.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.002999697021652744 Training loss: 7.591127872467041
2025-12-09 11:53:31.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.002999678381777791 Training loss: 8.318432807922363
2025-12-09 11:53:31.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.002999659185569369 Training loss: 7.986443996429443
2025-12-09 11:53:31.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0029996394330345996 Training loss: 7.623082637786865
2025-12-09 11:53:31.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0029996191241808113 Training loss: 7.776973724365234
2025-12-09 11:53:31.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.002999598259015537 Training loss: 8.20831298828125
2025-12-09 11:53:31.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.002999576837546517 Training loss: 7.726274013519287
2025-12-09 11:53:31.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0029995548597816983 Training loss: 7.628843784332275
2025-12-09 11:53:31.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.002999532325729234 Training loss: 7.7275471687316895
2025-12-09 11:53:31.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0029995092353974837 Training loss: 7.60299015045166
2025-12-09 11:53:31.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.002999485588795013 Training loss: 7.398255825042725
2025-12-09 11:53:31.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0029994613859305936 Training loss: 7.848791122436523
2025-12-09 11:53:31.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0029994366268132045 Training loss: 7.065235614776611
2025-12-09 11:53:31.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0029994113114520304 Training loss: 7.587403774261475
2025-12-09 11:53:32.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0029993854398564627 Training loss: 7.821102142333984
2025-12-09 11:53:32.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0029993590120360987 Training loss: 7.736555576324463
2025-12-09 11:53:32.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0029993320280007423 Training loss: 7.679067611694336
2025-12-09 11:53:32.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.002999304487760404 Training loss: 7.6537370681762695
2025-12-09 11:53:32.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0029992763913253002 Training loss: 7.763614654541016
2025-12-09 11:53:32.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.002999247738705854 Training loss: 7.414008140563965
2025-12-09 11:53:32.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0029992185299126946 Training loss: 7.751524448394775
2025-12-09 11:53:32.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0029991887649566565 Training loss: 7.878465175628662
2025-12-09 11:53:32.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002999158443848783 Training loss: 7.286486625671387
2025-12-09 11:53:32.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0029991275666003212 Training loss: 7.94081974029541
2025-12-09 11:53:32.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0029990961332227264 Training loss: 7.542720317840576
2025-12-09 11:53:32.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.002999064143727659 Training loss: 7.5607099533081055
2025-12-09 11:53:33.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0029990315981269864 Training loss: 7.566500663757324
2025-12-09 11:53:33.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0029989984964327813 Training loss: 7.464481830596924
2025-12-09 11:53:33.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0029989648386573244 Training loss: 7.676898002624512
2025-12-09 11:53:33.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.002998930624813101 Training loss: 7.3926520347595215
2025-12-09 11:53:33.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.002998895854912803 Training loss: 7.805450439453125
2025-12-09 11:53:33.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0029988605289693296 Training loss: 7.84431266784668
2025-12-09 11:53:33.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0029988246469957857 Training loss: 7.799361705780029
2025-12-09 11:53:33.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002998788209005482 Training loss: 7.307791233062744
2025-12-09 11:53:33.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.002998751215011936 Training loss: 6.627615928649902
2025-12-09 11:53:33.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0029987136650288706 Training loss: 7.715376377105713
2025-12-09 11:53:33.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.002998675559070217 Training loss: 7.437880516052246
2025-12-09 11:53:33.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.00299863689715011 Training loss: 7.533684730529785
2025-12-09 11:53:33.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0029985976792828934 Training loss: 6.226654052734375
2025-12-09 11:53:34.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0029985579054831145 Training loss: 7.632509708404541
2025-12-09 11:53:34.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0029985175757655286 Training loss: 7.741230010986328
2025-12-09 11:53:34.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.002998476690145097 Training loss: 8.21214771270752
2025-12-09 11:53:34.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0029984352486369867 Training loss: 7.780457496643066
2025-12-09 11:53:34.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002998393251256571 Training loss: 7.870061874389648
2025-12-09 11:53:34.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0029983506980194303 Training loss: 8.609644889831543
2025-12-09 11:53:34.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0029983075889413497 Training loss: 7.663912296295166
2025-12-09 11:53:34.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0029982639240383217 Training loss: 8.006247520446777
2025-12-09 11:53:34.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0029982197033265445 Training loss: 8.084010124206543
2025-12-09 11:53:34.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0029981749268224228 Training loss: 7.6661696434021
2025-12-09 11:53:34.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.0029981295945425666 Training loss: 7.66474723815918
2025-12-09 11:53:34.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002998083706503794 Training loss: 7.395224571228027
2025-12-09 11:53:34.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.002998037262723127 Training loss: 7.40098762512207
2025-12-09 11:53:35.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.002997990263217795 Training loss: 7.022368907928467
2025-12-09 11:53:35.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0029979427080052334 Training loss: 7.327201843261719
2025-12-09 11:53:35.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.002997894597103084 Training loss: 6.903649806976318
2025-12-09 11:53:35.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0029978459305291943 Training loss: 7.596585273742676
2025-12-09 11:53:35.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0029977967083016175 Training loss: 7.626648902893066
2025-12-09 11:53:35.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.002997746930438614 Training loss: 7.425100803375244
2025-12-09 11:53:35.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0029976965969586494 Training loss: 7.494718074798584
2025-12-09 11:53:35.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0029976457078803964 Training loss: 7.691190719604492
2025-12-09 11:53:35.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0029975942632227332 Training loss: 7.41135311126709
2025-12-09 11:53:35.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002997542263004744 Training loss: 7.63411808013916
2025-12-09 11:53:35.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.002997489707245719 Training loss: 8.134411811828613
2025-12-09 11:53:35.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0029974365959651544 Training loss: 7.093667030334473
2025-12-09 11:53:36.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0029973829291827544 Training loss: 7.7945427894592285
2025-12-09 11:53:36.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.002997328706918426 Training loss: 7.564464569091797
2025-12-09 11:53:36.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0029972739291922843 Training loss: 7.595522880554199
2025-12-09 11:53:36.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0029972185960246514 Training loss: 7.290305137634277
2025-12-09 11:53:36.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0029971627074360523 Training loss: 7.324220657348633
2025-12-09 11:53:36.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0029971062634472205 Training loss: 7.424458980560303
2025-12-09 11:53:36.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0029970492640790957 Training loss: 7.6274333000183105
2025-12-09 11:53:36.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.002996991709352822 Training loss: 7.396675109863281
2025-12-09 11:53:36.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0029969335992897513 Training loss: 7.556243419647217
2025-12-09 11:53:36.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0029968749339114404 Training loss: 7.063990116119385
2025-12-09 11:53:36.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0029968157132396513 Training loss: 7.599918842315674
2025-12-09 11:53:36.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002996755937296354 Training loss: 7.488903045654297
2025-12-09 11:53:36.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.002996695606103723 Training loss: 7.44418478012085
2025-12-09 11:53:37.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0029966347196841393 Training loss: 7.424961090087891
2025-12-09 11:53:37.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.00299657327806019 Training loss: 7.5695343017578125
2025-12-09 11:53:37.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0029965112812546683 Training loss: 7.913699626922607
2025-12-09 11:53:37.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0029964487292905725 Training loss: 7.017045021057129
2025-12-09 11:53:37.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0029963856221911075 Training loss: 7.4006171226501465
2025-12-09 11:53:37.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.002996321959979685 Training loss: 7.370578765869141
2025-12-09 11:53:37.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00299625774267992 Training loss: 6.558045864105225
2025-12-09 11:53:37.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0029961929703156364 Training loss: 7.2680816650390625
2025-12-09 11:53:37.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.002996127642910863 Training loss: 7.276308059692383
2025-12-09 11:53:37.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0029960617604898325 Training loss: 7.578907012939453
2025-12-09 11:53:37.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.002995995323076986 Training loss: 7.415459156036377
2025-12-09 11:53:37.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.002995928330696971 Training loss: 7.4008917808532715
2025-12-09 11:53:37.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.002995860783374638 Training loss: 7.534653186798096
2025-12-09 11:53:38.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0029957926811350452 Training loss: 7.644033432006836
2025-12-09 11:53:38.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0029957240240034567 Training loss: 7.40102481842041
2025-12-09 11:53:38.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0029956548120053422 Training loss: 8.063419342041016
2025-12-09 11:53:38.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0029955850451663765 Training loss: 7.52872896194458
2025-12-09 11:53:38.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0029955147235124417 Training loss: 7.471928596496582
2025-12-09 11:53:38.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.002995443847069625 Training loss: 7.450699329376221
2025-12-09 11:53:38.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0029953724158642185 Training loss: 7.486439228057861
2025-12-09 11:53:38.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0029953004299227213 Training loss: 7.479004383087158
2025-12-09 11:53:38.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.002995227889271838 Training loss: 7.439903259277344
2025-12-09 11:53:38.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.002995154793938479 Training loss: 7.644002914428711
2025-12-09 11:53:38.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0029950811439497607 Training loss: 7.554532527923584
2025-12-09 11:53:38.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0029950069393330043 Training loss: 7.325230121612549
2025-12-09 11:53:38.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.002994932180115737 Training loss: 6.688649654388428
2025-12-09 11:53:39.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0029948568663256928 Training loss: 7.599237442016602
2025-12-09 11:53:39.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0029947809979908114 Training loss: 7.848843097686768
2025-12-09 11:53:39.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.002994704575139236 Training loss: 7.067373275756836
2025-12-09 11:53:39.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.002994627597799318 Training loss: 7.554126739501953
2025-12-09 11:53:39.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0029945500659996132 Training loss: 7.533163070678711
2025-12-09 11:53:39.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0029944719797688844 Training loss: 7.664332389831543
2025-12-09 11:53:39.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.002994393339136098 Training loss: 7.336575031280518
2025-12-09 11:53:39.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0029943141441304277 Training loss: 7.773313522338867
2025-12-09 11:53:39.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0029942343947812517 Training loss: 7.667043685913086
2025-12-09 11:53:39.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.002994154091118156 Training loss: 7.395887851715088
2025-12-09 11:53:39.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0029940732331709295 Training loss: 7.360229015350342
2025-12-09 11:53:39.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0029939918209695676 Training loss: 7.845003128051758
2025-12-09 11:53:40.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0029939098545442733 Training loss: 7.07450008392334
2025-12-09 11:53:40.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0029938273339254516 Training loss: 7.576327323913574
2025-12-09 11:53:40.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.002993744259143717 Training loss: 7.247929096221924
2025-12-09 11:53:40.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.002993660630229886 Training loss: 6.405491352081299
2025-12-09 11:53:40.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0029935764472149833 Training loss: 7.626428127288818
2025-12-09 11:53:40.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0029934917101302376 Training loss: 7.415420055389404
2025-12-09 11:53:40.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.002993406419007084 Training loss: 7.897877216339111
2025-12-09 11:53:40.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0029933205738771626 Training loss: 7.071279525756836
2025-12-09 11:53:40.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0029932341747723194 Training loss: 7.487879276275635
2025-12-09 11:53:40.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.002993147221724606 Training loss: 7.145799160003662
2025-12-09 11:53:40.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0029930597147662785 Training loss: 7.133472919464111
2025-12-09 11:53:40.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0029929716539297997 Training loss: 7.676650047302246
2025-12-09 11:53:40.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0029928830392478376 Training loss: 7.220711708068848
2025-12-09 11:53:41.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.002992793870753265 Training loss: 7.202521800994873
2025-12-09 11:53:41.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0029927041484791614 Training loss: 8.034334182739258
2025-12-09 11:53:41.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00299261387245881 Training loss: 7.625834941864014
2025-12-09 11:53:41.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0029925230427257006 Training loss: 7.547395706176758
2025-12-09 11:53:41.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0029924316593135285 Training loss: 7.2985663414001465
2025-12-09 11:53:41.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0029923397222561938 Training loss: 7.543284893035889
2025-12-09 11:53:41.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0029922472315878023 Training loss: 7.241466045379639
2025-12-09 11:53:41.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.002992154187342665 Training loss: 7.293763160705566
2025-12-09 11:53:41.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.002992060589555299 Training loss: 7.183499813079834
2025-12-09 11:53:41.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0029919664382604253 Training loss: 7.631886005401611
2025-12-09 11:53:41.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0029918717334929718 Training loss: 7.900401592254639
2025-12-09 11:53:41.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00299177647528807 Training loss: 7.248276710510254
2025-12-09 11:53:41.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.002991680663681059 Training loss: 7.306434154510498
2025-12-09 11:53:42.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.002991584298707481 Training loss: 7.787003993988037
2025-12-09 11:53:42.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.002991487380403084 Training loss: 8.380019187927246
2025-12-09 11:53:42.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.002991389908803823 Training loss: 7.5019378662109375
2025-12-09 11:53:42.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0029912918839458554 Training loss: 7.5975661277771
2025-12-09 11:53:42.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.002991193305865547 Training loss: 7.508146286010742
2025-12-09 11:53:42.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0029910941745994657 Training loss: 8.92702865600586
2025-12-09 11:53:42.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0029909944901843864 Training loss: 7.033946990966797
2025-12-09 11:53:42.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.002990894252657289 Training loss: 7.306068420410156
2025-12-09 11:53:42.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0029907934620553595 Training loss: 7.69537353515625
2025-12-09 11:53:42.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0029906921184159863 Training loss: 7.6212968826293945
2025-12-09 11:53:42.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029905902217767654 Training loss: 7.243485927581787
2025-12-09 11:53:42.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029904877721754976 Training loss: 7.613526344299316
2025-12-09 11:53:43.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002990384769650188 Training loss: 7.296475887298584
2025-12-09 11:53:43.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0029902812142390475 Training loss: 7.583534240722656
2025-12-09 11:53:43.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0029901771059804923 Training loss: 7.342926979064941
2025-12-09 11:53:43.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0029900724449131427 Training loss: 7.528458595275879
2025-12-09 11:53:43.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0029899672310758248 Training loss: 7.107953071594238
2025-12-09 11:53:43.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0029898614645075695 Training loss: 7.16855001449585
2025-12-09 11:53:43.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.002989755145247613 Training loss: 6.782660961151123
2025-12-09 11:53:43.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002989648273335397 Training loss: 7.256213665008545
2025-12-09 11:53:43.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0029895408488105667 Training loss: 7.444056987762451
2025-12-09 11:53:43.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0029894328717129737 Training loss: 7.33244514465332
2025-12-09 11:53:43.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0029893243420826736 Training loss: 7.621094226837158
2025-12-09 11:53:43.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.002989215259959928 Training loss: 7.657789707183838
2025-12-09 11:53:43.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002989105625385203 Training loss: 7.535585403442383
2025-12-09 11:53:44.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.002988995438399169 Training loss: 7.299041748046875
2025-12-09 11:53:44.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0029888846990427024 Training loss: 7.062422275543213
2025-12-09 11:53:44.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.002988773407356884 Training loss: 7.346871852874756
2025-12-09 11:53:44.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0029886615633829996 Training loss: 7.4923601150512695
2025-12-09 11:53:44.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.002988549167162539 Training loss: 7.374239444732666
2025-12-09 11:53:44.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0029884362187371986 Training loss: 7.898052215576172
2025-12-09 11:53:44.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0029883227181488783 Training loss: 6.48508358001709
2025-12-09 11:53:44.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.002988208665439683 Training loss: 7.494927883148193
2025-12-09 11:53:44.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0029880940606519233 Training loss: 7.455460071563721
2025-12-09 11:53:44.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.002987978903828114 Training loss: 7.266217231750488
2025-12-09 11:53:44.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0029878631950109734 Training loss: 7.331582069396973
2025-12-09 11:53:44.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0029877469342434273 Training loss: 6.510400295257568
2025-12-09 11:53:44.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.002987630121568604 Training loss: 7.395120620727539
2025-12-09 11:53:45.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.002987512757029838 Training loss: 6.743677616119385
2025-12-09 11:53:45.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0029873948406706667 Training loss: 7.727278232574463
2025-12-09 11:53:45.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0029872763725348342 Training loss: 8.118131637573242
2025-12-09 11:53:45.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0029871573526662884 Training loss: 7.434891700744629
2025-12-09 11:53:45.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0029870377811091822 Training loss: 7.247894287109375
2025-12-09 11:53:45.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.002986917657907872 Training loss: 7.402978897094727
2025-12-09 11:53:45.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.00298679698310692 Training loss: 7.501755237579346
2025-12-09 11:53:45.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.002986675756751093 Training loss: 6.887330055236816
2025-12-09 11:53:45.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0029865539788853624 Training loss: 7.548000335693359
2025-12-09 11:53:45.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0029864316495549037 Training loss: 7.392025947570801
2025-12-09 11:53:45.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0029863087688050973 Training loss: 7.326822280883789
2025-12-09 11:53:45.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.002986185336681528 Training loss: 7.269261837005615
2025-12-09 11:53:46.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0029860613532299847 Training loss: 7.311373233795166
2025-12-09 11:53:46.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0029859368184964627 Training loss: 7.200954914093018
2025-12-09 11:53:46.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.002985811732527159 Training loss: 7.082763671875
2025-12-09 11:53:46.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0029856860953684774 Training loss: 6.711154460906982
2025-12-09 11:53:46.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0029855599070670253 Training loss: 7.738734722137451
2025-12-09 11:53:46.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0029854331676696143 Training loss: 7.01794958114624
2025-12-09 11:53:46.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0029853058772232608 Training loss: 6.983828544616699
2025-12-09 11:53:46.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0029851780357751856 Training loss: 7.210479259490967
2025-12-09 11:53:46.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.002985049643372814 Training loss: 7.216610908508301
2025-12-09 11:53:46.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0029849207000637755 Training loss: 7.168489456176758
2025-12-09 11:53:46.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0029847912058959033 Training loss: 7.020786285400391
2025-12-09 11:53:46.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.002984661160917237 Training loss: 6.844659805297852
2025-12-09 11:53:46.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.002984530565176018 Training loss: 7.143846035003662
2025-12-09 11:53:47.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.002984399418720694 Training loss: 7.537593841552734
2025-12-09 11:53:47.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0029842677215999158 Training loss: 7.296660423278809
2025-12-09 11:53:47.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.002984135473862539 Training loss: 7.499332904815674
2025-12-09 11:53:47.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.002984002675557623 Training loss: 7.32333517074585
2025-12-09 11:53:47.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.002983869326734432 Training loss: 7.057345390319824
2025-12-09 11:53:47.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0029837354274424347 Training loss: 7.256861686706543
2025-12-09 11:53:47.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.002983600977731303 Training loss: 7.0426764488220215
2025-12-09 11:53:47.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0029834659776509136 Training loss: 6.866648197174072
2025-12-09 11:53:47.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0029833304272513473 Training loss: 7.046931743621826
2025-12-09 11:53:47.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.002983194326582889 Training loss: 7.160009384155273
2025-12-09 11:53:47.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0029830576756960285 Training loss: 7.1536664962768555
2025-12-09 11:53:47.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0029829204746414577 Training loss: 7.573050498962402
2025-12-09 11:53:47.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0029827827234700744 Training loss: 6.3262505531311035
2025-12-09 11:53:48.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00298264442223298 Training loss: 6.868226528167725
2025-12-09 11:53:48.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0029825055709814803 Training loss: 7.312450408935547
2025-12-09 11:53:48.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.002982366169767084 Training loss: 6.902043342590332
2025-12-09 11:53:48.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.002982226218641505 Training loss: 7.137598037719727
2025-12-09 11:53:48.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.002982085717656661 Training loss: 7.2062177658081055
2025-12-09 11:53:48.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0029819446668646723 Training loss: 7.120816230773926
2025-12-09 11:53:48.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0029818030663178656 Training loss: 7.039079666137695
2025-12-09 11:53:48.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00298166091606877 Training loss: 7.1836042404174805
2025-12-09 11:53:48.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0029815182161701185 Training loss: 7.697516918182373
2025-12-09 11:53:48.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0029813749666748484 Training loss: 7.299827575683594
2025-12-09 11:53:48.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0029812311676361003 Training loss: 7.447654724121094
2025-12-09 11:53:48.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00298108681910722 Training loss: 7.206648349761963
2025-12-09 11:53:49.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0029809419211417553 Training loss: 7.315396785736084
2025-12-09 11:53:49.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0029807964737934593 Training loss: 7.200397491455078
2025-12-09 11:53:49.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0029806504771162884 Training loss: 7.19415283203125
2025-12-09 11:53:49.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0029805039311644028 Training loss: 7.055984973907471
2025-12-09 11:53:49.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.002980356835992166 Training loss: 7.421419620513916
2025-12-09 11:53:49.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0029802091916541463 Training loss: 7.269465446472168
2025-12-09 11:53:49.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.002980060998205115 Training loss: 7.039222240447998
2025-12-09 11:53:49.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0029799122557000466 Training loss: 7.34372615814209
2025-12-09 11:53:49.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0029797629641941203 Training loss: 7.419201850891113
2025-12-09 11:53:49.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.002979613123742719 Training loss: 7.105193138122559
2025-12-09 11:53:49.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0029794627344014277 Training loss: 7.334209442138672
2025-12-09 11:53:49.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.002979311796226037 Training loss: 6.574466705322266
2025-12-09 11:53:49.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00297916030927254 Training loss: 7.326198101043701
2025-12-09 11:53:50.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0029790082735971337 Training loss: 7.38875150680542
2025-12-09 11:53:50.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0029788556892562184 Training loss: 6.776617050170898
2025-12-09 11:53:50.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.002978702556306398 Training loss: 7.089386940002441
2025-12-09 11:53:50.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.00297854887480448 Training loss: 7.783587455749512
2025-12-09 11:53:50.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.002978394644807475 Training loss: 7.687809944152832
2025-12-09 11:53:50.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0029782398663725984 Training loss: 7.338481903076172
2025-12-09 11:53:50.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0029780845395572676 Training loss: 6.748720645904541
2025-12-09 11:53:50.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0029779286644191043 Training loss: 7.248665809631348
2025-12-09 11:53:50.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.002977772241015933 Training loss: 7.557005405426025
2025-12-09 11:53:50.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.002977615269405782 Training loss: 7.0027079582214355
2025-12-09 11:53:50.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0029774577496468825 Training loss: 7.302324295043945
2025-12-09 11:53:50.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0029772996817976696 Training loss: 7.170278072357178
2025-12-09 11:53:50.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.002977141065916781 Training loss: 7.093092441558838
2025-12-09 11:53:51.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0029769819020630597 Training loss: 7.343667507171631
2025-12-09 11:53:51.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0029768221902955485 Training loss: 7.256009101867676
2025-12-09 11:53:51.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.002976661930673497 Training loss: 6.893094062805176
2025-12-09 11:53:51.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.002976501123256355 Training loss: 7.347696781158447
2025-12-09 11:53:51.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0029763397681037787 Training loss: 7.05135440826416
2025-12-09 11:53:51.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0029761778652756246 Training loss: 7.293687343597412
2025-12-09 11:53:51.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0029760154148319538 Training loss: 6.94558048248291
2025-12-09 11:53:51.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0029758524168330305 Training loss: 7.412813663482666
2025-12-09 11:53:51.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.0029756888713393216 Training loss: 7.007347583770752
2025-12-09 11:53:51.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.002975524778411498 Training loss: 7.024962425231934
2025-12-09 11:53:51.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0029753601381104318 Training loss: 7.287906646728516
2025-12-09 11:53:51.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0029751949504972 Training loss: 7.149062156677246
2025-12-09 11:53:52.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0029750292156330823 Training loss: 6.592287540435791
2025-12-09 11:53:52.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0029748629335795604 Training loss: 6.991415500640869
2025-12-09 11:53:52.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.002974696104398321 Training loss: 7.1899800300598145
2025-12-09 11:53:52.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.002974528728151251 Training loss: 7.157639026641846
2025-12-09 11:53:52.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0029743608049004424 Training loss: 6.737515449523926
2025-12-09 11:53:52.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.002974192334708189 Training loss: 7.421802997589111
2025-12-09 11:53:52.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.002974023317636989 Training loss: 6.9662346839904785
2025-12-09 11:53:52.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0029738537537495413 Training loss: 6.865285873413086
2025-12-09 11:53:52.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.0029736836431087494 Training loss: 6.882233619689941
2025-12-09 11:53:52.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0029735129857777188 Training loss: 6.7503533363342285
2025-12-09 11:53:52.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.002973341781819758 Training loss: 7.7165703773498535
2025-12-09 11:53:52.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.002973170031298378 Training loss: 7.263189792633057
2025-12-09 11:53:52.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0029729977342772937 Training loss: 7.115932464599609
2025-12-09 11:53:53.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.0029728248908204207 Training loss: 7.5175700187683105
2025-12-09 11:53:53.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.0029726515009918793 Training loss: 7.187154293060303
2025-12-09 11:53:53.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0029724775648559913 Training loss: 7.260032653808594
2025-12-09 11:53:53.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0029723030824772814 Training loss: 6.915391445159912
2025-12-09 11:53:53.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.002972128053920478 Training loss: 6.278204441070557
2025-12-09 11:53:53.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00297195247925051 Training loss: 7.225526809692383
2025-12-09 11:53:53.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.0029717763585325103 Training loss: 7.173048496246338
2025-12-09 11:53:53.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.002971599691831815 Training loss: 7.237122535705566
2025-12-09 11:53:53.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.0029714224792139607 Training loss: 7.221039295196533
2025-12-09 11:53:53.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.002971244720744689 Training loss: 7.294764995574951
2025-12-09 11:53:53.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.0029710664164899416 Training loss: 6.871382236480713
2025-12-09 11:53:53.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0029708875665158643 Training loss: 7.155046463012695
2025-12-09 11:53:53.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0029707081708888047 Training loss: 7.086852073669434
2025-12-09 11:53:54.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0029705282296753127 Training loss: 6.984673976898193
2025-12-09 11:53:54.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.002970347742942141 Training loss: 7.236988067626953
2025-12-09 11:53:54.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.002970166710756245 Training loss: 7.471110820770264
2025-12-09 11:53:54.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.002969985133184781 Training loss: 7.265893459320068
2025-12-09 11:53:54.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0029698030102951094 Training loss: 6.940718650817871
2025-12-09 11:53:54.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0029696203421547916 Training loss: 7.32488489151001
2025-12-09 11:53:54.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0029694371288315913 Training loss: 7.260299205780029
2025-12-09 11:53:54.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.0029692533703934757 Training loss: 6.90404748916626
2025-12-09 11:53:54.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.002969069066908613 Training loss: 6.817226886749268
2025-12-09 11:53:54.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0029688842184453744 Training loss: 7.859225273132324
2025-12-09 11:53:54.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.002968698825072332 Training loss: 7.54714298248291
2025-12-09 11:53:54.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0029685128868582626 Training loss: 6.855736255645752
2025-12-09 11:53:55.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0029683264038721418 Training loss: 7.069423198699951
2025-12-09 11:53:55.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0029681393761831487 Training loss: 7.094863414764404
2025-12-09 11:53:55.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.002967951803860666 Training loss: 7.472152233123779
2025-12-09 11:53:55.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.002967763686974276 Training loss: 7.025598049163818
2025-12-09 11:53:55.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.002967575025593765 Training loss: 8.617219924926758
2025-12-09 11:53:55.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.00296738581978912 Training loss: 6.807998180389404
2025-12-09 11:53:55.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0029671960696305306 Training loss: 7.171267032623291
2025-12-09 11:53:55.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.002967005775188388 Training loss: 7.14969539642334
2025-12-09 11:53:55.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0029668149365332853 Training loss: 7.323563575744629
2025-12-09 11:53:55.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.002966623553736018 Training loss: 7.139639377593994
2025-12-09 11:53:55.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0029664316268675824 Training loss: 7.451192378997803
2025-12-09 11:53:55.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0029662391559991783 Training loss: 8.019515991210938
2025-12-09 11:53:55.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0029660461412022057 Training loss: 7.650644302368164
2025-12-09 11:53:56.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.002965852582548267 Training loss: 7.219245910644531
2025-12-09 11:53:56.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0029656584801091668 Training loss: 7.354191303253174
2025-12-09 11:53:56.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0029654638339569107 Training loss: 7.197206020355225
2025-12-09 11:53:56.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.002965268644163706 Training loss: 6.67415189743042
2025-12-09 11:53:56.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0029650729108019625 Training loss: 6.589515686035156
2025-12-09 11:53:56.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.002964876633944291 Training loss: 6.955427646636963
2025-12-09 11:53:56.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0029646798136635038 Training loss: 7.195757865905762
2025-12-09 11:53:56.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.002964482450032615 Training loss: 6.976341247558594
2025-12-09 11:53:56.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.002964284543124841 Training loss: 7.347676753997803
2025-12-09 11:53:56.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0029640860930135976 Training loss: 6.965733528137207
2025-12-09 11:53:56.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.002963887099772505 Training loss: 7.16593074798584
2025-12-09 11:53:56.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.0029636875634753827 Training loss: 6.923715114593506
2025-12-09 11:53:56.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.002963487484196253 Training loss: 6.991359710693359
2025-12-09 11:53:57.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.002963286862009338 Training loss: 7.184243679046631
2025-12-09 11:53:57.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0029630856969890627 Training loss: 6.983364105224609
2025-12-09 11:53:57.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0029628839892100536 Training loss: 6.927122592926025
2025-12-09 11:53:57.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.002962681738747137 Training loss: 7.054836750030518
2025-12-09 11:53:57.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.002962478945675342 Training loss: 7.03665018081665
2025-12-09 11:53:57.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.002962275610069898 Training loss: 7.157291889190674
2025-12-09 11:53:57.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.002962071732006237 Training loss: 6.633443355560303
2025-12-09 11:53:57.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00296186731155999 Training loss: 7.326756954193115
2025-12-09 11:53:57.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0029616623488069923 Training loss: 7.261470794677734
2025-12-09 11:53:57.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0029614568438232768 Training loss: 6.782707691192627
2025-12-09 11:53:57.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.0029612507966850807 Training loss: 6.789678573608398
2025-12-09 11:53:57.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0029610442074688398 Training loss: 7.049905300140381
2025-12-09 11:53:58.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0029608370762511937 Training loss: 7.4415812492370605
2025-12-09 11:53:58.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0029606294031089804 Training loss: 7.124997138977051
2025-12-09 11:53:58.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.00296042118811924 Training loss: 7.20446252822876
2025-12-09 11:53:58.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.002960212431359215 Training loss: 7.164221286773682
2025-12-09 11:53:58.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0029600031329063466 Training loss: 7.2825775146484375
2025-12-09 11:53:58.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.002959793292838278 Training loss: 6.946348667144775
2025-12-09 11:53:58.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.002959582911232853 Training loss: 7.127441883087158
2025-12-09 11:53:58.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0029593719881681173 Training loss: 7.3820295333862305
2025-12-09 11:53:58.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.002959160523722316 Training loss: 6.8303937911987305
2025-12-09 11:53:58.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0029589485179738963 Training loss: 7.089783668518066
2025-12-09 11:53:58.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0029587359710015054 Training loss: 7.56169319152832
2025-12-09 11:53:58.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0029585228828839915 Training loss: 7.184954643249512
2025-12-09 11:53:58.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.002958309253700404 Training loss: 6.985814571380615
2025-12-09 11:53:59.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.002958095083529992 Training loss: 7.302738666534424
2025-12-09 11:53:59.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0029578803724522058 Training loss: 7.39024543762207
2025-12-09 11:53:59.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.002957665120546697 Training loss: 6.949093818664551
2025-12-09 11:53:59.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0029574493278933175 Training loss: 6.641403675079346
2025-12-09 11:53:59.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.002957232994572119 Training loss: 6.1934494972229
2025-12-09 11:53:59.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0029570161206633546 Training loss: 7.016007423400879
2025-12-09 11:53:59.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0029567987062474772 Training loss: 7.057834148406982
2025-12-09 11:53:59.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.002956580751405141 Training loss: 6.7354230880737305
2025-12-09 11:53:59.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.002956362256217201 Training loss: 6.915372848510742
2025-12-09 11:53:59.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0029561432207647113 Training loss: 7.419905662536621
2025-12-09 11:53:59.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.002955923645128927 Training loss: 7.161588668823242
2025-12-09 11:53:59.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0029557035293913047 Training loss: 7.322515487670898
2025-12-09 11:53:59.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.0029554828736334995 Training loss: 6.929101467132568
2025-12-09 11:54:00.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0029552616779373684 Training loss: 6.868518352508545
2025-12-09 11:54:00.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0029550399423849674 Training loss: 6.697653293609619
2025-12-09 11:54:00.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.002954817667058554 Training loss: 6.762097358703613
2025-12-09 11:54:00.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.002954594852040585 Training loss: 6.750809669494629
2025-12-09 11:54:00.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0029543714974137178 Training loss: 6.816134929656982
2025-12-09 11:54:00.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.00295414760326081 Training loss: 7.328139305114746
2025-12-09 11:54:00.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.002953923169664919 Training loss: 7.047082901000977
2025-12-09 11:54:00.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.002953698196709303 Training loss: 6.8714118003845215
2025-12-09 11:54:00.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.00295347268447742 Training loss: 7.056852340698242
2025-12-09 11:54:00.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.002953246633052928 Training loss: 7.0615034103393555
2025-12-09 11:54:00.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0029530200425196837 Training loss: 7.06398868560791
2025-12-09 11:54:00.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0029527929129617467 Training loss: 7.310015678405762
2025-12-09 11:54:01.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.002952565244463374 Training loss: 6.898496627807617
2025-12-09 11:54:01.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0029523370371090235 Training loss: 7.318864345550537
2025-12-09 11:54:01.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.002952108290983353 Training loss: 7.225074768066406
2025-12-09 11:54:01.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.002951879006171221 Training loss: 6.97988748550415
2025-12-09 11:54:01.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0029516491827576833 Training loss: 6.929690361022949
2025-12-09 11:54:01.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.0029514188208279984 Training loss: 7.682606220245361
2025-12-09 11:54:01.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0029511879204676223 Training loss: 7.28427791595459
2025-12-09 11:54:01.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0029509564817622133 Training loss: 7.009479999542236
2025-12-09 11:54:01.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0029507245047976265 Training loss: 6.923266887664795
2025-12-09 11:54:01.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.002950491989659918 Training loss: 6.869012355804443
2025-12-09 11:54:01.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0029502589364353454 Training loss: 7.066961288452148
2025-12-09 11:54:01.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0029500253452103622 Training loss: 7.0842695236206055
2025-12-09 11:54:01.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.0029497912160716235 Training loss: 7.368770122528076
2025-12-09 11:54:02.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.002949556549105985 Training loss: 7.297372817993164
2025-12-09 11:54:02.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0029493213444005 Training loss: 6.564399242401123
2025-12-09 11:54:02.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.002949085602042422 Training loss: 7.489241123199463
2025-12-09 11:54:02.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0029488493221192045 Training loss: 7.131577968597412
2025-12-09 11:54:02.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.002948612504718499 Training loss: 6.963879108428955
2025-12-09 11:54:02.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0029483751499281585 Training loss: 6.589960098266602
2025-12-09 11:54:02.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0029481372578362332 Training loss: 6.750088691711426
2025-12-09 11:54:02.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.002947898828530974 Training loss: 7.050100326538086
2025-12-09 11:54:02.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.00294765986210083 Training loss: 7.482784271240234
2025-12-09 11:54:02.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0029474203586344516 Training loss: 6.847475051879883
2025-12-09 11:54:02.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.0029471803182206857 Training loss: 6.714810371398926
2025-12-09 11:54:02.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.0029469397409485807 Training loss: 6.9443840980529785
2025-12-09 11:54:02.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0029466986269073825 Training loss: 7.10349702835083
2025-12-09 11:54:03.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.0029464569761865366 Training loss: 7.231997966766357
2025-12-09 11:54:03.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.002946214788875689 Training loss: 6.997532844543457
2025-12-09 11:54:03.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0029459720650646826 Training loss: 6.828283786773682
2025-12-09 11:54:03.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0029457288048435606 Training loss: 6.780557155609131
2025-12-09 11:54:03.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.002945485008302565 Training loss: 7.066738605499268
2025-12-09 11:54:03.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0029452406755321363 Training loss: 7.235119342803955
2025-12-09 11:54:03.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0029449958066229145 Training loss: 6.692873477935791
2025-12-09 11:54:03.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0029447504016657383 Training loss: 6.965122699737549
2025-12-09 11:54:03.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.002944504460751645 Training loss: 6.990239143371582
2025-12-09 11:54:03.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.002944257983971871 Training loss: 6.930241107940674
2025-12-09 11:54:03.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.002944010971417851 Training loss: 7.051540374755859
2025-12-09 11:54:03.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00294376342318122 Training loss: 7.016505241394043
2025-12-09 11:54:04.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.00294351533935381 Training loss: 6.6431684494018555
2025-12-09 11:54:04.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.002943266720027652 Training loss: 7.51872444152832
2025-12-09 11:54:04.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0029430175652949762 Training loss: 7.099221229553223
2025-12-09 11:54:04.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0029427678752482114 Training loss: 6.722705841064453
2025-12-09 11:54:04.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0029425176499799843 Training loss: 7.192796230316162
2025-12-09 11:54:04.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.002942266889583121 Training loss: 6.701643466949463
2025-12-09 11:54:04.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.0029420155941506454 Training loss: 6.762726783752441
2025-12-09 11:54:04.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00294176376377578 Training loss: 7.011488437652588
2025-12-09 11:54:04.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0029415113985519466 Training loss: 6.938356876373291
2025-12-09 11:54:04.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0029412584985727642 Training loss: 7.816811561584473
2025-12-09 11:54:04.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.002941005063932051 Training loss: 7.048575401306152
2025-12-09 11:54:04.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.002940751094723823 Training loss: 6.901191711425781
2025-12-09 11:54:04.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0029404965910422953 Training loss: 6.761001110076904
2025-12-09 11:54:05.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0029402415529818805 Training loss: 6.9240264892578125
2025-12-09 11:54:05.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.0029399859806371895 Training loss: 7.232253551483154
2025-12-09 11:54:05.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.002939729874103032 Training loss: 7.2680253982543945
2025-12-09 11:54:05.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.002939473233474415 Training loss: 6.748453140258789
2025-12-09 11:54:05.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.002939216058846544 Training loss: 6.653972148895264
2025-12-09 11:54:05.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0029389583503148234 Training loss: 6.931906223297119
2025-12-09 11:54:05.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.0029387001079748537 Training loss: 6.851944923400879
2025-12-09 11:54:05.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0029384413319224366 Training loss: 7.1054229736328125
2025-12-09 11:54:05.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.002938182022253568 Training loss: 7.0363359451293945
2025-12-09 11:54:05.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.002937922179064445 Training loss: 6.860860347747803
2025-12-09 11:54:05.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.00293766180245146 Training loss: 6.908502101898193
2025-12-09 11:54:05.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0029374008925112057 Training loss: 6.890663146972656
2025-12-09 11:54:05.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0029371394493404707 Training loss: 7.066931247711182
2025-12-09 11:54:06.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0029368774730362426 Training loss: 7.495790481567383
2025-12-09 11:54:06.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.002936614963695706 Training loss: 7.260051250457764
2025-12-09 11:54:06.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.002936351921416244 Training loss: 7.417135715484619
2025-12-09 11:54:06.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0029360883462954362 Training loss: 7.8352885246276855
2025-12-09 11:54:06.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.002935824238431062 Training loss: 6.645907402038574
2025-12-09 11:54:06.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0029355595979210962 Training loss: 6.761205196380615
2025-12-09 11:54:06.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.002935294424863712 Training loss: 6.975984573364258
2025-12-09 11:54:06.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.002935028719357281 Training loss: 7.058735370635986
2025-12-09 11:54:06.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0029347624815003704 Training loss: 6.955937385559082
2025-12-09 11:54:06.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.0029344957113917472 Training loss: 6.832052230834961
2025-12-09 11:54:06.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.002934228409130374 Training loss: 7.398099422454834
2025-12-09 11:54:06.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0029339605748154125 Training loss: 7.069407939910889
2025-12-09 11:54:07.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0029336922085462193 Training loss: 7.2382707595825195
2025-12-09 11:54:07.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.002933423310422351 Training loss: 7.158868312835693
2025-12-09 11:54:07.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00293315388054356 Training loss: 6.674002647399902
2025-12-09 11:54:07.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.002932883919009796 Training loss: 7.853985786437988
2025-12-09 11:54:07.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.002932613425921207 Training loss: 7.09785270690918
2025-12-09 11:54:07.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.002932342401378137 Training loss: 6.53255558013916
2025-12-09 11:54:07.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0029320708454811267 Training loss: 6.709263801574707
2025-12-09 11:54:07.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.002931798758330916 Training loss: 7.810985565185547
2025-12-09 11:54:07.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.002931526140028441 Training loss: 7.236917972564697
2025-12-09 11:54:07.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0029312529906748326 Training loss: 6.929712772369385
2025-12-09 11:54:07.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0029309793103714224 Training loss: 7.177783966064453
2025-12-09 11:54:07.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.002930705099219736 Training loss: 6.435698986053467
2025-12-09 11:54:07.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0029304303573214983 Training loss: 7.0298261642456055
2025-12-09 11:54:08.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0029301550847786293 Training loss: 7.0920867919921875
2025-12-09 11:54:08.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0029298792816932462 Training loss: 7.314302444458008
2025-12-09 11:54:08.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0029296029481676636 Training loss: 6.757560729980469
2025-12-09 11:54:08.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0029293260843043924 Training loss: 7.1356682777404785
2025-12-09 11:54:08.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0029290486902061397 Training loss: 7.010060787200928
2025-12-09 11:54:08.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0029287707659758117 Training loss: 7.4418840408325195
2025-12-09 11:54:08.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0029284923117165076 Training loss: 6.560851573944092
2025-12-09 11:54:08.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0029282133275315265 Training loss: 7.1984639167785645
2025-12-09 11:54:08.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.0029279338135243626 Training loss: 7.897568702697754
2025-12-09 11:54:08.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0029276537697987062 Training loss: 6.759417533874512
2025-12-09 11:54:08.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0029273731964584446 Training loss: 7.147840976715088
2025-12-09 11:54:08.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0029270920936076625 Training loss: 6.971058368682861
2025-12-09 11:54:08.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0029268104613506397 Training loss: 6.936478137969971
2025-12-09 11:54:09.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0029265282997918535 Training loss: 6.749828815460205
2025-12-09 11:54:09.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.0029262456090359762 Training loss: 7.439068794250488
2025-12-09 11:54:09.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.002925962389187877 Training loss: 7.087524890899658
2025-12-09 11:54:09.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0029256786403526226 Training loss: 7.300724029541016
2025-12-09 11:54:09.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0029253943626354737 Training loss: 6.3298492431640625
2025-12-09 11:54:09.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0029251095561418894 Training loss: 6.830812454223633
2025-12-09 11:54:09.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0029248242209775235 Training loss: 6.714680194854736
2025-12-09 11:54:09.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.002924538357248226 Training loss: 6.812134265899658
2025-12-09 11:54:09.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0029242519650600437 Training loss: 6.895529270172119
2025-12-09 11:54:09.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.002923965044519219 Training loss: 6.73838472366333
2025-12-09 11:54:09.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.002923677595732191 Training loss: 7.053770065307617
2025-12-09 11:54:09.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0029233896188055933 Training loss: 7.102532386779785
2025-12-09 11:54:10.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0029231011138462566 Training loss: 6.932579517364502
2025-12-09 11:54:10.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0029228120809612072 Training loss: 7.304733753204346
2025-12-09 11:54:10.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.002922522520257667 Training loss: 6.798766136169434
2025-12-09 11:54:10.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0029222324318430542 Training loss: 6.952324390411377
2025-12-09 11:54:10.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0029219418158249826 Training loss: 7.241913318634033
2025-12-09 11:54:10.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0029216506723112614 Training loss: 7.052742004394531
2025-12-09 11:54:10.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0029213590014098953 Training loss: 6.784222602844238
2025-12-09 11:54:10.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.002921066803229085 Training loss: 7.457218170166016
2025-12-09 11:54:10.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.002920774077877228 Training loss: 6.7718915939331055
2025-12-09 11:54:10.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0029204808254629146 Training loss: 6.1637372970581055
2025-12-09 11:54:10.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.002920187046094933 Training loss: 6.888757705688477
2025-12-09 11:54:10.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.002919892739882266 Training loss: 7.38470983505249
2025-12-09 11:54:10.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0029195979069340924 Training loss: 6.9919328689575195
2025-12-09 11:54:11.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0029193025473597855 Training loss: 6.3826141357421875
2025-12-09 11:54:11.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0029190066612689142 Training loss: 6.941226959228516
2025-12-09 11:54:11.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0029187102487712433 Training loss: 7.077340126037598
2025-12-09 11:54:11.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0029184133099767326 Training loss: 7.216209888458252
2025-12-09 11:54:11.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.0029181158449955364 Training loss: 6.9664764404296875
2025-12-09 11:54:11.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0029178178539380054 Training loss: 7.218052864074707
2025-12-09 11:54:11.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0029175193369146844 Training loss: 6.9672698974609375
2025-12-09 11:54:11.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.002917220294036315 Training loss: 7.309445381164551
2025-12-09 11:54:11.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0029169207254138314 Training loss: 6.882728099822998
2025-12-09 11:54:11.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0029166206311583647 Training loss: 6.948824882507324
2025-12-09 11:54:11.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.00291632001138124 Training loss: 6.850742816925049
2025-12-09 11:54:11.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0029160188661939783 Training loss: 7.021442413330078
2025-12-09 11:54:11.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.002915717195708295 Training loss: 7.04585599899292
2025-12-09 11:54:12.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0029154150000361 Training loss: 6.763498306274414
2025-12-09 11:54:12.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0029151122792894987 Training loss: 7.630409240722656
2025-12-09 11:54:12.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.00291480903358079 Training loss: 7.160463809967041
2025-12-09 11:54:12.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00291450526302247 Training loss: 7.094379425048828
2025-12-09 11:54:12.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0029142009677272274 Training loss: 6.9634552001953125
2025-12-09 11:54:12.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0029138961478079456 Training loss: 7.124644756317139
2025-12-09 11:54:12.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.002913590803377704 Training loss: 6.723509788513184
2025-12-09 11:54:12.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0029132849345497756 Training loss: 7.086480140686035
2025-12-09 11:54:12.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0029129785414376275 Training loss: 7.074717044830322
2025-12-09 11:54:12.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0029126716241549225 Training loss: 6.889821529388428
2025-12-09 11:54:12.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0029123641828155173 Training loss: 6.89312219619751
2025-12-09 11:54:12.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0029120562175334627 Training loss: 6.7610907554626465
2025-12-09 11:54:13.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0029117477284230043 Training loss: 7.021039962768555
2025-12-09 11:54:13.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0029114387155985814 Training loss: 7.146274566650391
2025-12-09 11:54:13.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0029111291791748283 Training loss: 6.878828048706055
2025-12-09 11:54:13.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.002910819119266574 Training loss: 6.831043720245361
2025-12-09 11:54:13.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0029105085359888397 Training loss: 7.2762532234191895
2025-12-09 11:54:13.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.0029101974294568427 Training loss: 6.987800121307373
2025-12-09 11:54:13.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0029098857997859936 Training loss: 7.060516357421875
2025-12-09 11:54:13.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0029095736470918974 Training loss: 6.566768169403076
2025-12-09 11:54:13.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0029092609714903525 Training loss: 6.9873809814453125
2025-12-09 11:54:13.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.002908947773097352 Training loss: 7.317965030670166
2025-12-09 11:54:13.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.002908634052029083 Training loss: 7.1645188331604
2025-12-09 11:54:13.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0029083198084019256 Training loss: 6.586503028869629
2025-12-09 11:54:13.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0029080050423324543 Training loss: 6.098776340484619
2025-12-09 11:54:14.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.002907689753937438 Training loss: 6.825014591217041
2025-12-09 11:54:14.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.0029073739433338377 Training loss: 6.783503532409668
2025-12-09 11:54:14.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0029070576106388106 Training loss: 7.234841823577881
2025-12-09 11:54:14.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.002906740755969705 Training loss: 6.974328517913818
2025-12-09 11:54:14.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.002906423379444064 Training loss: 6.607484817504883
2025-12-09 11:54:14.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0029061054811796248 Training loss: 6.871148586273193
2025-12-09 11:54:14.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0029057870612943177 Training loss: 6.627816200256348
2025-12-09 11:54:14.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0029054681199062664 Training loss: 6.819278717041016
2025-12-09 11:54:14.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.002905148657133788 Training loss: 6.748235702514648
2025-12-09 11:54:14.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0029048286730953927 Training loss: 6.880241870880127
2025-12-09 11:54:14.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.002904508167909785 Training loss: 6.680698394775391
2025-12-09 11:54:14.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.002904187141695863 Training loss: 6.809781074523926
2025-12-09 11:54:14.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.002903865594572716 Training loss: 6.849064350128174
2025-12-09 11:54:15.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.002903543526659628 Training loss: 7.1836934089660645
2025-12-09 11:54:15.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.0029032209380760766 Training loss: 7.295605659484863
2025-12-09 11:54:15.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0029028978289417323 Training loss: 6.877119064331055
2025-12-09 11:54:15.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.002902574199376457 Training loss: 7.000710487365723
2025-12-09 11:54:15.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.002902250049500309 Training loss: 6.785236835479736
2025-12-09 11:54:15.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0029019253794335363 Training loss: 6.308955192565918
2025-12-09 11:54:15.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0029016001892965817 Training loss: 6.174111366271973
2025-12-09 11:54:15.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.00290127447921008 Training loss: 6.977548599243164
2025-12-09 11:54:15.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.0029009482492948608 Training loss: 6.9700846672058105
2025-12-09 11:54:15.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.002900621499671944 Training loss: 7.251511096954346
2025-12-09 11:54:15.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0029002942304625435 Training loss: 7.0190348625183105
2025-12-09 11:54:15.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0028999664417880657 Training loss: 6.9905619621276855
2025-12-09 11:54:16.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.0028996381337701104 Training loss: 6.579888343811035
2025-12-09 11:54:16.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0028993093065304695 Training loss: 7.08418607711792
2025-12-09 11:54:16.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.002898979960191127 Training loss: 7.003323078155518
2025-12-09 11:54:16.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.002898650094874261 Training loss: 6.742406845092773
2025-12-09 11:54:16.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00289831971070224 Training loss: 6.80391263961792
2025-12-09 11:54:16.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.002897988807797627 Training loss: 6.764802932739258
2025-12-09 11:54:16.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.002897657386283176 Training loss: 6.996060371398926
2025-12-09 11:54:16.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0028973254462818345 Training loss: 7.070812225341797
2025-12-09 11:54:16.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.002896992987916741 Training loss: 6.612070083618164
2025-12-09 11:54:16.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0028966600113112277 Training loss: 5.8864970207214355
2025-12-09 11:54:16.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.002896326516588819 Training loss: 6.585468769073486
