2025-12-09 12:11:14.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 4.865200519561768
2025-12-09 12:11:14.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 5.003516674041748
2025-12-09 12:11:14.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 4.846035003662109
2025-12-09 12:11:14.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 4.847034931182861
2025-12-09 12:11:14.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 4.87977409362793
2025-12-09 12:11:14.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 4.918314456939697
2025-12-09 12:11:14.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 4.871994495391846
2025-12-09 12:11:14.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 4.767423152923584
2025-12-09 12:11:14.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 4.933836936950684
2025-12-09 12:11:14.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 4.993866443634033
2025-12-09 12:11:14.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 4.648959636688232
2025-12-09 12:11:14.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 4.798664093017578
2025-12-09 12:11:14.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 4.826470851898193
2025-12-09 12:11:14.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 4.7629594802856445
2025-12-09 12:11:14.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 4.8664984703063965
2025-12-09 12:11:14.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 4.754019737243652
2025-12-09 12:11:14.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 4.848529815673828
2025-12-09 12:11:14.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 4.854944705963135
2025-12-09 12:11:14.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 4.897099494934082
2025-12-09 12:11:14.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 4.737712383270264
2025-12-09 12:11:14.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 4.889869689941406
2025-12-09 12:11:14.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 4.704140663146973
2025-12-09 12:11:14.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 4.784346580505371
2025-12-09 12:11:14.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 4.749177932739258
2025-12-09 12:11:14.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 4.758347034454346
2025-12-09 12:11:14.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 4.822022438049316
2025-12-09 12:11:14.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 4.749704360961914
2025-12-09 12:11:14.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 4.730852127075195
2025-12-09 12:11:14.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 4.797062397003174
2025-12-09 12:11:14.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 4.803155899047852
2025-12-09 12:11:14.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 4.616823673248291
2025-12-09 12:11:14.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 4.6409759521484375
2025-12-09 12:11:14.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 4.668203830718994
2025-12-09 12:11:14.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 4.668692588806152
2025-12-09 12:11:14.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 4.736132621765137
2025-12-09 12:11:14.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 4.804342269897461
2025-12-09 12:11:14.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 4.73070764541626
2025-12-09 12:11:14.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 4.654799461364746
2025-12-09 12:11:14.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 4.728949546813965
2025-12-09 12:11:14.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 4.748133182525635
2025-12-09 12:11:14.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 4.694742679595947
2025-12-09 12:11:14.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 4.600666046142578
2025-12-09 12:11:14.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 4.584481716156006
2025-12-09 12:11:15.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 4.516683101654053
2025-12-09 12:11:15.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 4.638516902923584
2025-12-09 12:11:15.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 4.659780025482178
2025-12-09 12:11:15.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 4.642886161804199
2025-12-09 12:11:15.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 4.544114589691162
2025-12-09 12:11:15.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 4.6020050048828125
2025-12-09 12:11:15.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 4.529401779174805
2025-12-09 12:11:15.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 4.645579814910889
2025-12-09 12:11:15.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 4.6602630615234375
2025-12-09 12:11:15.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 4.58096981048584
2025-12-09 12:11:15.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 4.6105756759643555
2025-12-09 12:11:15.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 4.595900058746338
2025-12-09 12:11:15.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 4.649106979370117
2025-12-09 12:11:15.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 4.602089881896973
2025-12-09 12:11:15.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 4.37040901184082
2025-12-09 12:11:15.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 4.514552116394043
2025-12-09 12:11:15.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 4.531003952026367
2025-12-09 12:11:15.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 4.473671913146973
2025-12-09 12:11:15.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 4.445855617523193
2025-12-09 12:11:15.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 4.4859137535095215
2025-12-09 12:11:15.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 4.536176681518555
2025-12-09 12:11:15.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 4.478714942932129
2025-12-09 12:11:15.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 4.435723304748535
2025-12-09 12:11:15.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 4.3065690994262695
2025-12-09 12:11:15.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 4.457512855529785
2025-12-09 12:11:15.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 4.362163543701172
2025-12-09 12:11:15.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 4.317525863647461
2025-12-09 12:11:15.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 4.300910472869873
2025-12-09 12:11:15.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 4.395002365112305
2025-12-09 12:11:15.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 4.4087934494018555
2025-12-09 12:11:15.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 4.368921279907227
2025-12-09 12:11:15.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 4.45265007019043
2025-12-09 12:11:15.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 4.320850372314453
2025-12-09 12:11:15.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 4.295417785644531
2025-12-09 12:11:15.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 4.3266801834106445
2025-12-09 12:11:15.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 4.34787130355835
2025-12-09 12:11:15.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 4.155981063842773
2025-12-09 12:11:15.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 4.458536624908447
2025-12-09 12:11:15.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 4.2238054275512695
2025-12-09 12:11:15.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 4.312494277954102
2025-12-09 12:11:15.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 4.198537826538086
2025-12-09 12:11:15.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 4.259096145629883
2025-12-09 12:11:15.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 4.2543816566467285
2025-12-09 12:11:15.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 4.313222885131836
2025-12-09 12:11:15.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 4.205333709716797
2025-12-09 12:11:15.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 4.241311550140381
2025-12-09 12:11:15.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 4.193734169006348
2025-12-09 12:11:15.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 4.348112106323242
2025-12-09 12:11:15.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 4.337839603424072
2025-12-09 12:11:15.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 4.043024063110352
2025-12-09 12:11:15.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 4.3376312255859375
2025-12-09 12:11:15.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 4.194945335388184
2025-12-09 12:11:15.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 4.206786632537842
2025-12-09 12:11:15.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 4.2800421714782715
2025-12-09 12:11:15.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 4.248509407043457
2025-12-09 12:11:15.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 4.269098281860352
2025-12-09 12:11:15.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 4.175448894500732
2025-12-09 12:11:15.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999125880491853 Training loss: 4.2689714431762695
2025-12-09 12:11:15.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0029996503623845394 Training loss: 4.195382595062256
2025-12-09 12:11:15.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029992133535682725 Training loss: 4.118780136108398
2025-12-09 12:11:15.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.002998601612533441 Training loss: 4.197338581085205
2025-12-09 12:11:15.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0029978152105780156 Training loss: 4.184132099151611
2025-12-09 12:11:15.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0029968542393565677 Training loss: 4.133236885070801
2025-12-09 12:11:15.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029957188108695894 Training loss: 4.0734944343566895
2025-12-09 12:11:15.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00299440905745044 Training loss: 4.209383964538574
2025-12-09 12:11:15.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.002992925131749921 Training loss: 4.0752763748168945
2025-12-09 12:11:15.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0029912672067184862 Training loss: 4.134471416473389
2025-12-09 12:11:15.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0029894354755860848 Training loss: 4.1162109375
2025-12-09 12:11:15.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0029874301518396378 Training loss: 4.009818077087402
2025-12-09 12:11:15.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029852514691981603 Training loss: 4.120541572570801
2025-12-09 12:11:15.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029828996815855183 Training loss: 4.05603551864624
2025-12-09 12:11:15.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.002980375063100836 Training loss: 4.105778217315674
2025-12-09 12:11:15.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00297767790798655 Training loss: 4.0528459548950195
2025-12-09 12:11:15.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.0029748085305941127 Training loss: 4.065720081329346
2025-12-09 12:11:15.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.002971767265347359 Training loss: 4.209921836853027
2025-12-09 12:11:15.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029685544667035256 Training loss: 4.048572063446045
2025-12-09 12:11:15.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0029651705091119423 Training loss: 4.16221284866333
2025-12-09 12:11:15.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0029616157869703894 Training loss: 4.069677829742432
2025-12-09 12:11:15.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002957890714579128 Training loss: 4.054923057556152
2025-12-09 12:11:15.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0029539957260926184 Training loss: 4.101183891296387
2025-12-09 12:11:15.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.002949931275468917 Training loss: 4.033324718475342
2025-12-09 12:11:15.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029456978364167667 Training loss: 4.004396915435791
2025-12-09 12:11:15.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0029412959023403904 Training loss: 4.0785932540893555
2025-12-09 12:11:15.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.002936725986281981 Training loss: 4.013464450836182
2025-12-09 12:11:15.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.002931988620861908 Training loss: 3.966420888900757
2025-12-09 12:11:15.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.002927084358216643 Training loss: 4.008319854736328
2025-12-09 12:11:15.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0029220137699344055 Training loss: 4.122683525085449
2025-12-09 12:11:15.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0029167774469885483 Training loss: 4.081767559051514
2025-12-09 12:11:15.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002911375999668675 Training loss: 4.107147693634033
2025-12-09 12:11:15.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.002905810057509516 Training loss: 4.025094032287598
2025-12-09 12:11:15.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.002900080269217554 Training loss: 4.082528114318848
2025-12-09 12:11:15.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.002894187302595419 Training loss: 4.086021900177002
2025-12-09 12:11:15.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0028881318444640564 Training loss: 3.972580909729004
2025-12-09 12:11:15.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0028819146005826766 Training loss: 4.136672496795654
2025-12-09 12:11:15.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0028755362955665015 Training loss: 4.092700958251953
2025-12-09 12:11:15.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0028689976728023103 Training loss: 3.9524998664855957
2025-12-09 12:11:15.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0028622994943617985 Training loss: 3.885427474975586
2025-12-09 12:11:15.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0028554425409127583 Training loss: 3.862367630004883
2025-12-09 12:11:15.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.002848427611628093 Training loss: 3.920224189758301
2025-12-09 12:11:15.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0028412555240926746 Training loss: 3.878828763961792
2025-12-09 12:11:15.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.002833927114208054 Training loss: 3.876875400543213
2025-12-09 12:11:15.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0028264432360950355 Training loss: 4.020354747772217
2025-12-09 12:11:15.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0028188047619941343 Training loss: 3.9788331985473633
2025-12-09 12:11:15.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0028110125821639137 Training loss: 3.9261364936828613
2025-12-09 12:11:15.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.002803067604777227 Training loss: 3.8259778022766113
2025-12-09 12:11:15.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0027949707558153703 Training loss: 3.9359354972839355
2025-12-09 12:11:15.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.002786722978960162 Training loss: 3.9635086059570312
2025-12-09 12:11:15.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.002778325235483954 Training loss: 3.9571926593780518
2025-12-09 12:11:15.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0027697785041376007 Training loss: 4.031590938568115
2025-12-09 12:11:15.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.002761083781036381 Training loss: 3.9734225273132324
2025-12-09 12:11:15.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.002752242079543907 Training loss: 3.964884042739868
2025-12-09 12:11:15.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002743254430154012 Training loss: 3.9235057830810547
2025-12-09 12:11:15.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.002734121880370652 Training loss: 4.012024879455566
2025-12-09 12:11:15.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0027248454945858164 Training loss: 3.9194774627685547
2025-12-09 12:11:15.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0027154263539554764 Training loss: 3.8432459831237793
2025-12-09 12:11:15.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.002705865556273575 Training loss: 3.890326738357544
2025-12-09 12:11:16.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.002696164215844081 Training loss: 3.9908926486968994
2025-12-09 12:11:16.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0026863234633511188 Training loss: 3.8958077430725098
2025-12-09 12:11:16.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0026763444457271837 Training loss: 4.127495765686035
2025-12-09 12:11:16.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.002666228326019474 Training loss: 4.046571254730225
2025-12-09 12:11:16.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.002655976283254334 Training loss: 3.907547950744629
2025-12-09 12:11:16.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0026455895122998404 Training loss: 4.044897079467773
2025-12-09 12:11:16.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002635069223726543 Training loss: 3.9700100421905518
2025-12-09 12:11:16.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.002624416643666371 Training loss: 3.879242420196533
2025-12-09 12:11:16.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0026136330136697305 Training loss: 3.931032180786133
2025-12-09 12:11:16.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0026027195905608006 Training loss: 3.809215545654297
2025-12-09 12:11:16.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0025916776462910542 Training loss: 3.9676735401153564
2025-12-09 12:11:16.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00258050846779101 Training loss: 3.881274700164795
2025-12-09 12:11:16.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0025692133568202442 Training loss: 3.8829851150512695
2025-12-09 12:11:16.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.002557793629815669 Training loss: 3.7896764278411865
2025-12-09 12:11:16.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0025462506177381045 Training loss: 3.9349992275238037
2025-12-09 12:11:16.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0025345856659171567 Training loss: 3.940248966217041
2025-12-09 12:11:16.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002522800133894418 Training loss: 3.936354160308838
2025-12-09 12:11:16.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0025108953952650164 Training loss: 3.9037184715270996
2025-12-09 12:11:16.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0024988728375175216 Training loss: 3.960911989212036
2025-12-09 12:11:16.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.002486733861872236 Training loss: 3.950523853302002
2025-12-09 12:11:16.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0024744798831178817 Training loss: 3.793215274810791
2025-12-09 12:11:16.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0024621123294467097 Training loss: 3.8099656105041504
2025-12-09 12:11:16.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.002449632642288045 Training loss: 3.988910675048828
2025-12-09 12:11:16.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002437042276140287 Training loss: 4.0135931968688965
2025-12-09 12:11:16.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0024243426984013913 Training loss: 3.8893444538116455
2025-12-09 12:11:16.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0024115353891978435 Training loss: 3.8520305156707764
2025-12-09 12:11:16.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.002398621841212154 Training loss: 3.970724582672119
2025-12-09 12:11:16.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.002385603559508884 Training loss: 3.7975127696990967
2025-12-09 12:11:16.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.002372482061359234 Training loss: 3.899089813232422
2025-12-09 12:11:16.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0023592588760642046 Training loss: 3.773437261581421
2025-12-09 12:11:16.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00234593554477636 Training loss: 3.7943735122680664
2025-12-09 12:11:16.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.002332513620320205 Training loss: 3.7504444122314453
2025-12-09 12:11:16.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.002318994667011207 Training loss: 3.8254213333129883
2025-12-09 12:11:16.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.002305380260473476 Training loss: 3.8248326778411865
2025-12-09 12:11:16.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002291671987456123 Training loss: 3.908243417739868
2025-12-09 12:11:16.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0022778714456483324 Training loss: 3.812270402908325
2025-12-09 12:11:16.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0022639802434931446 Training loss: 3.933288335800171
2025-12-09 12:11:16.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0022500000000000003 Training loss: 3.696514368057251
2025-12-09 12:11:16.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0022359323445560408 Training loss: 3.818056106567383
2025-12-09 12:11:16.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0022217789167362076 Training loss: 3.905412435531616
2025-12-09 12:11:16.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.002207541366112149 Training loss: 3.8531978130340576
2025-12-09 12:11:16.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0021932213520599654 Training loss: 3.880239963531494
2025-12-09 12:11:16.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0021788205435668085 Training loss: 3.7087652683258057
2025-12-09 12:11:16.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0021643406190363624 Training loss: 3.870549440383911
2025-12-09 12:11:16.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0021497832660932296 Training loss: 3.850036382675171
2025-12-09 12:11:16.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0021351501813862356 Training loss: 3.7770473957061768
2025-12-09 12:11:16.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0021204430703906873 Training loss: 3.900420904159546
2025-12-09 12:11:16.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0021056636472096026 Training loss: 3.7797224521636963
2025-12-09 12:11:16.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002090813634373931 Training loss: 3.928713798522949
2025-12-09 12:11:16.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0020758947626417943 Training loss: 3.7515947818756104
2025-12-09 12:11:16.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.002060908770796769 Training loss: 3.878295660018921
2025-12-09 12:11:16.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0020458574054452315 Training loss: 3.772263526916504
2025-12-09 12:11:16.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.002030742420812791 Training loss: 3.77453875541687
2025-12-09 12:11:16.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0020155655785398397 Training loss: 3.713388204574585
2025-12-09 12:11:16.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.002000328647476231 Training loss: 3.7492496967315674
2025-12-09 12:11:16.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.001985033403475123 Training loss: 3.6796765327453613
2025-12-09 12:11:16.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.001969681629186004 Training loss: 3.948488473892212
2025-12-09 12:11:16.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.001954275113846926 Training loss: 3.8402230739593506
2025-12-09 12:11:16.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0019388156530759713 Training loss: 3.7646565437316895
2025-12-09 12:11:16.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0019233050486619715 Training loss: 3.8859989643096924
2025-12-09 12:11:16.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0019077451083545144 Training loss: 3.8468658924102783
2025-12-09 12:11:16.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0018921376456532484 Training loss: 3.909980058670044
2025-12-09 12:11:16.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0018764844795965232 Training loss: 3.8529136180877686
2025-12-09 12:11:16.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0018607874345493807 Training loss: 3.9437146186828613
2025-12-09 12:11:16.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0018450483399909264 Training loss: 3.833292007446289
2025-12-09 12:11:16.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0018292690303011077 Training loss: 3.975245952606201
2025-12-09 12:11:16.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.001813451344546913 Training loss: 3.8426599502563477
2025-12-09 12:11:16.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0017975971262680348 Training loss: 3.7686123847961426
2025-12-09 12:11:16.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0017817082232620054 Training loss: 3.6021029949188232
2025-12-09 12:11:16.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0017657864873688344 Training loss: 3.68123197555542
2025-12-09 12:11:16.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0017498337742551818 Training loss: 3.728268623352051
2025-12-09 12:11:16.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0017338519431980798 Training loss: 3.7125933170318604
2025-12-09 12:11:16.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0017178428568682357 Training loss: 3.6876635551452637
2025-12-09 12:11:16.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.001701808381112938 Training loss: 3.839083671569824
2025-12-09 12:11:16.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0016857503847385955 Training loss: 3.774883508682251
2025-12-09 12:11:16.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0016696707392929266 Training loss: 3.9227681159973145
2025-12-09 12:11:16.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.001653571318846834 Training loss: 3.741102457046509
2025-12-09 12:11:16.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0016374539997759824 Training loss: 3.611140727996826
2025-12-09 12:11:16.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0016213206605421066 Training loss: 3.7666587829589844
2025-12-09 12:11:16.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.001605173181474081 Training loss: 3.7247698307037354
2025-12-09 12:11:16.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0015890134445487678 Training loss: 3.746859550476074
2025-12-09 12:11:16.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0015728433331716725 Training loss: 3.7799718379974365
2025-12-09 12:11:16.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0015566647319574351 Training loss: 3.8245632648468018
2025-12-09 12:11:16.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0015404795265101807 Training loss: 3.7405476570129395
2025-12-09 12:11:16.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0015242896032037524 Training loss: 3.732273578643799
2025-12-09 12:11:16.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0015080968489618568 Training loss: 3.7700366973876953
2025-12-09 12:11:16.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0014919031510381437 Training loss: 3.65207839012146
2025-12-09 12:11:16.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0014757103967962479 Training loss: 3.6237385272979736
2025-12-09 12:11:16.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0014595204734898198 Training loss: 3.655108690261841
2025-12-09 12:11:16.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0014433352680425654 Training loss: 3.5953290462493896
2025-12-09 12:11:16.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0014271566668283282 Training loss: 3.7952239513397217
2025-12-09 12:11:16.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.001410986555451232 Training loss: 3.734088659286499
2025-12-09 12:11:16.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0013948268185259188 Training loss: 3.70878267288208
2025-12-09 12:11:16.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.001378679339457894 Training loss: 3.6708717346191406
2025-12-09 12:11:16.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0013625460002240181 Training loss: 3.6540708541870117
2025-12-09 12:11:16.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0013464286811531662 Training loss: 3.78922438621521
2025-12-09 12:11:16.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0013303292607070737 Training loss: 3.756970167160034
2025-12-09 12:11:16.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.001314249615261405 Training loss: 3.8160412311553955
2025-12-09 12:11:16.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0012981916188870622 Training loss: 3.6546638011932373
2025-12-09 12:11:16.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.001282157143131765 Training loss: 3.7586829662323
2025-12-09 12:11:16.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00126614805680192 Training loss: 3.564440965652466
2025-12-09 12:11:16.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0012501662257448183 Training loss: 3.6702194213867188
2025-12-09 12:11:16.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.001234213512631166 Training loss: 3.7378716468811035
2025-12-09 12:11:16.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.001218291776737995 Training loss: 3.6862990856170654
2025-12-09 12:11:16.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0012024028737319652 Training loss: 3.8665690422058105
2025-12-09 12:11:16.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0011865486554530874 Training loss: 3.6094510555267334
2025-12-09 12:11:16.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.001170730969698893 Training loss: 3.7939352989196777
2025-12-09 12:11:16.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0011549516600090739 Training loss: 3.7086575031280518
2025-12-09 12:11:16.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00113921256545062 Training loss: 3.6620185375213623
2025-12-09 12:11:16.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0011235155204034769 Training loss: 3.6164095401763916
2025-12-09 12:11:16.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0011078623543467519 Training loss: 3.662523031234741
2025-12-09 12:11:16.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0010922548916454857 Training loss: 3.832707405090332
2025-12-09 12:11:16.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0010766949513380285 Training loss: 3.7165815830230713
2025-12-09 12:11:16.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.001061184346924029 Training loss: 3.8282582759857178
2025-12-09 12:11:16.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0010457248861530741 Training loss: 3.6399621963500977
2025-12-09 12:11:16.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0010303183708139964 Training loss: 3.779167413711548
2025-12-09 12:11:16.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0010149665965248776 Training loss: 3.731172561645508
2025-12-09 12:11:16.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009996713525237694 Training loss: 3.5289666652679443
2025-12-09 12:11:17.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0009844344214601601 Training loss: 3.6931207180023193
2025-12-09 12:11:17.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.000969257579187209 Training loss: 3.632047176361084
2025-12-09 12:11:17.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009541425945547689 Training loss: 3.632629156112671
2025-12-09 12:11:17.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.000939091229203231 Training loss: 3.6704788208007812
2025-12-09 12:11:17.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0009241052373582058 Training loss: 3.807861089706421
2025-12-09 12:11:17.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009091863656260695 Training loss: 3.61881947517395
2025-12-09 12:11:17.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0008943363527903977 Training loss: 3.614280939102173
2025-12-09 12:11:17.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0008795569296093132 Training loss: 3.6600985527038574
2025-12-09 12:11:17.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0008648498186137653 Training loss: 3.725537061691284
2025-12-09 12:11:17.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0008502167339067705 Training loss: 3.5643680095672607
2025-12-09 12:11:17.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0008356593809636371 Training loss: 3.706554412841797
2025-12-09 12:11:17.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0008211794564331918 Training loss: 3.715320348739624
2025-12-09 12:11:17.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0008067786479400346 Training loss: 3.7779767513275146
2025-12-09 12:11:17.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0007924586338878512 Training loss: 3.6570370197296143
2025-12-09 12:11:17.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0007782210832637924 Training loss: 3.639760971069336
2025-12-09 12:11:17.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0007640676554439594 Training loss: 3.590991497039795
2025-12-09 12:11:17.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0007500000000000003 Training loss: 3.603179454803467
2025-12-09 12:11:17.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.000736019756506856 Training loss: 3.6473493576049805
2025-12-09 12:11:17.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.000722128554351668 Training loss: 3.675332546234131
2025-12-09 12:11:17.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0007083280125438767 Training loss: 3.605457305908203
2025-12-09 12:11:17.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0006946197395265243 Training loss: 3.629565715789795
2025-12-09 12:11:17.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0006810053329887928 Training loss: 3.686429023742676
2025-12-09 12:11:17.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0006674863796797954 Training loss: 3.531639814376831
2025-12-09 12:11:17.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0006540644552236401 Training loss: 3.6169493198394775
2025-12-09 12:11:17.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0006407411239357954 Training loss: 3.478607416152954
2025-12-09 12:11:17.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0006275179386407663 Training loss: 3.7347211837768555
2025-12-09 12:11:17.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0006143964404911165 Training loss: 3.551790714263916
2025-12-09 12:11:17.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0006013781587878463 Training loss: 3.6628663539886475
2025-12-09 12:11:17.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0005884646108021563 Training loss: 3.7082033157348633
2025-12-09 12:11:17.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.000575657301598609 Training loss: 3.6681227684020996
2025-12-09 12:11:17.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0005629577238597133 Training loss: 3.7171988487243652
2025-12-09 12:11:17.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0005503673577119553 Training loss: 3.631030797958374
2025-12-09 12:11:17.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0005378876705532904 Training loss: 3.7543561458587646
2025-12-09 12:11:17.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0005255201168821184 Training loss: 3.7378268241882324
2025-12-09 12:11:17.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0005132661381277644 Training loss: 3.526942491531372
2025-12-09 12:11:17.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0005011271624824787 Training loss: 3.7059240341186523
2025-12-09 12:11:17.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0004891046047349837 Training loss: 3.670167922973633
2025-12-09 12:11:17.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00047719986610558236 Training loss: 3.70457124710083
2025-12-09 12:11:17.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00046541433408284357 Training loss: 3.7292397022247314
2025-12-09 12:11:17.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00045374938226189584 Training loss: 3.6711528301239014
2025-12-09 12:11:17.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.00044220637018433165 Training loss: 3.720659017562866
2025-12-09 12:11:17.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00043078664317975653 Training loss: 3.769650936126709
2025-12-09 12:11:17.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0004194915322089899 Training loss: 3.691012144088745
2025-12-09 12:11:17.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00040832235370894606 Training loss: 3.6157045364379883
2025-12-09 12:11:17.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0003972804094391998 Training loss: 3.721829891204834
2025-12-09 12:11:17.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0003863669863302698 Training loss: 3.626802682876587
2025-12-09 12:11:17.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00037558335633362935 Training loss: 3.6349053382873535
2025-12-09 12:11:17.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.000364930776273457 Training loss: 3.6700546741485596
2025-12-09 12:11:17.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0003544104877001596 Training loss: 3.6728365421295166
2025-12-09 12:11:17.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0003440237167456663 Training loss: 3.7096567153930664
2025-12-09 12:11:17.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0003337716739805264 Training loss: 3.6453793048858643
2025-12-09 12:11:17.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00032365555427281634 Training loss: 3.65512752532959
2025-12-09 12:11:17.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00031367653664888173 Training loss: 3.6861917972564697
2025-12-09 12:11:17.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0003038357841559191 Training loss: 3.7061915397644043
2025-12-09 12:11:17.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00029413444372642495 Training loss: 3.5000641345977783
2025-12-09 12:11:17.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.00028457364604452375 Training loss: 3.5359504222869873
2025-12-09 12:11:17.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.00027515450541418343 Training loss: 3.5243031978607178
2025-12-09 12:11:17.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.0002658781196293482 Training loss: 3.695631742477417
2025-12-09 12:11:17.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0002567455698459882 Training loss: 3.811934232711792
2025-12-09 12:11:17.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00024775792045609354 Training loss: 3.5547783374786377
2025-12-09 12:11:17.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.00023891621896361882 Training loss: 3.650632858276367
2025-12-09 12:11:17.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.00023022149586239971 Training loss: 3.685685634613037
2025-12-09 12:11:17.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.00022167476451604625 Training loss: 3.565363883972168
2025-12-09 12:11:17.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.00021327702103983865 Training loss: 3.6627588272094727
2025-12-09 12:11:17.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.00020502924418463014 Training loss: 3.623643159866333
2025-12-09 12:11:17.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0001969323952227733 Training loss: 3.7307283878326416
2025-12-09 12:11:17.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.00018898741783608642 Training loss: 3.743192434310913
2025-12-09 12:11:17.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00018119523800586568 Training loss: 3.584869384765625
2025-12-09 12:11:17.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.00017355676390496484 Training loss: 3.6303141117095947
2025-12-09 12:11:17.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0001660728857919464 Training loss: 3.6914708614349365
2025-12-09 12:11:17.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00015874447590732537 Training loss: 3.6648669242858887
2025-12-09 12:11:17.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0001515723883719072 Training loss: 3.5884857177734375
2025-12-09 12:11:17.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00014455745908724228 Training loss: 3.716402530670166
2025-12-09 12:11:17.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0001377005056382018 Training loss: 3.6422224044799805
2025-12-09 12:11:17.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00013100232719768994 Training loss: 3.4899890422821045
2025-12-09 12:11:17.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00012446370443349862 Training loss: 3.7910196781158447
2025-12-09 12:11:17.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0001180853994173236 Training loss: 3.553525447845459
2025-12-09 12:11:17.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.00011186815553594382 Training loss: 3.667499303817749
2025-12-09 12:11:17.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0001058126974045811 Training loss: 3.6726555824279785
2025-12-09 12:11:17.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.991973078244637e-05 Training loss: 3.667052984237671
2025-12-09 12:11:17.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.418994249048473e-05 Training loss: 3.6212940216064453
2025-12-09 12:11:17.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 8.862400033132573e-05 Training loss: 3.525625467300415
2025-12-09 12:11:17.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 8.322255301145204e-05 Training loss: 3.6613752841949463
2025-12-09 12:11:17.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 7.798623006559435e-05 Training loss: 3.64807391166687
2025-12-09 12:11:17.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 7.291564178335719e-05 Training loss: 3.6626946926116943
2025-12-09 12:11:17.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 6.801137913809213e-05 Training loss: 3.759507417678833
2025-12-09 12:11:17.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 6.327401371801944e-05 Training loss: 3.6763875484466553
2025-12-09 12:11:17.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 5.870409765960966e-05 Training loss: 3.517735004425049
2025-12-09 12:11:17.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 5.430216358323309e-05 Training loss: 3.740917444229126
2025-12-09 12:11:17.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 5.00687245310833e-05 Training loss: 3.691002368927002
2025-12-09 12:11:17.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 4.60042739073816e-05 Training loss: 3.753091812133789
2025-12-09 12:11:17.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 4.2109285420872056e-05 Training loss: 3.7970335483551025
2025-12-09 12:11:17.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 3.8384213029610984e-05 Training loss: 3.6770973205566406
2025-12-09 12:11:17.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 3.4829490888057425e-05 Training loss: 3.786064624786377
2025-12-09 12:11:17.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 3.144553329647448e-05 Training loss: 3.731086254119873
2025-12-09 12:11:17.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 2.823273465264142e-05 Training loss: 3.7673516273498535
2025-12-09 12:11:17.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 2.5191469405887624e-05 Training loss: 3.8123297691345215
2025-12-09 12:11:17.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 2.2322092013450313e-05 Training loss: 3.526984214782715
2025-12-09 12:11:17.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 1.9624936899163947e-05 Training loss: 3.717155694961548
2025-12-09 12:11:17.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 1.7100318414482063e-05 Training loss: 3.8006625175476074
2025-12-09 12:11:17.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 1.4748530801840077e-05 Training loss: 3.5948002338409424
2025-12-09 12:11:17.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 1.2569848160362384e-05 Training loss: 3.5763020515441895
2025-12-09 12:11:17.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 1.0564524413915422e-05 Training loss: 3.5917298793792725
2025-12-09 12:11:17.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 8.732793281513663e-06 Training loss: 3.6813905239105225
2025-12-09 12:11:17.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 7.07486825007908e-06 Training loss: 3.664419651031494
2025-12-09 12:11:17.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 5.590942549560052e-06 Training loss: 3.6167490482330322
2025-12-09 12:11:17.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 4.281189130410535e-06 Training loss: 3.5333473682403564
2025-12-09 12:11:17.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 3.145760643432527e-06 Training loss: 3.5475738048553467
2025-12-09 12:11:17.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 2.184789421984634e-06 Training loss: 3.6816697120666504
2025-12-09 12:11:17.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 1.3983874665589036e-06 Training loss: 3.5150558948516846
2025-12-09 12:11:17.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 7.866464317276001e-07 Training loss: 3.660283088684082
2025-12-09 12:11:17.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 3.496376154604186e-07 Training loss: 3.6694021224975586
2025-12-09 12:11:17.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 8.741195081479747e-08 Training loss: 3.647266387939453
2025-12-09 12:11:18.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 3.6059932708740234
