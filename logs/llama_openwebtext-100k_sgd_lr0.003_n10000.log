2025-12-09 13:04:28.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 12.163979530334473
2025-12-09 13:04:28.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 12.187880516052246
2025-12-09 13:04:29.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 12.197481155395508
2025-12-09 13:04:29.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 12.192538261413574
2025-12-09 13:04:30.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 12.158312797546387
2025-12-09 13:04:30.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 12.183597564697266
2025-12-09 13:04:30.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 12.242986679077148
2025-12-09 13:04:31.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 12.140340805053711
2025-12-09 13:04:31.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 12.123865127563477
2025-12-09 13:04:31.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 12.141407012939453
2025-12-09 13:04:32.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 12.12791633605957
2025-12-09 13:04:32.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 12.158493041992188
2025-12-09 13:04:33.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 12.15687370300293
2025-12-09 13:04:33.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 12.133540153503418
2025-12-09 13:04:33.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 12.06472396850586
2025-12-09 13:04:34.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 12.127336502075195
2025-12-09 13:04:34.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 12.098105430603027
2025-12-09 13:04:34.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 12.104207038879395
2025-12-09 13:04:35.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 11.969590187072754
2025-12-09 13:04:35.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 12.01125717163086
2025-12-09 13:04:36.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 12.064716339111328
2025-12-09 13:04:36.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 11.90658950805664
2025-12-09 13:04:36.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 11.972935676574707
2025-12-09 13:04:37.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 11.906488418579102
2025-12-09 13:04:37.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 11.819367408752441
2025-12-09 13:04:37.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 11.82531452178955
2025-12-09 13:04:38.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 11.712775230407715
2025-12-09 13:04:38.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 11.538509368896484
2025-12-09 13:04:39.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 11.772583961486816
2025-12-09 13:04:39.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 11.324129104614258
2025-12-09 13:04:39.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 11.619688987731934
2025-12-09 13:04:40.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 11.256367683410645
2025-12-09 13:04:40.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 11.18386173248291
2025-12-09 13:04:40.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 10.844184875488281
2025-12-09 13:04:41.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 11.35335922241211
2025-12-09 13:04:41.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 10.721540451049805
2025-12-09 13:04:41.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 10.723678588867188
2025-12-09 13:04:42.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 10.806222915649414
2025-12-09 13:04:42.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 10.9032564163208
2025-12-09 13:04:43.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 10.29613971710205
2025-12-09 13:04:43.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 10.465530395507812
2025-12-09 13:04:43.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 10.581889152526855
2025-12-09 13:04:44.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 10.137469291687012
2025-12-09 13:04:44.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 10.076581001281738
2025-12-09 13:04:44.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 10.292071342468262
2025-12-09 13:04:45.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 9.92024040222168
2025-12-09 13:04:45.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 10.176229476928711
2025-12-09 13:04:46.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 10.199204444885254
2025-12-09 13:04:46.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 10.003252029418945
2025-12-09 13:04:46.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 10.032995223999023
2025-12-09 13:04:47.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 10.026782989501953
2025-12-09 13:04:47.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 10.020535469055176
2025-12-09 13:04:47.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 9.888640403747559
2025-12-09 13:04:48.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 9.584131240844727
2025-12-09 13:04:48.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 10.541848182678223
2025-12-09 13:04:49.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 9.836770057678223
2025-12-09 13:04:49.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 9.702540397644043
2025-12-09 13:04:49.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 9.863363265991211
2025-12-09 13:04:50.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 9.664637565612793
2025-12-09 13:04:50.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 9.542978286743164
2025-12-09 13:04:50.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 9.583080291748047
2025-12-09 13:04:51.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 9.578849792480469
2025-12-09 13:04:51.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 9.973373413085938
2025-12-09 13:04:52.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 9.79847526550293
2025-12-09 13:04:52.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 9.603888511657715
2025-12-09 13:04:52.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 9.375127792358398
2025-12-09 13:04:53.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 9.582247734069824
2025-12-09 13:04:53.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 9.478677749633789
2025-12-09 13:04:53.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 9.370415687561035
2025-12-09 13:04:54.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 9.338805198669434
2025-12-09 13:04:54.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 9.136085510253906
2025-12-09 13:04:54.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 9.720562934875488
2025-12-09 13:04:55.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 9.479307174682617
2025-12-09 13:04:55.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 9.296883583068848
2025-12-09 13:04:56.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 9.578920364379883
2025-12-09 13:04:56.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 9.212071418762207
2025-12-09 13:04:56.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 9.455487251281738
2025-12-09 13:04:57.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 9.378890991210938
2025-12-09 13:04:57.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 9.423171997070312
2025-12-09 13:04:57.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 9.19096565246582
2025-12-09 13:04:58.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 9.394083976745605
2025-12-09 13:04:58.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 9.272732734680176
2025-12-09 13:04:59.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 9.206514358520508
2025-12-09 13:04:59.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 9.094009399414062
2025-12-09 13:04:59.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 9.232072830200195
2025-12-09 13:05:00.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 9.244791984558105
2025-12-09 13:05:00.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 10.210721015930176
2025-12-09 13:05:00.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 9.478326797485352
2025-12-09 13:05:01.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 9.231222152709961
2025-12-09 13:05:01.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 9.198280334472656
2025-12-09 13:05:02.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 9.04789924621582
2025-12-09 13:05:02.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 9.097054481506348
2025-12-09 13:05:02.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 9.20063591003418
2025-12-09 13:05:03.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 9.328718185424805
2025-12-09 13:05:03.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 9.054551124572754
2025-12-09 13:05:03.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 9.218976974487305
2025-12-09 13:05:04.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 9.033333778381348
2025-12-09 13:05:04.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 9.105785369873047
2025-12-09 13:05:05.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 9.12907600402832
2025-12-09 13:05:05.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 9.071802139282227
2025-12-09 13:05:05.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999997089396424 Training loss: 9.212545394897461
2025-12-09 13:05:06.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002999998835758683 Training loss: 8.888249397277832
2025-12-09 13:05:06.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029999973804574606 Training loss: 9.123733520507812
2025-12-09 13:05:06.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0029999953430365394 Training loss: 9.005468368530273
2025-12-09 13:05:07.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.002999992723496711 Training loss: 9.03601360321045
2025-12-09 13:05:07.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.002999989521838991 Training loss: 9.147802352905273
2025-12-09 13:05:07.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029999857380646226 Training loss: 8.822027206420898
2025-12-09 13:05:08.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.002999981372175074 Training loss: 9.229202270507812
2025-12-09 13:05:08.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0029999764241720396 Training loss: 9.033212661743164
2025-12-09 13:05:09.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0029999708940574394 Training loss: 9.230890274047852
2025-12-09 13:05:09.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0029999647818334195 Training loss: 8.722346305847168
2025-12-09 13:05:09.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.002999958087502352 Training loss: 9.025964736938477
2025-12-09 13:05:10.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029999508110668356 Training loss: 9.147625923156738
2025-12-09 13:05:10.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029999429525296934 Training loss: 9.235187530517578
2025-12-09 13:05:10.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0029999345118939752 Training loss: 8.974577903747559
2025-12-09 13:05:11.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0029999254891629563 Training loss: 8.951959609985352
2025-12-09 13:05:11.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.002999915884340139 Training loss: 8.881974220275879
2025-12-09 13:05:12.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0029999056974292504 Training loss: 9.076031684875488
2025-12-09 13:05:12.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029998949284342435 Training loss: 9.049019813537598
2025-12-09 13:05:12.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.002999883577359298 Training loss: 8.9442777633667
2025-12-09 13:05:13.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.002999871644208819 Training loss: 9.138118743896484
2025-12-09 13:05:13.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002999859128987437 Training loss: 9.011968612670898
2025-12-09 13:05:13.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00299984603170001 Training loss: 9.166598320007324
2025-12-09 13:05:14.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0029998323523516197 Training loss: 8.844850540161133
2025-12-09 13:05:14.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029998180909475754 Training loss: 8.812294960021973
2025-12-09 13:05:15.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.002999803247493411 Training loss: 9.132085800170898
2025-12-09 13:05:15.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.002999787821994888 Training loss: 8.8753662109375
2025-12-09 13:05:15.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0029997718144579915 Training loss: 9.183545112609863
2025-12-09 13:05:16.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0029997552248889354 Training loss: 8.715680122375488
2025-12-09 13:05:16.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.002999738053294156 Training loss: 9.530058860778809
2025-12-09 13:05:16.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0029997202996803183 Training loss: 8.8114595413208
2025-12-09 13:05:17.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002999701964054312 Training loss: 9.358261108398438
2025-12-09 13:05:17.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0029996830464232523 Training loss: 8.915806770324707
2025-12-09 13:05:18.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0029996635467944813 Training loss: 9.232075691223145
2025-12-09 13:05:18.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0029996434651755662 Training loss: 8.863179206848145
2025-12-09 13:05:18.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0029996228015743004 Training loss: 8.827253341674805
2025-12-09 13:05:19.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.002999601555998703 Training loss: 9.039045333862305
2025-12-09 13:05:19.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.002999579728457019 Training loss: 8.85445785522461
2025-12-09 13:05:19.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.002999557318957719 Training loss: 8.903572082519531
2025-12-09 13:05:20.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0029995343275095003 Training loss: 8.950624465942383
2025-12-09 13:05:20.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0029995107541212845 Training loss: 8.984630584716797
2025-12-09 13:05:20.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.002999486598802221 Training loss: 8.979615211486816
2025-12-09 13:05:21.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0029994618615616832 Training loss: 9.102749824523926
2025-12-09 13:05:21.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.002999436542409272 Training loss: 8.867372512817383
2025-12-09 13:05:22.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0029994106413548122 Training loss: 8.87325668334961
2025-12-09 13:05:22.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0029993841584083558 Training loss: 8.708415031433105
2025-12-09 13:05:22.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.002999357093580181 Training loss: 8.65829849243164
2025-12-09 13:05:23.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0029993294468807904 Training loss: 9.013705253601074
2025-12-09 13:05:23.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0029993012183209137 Training loss: 8.977921485900879
2025-12-09 13:05:23.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0029992724079115052 Training loss: 9.010900497436523
2025-12-09 13:05:24.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.002999243015663746 Training loss: 8.781085014343262
2025-12-09 13:05:24.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0029992130415890427 Training loss: 8.872142791748047
2025-12-09 13:05:25.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.002999182485699028 Training loss: 8.9130859375
2025-12-09 13:05:25.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0029991513480055595 Training loss: 8.966861724853516
2025-12-09 13:05:25.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002999119628520721 Training loss: 9.50719165802002
2025-12-09 13:05:26.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0029990873272568224 Training loss: 9.005294799804688
2025-12-09 13:05:26.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0029990544442264 Training loss: 8.955747604370117
2025-12-09 13:05:26.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.002999020979442214 Training loss: 9.077376365661621
2025-12-09 13:05:27.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.002998986932917252 Training loss: 9.007561683654785
2025-12-09 13:05:27.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.002998952304664726 Training loss: 8.786760330200195
2025-12-09 13:05:28.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.002998917094698076 Training loss: 8.780332565307617
2025-12-09 13:05:28.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.002998881303030965 Training loss: 8.626286506652832
2025-12-09 13:05:28.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0029988449296772836 Training loss: 8.782966613769531
2025-12-09 13:05:29.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.002998807974651147 Training loss: 8.759478569030762
2025-12-09 13:05:29.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0029987704379668976 Training loss: 8.983901023864746
2025-12-09 13:05:29.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002998732319639102 Training loss: 8.714378356933594
2025-12-09 13:05:30.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0029986936196825537 Training loss: 8.81999397277832
2025-12-09 13:05:30.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.002998654338112271 Training loss: 8.898782730102539
2025-12-09 13:05:30.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0029986144749434987 Training loss: 9.342625617980957
2025-12-09 13:05:31.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0029985740301917063 Training loss: 9.009204864501953
2025-12-09 13:05:31.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00299853300387259 Training loss: 8.847990036010742
2025-12-09 13:05:32.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0029984913960020712 Training loss: 9.020313262939453
2025-12-09 13:05:32.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0029984492065962976 Training loss: 8.530567169189453
2025-12-09 13:05:32.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0029984064356716415 Training loss: 8.886796951293945
2025-12-09 13:05:33.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0029983630832447015 Training loss: 9.233400344848633
2025-12-09 13:05:33.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002998319149332302 Training loss: 8.944657325744629
2025-12-09 13:05:33.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0029982746339514933 Training loss: 8.680294036865234
2025-12-09 13:05:34.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0029982295371195496 Training loss: 8.78189754486084
2025-12-09 13:05:34.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.002998183858853974 Training loss: 8.879965782165527
2025-12-09 13:05:35.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0029981375991724917 Training loss: 8.910370826721191
2025-12-09 13:05:35.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0029980907580930563 Training loss: 8.94847297668457
2025-12-09 13:05:35.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.002998043335633845 Training loss: 8.89465618133545
2025-12-09 13:05:36.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002997995331813262 Training loss: 8.971650123596191
2025-12-09 13:05:36.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.002997946746649937 Training loss: 9.241476058959961
2025-12-09 13:05:36.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0029978975801627245 Training loss: 8.834799766540527
2025-12-09 13:05:37.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0029978478323707046 Training loss: 8.793237686157227
2025-12-09 13:05:37.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0029977975032931844 Training loss: 9.75260066986084
2025-12-09 13:05:38.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.002997746592949695 Training loss: 8.817691802978516
2025-12-09 13:05:38.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.002997695101359994 Training loss: 8.723430633544922
2025-12-09 13:05:38.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.002997643028544064 Training loss: 8.867053985595703
2025-12-09 13:05:39.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0029975903745221143 Training loss: 8.848877906799316
2025-12-09 13:05:39.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.002997537139314578 Training loss: 9.375770568847656
2025-12-09 13:05:39.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0029974833229421145 Training loss: 9.093452453613281
2025-12-09 13:05:40.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002997428925425609 Training loss: 8.966575622558594
2025-12-09 13:05:40.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0029973739467861736 Training loss: 9.188523292541504
2025-12-09 13:05:41.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.002997318387045142 Training loss: 9.423455238342285
2025-12-09 13:05:41.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0029972622462240777 Training loss: 8.663501739501953
2025-12-09 13:05:41.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0029972055243447666 Training loss: 8.809893608093262
2025-12-09 13:05:42.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.002997148221429223 Training loss: 8.991144180297852
2025-12-09 13:05:42.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.002997090337499683 Training loss: 9.201077461242676
2025-12-09 13:05:42.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0029970318725786116 Training loss: 9.175832748413086
2025-12-09 13:05:43.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0029969728266886976 Training loss: 9.080050468444824
2025-12-09 13:05:43.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0029969131998528555 Training loss: 8.771710395812988
2025-12-09 13:05:43.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0029968529920942253 Training loss: 8.59328556060791
2025-12-09 13:05:44.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0029967922034361727 Training loss: 8.823962211608887
2025-12-09 13:05:44.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.002996730833902288 Training loss: 8.804986000061035
2025-12-09 13:05:45.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.002996668883516388 Training loss: 8.956547737121582
2025-12-09 13:05:45.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002996606352302514 Training loss: 8.864544868469238
2025-12-09 13:05:45.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.002996543240284934 Training loss: 8.970370292663574
2025-12-09 13:05:46.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0029964795474881397 Training loss: 8.702701568603516
2025-12-09 13:05:46.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.002996415273936849 Training loss: 8.979013442993164
2025-12-09 13:05:46.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.002996350419656006 Training loss: 8.707541465759277
2025-12-09 13:05:47.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0029962849846707786 Training loss: 8.840742111206055
2025-12-09 13:05:47.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0029962189690065613 Training loss: 8.887533187866211
2025-12-09 13:05:48.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.0029961523726889736 Training loss: 8.694497108459473
2025-12-09 13:05:48.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0029960851957438594 Training loss: 8.774003028869629
2025-12-09 13:05:48.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.00299601743819729 Training loss: 9.262372970581055
2025-12-09 13:05:49.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0029959491000755597 Training loss: 8.94626235961914
2025-12-09 13:05:49.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0029958801814051897 Training loss: 9.649081230163574
2025-12-09 13:05:49.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.002995810682212926 Training loss: 8.654716491699219
2025-12-09 13:05:50.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0029957406025257396 Training loss: 8.900941848754883
2025-12-09 13:05:50.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.002995669942370827 Training loss: 8.654862403869629
2025-12-09 13:05:51.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0029955987017756106 Training loss: 8.94970989227295
2025-12-09 13:05:51.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0029955268807677375 Training loss: 9.372591018676758
2025-12-09 13:05:51.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.002995454479375079 Training loss: 9.402376174926758
2025-12-09 13:05:52.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0029953814976257337 Training loss: 8.592668533325195
2025-12-09 13:05:52.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.002995307935548024 Training loss: 9.206732749938965
2025-12-09 13:05:52.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.002995233793170498 Training loss: 8.711031913757324
2025-12-09 13:05:53.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0029951590705219284 Training loss: 8.96481990814209
2025-12-09 13:05:53.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0029950837676313144 Training loss: 8.679619789123535
2025-12-09 13:05:54.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0029950078845278794 Training loss: 9.033087730407715
2025-12-09 13:05:54.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0029949314212410717 Training loss: 8.729411125183105
2025-12-09 13:05:54.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0029948543778005655 Training loss: 8.863478660583496
2025-12-09 13:05:55.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00299477675423626 Training loss: 9.193373680114746
2025-12-09 13:05:55.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.002994698550578279 Training loss: 8.661051750183105
2025-12-09 13:05:55.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0029946197668569725 Training loss: 8.595980644226074
2025-12-09 13:05:56.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.002994540403102914 Training loss: 8.716485977172852
2025-12-09 13:05:56.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0029944604593469034 Training loss: 8.787408828735352
2025-12-09 13:05:56.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0029943799356199658 Training loss: 8.881027221679688
2025-12-09 13:05:57.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0029942988319533507 Training loss: 8.867186546325684
2025-12-09 13:05:57.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.002994217148378532 Training loss: 9.190165519714355
2025-12-09 13:05:58.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.002994134884927211 Training loss: 9.25216293334961
2025-12-09 13:05:58.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.002994052041631311 Training loss: 8.89947509765625
2025-12-09 13:05:58.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0029939686185229825 Training loss: 8.707476615905762
2025-12-09 13:05:59.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0029938846156346006 Training loss: 8.949732780456543
2025-12-09 13:05:59.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.002993800032998765 Training loss: 8.82975959777832
2025-12-09 13:05:59.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.002993714870648301 Training loss: 9.0465669631958
2025-12-09 13:06:00.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0029936291286162577 Training loss: 9.139333724975586
2025-12-09 13:06:00.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00299354280693591 Training loss: 9.107725143432617
2025-12-09 13:06:01.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.002993455905640758 Training loss: 8.981416702270508
2025-12-09 13:06:01.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0029933684247645267 Training loss: 9.011295318603516
2025-12-09 13:06:01.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.002993280364341165 Training loss: 8.797030448913574
2025-12-09 13:06:02.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.002993191724404848 Training loss: 8.932859420776367
2025-12-09 13:06:02.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0029931025049899744 Training loss: 8.805756568908691
2025-12-09 13:06:02.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.002993012706131169 Training loss: 9.111680030822754
2025-12-09 13:06:03.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.002992922327863281 Training loss: 8.933418273925781
2025-12-09 13:06:03.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.002992831370221385 Training loss: 8.863744735717773
2025-12-09 13:06:04.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.002992739833240779 Training loss: 9.20535945892334
2025-12-09 13:06:04.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0029926477169569866 Training loss: 8.938755989074707
2025-12-09 13:06:04.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0029925550214057565 Training loss: 9.00355052947998
2025-12-09 13:06:05.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.002992461746623063 Training loss: 9.324723243713379
2025-12-09 13:06:05.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0029923678926451034 Training loss: 8.620735168457031
2025-12-09 13:06:05.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.0029922734595083005 Training loss: 9.128188133239746
2025-12-09 13:06:06.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.002992178447249302 Training loss: 8.849163055419922
2025-12-09 13:06:06.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0029920828559049806 Training loss: 8.69640827178955
2025-12-09 13:06:07.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0029919866855124336 Training loss: 8.877288818359375
2025-12-09 13:06:07.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0029918899361089826 Training loss: 8.815183639526367
2025-12-09 13:06:07.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0029917926077321732 Training loss: 8.670852661132812
2025-12-09 13:06:08.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.002991694700419778 Training loss: 8.8988618850708
2025-12-09 13:06:08.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.002991596214209793 Training loss: 8.814602851867676
2025-12-09 13:06:08.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0029914971491404375 Training loss: 9.315323829650879
2025-12-09 13:06:09.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0029913975052501575 Training loss: 8.828681945800781
2025-12-09 13:06:09.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.002991297282577623 Training loss: 8.974746704101562
2025-12-09 13:06:09.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0029911964811617287 Training loss: 8.946521759033203
2025-12-09 13:06:10.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0029910951010415927 Training loss: 9.178020477294922
2025-12-09 13:06:10.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0029909931422565593 Training loss: 8.492457389831543
2025-12-09 13:06:11.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0029908906048461965 Training loss: 9.260757446289062
2025-12-09 13:06:11.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.002990787488850297 Training loss: 8.985660552978516
2025-12-09 13:06:11.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0029906837943088787 Training loss: 8.761630058288574
2025-12-09 13:06:12.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0029905795212621824 Training loss: 8.969803810119629
2025-12-09 13:06:12.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.002990474669750676 Training loss: 8.887656211853027
2025-12-09 13:06:12.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.002990369239815048 Training loss: 9.203179359436035
2025-12-09 13:06:13.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.002990263231496216 Training loss: 8.892528533935547
2025-12-09 13:06:13.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029901566448353183 Training loss: 8.86751937866211
2025-12-09 13:06:14.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029900494798737196 Training loss: 8.917498588562012
2025-12-09 13:06:14.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002989941736653009 Training loss: 8.941184043884277
2025-12-09 13:06:14.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.002989833415214999 Training loss: 8.975016593933105
2025-12-09 13:06:15.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0029897245156017267 Training loss: 9.161107063293457
2025-12-09 13:06:15.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.002989615037855454 Training loss: 9.115171432495117
2025-12-09 13:06:15.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0029895049820186682 Training loss: 8.987137794494629
2025-12-09 13:06:16.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0029893943481340787 Training loss: 9.495384216308594
2025-12-09 13:06:16.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0029892831362446203 Training loss: 8.961274147033691
2025-12-09 13:06:17.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002989171346393453 Training loss: 8.829386711120605
2025-12-09 13:06:17.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00298905897862396 Training loss: 8.875685691833496
2025-12-09 13:06:17.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0029889460329797484 Training loss: 9.045869827270508
2025-12-09 13:06:18.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0029888325095046506 Training loss: 8.89115047454834
2025-12-09 13:06:18.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0029887184082427226 Training loss: 8.910202980041504
2025-12-09 13:06:18.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002988603729238246 Training loss: 8.929847717285156
2025-12-09 13:06:19.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0029884884725357237 Training loss: 9.128677368164062
2025-12-09 13:06:19.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0029883726381798865 Training loss: 8.948111534118652
2025-12-09 13:06:20.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0029882562262156854 Training loss: 9.048996925354004
2025-12-09 13:06:20.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.002988139236688299 Training loss: 8.968341827392578
2025-12-09 13:06:20.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0029880216696431287 Training loss: 9.263120651245117
2025-12-09 13:06:21.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0029879035251257993 Training loss: 9.001022338867188
2025-12-09 13:06:21.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.002987784803182161 Training loss: 8.946195602416992
2025-12-09 13:06:21.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0029876655038582863 Training loss: 9.155648231506348
2025-12-09 13:06:22.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0029875456272004746 Training loss: 9.32477855682373
2025-12-09 13:06:22.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0029874251732552462 Training loss: 9.244153022766113
2025-12-09 13:06:23.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.002987304142069348 Training loss: 9.200175285339355
2025-12-09 13:06:23.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.002987182533689749 Training loss: 8.719306945800781
2025-12-09 13:06:23.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0029870603481636443 Training loss: 8.931982040405273
2025-12-09 13:06:24.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0029869375855384505 Training loss: 9.15953254699707
2025-12-09 13:06:24.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0029868142458618096 Training loss: 9.001313209533691
2025-12-09 13:06:24.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0029866903291815875 Training loss: 8.985647201538086
2025-12-09 13:06:25.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.002986565835545874 Training loss: 8.889851570129395
2025-12-09 13:06:25.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0029864407650029823 Training loss: 9.018586158752441
2025-12-09 13:06:25.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00298631511760145 Training loss: 8.998501777648926
2025-12-09 13:06:26.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0029861888933900385 Training loss: 9.50135326385498
2025-12-09 13:06:26.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.002986062092417733 Training loss: 8.843461036682129
2025-12-09 13:06:27.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0029859347147337422 Training loss: 9.11274528503418
2025-12-09 13:06:27.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.002985806760387499 Training loss: 8.92713451385498
2025-12-09 13:06:27.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00298567822942866 Training loss: 8.67066478729248
2025-12-09 13:06:28.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0029855491219071056 Training loss: 9.010038375854492
2025-12-09 13:06:28.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00298541943787294 Training loss: 8.90786361694336
2025-12-09 13:06:28.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.002985289177376491 Training loss: 8.69865894317627
2025-12-09 13:06:29.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00298515834046831 Training loss: 9.029173851013184
2025-12-09 13:06:29.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.002985026927199172 Training loss: 9.043197631835938
2025-12-09 13:06:30.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0029848949376200767 Training loss: 9.000611305236816
2025-12-09 13:06:30.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0029847623717822462 Training loss: 8.838428497314453
2025-12-09 13:06:30.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0029846292297371264 Training loss: 8.941943168640137
2025-12-09 13:06:31.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.002984495511536388 Training loss: 9.332212448120117
2025-12-09 13:06:31.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0029843612172319235 Training loss: 9.05852222442627
2025-12-09 13:06:31.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.002984226346875851 Training loss: 9.320684432983398
2025-12-09 13:06:32.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0029840909005205093 Training loss: 9.097452163696289
2025-12-09 13:06:32.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.002983954878218464 Training loss: 9.418290138244629
2025-12-09 13:06:33.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.002983818280022502 Training loss: 9.337719917297363
2025-12-09 13:06:33.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0029836811059856354 Training loss: 9.073234558105469
2025-12-09 13:06:33.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0029835433561610975 Training loss: 8.895039558410645
2025-12-09 13:06:34.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0029834050306023468 Training loss: 8.970104217529297
2025-12-09 13:06:34.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0029832661293630646 Training loss: 9.048867225646973
2025-12-09 13:06:34.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.002983126652497156 Training loss: 9.036322593688965
2025-12-09 13:06:35.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.002982986600058749 Training loss: 9.050178527832031
2025-12-09 13:06:35.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0029828459721021965 Training loss: 9.309608459472656
2025-12-09 13:06:36.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.002982704768682071 Training loss: 8.89785385131836
2025-12-09 13:06:36.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0029825629898531728 Training loss: 9.44837760925293
2025-12-09 13:06:36.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.002982420635670523 Training loss: 8.806037902832031
2025-12-09 13:06:37.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.002982277706189366 Training loss: 9.19090747833252
2025-12-09 13:06:37.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00298213420146517 Training loss: 8.958671569824219
2025-12-09 13:06:37.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0029819901215536273 Training loss: 9.050148963928223
2025-12-09 13:06:38.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.002981845466510651 Training loss: 8.929128646850586
2025-12-09 13:06:38.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0029817002363923804 Training loss: 9.012980461120605
2025-12-09 13:06:38.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.002981554431255176 Training loss: 9.035378456115723
2025-12-09 13:06:39.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.002981408051155621 Training loss: 8.990767478942871
2025-12-09 13:06:39.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.002981261096150524 Training loss: 9.150976181030273
2025-12-09 13:06:40.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.002981113566296915 Training loss: 8.857691764831543
2025-12-09 13:06:40.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0029809654616520464 Training loss: 8.742859840393066
2025-12-09 13:06:40.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.0029808167822733956 Training loss: 8.941946983337402
2025-12-09 13:06:41.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0029806675282186626 Training loss: 9.03020191192627
2025-12-09 13:06:41.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.002980517699545769 Training loss: 8.924108505249023
2025-12-09 13:06:41.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0029803672963128617 Training loss: 9.200996398925781
2025-12-09 13:06:42.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0029802163185783073 Training loss: 9.008383750915527
2025-12-09 13:06:42.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0029800647664006996 Training loss: 9.210173606872559
2025-12-09 13:06:43.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.002979912639838851 Training loss: 9.291472434997559
2025-12-09 13:06:43.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0029797599389518002 Training loss: 8.961036682128906
2025-12-09 13:06:43.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0029796066637988072 Training loss: 8.910042762756348
2025-12-09 13:06:44.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.002979452814439354 Training loss: 9.268877029418945
2025-12-09 13:06:44.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0029792983909331487 Training loss: 9.256489753723145
2025-12-09 13:06:44.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0029791433933401175 Training loss: 9.224494934082031
2025-12-09 13:06:45.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0029789878217204137 Training loss: 8.729438781738281
2025-12-09 13:06:45.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0029788316761344114 Training loss: 8.97311019897461
2025-12-09 13:06:46.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0029786749566427066 Training loss: 9.481467247009277
2025-12-09 13:06:46.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00297851766330612 Training loss: 9.224102973937988
2025-12-09 13:06:46.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.002978359796185695 Training loss: 8.923099517822266
2025-12-09 13:06:47.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0029782013553426943 Training loss: 9.119357109069824
2025-12-09 13:06:47.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0029780423408386075 Training loss: 8.959244728088379
2025-12-09 13:06:47.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0029778827527351445 Training loss: 8.898934364318848
2025-12-09 13:06:48.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0029777225910942386 Training loss: 9.194144248962402
2025-12-09 13:06:48.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.002977561855978045 Training loss: 9.146998405456543
2025-12-09 13:06:49.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.002977400547448942 Training loss: 9.08555793762207
2025-12-09 13:06:49.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0029772386655695306 Training loss: 8.801037788391113
2025-12-09 13:06:49.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0029770762104026336 Training loss: 9.121355056762695
2025-12-09 13:06:50.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.002976913182011297 Training loss: 9.019593238830566
2025-12-09 13:06:50.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0029767495804587886 Training loss: 9.12291431427002
2025-12-09 13:06:50.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.002976585405808599 Training loss: 9.110084533691406
2025-12-09 13:06:51.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0029764206581244412 Training loss: 8.971714973449707
2025-12-09 13:06:51.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.002976255337470251 Training loss: 9.172344207763672
2025-12-09 13:06:51.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.002976089443910186 Training loss: 9.301168441772461
2025-12-09 13:06:52.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0029759229775086255 Training loss: 9.632884979248047
2025-12-09 13:06:52.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0029757559383301727 Training loss: 9.61117172241211
2025-12-09 13:06:53.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.0029755883264396517 Training loss: 9.042818069458008
2025-12-09 13:06:53.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00297542014190211 Training loss: 9.047318458557129
2025-12-09 13:06:53.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0029752513847828162 Training loss: 9.207773208618164
2025-12-09 13:06:54.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0029750820551472617 Training loss: 8.945716857910156
2025-12-09 13:06:54.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0029749121530611602 Training loss: 9.138672828674316
2025-12-09 13:06:54.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0029747416785904472 Training loss: 9.362045288085938
2025-12-09 13:06:55.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.002974570631801281 Training loss: 8.991950035095215
2025-12-09 13:06:55.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0029743990127600413 Training loss: 8.848807334899902
2025-12-09 13:06:56.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.002974226821533329 Training loss: 9.114556312561035
2025-12-09 13:06:56.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0029740540581879703 Training loss: 8.970256805419922
2025-12-09 13:06:56.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0029738807227910093 Training loss: 8.929178237915039
2025-12-09 13:06:57.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.002973706815409715 Training loss: 8.70960807800293
2025-12-09 13:06:57.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0029735323361115775 Training loss: 9.177693367004395
2025-12-09 13:06:57.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.002973357284964309 Training loss: 9.048670768737793
2025-12-09 13:06:58.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0029731816620358425 Training loss: 9.151190757751465
2025-12-09 13:06:58.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.002973005467394334 Training loss: 9.17270278930664
2025-12-09 13:06:59.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0029728287011081627 Training loss: 9.519801139831543
2025-12-09 13:06:59.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.002972651363245927 Training loss: 9.10861873626709
2025-12-09 13:06:59.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.002972473453876448 Training loss: 9.07662296295166
2025-12-09 13:07:00.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0029722949730687687 Training loss: 9.048737525939941
2025-12-09 13:07:00.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0029721159208921546 Training loss: 9.134045600891113
2025-12-09 13:07:00.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.0029719362974160927 Training loss: 9.167612075805664
2025-12-09 13:07:01.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0029717561027102907 Training loss: 9.137699127197266
2025-12-09 13:07:01.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.002971575336844679 Training loss: 9.109286308288574
2025-12-09 13:07:02.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.002971393999889409 Training loss: 9.455278396606445
2025-12-09 13:07:02.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.002971212091914854 Training loss: 9.373306274414062
2025-12-09 13:07:02.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0029710296129916093 Training loss: 9.793752670288086
2025-12-09 13:07:03.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0029708465631904913 Training loss: 9.184578895568848
2025-12-09 13:07:03.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.002970662942582538 Training loss: 9.237112045288086
2025-12-09 13:07:03.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.002970478751239009 Training loss: 9.122115135192871
2025-12-09 13:07:04.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0029702939892313853 Training loss: 9.431632995605469
2025-12-09 13:07:04.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.002970108656631369 Training loss: 9.137785911560059
2025-12-09 13:07:04.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.002969922753510885 Training loss: 9.32498550415039
2025-12-09 13:07:05.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.002969736279942078 Training loss: 9.570134162902832
2025-12-09 13:07:05.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.002969549235997315 Training loss: 9.01191520690918
2025-12-09 13:07:06.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.002969361621749184 Training loss: 9.132668495178223
2025-12-09 13:07:06.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.002969173437270495 Training loss: 9.167933464050293
2025-12-09 13:07:06.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0029689846826342773 Training loss: 9.21306324005127
2025-12-09 13:07:07.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.002968795357913784 Training loss: 9.103225708007812
2025-12-09 13:07:07.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0029686054631824885 Training loss: 9.221817970275879
2025-12-09 13:07:07.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0029684149985140847 Training loss: 9.294108390808105
2025-12-09 13:07:08.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0029682239639824883 Training loss: 9.236928939819336
2025-12-09 13:07:08.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.002968032359661836 Training loss: 9.33789348602295
2025-12-09 13:07:09.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.002967840185626486 Training loss: 9.134322166442871
2025-12-09 13:07:09.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0029676474419510174 Training loss: 9.120784759521484
2025-12-09 13:07:09.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0029674541287102296 Training loss: 9.390951156616211
2025-12-09 13:07:10.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.002967260245979144 Training loss: 9.687931060791016
2025-12-09 13:07:10.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.002967065793833003 Training loss: 9.139220237731934
2025-12-09 13:07:10.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.002966870772347269 Training loss: 9.29397201538086
2025-12-09 13:07:11.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0029666751815976273 Training loss: 8.958434104919434
2025-12-09 13:07:11.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0029664790216599813 Training loss: 9.373103141784668
2025-12-09 13:07:12.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0029662822926104578 Training loss: 9.215309143066406
2025-12-09 13:07:12.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0029660849945254038 Training loss: 9.072020530700684
2025-12-09 13:07:12.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0029658871274813856 Training loss: 9.638751983642578
2025-12-09 13:07:13.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0029656886915551926 Training loss: 9.081388473510742
2025-12-09 13:07:13.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0029654896868238335 Training loss: 9.059155464172363
2025-12-09 13:07:13.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0029652901133645384 Training loss: 9.252290725708008
2025-12-09 13:07:14.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0029650899712547574 Training loss: 9.367232322692871
2025-12-09 13:07:14.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0029648892605721624 Training loss: 9.335885047912598
2025-12-09 13:07:15.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0029646879813946445 Training loss: 9.181655883789062
2025-12-09 13:07:15.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.002964486133800317 Training loss: 9.37773609161377
2025-12-09 13:07:15.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0029642837178675122 Training loss: 9.125265121459961
2025-12-09 13:07:16.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.002964080733674784 Training loss: 9.191108703613281
2025-12-09 13:07:16.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0029638771813009076 Training loss: 9.036112785339355
2025-12-09 13:07:16.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.002963673060824877 Training loss: 9.295762062072754
2025-12-09 13:07:17.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0029634683723259066 Training loss: 9.072941780090332
2025-12-09 13:07:17.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0029632631158834333 Training loss: 9.732319831848145
2025-12-09 13:07:17.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0029630572915771117 Training loss: 9.108360290527344
2025-12-09 13:07:18.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.00296285089948682 Training loss: 9.447983741760254
2025-12-09 13:07:18.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0029626439396926536 Training loss: 9.148836135864258
2025-12-09 13:07:19.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0029624364122749296 Training loss: 9.248164176940918
2025-12-09 13:07:19.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0029622283173141866 Training loss: 9.188721656799316
2025-12-09 13:07:19.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00296201965489118 Training loss: 9.090373992919922
2025-12-09 13:07:20.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00296181042508689 Training loss: 9.712789535522461
2025-12-09 13:07:20.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.0029616006279825128 Training loss: 9.473995208740234
2025-12-09 13:07:20.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.002961390263659467 Training loss: 9.088619232177734
2025-12-09 13:07:21.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0029611793321993912 Training loss: 9.340551376342773
2025-12-09 13:07:21.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.0029609678336841444 Training loss: 9.157210350036621
2025-12-09 13:07:22.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0029607557681958037 Training loss: 9.35989761352539
2025-12-09 13:07:22.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0029605431358166686 Training loss: 9.123152732849121
2025-12-09 13:07:22.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.002960329936629257 Training loss: 9.312801361083984
2025-12-09 13:07:23.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.002960116170716308 Training loss: 9.462320327758789
2025-12-09 13:07:23.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0029599018381607787 Training loss: 9.244288444519043
2025-12-09 13:07:23.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0029596869390458485 Training loss: 9.123926162719727
2025-12-09 13:07:24.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.002959471473454915 Training loss: 9.4410982131958
2025-12-09 13:07:24.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0029592554414715967 Training loss: 9.23134994506836
2025-12-09 13:07:25.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.002959038843179731 Training loss: 9.245882034301758
2025-12-09 13:07:25.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0029588216786633763 Training loss: 9.791057586669922
2025-12-09 13:07:25.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0029586039480068087 Training loss: 9.15340805053711
2025-12-09 13:07:26.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0029583856512945257 Training loss: 9.361910820007324
2025-12-09 13:07:26.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0029581667886112435 Training loss: 9.182165145874023
2025-12-09 13:07:26.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0029579473600418998 Training loss: 9.573832511901855
2025-12-09 13:07:27.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0029577273656716495 Training loss: 9.241185188293457
2025-12-09 13:07:27.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0029575068055858675 Training loss: 9.285372734069824
2025-12-09 13:07:28.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.0029572856798701507 Training loss: 9.122091293334961
2025-12-09 13:07:28.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0029570639886103123 Training loss: 9.57144832611084
2025-12-09 13:07:28.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0029568417318923865 Training loss: 9.34541130065918
2025-12-09 13:07:29.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.002956618909802627 Training loss: 9.185590744018555
2025-12-09 13:07:29.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0029563955224275068 Training loss: 9.254534721374512
2025-12-09 13:07:29.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0029561715698537185 Training loss: 9.510101318359375
2025-12-09 13:07:30.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0029559470521681726 Training loss: 9.185604095458984
2025-12-09 13:07:30.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.002955721969458001 Training loss: 9.285122871398926
2025-12-09 13:07:31.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0029554963218105536 Training loss: 9.33714485168457
2025-12-09 13:07:31.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0029552701093133998 Training loss: 9.34394359588623
2025-12-09 13:07:31.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0029550433320543286 Training loss: 9.332919120788574
2025-12-09 13:07:32.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0029548159901213473 Training loss: 9.053221702575684
2025-12-09 13:07:32.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.0029545880836026835 Training loss: 9.51785945892334
2025-12-09 13:07:32.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0029543596125867827 Training loss: 9.336798667907715
2025-12-09 13:07:33.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00295413057716231 Training loss: 9.311530113220215
2025-12-09 13:07:33.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.00295390097741815 Training loss: 9.563076972961426
2025-12-09 13:07:33.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0029536708134434058 Training loss: 9.298240661621094
2025-12-09 13:07:34.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.002953440085327399 Training loss: 9.148079872131348
2025-12-09 13:07:34.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0029532087931596718 Training loss: 9.429924011230469
2025-12-09 13:07:35.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0029529769370299826 Training loss: 9.419052124023438
2025-12-09 13:07:35.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0029527445170283114 Training loss: 9.238582611083984
2025-12-09 13:07:35.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.002952511533244856 Training loss: 9.117767333984375
2025-12-09 13:07:36.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0029522779857700326 Training loss: 9.227344512939453
2025-12-09 13:07:36.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0029520438746944754 Training loss: 9.519506454467773
2025-12-09 13:07:36.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00295180920010904 Training loss: 9.356871604919434
2025-12-09 13:07:37.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.0029515739621047976 Training loss: 9.040719985961914
2025-12-09 13:07:37.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0029513381607730402 Training loss: 9.481520652770996
2025-12-09 13:07:38.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.002951101796205278 Training loss: 9.292869567871094
2025-12-09 13:07:38.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0029508648684932392 Training loss: 9.54655647277832
2025-12-09 13:07:38.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0029506273777288703 Training loss: 9.179763793945312
2025-12-09 13:07:39.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.002950389324004337 Training loss: 9.317381858825684
2025-12-09 13:07:39.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.002950150707412024 Training loss: 9.600811958312988
2025-12-09 13:07:39.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.002949911528044533 Training loss: 9.243979454040527
2025-12-09 13:07:40.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0029496717859946848 Training loss: 9.447141647338867
2025-12-09 13:07:40.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0029494314813555194 Training loss: 9.36429214477539
2025-12-09 13:07:41.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.002949190614220294 Training loss: 9.334230422973633
2025-12-09 13:07:41.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.002948949184682484 Training loss: 9.782920837402344
2025-12-09 13:07:41.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0029487071928357834 Training loss: 9.937284469604492
2025-12-09 13:07:42.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0029484646387741057 Training loss: 10.142605781555176
2025-12-09 13:07:42.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0029482215225915803 Training loss: 9.229917526245117
2025-12-09 13:07:42.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0029479778443825553 Training loss: 9.299327850341797
2025-12-09 13:07:43.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.002947733604241599 Training loss: 9.15746784210205
2025-12-09 13:07:43.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.002947488802263496 Training loss: 9.237639427185059
2025-12-09 13:07:44.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0029472434385432477 Training loss: 9.264547348022461
2025-12-09 13:07:44.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0029469975131760765 Training loss: 9.474387168884277
2025-12-09 13:07:44.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0029467510262574203 Training loss: 9.444198608398438
2025-12-09 13:07:45.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0029465039778829366 Training loss: 9.350851058959961
2025-12-09 13:07:45.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0029462563681484995 Training loss: 9.390048027038574
2025-12-09 13:07:45.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.002946008197150202 Training loss: 9.322746276855469
2025-12-09 13:07:46.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0029457594649843536 Training loss: 9.316773414611816
2025-12-09 13:07:46.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0029455101717474836 Training loss: 9.406571388244629
2025-12-09 13:07:46.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0029452603175363365 Training loss: 9.11057186126709
2025-12-09 13:07:47.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0029450099024478766 Training loss: 9.487923622131348
2025-12-09 13:07:47.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.002944758926579285 Training loss: 9.301076889038086
2025-12-09 13:07:48.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.002944507390027961 Training loss: 9.255971908569336
2025-12-09 13:07:48.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0029442552928915203 Training loss: 9.395228385925293
2025-12-09 13:07:48.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.002944002635267797 Training loss: 9.335047721862793
2025-12-09 13:07:49.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.002943749417254843 Training loss: 9.255499839782715
2025-12-09 13:07:49.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0029434956389509264 Training loss: 9.278979301452637
2025-12-09 13:07:49.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0029432413004545346 Training loss: 9.703943252563477
2025-12-09 13:07:50.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.002942986401864371 Training loss: 9.26502799987793
2025-12-09 13:07:50.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.002942730943279357 Training loss: 9.299877166748047
2025-12-09 13:07:51.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0029424749247986305 Training loss: 9.656344413757324
2025-12-09 13:07:51.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.002942218346521548 Training loss: 9.308416366577148
2025-12-09 13:07:51.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.0029419612085476816 Training loss: 9.53298568725586
2025-12-09 13:07:52.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0029417035109768224 Training loss: 9.225993156433105
2025-12-09 13:07:52.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.002941445253908978 Training loss: 9.366342544555664
2025-12-09 13:07:52.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.0029411864374443717 Training loss: 9.260749816894531
2025-12-09 13:07:53.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.002940927061683446 Training loss: 9.390158653259277
2025-12-09 13:07:53.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.002940667126726859 Training loss: 9.858668327331543
2025-12-09 13:07:54.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0029404066326754875 Training loss: 9.802103996276855
2025-12-09 13:07:54.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0029401455796304234 Training loss: 9.341259956359863
2025-12-09 13:07:54.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0029398839676929756 Training loss: 9.547218322753906
2025-12-09 13:07:55.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.002939621796964672 Training loss: 9.322829246520996
2025-12-09 13:07:55.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.002939359067547255 Training loss: 9.328566551208496
2025-12-09 13:07:55.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.002939095779542685 Training loss: 9.349004745483398
2025-12-09 13:07:56.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0029388319330531385 Training loss: 9.26180648803711
2025-12-09 13:07:56.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0029385675281810106 Training loss: 9.384979248046875
2025-12-09 13:07:57.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.00293830256502891 Training loss: 9.144707679748535
2025-12-09 13:07:57.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0029380370436996642 Training loss: 9.281563758850098
2025-12-09 13:07:57.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0029377709642963174 Training loss: 9.265436172485352
2025-12-09 13:07:58.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0029375043269221296 Training loss: 9.350130081176758
2025-12-09 13:07:58.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.002937237131680577 Training loss: 9.123127937316895
2025-12-09 13:07:58.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.002936969378675354 Training loss: 9.231266021728516
2025-12-09 13:07:59.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0029367010680103685 Training loss: 9.52138900756836
2025-12-09 13:07:59.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0029364321997897482 Training loss: 9.335123062133789
2025-12-09 13:08:00.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0029361627741178358 Training loss: 9.22933292388916
2025-12-09 13:08:00.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.002935892791099189 Training loss: 9.283804893493652
2025-12-09 13:08:00.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.002935622250838583 Training loss: 9.314970970153809
2025-12-09 13:08:01.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0029353511534410104 Training loss: 9.506027221679688
2025-12-09 13:08:01.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.002935079499011677 Training loss: 9.371347427368164
2025-12-09 13:08:01.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0029348072876560086 Training loss: 9.184720993041992
2025-12-09 13:08:02.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0029345345194796437 Training loss: 9.238716125488281
2025-12-09 13:08:02.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0029342611945884388 Training loss: 9.262458801269531
2025-12-09 13:08:02.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0029339873130884656 Training loss: 9.542356491088867
2025-12-09 13:08:03.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0029337128750860125 Training loss: 9.216835975646973
2025-12-09 13:08:03.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0029334378806875837 Training loss: 9.312047004699707
2025-12-09 13:08:04.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.002933162329999899 Training loss: 9.468718528747559
2025-12-09 13:08:04.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.002932886223129894 Training loss: 9.182085037231445
2025-12-09 13:08:04.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0029326095601847203 Training loss: 9.274104118347168
2025-12-09 13:08:05.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0029323323412717463 Training loss: 9.471519470214844
2025-12-09 13:08:05.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0029320545664985537 Training loss: 9.277782440185547
2025-12-09 13:08:05.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0029317762359729427 Training loss: 9.32413101196289
2025-12-09 13:08:06.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.002931497349802928 Training loss: 9.462691307067871
2025-12-09 13:08:06.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.002931217908096739 Training loss: 9.188005447387695
2025-12-09 13:08:07.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0029309379109628223 Training loss: 9.223575592041016
2025-12-09 13:08:07.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0029306573585098387 Training loss: 9.282769203186035
2025-12-09 13:08:07.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0029303762508466656 Training loss: 9.279468536376953
2025-12-09 13:08:08.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0029300945880823956 Training loss: 9.300045013427734
2025-12-09 13:08:08.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0029298123703263364 Training loss: 9.643881797790527
2025-12-09 13:08:08.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.002929529597688011 Training loss: 9.093912124633789
2025-12-09 13:08:09.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0029292462702771574 Training loss: 9.190889358520508
2025-12-09 13:08:09.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0029289623882037302 Training loss: 9.242181777954102
2025-12-09 13:08:10.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0029286779515778983 Training loss: 9.164554595947266
2025-12-09 13:08:10.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0029283929605100458 Training loss: 9.333365440368652
2025-12-09 13:08:10.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0029281074151107727 Training loss: 9.340275764465332
2025-12-09 13:08:11.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.002927821315490893 Training loss: 9.223601341247559
2025-12-09 13:08:11.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0029275346617614363 Training loss: 9.461601257324219
2025-12-09 13:08:11.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.002927247454033648 Training loss: 9.145842552185059
2025-12-09 13:08:12.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.002926959692418988 Training loss: 9.1737060546875
2025-12-09 13:08:12.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0029266713770291293 Training loss: 9.092184066772461
2025-12-09 13:08:13.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0029263825079759638 Training loss: 9.300651550292969
2025-12-09 13:08:13.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0029260930853715937 Training loss: 9.175888061523438
2025-12-09 13:08:13.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0029258031093283396 Training loss: 9.552647590637207
2025-12-09 13:08:14.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0029255125799587355 Training loss: 9.325433731079102
2025-12-09 13:08:14.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.002925221497375529 Training loss: 9.222614288330078
2025-12-09 13:08:14.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0029249298616916856 Training loss: 9.306270599365234
2025-12-09 13:08:15.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.002924637673020382 Training loss: 9.281845092773438
2025-12-09 13:08:15.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.002924344931475011 Training loss: 9.487692832946777
2025-12-09 13:08:16.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0029240516371691803 Training loss: 9.2279052734375
2025-12-09 13:08:16.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.002923757790216711 Training loss: 9.474321365356445
2025-12-09 13:08:16.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0029234633907316405 Training loss: 9.34521770477295
2025-12-09 13:08:17.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0029231684388282184 Training loss: 9.287054061889648
2025-12-09 13:08:17.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00292287293462091 Training loss: 9.269393920898438
2025-12-09 13:08:17.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0029225768782243956 Training loss: 8.919231414794922
2025-12-09 13:08:18.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0029222802697535678 Training loss: 9.363842964172363
2025-12-09 13:08:18.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.002921983109323535 Training loss: 9.038269996643066
2025-12-09 13:08:18.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0029216853970496196 Training loss: 9.674710273742676
2025-12-09 13:08:19.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0029213871330473575 Training loss: 9.287984848022461
2025-12-09 13:08:19.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.002921088317432499 Training loss: 9.18457317352295
2025-12-09 13:08:20.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0029207889503210095 Training loss: 9.523695945739746
2025-12-09 13:08:20.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.002920489031829067 Training loss: 9.332012176513672
2025-12-09 13:08:20.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.002920188562073063 Training loss: 9.250235557556152
2025-12-09 13:08:21.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0029198875411696056 Training loss: 9.315071105957031
2025-12-09 13:08:21.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0029195859692355145 Training loss: 9.073363304138184
2025-12-09 13:08:21.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.002919283846387824 Training loss: 9.250810623168945
2025-12-09 13:08:22.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0029189811727437813 Training loss: 9.110254287719727
2025-12-09 13:08:22.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.002918677948420849 Training loss: 9.624336242675781
2025-12-09 13:08:23.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0029183741735367024 Training loss: 9.228384017944336
2025-12-09 13:08:23.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0029180698482092304 Training loss: 9.107142448425293
2025-12-09 13:08:23.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0029177649725565355 Training loss: 9.220852851867676
2025-12-09 13:08:24.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0029174595466969345 Training loss: 8.969204902648926
2025-12-09 13:08:24.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0029171535707489572 Training loss: 9.373642921447754
2025-12-09 13:08:24.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0029168470448313463 Training loss: 9.135238647460938
2025-12-09 13:08:25.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.002916539969063059 Training loss: 9.117030143737793
2025-12-09 13:08:25.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0029162323435632654 Training loss: 9.176196098327637
2025-12-09 13:08:26.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.002915924168451349 Training loss: 9.809307098388672
2025-12-09 13:08:26.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.002915615443846906 Training loss: 9.319313049316406
2025-12-09 13:08:26.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0029153061698697475 Training loss: 9.095032691955566
2025-12-09 13:08:27.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0029149963466398956 Training loss: 9.202287673950195
2025-12-09 13:08:27.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.002914685974277587 Training loss: 9.283754348754883
2025-12-09 13:08:27.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.002914375052903271 Training loss: 9.107033729553223
2025-12-09 13:08:28.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.002914063582637611 Training loss: 9.167243957519531
2025-12-09 13:08:28.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.002913751563601481 Training loss: 9.20693302154541
2025-12-09 13:08:29.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0029134389959159708 Training loss: 9.177382469177246
2025-12-09 13:08:29.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0029131258797023816 Training loss: 9.419050216674805
2025-12-09 13:08:29.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0029128122150822266 Training loss: 9.311614990234375
2025-12-09 13:08:30.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0029124980021772344 Training loss: 8.98601245880127
2025-12-09 13:08:30.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0029121832411093443 Training loss: 8.641982078552246
2025-12-09 13:08:30.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0029118679320007087 Training loss: 9.410351753234863
2025-12-09 13:08:31.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0029115520749736935 Training loss: 9.004199981689453
2025-12-09 13:08:31.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0029112356701508756 Training loss: 9.362251281738281
2025-12-09 13:08:32.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0029109187176550463 Training loss: 9.10265064239502
2025-12-09 13:08:32.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0029106012176092085 Training loss: 9.758932113647461
2025-12-09 13:08:32.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0029102831701365785 Training loss: 9.256242752075195
2025-12-09 13:08:33.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0029099645753605827 Training loss: 8.891318321228027
2025-12-09 13:08:33.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.002909645433404863 Training loss: 9.28231430053711
2025-12-09 13:08:33.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0029093257443932713 Training loss: 9.26772689819336
2025-12-09 13:08:34.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0029090055084498734 Training loss: 8.99370288848877
2025-12-09 13:08:34.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0029086847256989457 Training loss: 8.965267181396484
2025-12-09 13:08:34.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0029083633962649785 Training loss: 9.048850059509277
2025-12-09 13:08:35.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0029080415202726727 Training loss: 9.026113510131836
2025-12-09 13:08:35.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0029077190978469432 Training loss: 9.78535270690918
2025-12-09 13:08:36.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0029073961291129153 Training loss: 8.918665885925293
2025-12-09 13:08:36.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0029070726141959265 Training loss: 9.205900192260742
2025-12-09 13:08:36.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.002906748553221527 Training loss: 9.092613220214844
2025-12-09 13:08:37.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0029064239463154782 Training loss: 9.622781753540039
2025-12-09 13:08:37.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.002906098793603754 Training loss: 9.207123756408691
2025-12-09 13:08:37.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.00290577309521254 Training loss: 9.32422161102295
2025-12-09 13:08:38.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.002905446851268233 Training loss: 9.367684364318848
2025-12-09 13:08:38.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.002905120061897442 Training loss: 8.993806838989258
2025-12-09 13:08:39.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.002904792727226987 Training loss: 8.896286964416504
2025-12-09 13:08:39.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.002904464847383902 Training loss: 9.8137788772583
2025-12-09 13:08:39.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.002904136422495429 Training loss: 9.072197914123535
2025-12-09 13:08:40.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0029038074526890243 Training loss: 8.761316299438477
2025-12-09 13:08:40.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.002903477938092354 Training loss: 8.845333099365234
2025-12-09 13:08:40.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.002903147878833296 Training loss: 9.285897254943848
2025-12-09 13:08:41.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.002902817275039941 Training loss: 9.096366882324219
2025-12-09 13:08:41.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0029024861268405894 Training loss: 8.862618446350098
2025-12-09 13:08:42.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0029021544343637525 Training loss: 9.097819328308105
2025-12-09 13:08:42.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0029018221977381554 Training loss: 8.891399383544922
2025-12-09 13:08:42.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0029014894170927307 Training loss: 8.945721626281738
2025-12-09 13:08:43.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0029011560925566253 Training loss: 9.075552940368652
2025-12-09 13:08:43.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.002900822224259196 Training loss: 8.920930862426758
2025-12-09 13:08:43.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0029004878123300095 Training loss: 9.224239349365234
2025-12-09 13:08:44.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0029001528568988457 Training loss: 9.021904945373535
2025-12-09 13:08:44.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.0028998173580956936 Training loss: 8.934834480285645
2025-12-09 13:08:45.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.002899481316050754 Training loss: 8.893678665161133
2025-12-09 13:08:45.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0028991447308944385 Training loss: 8.94655704498291
2025-12-09 13:08:45.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.002898807602757369 Training loss: 8.925084114074707
2025-12-09 13:08:46.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0028984699317703777 Training loss: 8.663744926452637
2025-12-09 13:08:46.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0028981317180645093 Training loss: 8.796300888061523
2025-12-09 13:08:46.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.002897792961771017 Training loss: 8.807611465454102
2025-12-09 13:08:47.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.002897453663021366 Training loss: 8.7274751663208
2025-12-09 13:08:47.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.002897113821947231 Training loss: 8.996378898620605
2025-12-09 13:08:48.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.0028967734386804982 Training loss: 8.735523223876953
2025-12-09 13:08:48.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.002896432513353264 Training loss: 8.657157897949219
2025-12-09 13:08:48.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.002896091046097834 Training loss: 8.836763381958008
2025-12-09 13:08:49.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0028957490370467255 Training loss: 8.694015502929688
2025-12-09 13:08:49.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0028954064863326652 Training loss: 8.624835968017578
2025-12-09 13:08:49.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.002895063394088591 Training loss: 8.85757064819336
2025-12-09 13:08:50.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0028947197604476493 Training loss: 9.354945182800293
2025-12-09 13:08:50.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.002894375585543199 Training loss: 8.961494445800781
2025-12-09 13:08:51.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0028940308695088062 Training loss: 8.846656799316406
2025-12-09 13:08:51.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.002893685612478249 Training loss: 8.852751731872559
2025-12-09 13:08:51.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0028933398145855158 Training loss: 8.335862159729004
2025-12-09 13:08:52.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0028929934759648022 Training loss: 8.739389419555664
2025-12-09 13:08:52.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0028926465967505175 Training loss: 8.662257194519043
2025-12-09 13:08:52.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.002892299177077277 Training loss: 8.875760078430176
2025-12-09 13:08:53.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0028919512170799085 Training loss: 8.533164978027344
2025-12-09 13:08:53.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0028916027168934483 Training loss: 8.694448471069336
2025-12-09 13:08:54.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0028912536766531423 Training loss: 8.569092750549316
2025-12-09 13:08:54.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.0028909040964944462 Training loss: 8.755568504333496
2025-12-09 13:08:54.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.002890553976553025 Training loss: 8.501291275024414
2025-12-09 13:08:55.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.002890203316964755 Training loss: 9.033110618591309
2025-12-09 13:08:55.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.002889852117865718 Training loss: 8.790454864501953
2025-12-09 13:08:55.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.002889500379392209 Training loss: 8.745816230773926
2025-12-09 13:08:56.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.0028891481016807305 Training loss: 8.79178524017334
2025-12-09 13:08:56.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.0028887952848679946 Training loss: 8.749866485595703
2025-12-09 13:08:56.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.002888441929090922 Training loss: 8.841022491455078
2025-12-09 13:08:57.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.002888088034486645 Training loss: 8.711356163024902
2025-12-09 13:08:57.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0028877336011925007 Training loss: 8.613302230834961
2025-12-09 13:08:58.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.00288737862934604 Training loss: 8.679868698120117
2025-12-09 13:08:58.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.002887023119085019 Training loss: 8.656620979309082
2025-12-09 13:08:58.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.002886667070547405 Training loss: 8.613757133483887
2025-12-09 13:08:59.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.002886310483871373 Training loss: 8.9315824508667
2025-12-09 13:08:59.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.0028859533591953077 Training loss: 8.53674030303955
2025-12-09 13:08:59.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.0028855956966578025 Training loss: 8.691472053527832
2025-12-09 13:09:00.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.002885237496397659 Training loss: 8.659598350524902
2025-12-09 13:09:00.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.0028848787585538872 Training loss: 8.629545211791992
2025-12-09 13:09:01.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.0028845194832657064 Training loss: 8.336341857910156
2025-12-09 13:09:01.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.002884159670672545 Training loss: 8.53593921661377
2025-12-09 13:09:01.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.002883799320914039 Training loss: 9.414990425109863
