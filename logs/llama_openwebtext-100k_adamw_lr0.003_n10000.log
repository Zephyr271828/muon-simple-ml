2025-12-09 12:03:03.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 12.168539047241211
2025-12-09 12:03:04.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 12.16819953918457
2025-12-09 12:03:04.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 12.037919044494629
2025-12-09 12:03:04.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 11.866774559020996
2025-12-09 12:03:05.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 11.132781028747559
2025-12-09 12:03:05.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 10.501056671142578
2025-12-09 12:03:06.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 10.909903526306152
2025-12-09 12:03:06.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 11.324875831604004
2025-12-09 12:03:06.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 10.779099464416504
2025-12-09 12:03:07.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 10.287450790405273
2025-12-09 12:03:07.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 9.537697792053223
2025-12-09 12:03:07.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 9.169065475463867
2025-12-09 12:03:08.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 8.947460174560547
2025-12-09 12:03:08.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 9.165604591369629
2025-12-09 12:03:09.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 8.727946281433105
2025-12-09 12:03:09.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 8.566184043884277
2025-12-09 12:03:09.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 8.552005767822266
2025-12-09 12:03:10.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 8.304247856140137
2025-12-09 12:03:10.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 8.595519065856934
2025-12-09 12:03:11.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 8.630112648010254
2025-12-09 12:03:11.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 8.826141357421875
2025-12-09 12:03:11.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 8.762600898742676
2025-12-09 12:03:12.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 8.71163558959961
2025-12-09 12:03:12.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 8.954106330871582
2025-12-09 12:03:12.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 9.006125450134277
2025-12-09 12:03:13.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 9.220657348632812
2025-12-09 12:03:13.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 8.668848991394043
2025-12-09 12:03:14.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 9.196259498596191
2025-12-09 12:03:14.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 9.536850929260254
2025-12-09 12:03:14.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 8.786057472229004
2025-12-09 12:03:15.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 9.319596290588379
2025-12-09 12:03:15.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 8.588491439819336
2025-12-09 12:03:16.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 8.504171371459961
2025-12-09 12:03:16.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 8.43568229675293
2025-12-09 12:03:16.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 8.167017936706543
2025-12-09 12:03:17.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 8.625432014465332
2025-12-09 12:03:17.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 8.983650207519531
2025-12-09 12:03:17.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 8.433639526367188
2025-12-09 12:03:18.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 9.547548294067383
2025-12-09 12:03:18.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 9.040298461914062
2025-12-09 12:03:19.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 8.52469253540039
2025-12-09 12:03:19.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 9.107250213623047
2025-12-09 12:03:19.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 7.9643402099609375
2025-12-09 12:03:20.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 8.623324394226074
2025-12-09 12:03:20.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 8.35185718536377
2025-12-09 12:03:21.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 10.290319442749023
2025-12-09 12:03:21.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 8.882758140563965
2025-12-09 12:03:21.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 8.739002227783203
2025-12-09 12:03:22.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 9.291352272033691
2025-12-09 12:03:22.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 9.047734260559082
2025-12-09 12:03:22.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 8.590036392211914
2025-12-09 12:03:23.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 8.729555130004883
2025-12-09 12:03:23.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 8.38708209991455
2025-12-09 12:03:24.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 9.431687355041504
2025-12-09 12:03:24.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 8.596596717834473
2025-12-09 12:03:24.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 8.54660415649414
2025-12-09 12:03:25.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 8.927042961120605
2025-12-09 12:03:25.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 8.542497634887695
2025-12-09 12:03:26.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 8.246445655822754
2025-12-09 12:03:26.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 8.93012523651123
2025-12-09 12:03:26.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 9.44402027130127
2025-12-09 12:03:27.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 9.078843116760254
2025-12-09 12:03:27.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 8.749868392944336
2025-12-09 12:03:27.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 9.407398223876953
2025-12-09 12:03:28.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 8.895334243774414
2025-12-09 12:03:28.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 8.77794361114502
2025-12-09 12:03:29.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 8.965127944946289
2025-12-09 12:03:29.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 8.996365547180176
2025-12-09 12:03:29.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 8.769930839538574
2025-12-09 12:03:30.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 9.177145004272461
2025-12-09 12:03:30.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 8.572196960449219
2025-12-09 12:03:31.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 8.257353782653809
2025-12-09 12:03:31.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 8.924062728881836
2025-12-09 12:03:31.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 8.721534729003906
2025-12-09 12:03:32.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 7.9420928955078125
2025-12-09 12:03:32.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 8.19239330291748
2025-12-09 12:03:32.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 8.432991027832031
2025-12-09 12:03:33.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 8.284586906433105
2025-12-09 12:03:33.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 8.519695281982422
2025-12-09 12:03:34.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 8.425789833068848
2025-12-09 12:03:34.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 8.494449615478516
2025-12-09 12:03:34.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 8.838717460632324
2025-12-09 12:03:35.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 8.303451538085938
2025-12-09 12:03:35.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 8.70512580871582
2025-12-09 12:03:36.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 8.345458030700684
2025-12-09 12:03:36.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 8.493965148925781
2025-12-09 12:03:36.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 8.791851043701172
2025-12-09 12:03:37.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 8.378778457641602
2025-12-09 12:03:37.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 7.780649662017822
2025-12-09 12:03:37.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 8.828789710998535
2025-12-09 12:03:38.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 8.034598350524902
2025-12-09 12:03:38.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 9.247048377990723
2025-12-09 12:03:39.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 8.231432914733887
2025-12-09 12:03:39.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 8.33799934387207
2025-12-09 12:03:39.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 9.215999603271484
2025-12-09 12:03:40.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 7.661698341369629
2025-12-09 12:03:40.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 8.48477554321289
2025-12-09 12:03:41.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 8.314648628234863
2025-12-09 12:03:41.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 8.27014446258545
2025-12-09 12:03:41.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 8.441388130187988
2025-12-09 12:03:42.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999997089396424 Training loss: 8.920764923095703
2025-12-09 12:03:42.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002999998835758683 Training loss: 8.430259704589844
2025-12-09 12:03:43.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029999973804574606 Training loss: 8.438392639160156
2025-12-09 12:03:43.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0029999953430365394 Training loss: 8.107978820800781
2025-12-09 12:03:43.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.002999992723496711 Training loss: 8.731163024902344
2025-12-09 12:03:44.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.002999989521838991 Training loss: 8.507635116577148
2025-12-09 12:03:44.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029999857380646226 Training loss: 8.735038757324219
2025-12-09 12:03:44.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.002999981372175074 Training loss: 8.25521469116211
2025-12-09 12:03:45.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0029999764241720396 Training loss: 8.368293762207031
2025-12-09 12:03:45.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0029999708940574394 Training loss: 8.17463207244873
2025-12-09 12:03:46.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0029999647818334195 Training loss: 8.11275577545166
2025-12-09 12:03:46.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.002999958087502352 Training loss: 8.225893020629883
2025-12-09 12:03:46.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029999508110668356 Training loss: 8.092479705810547
2025-12-09 12:03:47.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029999429525296934 Training loss: 8.273438453674316
2025-12-09 12:03:47.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0029999345118939752 Training loss: 7.915706157684326
2025-12-09 12:03:48.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0029999254891629563 Training loss: 8.77283000946045
2025-12-09 12:03:48.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.002999915884340139 Training loss: 8.177062034606934
2025-12-09 12:03:48.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0029999056974292504 Training loss: 8.396835327148438
2025-12-09 12:03:49.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029998949284342435 Training loss: 9.833806037902832
2025-12-09 12:03:49.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.002999883577359298 Training loss: 8.897649765014648
2025-12-09 12:03:49.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.002999871644208819 Training loss: 8.229412078857422
2025-12-09 12:03:50.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002999859128987437 Training loss: 8.146883010864258
2025-12-09 12:03:50.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00299984603170001 Training loss: 7.996079921722412
2025-12-09 12:03:51.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0029998323523516197 Training loss: 8.525043487548828
2025-12-09 12:03:51.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029998180909475754 Training loss: 8.235027313232422
2025-12-09 12:03:51.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.002999803247493411 Training loss: 7.9242634773254395
2025-12-09 12:03:52.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.002999787821994888 Training loss: 8.285581588745117
2025-12-09 12:03:52.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0029997718144579915 Training loss: 8.411406517028809
2025-12-09 12:03:53.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0029997552248889354 Training loss: 8.474655151367188
2025-12-09 12:03:53.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.002999738053294156 Training loss: 8.199769020080566
2025-12-09 12:03:53.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0029997202996803183 Training loss: 8.282227516174316
2025-12-09 12:03:54.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002999701964054312 Training loss: 8.583748817443848
2025-12-09 12:03:54.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0029996830464232523 Training loss: 7.872481822967529
2025-12-09 12:03:54.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0029996635467944813 Training loss: 8.251551628112793
2025-12-09 12:03:55.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0029996434651755662 Training loss: 8.194286346435547
2025-12-09 12:03:55.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0029996228015743004 Training loss: 7.8773908615112305
2025-12-09 12:03:56.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.002999601555998703 Training loss: 8.045247077941895
2025-12-09 12:03:56.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.002999579728457019 Training loss: 8.084756851196289
2025-12-09 12:03:56.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.002999557318957719 Training loss: 8.172513961791992
2025-12-09 12:03:57.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0029995343275095003 Training loss: 8.150802612304688
2025-12-09 12:03:57.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0029995107541212845 Training loss: 7.973457336425781
2025-12-09 12:03:58.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.002999486598802221 Training loss: 8.001011848449707
2025-12-09 12:03:58.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0029994618615616832 Training loss: 7.8259172439575195
2025-12-09 12:03:58.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.002999436542409272 Training loss: 7.846826553344727
2025-12-09 12:03:59.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0029994106413548122 Training loss: 8.879945755004883
2025-12-09 12:03:59.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0029993841584083558 Training loss: 8.075441360473633
2025-12-09 12:04:00.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.002999357093580181 Training loss: 8.641799926757812
2025-12-09 12:04:00.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0029993294468807904 Training loss: 7.669943332672119
2025-12-09 12:04:00.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0029993012183209137 Training loss: 8.171186447143555
2025-12-09 12:04:01.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0029992724079115052 Training loss: 7.914414405822754
2025-12-09 12:04:01.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.002999243015663746 Training loss: 8.261052131652832
2025-12-09 12:04:01.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0029992130415890427 Training loss: 7.72782039642334
2025-12-09 12:04:02.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.002999182485699028 Training loss: 7.434980869293213
2025-12-09 12:04:02.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0029991513480055595 Training loss: 8.562345504760742
2025-12-09 12:04:03.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002999119628520721 Training loss: 7.908749580383301
2025-12-09 12:04:03.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0029990873272568224 Training loss: 7.923461437225342
2025-12-09 12:04:03.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0029990544442264 Training loss: 7.739980220794678
2025-12-09 12:04:04.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.002999020979442214 Training loss: 8.439451217651367
2025-12-09 12:04:04.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.002998986932917252 Training loss: 7.963468551635742
2025-12-09 12:04:05.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.002998952304664726 Training loss: 7.8594889640808105
2025-12-09 12:04:05.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.002998917094698076 Training loss: 7.941227436065674
2025-12-09 12:04:05.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.002998881303030965 Training loss: 8.210864067077637
2025-12-09 12:04:06.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0029988449296772836 Training loss: 9.012285232543945
2025-12-09 12:04:06.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.002998807974651147 Training loss: 8.012402534484863
2025-12-09 12:04:06.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0029987704379668976 Training loss: 8.495115280151367
2025-12-09 12:04:07.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002998732319639102 Training loss: 8.143921852111816
2025-12-09 12:04:07.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0029986936196825537 Training loss: 7.936934471130371
2025-12-09 12:04:08.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.002998654338112271 Training loss: 8.341075897216797
2025-12-09 12:04:08.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0029986144749434987 Training loss: 7.744725227355957
2025-12-09 12:04:08.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0029985740301917063 Training loss: 7.9021830558776855
2025-12-09 12:04:09.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00299853300387259 Training loss: 7.797260284423828
2025-12-09 12:04:09.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0029984913960020712 Training loss: 8.445213317871094
2025-12-09 12:04:10.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0029984492065962976 Training loss: 8.073799133300781
2025-12-09 12:04:10.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0029984064356716415 Training loss: 7.8042311668396
2025-12-09 12:04:10.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0029983630832447015 Training loss: 7.514059066772461
2025-12-09 12:04:11.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002998319149332302 Training loss: 7.586207866668701
2025-12-09 12:04:11.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0029982746339514933 Training loss: 7.7270402908325195
2025-12-09 12:04:12.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0029982295371195496 Training loss: 7.470876693725586
2025-12-09 12:04:12.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.002998183858853974 Training loss: 7.872135162353516
2025-12-09 12:04:12.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0029981375991724917 Training loss: 7.962465763092041
2025-12-09 12:04:13.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0029980907580930563 Training loss: 8.017176628112793
2025-12-09 12:04:13.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.002998043335633845 Training loss: 8.049196243286133
2025-12-09 12:04:13.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002997995331813262 Training loss: 8.615678787231445
2025-12-09 12:04:14.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.002997946746649937 Training loss: 7.981034755706787
2025-12-09 12:04:14.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0029978975801627245 Training loss: 7.689546585083008
2025-12-09 12:04:15.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0029978478323707046 Training loss: 7.913559913635254
2025-12-09 12:04:15.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0029977975032931844 Training loss: 7.723172664642334
2025-12-09 12:04:15.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.002997746592949695 Training loss: 7.924260139465332
2025-12-09 12:04:16.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.002997695101359994 Training loss: 8.46692943572998
2025-12-09 12:04:16.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.002997643028544064 Training loss: 8.002805709838867
2025-12-09 12:04:17.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0029975903745221143 Training loss: 7.722670555114746
2025-12-09 12:04:17.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.002997537139314578 Training loss: 7.953119277954102
2025-12-09 12:04:17.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0029974833229421145 Training loss: 8.118199348449707
2025-12-09 12:04:18.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002997428925425609 Training loss: 7.675201416015625
2025-12-09 12:04:18.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0029973739467861736 Training loss: 7.726465702056885
2025-12-09 12:04:18.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.002997318387045142 Training loss: 8.373549461364746
2025-12-09 12:04:19.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0029972622462240777 Training loss: 8.099431037902832
2025-12-09 12:04:19.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0029972055243447666 Training loss: 8.293815612792969
2025-12-09 12:04:20.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.002997148221429223 Training loss: 7.552394390106201
2025-12-09 12:04:20.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.002997090337499683 Training loss: 7.725776672363281
2025-12-09 12:04:20.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0029970318725786116 Training loss: 8.005001068115234
2025-12-09 12:04:21.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0029969728266886976 Training loss: 7.414003849029541
2025-12-09 12:04:21.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0029969131998528555 Training loss: 8.69561767578125
2025-12-09 12:04:22.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0029968529920942253 Training loss: 7.789858341217041
2025-12-09 12:04:22.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0029967922034361727 Training loss: 8.108770370483398
2025-12-09 12:04:22.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.002996730833902288 Training loss: 8.030241012573242
2025-12-09 12:04:23.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.002996668883516388 Training loss: 7.631700038909912
2025-12-09 12:04:23.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002996606352302514 Training loss: 8.18083381652832
2025-12-09 12:04:24.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.002996543240284934 Training loss: 7.555999755859375
2025-12-09 12:04:24.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0029964795474881397 Training loss: 7.8999738693237305
2025-12-09 12:04:24.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.002996415273936849 Training loss: 7.998547554016113
2025-12-09 12:04:25.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.002996350419656006 Training loss: 7.705105304718018
2025-12-09 12:04:25.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0029962849846707786 Training loss: 7.6308794021606445
2025-12-09 12:04:25.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0029962189690065613 Training loss: 8.414258003234863
2025-12-09 12:04:26.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.0029961523726889736 Training loss: 7.651503086090088
2025-12-09 12:04:26.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0029960851957438594 Training loss: 7.965803146362305
2025-12-09 12:04:27.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.00299601743819729 Training loss: 8.000645637512207
2025-12-09 12:04:27.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0029959491000755597 Training loss: 7.608913898468018
2025-12-09 12:04:27.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0029958801814051897 Training loss: 10.589278221130371
2025-12-09 12:04:28.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.002995810682212926 Training loss: 7.863589286804199
2025-12-09 12:04:28.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0029957406025257396 Training loss: 7.5029802322387695
2025-12-09 12:04:29.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.002995669942370827 Training loss: 8.384268760681152
2025-12-09 12:04:29.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0029955987017756106 Training loss: 8.2410306930542
2025-12-09 12:04:29.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0029955268807677375 Training loss: 7.86726188659668
2025-12-09 12:04:30.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.002995454479375079 Training loss: 8.017538070678711
2025-12-09 12:04:30.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0029953814976257337 Training loss: 7.823336601257324
2025-12-09 12:04:30.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.002995307935548024 Training loss: 8.36798095703125
2025-12-09 12:04:31.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.002995233793170498 Training loss: 7.569406509399414
2025-12-09 12:04:31.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0029951590705219284 Training loss: 7.785573959350586
2025-12-09 12:04:32.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0029950837676313144 Training loss: 7.390821933746338
2025-12-09 12:04:32.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0029950078845278794 Training loss: 7.918975830078125
2025-12-09 12:04:32.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0029949314212410717 Training loss: 7.939461708068848
2025-12-09 12:04:33.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0029948543778005655 Training loss: 7.736032485961914
2025-12-09 12:04:33.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00299477675423626 Training loss: 8.039152145385742
2025-12-09 12:04:34.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.002994698550578279 Training loss: 7.648166656494141
2025-12-09 12:04:34.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0029946197668569725 Training loss: 8.14023494720459
2025-12-09 12:04:34.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.002994540403102914 Training loss: 7.832481384277344
2025-12-09 12:04:35.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0029944604593469034 Training loss: 7.586228370666504
2025-12-09 12:04:35.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0029943799356199658 Training loss: 7.349318981170654
2025-12-09 12:04:36.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0029942988319533507 Training loss: 8.341889381408691
2025-12-09 12:04:36.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.002994217148378532 Training loss: 8.054177284240723
2025-12-09 12:04:36.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.002994134884927211 Training loss: 7.2953596115112305
2025-12-09 12:04:37.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.002994052041631311 Training loss: 7.475031852722168
2025-12-09 12:04:37.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0029939686185229825 Training loss: 7.728867530822754
2025-12-09 12:04:37.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0029938846156346006 Training loss: 7.805543422698975
2025-12-09 12:04:38.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.002993800032998765 Training loss: 7.852500915527344
2025-12-09 12:04:38.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.002993714870648301 Training loss: 8.021766662597656
2025-12-09 12:04:39.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0029936291286162577 Training loss: 7.51059627532959
2025-12-09 12:04:39.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00299354280693591 Training loss: 8.137961387634277
2025-12-09 12:04:39.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.002993455905640758 Training loss: 8.051043510437012
2025-12-09 12:04:40.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0029933684247645267 Training loss: 7.994422912597656
2025-12-09 12:04:40.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.002993280364341165 Training loss: 7.505424976348877
2025-12-09 12:04:41.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.002993191724404848 Training loss: 7.620665073394775
2025-12-09 12:04:41.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0029931025049899744 Training loss: 7.379814147949219
2025-12-09 12:04:41.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.002993012706131169 Training loss: 7.545896053314209
2025-12-09 12:04:42.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.002992922327863281 Training loss: 8.857083320617676
2025-12-09 12:04:42.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.002992831370221385 Training loss: 8.27722454071045
2025-12-09 12:04:42.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.002992739833240779 Training loss: 7.337457656860352
2025-12-09 12:04:43.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0029926477169569866 Training loss: 7.7116007804870605
2025-12-09 12:04:43.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0029925550214057565 Training loss: 8.003059387207031
2025-12-09 12:04:44.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.002992461746623063 Training loss: 7.640996932983398
2025-12-09 12:04:44.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0029923678926451034 Training loss: 7.822957992553711
2025-12-09 12:04:44.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.0029922734595083005 Training loss: 7.693032264709473
2025-12-09 12:04:45.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.002992178447249302 Training loss: 7.226501941680908
2025-12-09 12:04:45.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0029920828559049806 Training loss: 7.243802547454834
2025-12-09 12:04:46.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0029919866855124336 Training loss: 8.280625343322754
2025-12-09 12:04:46.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0029918899361089826 Training loss: 7.97306489944458
2025-12-09 12:04:46.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0029917926077321732 Training loss: 7.751775741577148
2025-12-09 12:04:47.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.002991694700419778 Training loss: 7.622353553771973
2025-12-09 12:04:47.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.002991596214209793 Training loss: 7.156530857086182
2025-12-09 12:04:48.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0029914971491404375 Training loss: 7.426365852355957
2025-12-09 12:04:48.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0029913975052501575 Training loss: 7.789122581481934
2025-12-09 12:04:48.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.002991297282577623 Training loss: 8.08400821685791
2025-12-09 12:04:49.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0029911964811617287 Training loss: 7.8817524909973145
2025-12-09 12:04:49.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0029910951010415927 Training loss: 7.866608619689941
2025-12-09 12:04:49.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0029909931422565593 Training loss: 8.159562110900879
2025-12-09 12:04:50.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0029908906048461965 Training loss: 8.992995262145996
2025-12-09 12:04:50.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.002990787488850297 Training loss: 8.103958129882812
2025-12-09 12:04:51.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0029906837943088787 Training loss: 7.6525115966796875
2025-12-09 12:04:51.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0029905795212621824 Training loss: 7.5293684005737305
2025-12-09 12:04:51.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.002990474669750676 Training loss: 7.2724609375
2025-12-09 12:04:52.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.002990369239815048 Training loss: 7.933836460113525
2025-12-09 12:04:52.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.002990263231496216 Training loss: 7.956457614898682
2025-12-09 12:04:53.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029901566448353183 Training loss: 8.278911590576172
2025-12-09 12:04:53.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029900494798737196 Training loss: 7.508092403411865
2025-12-09 12:04:53.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002989941736653009 Training loss: 7.729472637176514
2025-12-09 12:04:54.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.002989833415214999 Training loss: 7.539694786071777
2025-12-09 12:04:54.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0029897245156017267 Training loss: 7.680337905883789
2025-12-09 12:04:55.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.002989615037855454 Training loss: 8.135112762451172
2025-12-09 12:04:55.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0029895049820186682 Training loss: 7.6096673011779785
2025-12-09 12:04:55.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0029893943481340787 Training loss: 8.056486129760742
2025-12-09 12:04:56.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0029892831362446203 Training loss: 7.761537075042725
2025-12-09 12:04:56.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002989171346393453 Training loss: 7.46617317199707
2025-12-09 12:04:56.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00298905897862396 Training loss: 8.127654075622559
2025-12-09 12:04:57.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0029889460329797484 Training loss: 7.715699195861816
2025-12-09 12:04:57.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0029888325095046506 Training loss: 7.3484272956848145
2025-12-09 12:04:58.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0029887184082427226 Training loss: 7.831009387969971
2025-12-09 12:04:58.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002988603729238246 Training loss: 7.706952095031738
2025-12-09 12:04:58.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0029884884725357237 Training loss: 7.648706436157227
2025-12-09 12:04:59.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0029883726381798865 Training loss: 7.493594169616699
2025-12-09 12:04:59.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0029882562262156854 Training loss: 7.95682954788208
2025-12-09 12:05:00.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.002988139236688299 Training loss: 7.3254618644714355
2025-12-09 12:05:00.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0029880216696431287 Training loss: 8.394657135009766
2025-12-09 12:05:00.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0029879035251257993 Training loss: 7.174067497253418
2025-12-09 12:05:01.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.002987784803182161 Training loss: 7.8452324867248535
2025-12-09 12:05:01.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0029876655038582863 Training loss: 7.305748462677002
2025-12-09 12:05:01.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0029875456272004746 Training loss: 7.810461521148682
2025-12-09 12:05:02.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0029874251732552462 Training loss: 7.566861629486084
2025-12-09 12:05:02.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.002987304142069348 Training loss: 7.763727188110352
2025-12-09 12:05:03.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.002987182533689749 Training loss: 7.7091288566589355
2025-12-09 12:05:03.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0029870603481636443 Training loss: 7.600371360778809
2025-12-09 12:05:03.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0029869375855384505 Training loss: 7.76673698425293
2025-12-09 12:05:04.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0029868142458618096 Training loss: 7.615748405456543
2025-12-09 12:05:04.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0029866903291815875 Training loss: 7.688324451446533
2025-12-09 12:05:05.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.002986565835545874 Training loss: 7.422463893890381
2025-12-09 12:05:05.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0029864407650029823 Training loss: 7.844451904296875
2025-12-09 12:05:05.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00298631511760145 Training loss: 7.806727409362793
2025-12-09 12:05:06.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0029861888933900385 Training loss: 8.299793243408203
2025-12-09 12:05:06.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.002986062092417733 Training loss: 7.545504570007324
2025-12-09 12:05:07.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0029859347147337422 Training loss: 7.819276332855225
2025-12-09 12:05:07.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.002985806760387499 Training loss: 7.5803046226501465
2025-12-09 12:05:07.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00298567822942866 Training loss: 7.617293357849121
2025-12-09 12:05:08.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0029855491219071056 Training loss: 7.3759918212890625
2025-12-09 12:05:08.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00298541943787294 Training loss: 7.738917827606201
2025-12-09 12:05:08.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.002985289177376491 Training loss: 7.319075107574463
2025-12-09 12:05:09.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00298515834046831 Training loss: 7.487687587738037
2025-12-09 12:05:09.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.002985026927199172 Training loss: 8.2134370803833
2025-12-09 12:05:10.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0029848949376200767 Training loss: 6.907441139221191
2025-12-09 12:05:10.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0029847623717822462 Training loss: 7.682377338409424
2025-12-09 12:05:10.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0029846292297371264 Training loss: 7.180931568145752
2025-12-09 12:05:11.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.002984495511536388 Training loss: 7.741201877593994
2025-12-09 12:05:11.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0029843612172319235 Training loss: 7.836179256439209
2025-12-09 12:05:12.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.002984226346875851 Training loss: 7.574856758117676
2025-12-09 12:05:12.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0029840909005205093 Training loss: 8.042719841003418
2025-12-09 12:05:12.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.002983954878218464 Training loss: 7.548962593078613
2025-12-09 12:05:13.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.002983818280022502 Training loss: 7.876096248626709
2025-12-09 12:05:13.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0029836811059856354 Training loss: 7.249215602874756
2025-12-09 12:05:13.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0029835433561610975 Training loss: 7.597513675689697
2025-12-09 12:05:14.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0029834050306023468 Training loss: 8.307367324829102
2025-12-09 12:05:14.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0029832661293630646 Training loss: 7.978440761566162
2025-12-09 12:05:15.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.002983126652497156 Training loss: 7.653257369995117
2025-12-09 12:05:15.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.002982986600058749 Training loss: 7.481471061706543
2025-12-09 12:05:15.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0029828459721021965 Training loss: 7.179604530334473
2025-12-09 12:05:16.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.002982704768682071 Training loss: 7.169223308563232
2025-12-09 12:05:16.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0029825629898531728 Training loss: 7.490571975708008
2025-12-09 12:05:17.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.002982420635670523 Training loss: 7.63946533203125
2025-12-09 12:05:17.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.002982277706189366 Training loss: 7.1183671951293945
2025-12-09 12:05:17.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00298213420146517 Training loss: 8.199593544006348
2025-12-09 12:05:18.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0029819901215536273 Training loss: 7.291031360626221
2025-12-09 12:05:18.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.002981845466510651 Training loss: 7.349200248718262
2025-12-09 12:05:19.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0029817002363923804 Training loss: 7.419382572174072
2025-12-09 12:05:19.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.002981554431255176 Training loss: 7.87661600112915
2025-12-09 12:05:19.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.002981408051155621 Training loss: 7.564476013183594
2025-12-09 12:05:20.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.002981261096150524 Training loss: 7.582244873046875
2025-12-09 12:05:20.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.002981113566296915 Training loss: 7.408059120178223
2025-12-09 12:05:20.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0029809654616520464 Training loss: 8.064220428466797
2025-12-09 12:05:21.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.0029808167822733956 Training loss: 7.836197853088379
2025-12-09 12:05:21.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0029806675282186626 Training loss: 7.505942344665527
2025-12-09 12:05:22.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.002980517699545769 Training loss: 7.756376266479492
2025-12-09 12:05:22.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0029803672963128617 Training loss: 7.333209037780762
2025-12-09 12:05:22.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0029802163185783073 Training loss: 7.273781776428223
2025-12-09 12:05:23.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0029800647664006996 Training loss: 7.389610767364502
2025-12-09 12:05:23.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.002979912639838851 Training loss: 7.32506799697876
2025-12-09 12:05:24.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0029797599389518002 Training loss: 7.353185653686523
2025-12-09 12:05:24.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0029796066637988072 Training loss: 7.822593688964844
2025-12-09 12:05:24.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.002979452814439354 Training loss: 7.376995086669922
2025-12-09 12:05:25.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0029792983909331487 Training loss: 7.789217472076416
2025-12-09 12:05:25.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0029791433933401175 Training loss: 7.300959587097168
2025-12-09 12:05:26.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0029789878217204137 Training loss: 7.294981002807617
2025-12-09 12:05:26.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0029788316761344114 Training loss: 7.471282958984375
2025-12-09 12:05:26.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0029786749566427066 Training loss: 8.041264533996582
2025-12-09 12:05:27.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00297851766330612 Training loss: 8.354113578796387
2025-12-09 12:05:27.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.002978359796185695 Training loss: 7.608892440795898
2025-12-09 12:05:27.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0029782013553426943 Training loss: 7.225861549377441
2025-12-09 12:05:28.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0029780423408386075 Training loss: 6.952423572540283
2025-12-09 12:05:28.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0029778827527351445 Training loss: 7.2568888664245605
2025-12-09 12:05:29.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0029777225910942386 Training loss: 7.5060296058654785
2025-12-09 12:05:29.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.002977561855978045 Training loss: 7.765711784362793
2025-12-09 12:05:29.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.002977400547448942 Training loss: 7.726012229919434
2025-12-09 12:05:30.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0029772386655695306 Training loss: 7.302404880523682
2025-12-09 12:05:30.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0029770762104026336 Training loss: 7.625859260559082
2025-12-09 12:05:31.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.002976913182011297 Training loss: 7.466353893280029
2025-12-09 12:05:31.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0029767495804587886 Training loss: 7.536914348602295
2025-12-09 12:05:31.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.002976585405808599 Training loss: 7.389770030975342
2025-12-09 12:05:32.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0029764206581244412 Training loss: 7.626290321350098
2025-12-09 12:05:32.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.002976255337470251 Training loss: 7.309196949005127
2025-12-09 12:05:32.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.002976089443910186 Training loss: 7.403093338012695
2025-12-09 12:05:33.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0029759229775086255 Training loss: 7.339930534362793
2025-12-09 12:05:33.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0029757559383301727 Training loss: 7.68182373046875
2025-12-09 12:05:34.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.0029755883264396517 Training loss: 7.522730350494385
2025-12-09 12:05:34.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00297542014190211 Training loss: 7.131068706512451
2025-12-09 12:05:34.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0029752513847828162 Training loss: 7.468193054199219
2025-12-09 12:05:35.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0029750820551472617 Training loss: 8.001252174377441
2025-12-09 12:05:35.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0029749121530611602 Training loss: 7.436176300048828
2025-12-09 12:05:36.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0029747416785904472 Training loss: 7.045905590057373
2025-12-09 12:05:36.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.002974570631801281 Training loss: 7.395650386810303
2025-12-09 12:05:36.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0029743990127600413 Training loss: 7.311349868774414
2025-12-09 12:05:37.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.002974226821533329 Training loss: 7.1699934005737305
2025-12-09 12:05:37.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0029740540581879703 Training loss: 7.320450305938721
2025-12-09 12:05:38.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0029738807227910093 Training loss: 7.338700771331787
2025-12-09 12:05:38.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.002973706815409715 Training loss: 7.389975070953369
2025-12-09 12:05:38.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0029735323361115775 Training loss: 6.899746894836426
2025-12-09 12:05:39.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.002973357284964309 Training loss: 7.45779275894165
2025-12-09 12:05:39.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0029731816620358425 Training loss: 7.320603370666504
2025-12-09 12:05:39.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.002973005467394334 Training loss: 7.0971245765686035
2025-12-09 12:05:40.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0029728287011081627 Training loss: 7.585000038146973
2025-12-09 12:05:40.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.002972651363245927 Training loss: 8.003585815429688
2025-12-09 12:05:41.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.002972473453876448 Training loss: 7.5009918212890625
2025-12-09 12:05:41.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0029722949730687687 Training loss: 7.148714542388916
2025-12-09 12:05:41.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0029721159208921546 Training loss: 7.2114667892456055
2025-12-09 12:05:42.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.0029719362974160927 Training loss: 6.732142925262451
2025-12-09 12:05:42.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0029717561027102907 Training loss: 7.408398151397705
2025-12-09 12:05:43.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.002971575336844679 Training loss: 8.902434349060059
2025-12-09 12:05:43.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.002971393999889409 Training loss: 7.768752574920654
2025-12-09 12:05:43.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.002971212091914854 Training loss: 7.617345809936523
2025-12-09 12:05:44.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0029710296129916093 Training loss: 7.871537685394287
2025-12-09 12:05:44.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0029708465631904913 Training loss: 7.124324798583984
2025-12-09 12:05:45.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.002970662942582538 Training loss: 7.655190467834473
2025-12-09 12:05:45.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.002970478751239009 Training loss: 7.5186638832092285
2025-12-09 12:05:45.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0029702939892313853 Training loss: 7.78718900680542
2025-12-09 12:05:46.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.002970108656631369 Training loss: 7.751289367675781
2025-12-09 12:05:46.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.002969922753510885 Training loss: 7.154994010925293
2025-12-09 12:05:46.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.002969736279942078 Training loss: 7.428118705749512
2025-12-09 12:05:47.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.002969549235997315 Training loss: 7.405599117279053
2025-12-09 12:05:47.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.002969361621749184 Training loss: 7.333964824676514
2025-12-09 12:05:48.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.002969173437270495 Training loss: 7.346081733703613
2025-12-09 12:05:48.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0029689846826342773 Training loss: 7.726729393005371
2025-12-09 12:05:48.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.002968795357913784 Training loss: 7.237308025360107
2025-12-09 12:05:49.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0029686054631824885 Training loss: 7.451004505157471
2025-12-09 12:05:49.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0029684149985140847 Training loss: 7.9895806312561035
2025-12-09 12:05:50.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0029682239639824883 Training loss: 6.756377220153809
2025-12-09 12:05:50.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.002968032359661836 Training loss: 7.640073299407959
2025-12-09 12:05:50.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.002967840185626486 Training loss: 7.768279552459717
2025-12-09 12:05:51.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0029676474419510174 Training loss: 7.023130893707275
2025-12-09 12:05:51.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0029674541287102296 Training loss: 7.31964635848999
2025-12-09 12:05:51.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.002967260245979144 Training loss: 7.1235857009887695
2025-12-09 12:05:52.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.002967065793833003 Training loss: 7.395777702331543
2025-12-09 12:05:52.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.002966870772347269 Training loss: 7.232073783874512
2025-12-09 12:05:53.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0029666751815976273 Training loss: 6.5179924964904785
2025-12-09 12:05:53.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0029664790216599813 Training loss: 7.254438877105713
2025-12-09 12:05:53.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0029662822926104578 Training loss: 7.463017463684082
2025-12-09 12:05:54.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0029660849945254038 Training loss: 7.157426357269287
2025-12-09 12:05:54.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0029658871274813856 Training loss: 7.247989654541016
2025-12-09 12:05:55.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0029656886915551926 Training loss: 7.298454284667969
2025-12-09 12:05:55.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0029654896868238335 Training loss: 7.49432897567749
2025-12-09 12:05:55.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0029652901133645384 Training loss: 7.451444149017334
2025-12-09 12:05:56.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0029650899712547574 Training loss: 7.107768535614014
2025-12-09 12:05:56.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0029648892605721624 Training loss: 7.149296760559082
2025-12-09 12:05:57.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0029646879813946445 Training loss: 7.650585651397705
2025-12-09 12:05:57.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.002964486133800317 Training loss: 7.033329963684082
2025-12-09 12:05:57.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0029642837178675122 Training loss: 7.348543167114258
2025-12-09 12:05:58.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.002964080733674784 Training loss: 7.289059162139893
2025-12-09 12:05:58.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0029638771813009076 Training loss: 7.447580814361572
2025-12-09 12:05:58.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.002963673060824877 Training loss: 7.296175956726074
2025-12-09 12:05:59.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0029634683723259066 Training loss: 7.086608409881592
2025-12-09 12:05:59.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0029632631158834333 Training loss: 7.2053446769714355
2025-12-09 12:06:00.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0029630572915771117 Training loss: 7.334184169769287
2025-12-09 12:06:00.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.00296285089948682 Training loss: 7.1627960205078125
2025-12-09 12:06:00.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0029626439396926536 Training loss: 7.1093597412109375
2025-12-09 12:06:01.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0029624364122749296 Training loss: 7.300909996032715
2025-12-09 12:06:01.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0029622283173141866 Training loss: 7.191904544830322
2025-12-09 12:06:02.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00296201965489118 Training loss: 7.846867561340332
2025-12-09 12:06:02.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00296181042508689 Training loss: 7.014070510864258
2025-12-09 12:06:02.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.0029616006279825128 Training loss: 7.225979804992676
2025-12-09 12:06:03.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.002961390263659467 Training loss: 7.304464340209961
2025-12-09 12:06:03.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0029611793321993912 Training loss: 7.617890357971191
2025-12-09 12:06:03.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.0029609678336841444 Training loss: 7.23627233505249
2025-12-09 12:06:04.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0029607557681958037 Training loss: 7.489741325378418
2025-12-09 12:06:04.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0029605431358166686 Training loss: 7.032442569732666
2025-12-09 12:06:05.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.002960329936629257 Training loss: 7.046566963195801
2025-12-09 12:06:05.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.002960116170716308 Training loss: 7.50184965133667
2025-12-09 12:06:05.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0029599018381607787 Training loss: 6.89267635345459
2025-12-09 12:06:06.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0029596869390458485 Training loss: 7.305286884307861
2025-12-09 12:06:06.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.002959471473454915 Training loss: 7.488260746002197
2025-12-09 12:06:07.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0029592554414715967 Training loss: 7.347727298736572
2025-12-09 12:06:07.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.002959038843179731 Training loss: 7.846968173980713
2025-12-09 12:06:07.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0029588216786633763 Training loss: 7.436587333679199
2025-12-09 12:06:08.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0029586039480068087 Training loss: 7.119050025939941
2025-12-09 12:06:08.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0029583856512945257 Training loss: 7.155237197875977
2025-12-09 12:06:09.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0029581667886112435 Training loss: 7.19290828704834
2025-12-09 12:06:09.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0029579473600418998 Training loss: 7.2276411056518555
2025-12-09 12:06:09.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0029577273656716495 Training loss: 7.408811092376709
2025-12-09 12:06:10.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0029575068055858675 Training loss: 8.033695220947266
2025-12-09 12:06:10.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.0029572856798701507 Training loss: 7.524200916290283
2025-12-09 12:06:10.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0029570639886103123 Training loss: 7.431853771209717
2025-12-09 12:06:11.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0029568417318923865 Training loss: 7.144318580627441
2025-12-09 12:06:11.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.002956618909802627 Training loss: 7.884860038757324
2025-12-09 12:06:12.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0029563955224275068 Training loss: 7.166338920593262
2025-12-09 12:06:12.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0029561715698537185 Training loss: 7.283447742462158
2025-12-09 12:06:12.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0029559470521681726 Training loss: 7.237269878387451
2025-12-09 12:06:13.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.002955721969458001 Training loss: 7.195850849151611
2025-12-09 12:06:13.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0029554963218105536 Training loss: 8.375782012939453
2025-12-09 12:06:14.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0029552701093133998 Training loss: 7.225215911865234
2025-12-09 12:06:14.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0029550433320543286 Training loss: 7.370856285095215
2025-12-09 12:06:14.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0029548159901213473 Training loss: 7.209486961364746
2025-12-09 12:06:15.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.0029545880836026835 Training loss: 6.958374977111816
2025-12-09 12:06:15.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0029543596125867827 Training loss: 7.23484468460083
2025-12-09 12:06:16.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00295413057716231 Training loss: 6.9928812980651855
2025-12-09 12:06:16.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.00295390097741815 Training loss: 7.421380996704102
2025-12-09 12:06:16.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0029536708134434058 Training loss: 6.920261859893799
2025-12-09 12:06:17.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.002953440085327399 Training loss: 6.782752513885498
2025-12-09 12:06:17.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0029532087931596718 Training loss: 7.15894889831543
2025-12-09 12:06:17.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0029529769370299826 Training loss: 7.720632553100586
2025-12-09 12:06:18.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0029527445170283114 Training loss: 7.119885444641113
2025-12-09 12:06:18.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.002952511533244856 Training loss: 7.73104190826416
2025-12-09 12:06:19.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0029522779857700326 Training loss: 7.2258992195129395
2025-12-09 12:06:19.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0029520438746944754 Training loss: 7.290097713470459
2025-12-09 12:06:19.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00295180920010904 Training loss: 7.07147216796875
2025-12-09 12:06:20.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.0029515739621047976 Training loss: 6.9002604484558105
2025-12-09 12:06:20.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0029513381607730402 Training loss: 7.172797203063965
2025-12-09 12:06:21.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.002951101796205278 Training loss: 7.124924182891846
2025-12-09 12:06:21.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0029508648684932392 Training loss: 7.275188446044922
2025-12-09 12:06:21.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0029506273777288703 Training loss: 7.34880256652832
2025-12-09 12:06:22.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.002950389324004337 Training loss: 6.855504035949707
2025-12-09 12:06:22.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.002950150707412024 Training loss: 7.142389297485352
2025-12-09 12:06:22.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.002949911528044533 Training loss: 7.38575553894043
2025-12-09 12:06:23.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0029496717859946848 Training loss: 7.619508266448975
2025-12-09 12:06:23.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0029494314813555194 Training loss: 6.3888163566589355
2025-12-09 12:06:24.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.002949190614220294 Training loss: 7.212879180908203
2025-12-09 12:06:24.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.002948949184682484 Training loss: 7.439016342163086
2025-12-09 12:06:24.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0029487071928357834 Training loss: 7.092256546020508
2025-12-09 12:06:25.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0029484646387741057 Training loss: 7.333472728729248
2025-12-09 12:06:25.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0029482215225915803 Training loss: 7.116616725921631
2025-12-09 12:06:26.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0029479778443825553 Training loss: 7.2615790367126465
2025-12-09 12:06:26.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.002947733604241599 Training loss: 6.992416858673096
2025-12-09 12:06:26.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.002947488802263496 Training loss: 8.073732376098633
2025-12-09 12:06:27.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0029472434385432477 Training loss: 7.128294944763184
2025-12-09 12:06:27.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0029469975131760765 Training loss: 7.354494571685791
2025-12-09 12:06:28.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0029467510262574203 Training loss: 7.371677875518799
2025-12-09 12:06:28.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0029465039778829366 Training loss: 6.922715663909912
2025-12-09 12:06:28.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0029462563681484995 Training loss: 7.180916786193848
2025-12-09 12:06:29.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.002946008197150202 Training loss: 7.357366561889648
2025-12-09 12:06:29.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0029457594649843536 Training loss: 7.437880992889404
2025-12-09 12:06:29.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0029455101717474836 Training loss: 7.210668563842773
2025-12-09 12:06:30.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0029452603175363365 Training loss: 7.654489040374756
2025-12-09 12:06:30.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0029450099024478766 Training loss: 7.166394233703613
2025-12-09 12:06:31.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.002944758926579285 Training loss: 6.847701549530029
2025-12-09 12:06:31.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.002944507390027961 Training loss: 7.345418453216553
2025-12-09 12:06:31.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0029442552928915203 Training loss: 7.088543891906738
2025-12-09 12:06:32.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.002944002635267797 Training loss: 7.093252182006836
2025-12-09 12:06:32.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.002943749417254843 Training loss: 7.03333044052124
2025-12-09 12:06:33.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0029434956389509264 Training loss: 7.18618631362915
2025-12-09 12:06:33.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0029432413004545346 Training loss: 7.330366134643555
2025-12-09 12:06:33.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.002942986401864371 Training loss: 7.33860445022583
2025-12-09 12:06:34.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.002942730943279357 Training loss: 7.507803916931152
2025-12-09 12:06:34.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0029424749247986305 Training loss: 7.973235130310059
2025-12-09 12:06:35.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.002942218346521548 Training loss: 7.070809364318848
2025-12-09 12:06:35.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.0029419612085476816 Training loss: 7.292600631713867
2025-12-09 12:06:35.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0029417035109768224 Training loss: 7.390521049499512
2025-12-09 12:06:36.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.002941445253908978 Training loss: 7.676778793334961
2025-12-09 12:06:36.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.0029411864374443717 Training loss: 7.302097320556641
2025-12-09 12:06:36.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.002940927061683446 Training loss: 7.241367340087891
2025-12-09 12:06:37.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.002940667126726859 Training loss: 7.251165390014648
2025-12-09 12:06:37.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0029404066326754875 Training loss: 7.566499710083008
2025-12-09 12:06:38.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0029401455796304234 Training loss: 7.087713241577148
2025-12-09 12:06:38.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0029398839676929756 Training loss: 7.299651622772217
2025-12-09 12:06:38.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.002939621796964672 Training loss: 7.326873302459717
2025-12-09 12:06:39.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.002939359067547255 Training loss: 6.9446516036987305
2025-12-09 12:06:39.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.002939095779542685 Training loss: 6.850310802459717
2025-12-09 12:06:40.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0029388319330531385 Training loss: 6.803328037261963
2025-12-09 12:06:40.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0029385675281810106 Training loss: 7.232563495635986
2025-12-09 12:06:40.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.00293830256502891 Training loss: 7.513540267944336
2025-12-09 12:06:41.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0029380370436996642 Training loss: 7.026556015014648
2025-12-09 12:06:41.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0029377709642963174 Training loss: 7.124156475067139
2025-12-09 12:06:41.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0029375043269221296 Training loss: 7.421077728271484
2025-12-09 12:06:42.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.002937237131680577 Training loss: 7.1109299659729
2025-12-09 12:06:42.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.002936969378675354 Training loss: 6.828606128692627
2025-12-09 12:06:43.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0029367010680103685 Training loss: 6.760356426239014
2025-12-09 12:06:43.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0029364321997897482 Training loss: 7.164602756500244
2025-12-09 12:06:43.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0029361627741178358 Training loss: 7.13765811920166
2025-12-09 12:06:44.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.002935892791099189 Training loss: 7.403748512268066
2025-12-09 12:06:44.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.002935622250838583 Training loss: 7.251437187194824
2025-12-09 12:06:45.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0029353511534410104 Training loss: 7.149361610412598
2025-12-09 12:06:45.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.002935079499011677 Training loss: 7.264241695404053
2025-12-09 12:06:45.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0029348072876560086 Training loss: 7.174276828765869
2025-12-09 12:06:46.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0029345345194796437 Training loss: 6.982966423034668
2025-12-09 12:06:46.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0029342611945884388 Training loss: 7.1624627113342285
2025-12-09 12:06:47.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0029339873130884656 Training loss: 7.153261184692383
2025-12-09 12:06:47.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0029337128750860125 Training loss: 6.860299110412598
2025-12-09 12:06:47.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0029334378806875837 Training loss: 7.186639785766602
2025-12-09 12:06:48.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.002933162329999899 Training loss: 7.285192012786865
2025-12-09 12:06:48.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.002932886223129894 Training loss: 7.806605339050293
2025-12-09 12:06:48.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0029326095601847203 Training loss: 6.94173002243042
2025-12-09 12:06:49.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0029323323412717463 Training loss: 7.13330602645874
2025-12-09 12:06:49.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0029320545664985537 Training loss: 7.086843490600586
2025-12-09 12:06:50.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0029317762359729427 Training loss: 7.030515193939209
2025-12-09 12:06:50.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.002931497349802928 Training loss: 7.6529412269592285
2025-12-09 12:06:50.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.002931217908096739 Training loss: 7.423081398010254
2025-12-09 12:06:51.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0029309379109628223 Training loss: 7.398996353149414
2025-12-09 12:06:51.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0029306573585098387 Training loss: 7.409176349639893
2025-12-09 12:06:52.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0029303762508466656 Training loss: 7.090560436248779
2025-12-09 12:06:52.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0029300945880823956 Training loss: 7.277975559234619
2025-12-09 12:06:52.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0029298123703263364 Training loss: 7.248349666595459
2025-12-09 12:06:53.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.002929529597688011 Training loss: 7.543102264404297
2025-12-09 12:06:53.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0029292462702771574 Training loss: 7.105649948120117
2025-12-09 12:06:54.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0029289623882037302 Training loss: 7.066293716430664
2025-12-09 12:06:54.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0029286779515778983 Training loss: 7.199514389038086
2025-12-09 12:06:54.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0029283929605100458 Training loss: 7.065055847167969
2025-12-09 12:06:55.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0029281074151107727 Training loss: 7.4436750411987305
2025-12-09 12:06:55.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.002927821315490893 Training loss: 7.350640773773193
2025-12-09 12:06:55.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0029275346617614363 Training loss: 7.0805816650390625
2025-12-09 12:06:56.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.002927247454033648 Training loss: 7.306181907653809
2025-12-09 12:06:56.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.002926959692418988 Training loss: 6.839278697967529
2025-12-09 12:06:57.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0029266713770291293 Training loss: 6.8165740966796875
2025-12-09 12:06:57.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0029263825079759638 Training loss: 7.676717758178711
2025-12-09 12:06:57.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0029260930853715937 Training loss: 6.956326007843018
2025-12-09 12:06:58.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0029258031093283396 Training loss: 7.638692378997803
2025-12-09 12:06:58.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0029255125799587355 Training loss: 6.9899396896362305
2025-12-09 12:06:59.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.002925221497375529 Training loss: 7.13065767288208
2025-12-09 12:06:59.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0029249298616916856 Training loss: 6.8950114250183105
2025-12-09 12:06:59.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.002924637673020382 Training loss: 7.289898872375488
2025-12-09 12:07:00.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.002924344931475011 Training loss: 6.83799409866333
2025-12-09 12:07:00.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0029240516371691803 Training loss: 7.566865921020508
2025-12-09 12:07:00.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.002923757790216711 Training loss: 7.742667198181152
2025-12-09 12:07:01.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0029234633907316405 Training loss: 7.286265850067139
2025-12-09 12:07:01.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0029231684388282184 Training loss: 7.028067588806152
2025-12-09 12:07:02.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00292287293462091 Training loss: 7.12000846862793
2025-12-09 12:07:02.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0029225768782243956 Training loss: 7.166844844818115
2025-12-09 12:07:02.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0029222802697535678 Training loss: 7.464938163757324
2025-12-09 12:07:03.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.002921983109323535 Training loss: 7.2643818855285645
2025-12-09 12:07:03.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0029216853970496196 Training loss: 7.4419097900390625
2025-12-09 12:07:04.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0029213871330473575 Training loss: 7.8969621658325195
2025-12-09 12:07:04.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.002921088317432499 Training loss: 7.174073219299316
2025-12-09 12:07:04.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0029207889503210095 Training loss: 6.9609808921813965
2025-12-09 12:07:05.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.002920489031829067 Training loss: 7.06082010269165
2025-12-09 12:07:05.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.002920188562073063 Training loss: 7.316932201385498
2025-12-09 12:07:06.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0029198875411696056 Training loss: 6.999670505523682
2025-12-09 12:07:06.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0029195859692355145 Training loss: 7.121528148651123
2025-12-09 12:07:06.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.002919283846387824 Training loss: 7.2197957038879395
2025-12-09 12:07:07.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0029189811727437813 Training loss: 7.164552688598633
2025-12-09 12:07:07.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.002918677948420849 Training loss: 7.0397515296936035
2025-12-09 12:07:07.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0029183741735367024 Training loss: 6.808945178985596
2025-12-09 12:07:08.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0029180698482092304 Training loss: 7.168923377990723
2025-12-09 12:07:08.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0029177649725565355 Training loss: 7.102568626403809
2025-12-09 12:07:09.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0029174595466969345 Training loss: 7.117680072784424
2025-12-09 12:07:09.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0029171535707489572 Training loss: 7.3757452964782715
2025-12-09 12:07:09.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0029168470448313463 Training loss: 7.367024898529053
2025-12-09 12:07:10.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.002916539969063059 Training loss: 7.94282341003418
2025-12-09 12:07:10.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0029162323435632654 Training loss: 6.733090400695801
2025-12-09 12:07:11.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.002915924168451349 Training loss: 7.2844743728637695
2025-12-09 12:07:11.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.002915615443846906 Training loss: 7.081639289855957
2025-12-09 12:07:11.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0029153061698697475 Training loss: 7.375520706176758
2025-12-09 12:07:12.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0029149963466398956 Training loss: 7.260066986083984
2025-12-09 12:07:12.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.002914685974277587 Training loss: 7.250692844390869
2025-12-09 12:07:13.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.002914375052903271 Training loss: 7.169824123382568
2025-12-09 12:07:13.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.002914063582637611 Training loss: 7.417738437652588
2025-12-09 12:07:13.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.002913751563601481 Training loss: 7.178385257720947
2025-12-09 12:07:14.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0029134389959159708 Training loss: 6.909395217895508
2025-12-09 12:07:14.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0029131258797023816 Training loss: 7.236818313598633
2025-12-09 12:07:14.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0029128122150822266 Training loss: 7.195979595184326
2025-12-09 12:07:15.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0029124980021772344 Training loss: 7.399289131164551
2025-12-09 12:07:15.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0029121832411093443 Training loss: 6.9441752433776855
2025-12-09 12:07:16.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0029118679320007087 Training loss: 7.372409343719482
2025-12-09 12:07:16.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0029115520749736935 Training loss: 7.114468574523926
2025-12-09 12:07:16.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0029112356701508756 Training loss: 7.333223819732666
2025-12-09 12:07:17.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0029109187176550463 Training loss: 7.319642066955566
2025-12-09 12:07:17.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0029106012176092085 Training loss: 7.290993690490723
2025-12-09 12:07:18.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0029102831701365785 Training loss: 7.1789631843566895
2025-12-09 12:07:18.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0029099645753605827 Training loss: 7.095932960510254
2025-12-09 12:07:18.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.002909645433404863 Training loss: 7.291586875915527
2025-12-09 12:07:19.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0029093257443932713 Training loss: 6.7524847984313965
2025-12-09 12:07:19.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0029090055084498734 Training loss: 7.645443439483643
2025-12-09 12:07:19.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0029086847256989457 Training loss: 6.90233039855957
2025-12-09 12:07:20.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0029083633962649785 Training loss: 7.306560516357422
2025-12-09 12:07:20.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0029080415202726727 Training loss: 7.282541275024414
2025-12-09 12:07:21.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0029077190978469432 Training loss: 7.126474857330322
2025-12-09 12:07:21.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0029073961291129153 Training loss: 7.162455081939697
2025-12-09 12:07:21.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0029070726141959265 Training loss: 6.929574012756348
2025-12-09 12:07:22.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.002906748553221527 Training loss: 7.673549652099609
2025-12-09 12:07:22.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0029064239463154782 Training loss: 6.829510688781738
2025-12-09 12:07:23.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.002906098793603754 Training loss: 7.009186744689941
2025-12-09 12:07:23.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.00290577309521254 Training loss: 7.394894599914551
2025-12-09 12:07:23.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.002905446851268233 Training loss: 7.065290927886963
2025-12-09 12:07:24.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.002905120061897442 Training loss: 7.316622257232666
2025-12-09 12:07:24.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.002904792727226987 Training loss: 6.57033634185791
2025-12-09 12:07:25.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.002904464847383902 Training loss: 7.300996780395508
2025-12-09 12:07:25.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.002904136422495429 Training loss: 6.92333459854126
2025-12-09 12:07:25.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0029038074526890243 Training loss: 7.121811389923096
2025-12-09 12:07:26.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.002903477938092354 Training loss: 7.183277130126953
2025-12-09 12:07:26.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.002903147878833296 Training loss: 6.963923454284668
2025-12-09 12:07:26.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.002902817275039941 Training loss: 7.299398899078369
2025-12-09 12:07:27.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0029024861268405894 Training loss: 7.058904647827148
2025-12-09 12:07:27.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0029021544343637525 Training loss: 7.815908908843994
2025-12-09 12:07:28.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0029018221977381554 Training loss: 7.220495223999023
2025-12-09 12:07:28.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0029014894170927307 Training loss: 7.150101184844971
2025-12-09 12:07:28.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0029011560925566253 Training loss: 7.024643421173096
2025-12-09 12:07:29.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.002900822224259196 Training loss: 6.723631381988525
2025-12-09 12:07:29.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0029004878123300095 Training loss: 7.204756259918213
