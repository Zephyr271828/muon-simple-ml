2025-12-09 12:07:48.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 4.976489067077637
2025-12-09 12:07:48.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 4.871828079223633
2025-12-09 12:07:48.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 4.833873271942139
2025-12-09 12:07:48.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 4.796751976013184
2025-12-09 12:07:48.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 4.9385504722595215
2025-12-09 12:07:48.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 4.71798849105835
2025-12-09 12:07:48.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 4.780460834503174
2025-12-09 12:07:48.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 4.777891635894775
2025-12-09 12:07:48.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 4.770389556884766
2025-12-09 12:07:48.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 4.764874458312988
2025-12-09 12:07:48.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 4.824403762817383
2025-12-09 12:07:48.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 4.593798637390137
2025-12-09 12:07:48.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 4.676577568054199
2025-12-09 12:07:48.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 4.592828273773193
2025-12-09 12:07:48.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 4.62206506729126
2025-12-09 12:07:48.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 4.566178321838379
2025-12-09 12:07:48.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 4.4727067947387695
2025-12-09 12:07:48.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 4.373369216918945
2025-12-09 12:07:48.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 4.412664890289307
2025-12-09 12:07:48.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 4.519961833953857
2025-12-09 12:07:48.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 4.466067790985107
2025-12-09 12:07:48.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 4.522732257843018
2025-12-09 12:07:48.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 4.431121349334717
2025-12-09 12:07:48.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 4.385066986083984
2025-12-09 12:07:48.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 4.540459632873535
2025-12-09 12:07:48.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 4.4341959953308105
2025-12-09 12:07:48.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 4.327855110168457
2025-12-09 12:07:48.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 4.465521335601807
2025-12-09 12:07:48.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 4.40708589553833
2025-12-09 12:07:48.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 4.438540935516357
2025-12-09 12:07:48.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 4.331910610198975
2025-12-09 12:07:48.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 4.305478572845459
2025-12-09 12:07:48.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 4.286745548248291
2025-12-09 12:07:48.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 4.270376682281494
2025-12-09 12:07:48.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 4.195029258728027
2025-12-09 12:07:48.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 4.151246547698975
2025-12-09 12:07:48.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 4.074104309082031
2025-12-09 12:07:48.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 4.164137363433838
2025-12-09 12:07:48.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 4.202669620513916
2025-12-09 12:07:48.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 4.164398193359375
2025-12-09 12:07:48.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 4.131859302520752
2025-12-09 12:07:48.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 3.944741725921631
2025-12-09 12:07:48.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 4.134754180908203
2025-12-09 12:07:48.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 4.083258628845215
2025-12-09 12:07:48.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 4.041276931762695
2025-12-09 12:07:48.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 4.092491149902344
2025-12-09 12:07:48.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 4.25663423538208
2025-12-09 12:07:48.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 4.144016265869141
2025-12-09 12:07:48.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 3.987274169921875
2025-12-09 12:07:48.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 3.9252512454986572
2025-12-09 12:07:48.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 3.997861385345459
2025-12-09 12:07:48.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 4.01151704788208
2025-12-09 12:07:49.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 4.1146697998046875
2025-12-09 12:07:49.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 4.101168632507324
2025-12-09 12:07:49.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 4.044936656951904
2025-12-09 12:07:49.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 4.238252639770508
2025-12-09 12:07:49.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 4.293873310089111
2025-12-09 12:07:49.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 3.9733498096466064
2025-12-09 12:07:49.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 4.067327976226807
2025-12-09 12:07:49.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 4.072841644287109
2025-12-09 12:07:49.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 3.7088842391967773
2025-12-09 12:07:49.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 3.972538948059082
2025-12-09 12:07:49.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 4.171774387359619
2025-12-09 12:07:49.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 4.108836650848389
2025-12-09 12:07:49.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 4.00313138961792
2025-12-09 12:07:49.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 3.922978639602661
2025-12-09 12:07:49.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 3.979597806930542
2025-12-09 12:07:49.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 4.070082664489746
2025-12-09 12:07:49.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 3.8773226737976074
2025-12-09 12:07:49.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 3.997455596923828
2025-12-09 12:07:49.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 4.16917085647583
2025-12-09 12:07:49.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 4.026566028594971
2025-12-09 12:07:49.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 4.110862731933594
2025-12-09 12:07:49.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 3.8913488388061523
2025-12-09 12:07:49.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 3.8904716968536377
2025-12-09 12:07:49.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 4.198330402374268
2025-12-09 12:07:49.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 3.949608325958252
2025-12-09 12:07:49.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 3.9902842044830322
2025-12-09 12:07:49.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 3.9625163078308105
2025-12-09 12:07:49.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 3.9247171878814697
2025-12-09 12:07:49.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 3.9613633155822754
2025-12-09 12:07:49.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 3.936459541320801
2025-12-09 12:07:49.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 3.9434335231781006
2025-12-09 12:07:49.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 4.003429889678955
2025-12-09 12:07:49.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 4.045229911804199
2025-12-09 12:07:49.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 4.1507768630981445
2025-12-09 12:07:49.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 3.9612579345703125
2025-12-09 12:07:49.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 4.141915798187256
2025-12-09 12:07:49.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 4.057850360870361
2025-12-09 12:07:49.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 3.9975638389587402
2025-12-09 12:07:49.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 3.9337759017944336
2025-12-09 12:07:49.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 3.741051197052002
2025-12-09 12:07:49.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 3.8111133575439453
2025-12-09 12:07:49.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 3.90574049949646
2025-12-09 12:07:49.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 3.8241982460021973
2025-12-09 12:07:49.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 4.099527835845947
2025-12-09 12:07:49.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 4.217360973358154
2025-12-09 12:07:49.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 3.933560848236084
2025-12-09 12:07:49.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 3.8104746341705322
2025-12-09 12:07:49.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 3.862635850906372
2025-12-09 12:07:49.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999125880491853 Training loss: 3.8321566581726074
2025-12-09 12:07:49.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0029996503623845394 Training loss: 3.9852561950683594
2025-12-09 12:07:49.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029992133535682725 Training loss: 3.91964054107666
2025-12-09 12:07:49.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.002998601612533441 Training loss: 3.9017205238342285
2025-12-09 12:07:49.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0029978152105780156 Training loss: 3.7395377159118652
2025-12-09 12:07:49.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0029968542393565677 Training loss: 3.8708083629608154
2025-12-09 12:07:49.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029957188108695894 Training loss: 4.130891799926758
2025-12-09 12:07:49.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00299440905745044 Training loss: 3.8461875915527344
2025-12-09 12:07:49.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.002992925131749921 Training loss: 3.9801337718963623
2025-12-09 12:07:49.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0029912672067184862 Training loss: 4.060948371887207
2025-12-09 12:07:49.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0029894354755860848 Training loss: 3.867499351501465
2025-12-09 12:07:49.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0029874301518396378 Training loss: 3.798621654510498
2025-12-09 12:07:49.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029852514691981603 Training loss: 3.9012553691864014
2025-12-09 12:07:49.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029828996815855183 Training loss: 3.852998733520508
2025-12-09 12:07:49.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.002980375063100836 Training loss: 3.7366113662719727
2025-12-09 12:07:49.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00297767790798655 Training loss: 3.8473942279815674
2025-12-09 12:07:49.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.0029748085305941127 Training loss: 3.7413647174835205
2025-12-09 12:07:49.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.002971767265347359 Training loss: 3.5863656997680664
2025-12-09 12:07:49.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029685544667035256 Training loss: 3.7863004207611084
2025-12-09 12:07:49.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0029651705091119423 Training loss: 3.8103599548339844
2025-12-09 12:07:49.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0029616157869703894 Training loss: 3.838618040084839
2025-12-09 12:07:49.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002957890714579128 Training loss: 3.9554972648620605
2025-12-09 12:07:49.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0029539957260926184 Training loss: 3.5868284702301025
2025-12-09 12:07:49.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.002949931275468917 Training loss: 3.9129018783569336
2025-12-09 12:07:49.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029456978364167667 Training loss: 3.7272021770477295
2025-12-09 12:07:49.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0029412959023403904 Training loss: 3.950617551803589
2025-12-09 12:07:49.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.002936725986281981 Training loss: 3.8272273540496826
2025-12-09 12:07:49.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.002931988620861908 Training loss: 3.837439775466919
2025-12-09 12:07:49.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.002927084358216643 Training loss: 3.726689100265503
2025-12-09 12:07:49.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0029220137699344055 Training loss: 3.6944782733917236
2025-12-09 12:07:49.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0029167774469885483 Training loss: 3.8530449867248535
2025-12-09 12:07:49.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002911375999668675 Training loss: 3.7947661876678467
2025-12-09 12:07:49.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.002905810057509516 Training loss: 3.6937084197998047
2025-12-09 12:07:49.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.002900080269217554 Training loss: 3.8164548873901367
2025-12-09 12:07:49.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.002894187302595419 Training loss: 3.641418933868408
2025-12-09 12:07:49.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0028881318444640564 Training loss: 3.855508327484131
2025-12-09 12:07:49.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0028819146005826766 Training loss: 3.802382469177246
2025-12-09 12:07:49.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0028755362955665015 Training loss: 3.8086493015289307
2025-12-09 12:07:49.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0028689976728023103 Training loss: 3.863269805908203
2025-12-09 12:07:49.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0028622994943617985 Training loss: 3.9153971672058105
2025-12-09 12:07:49.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0028554425409127583 Training loss: 3.833392858505249
2025-12-09 12:07:49.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.002848427611628093 Training loss: 3.668972969055176
2025-12-09 12:07:49.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0028412555240926746 Training loss: 3.7550246715545654
2025-12-09 12:07:49.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.002833927114208054 Training loss: 3.79451322555542
2025-12-09 12:07:49.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0028264432360950355 Training loss: 3.7942068576812744
2025-12-09 12:07:49.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0028188047619941343 Training loss: 3.856816530227661
2025-12-09 12:07:49.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0028110125821639137 Training loss: 3.8927440643310547
2025-12-09 12:07:49.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.002803067604777227 Training loss: 3.9720232486724854
2025-12-09 12:07:49.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0027949707558153703 Training loss: 3.807439088821411
2025-12-09 12:07:49.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.002786722978960162 Training loss: 3.986947536468506
2025-12-09 12:07:49.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.002778325235483954 Training loss: 3.92338490486145
2025-12-09 12:07:49.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0027697785041376007 Training loss: 3.5982165336608887
2025-12-09 12:07:49.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.002761083781036381 Training loss: 3.842644453048706
2025-12-09 12:07:49.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.002752242079543907 Training loss: 3.6788625717163086
2025-12-09 12:07:49.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002743254430154012 Training loss: 3.8541183471679688
2025-12-09 12:07:49.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.002734121880370652 Training loss: 3.9690964221954346
2025-12-09 12:07:49.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0027248454945858164 Training loss: 3.681166887283325
2025-12-09 12:07:49.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0027154263539554764 Training loss: 3.7886128425598145
2025-12-09 12:07:49.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.002705865556273575 Training loss: 3.6888587474823
2025-12-09 12:07:50.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.002696164215844081 Training loss: 3.484968900680542
2025-12-09 12:07:50.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0026863234633511188 Training loss: 3.666774272918701
2025-12-09 12:07:50.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0026763444457271837 Training loss: 3.6661553382873535
2025-12-09 12:07:50.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.002666228326019474 Training loss: 3.681952476501465
2025-12-09 12:07:50.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.002655976283254334 Training loss: 3.438758134841919
2025-12-09 12:07:50.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0026455895122998404 Training loss: 3.7454261779785156
2025-12-09 12:07:50.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002635069223726543 Training loss: 3.5808801651000977
2025-12-09 12:07:50.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.002624416643666371 Training loss: 3.743238925933838
2025-12-09 12:07:50.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0026136330136697305 Training loss: 3.75632381439209
2025-12-09 12:07:50.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0026027195905608006 Training loss: 3.607544183731079
2025-12-09 12:07:50.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0025916776462910542 Training loss: 3.4888787269592285
2025-12-09 12:07:50.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00258050846779101 Training loss: 3.551607131958008
2025-12-09 12:07:50.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0025692133568202442 Training loss: 3.6547861099243164
2025-12-09 12:07:50.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.002557793629815669 Training loss: 3.627504825592041
2025-12-09 12:07:50.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0025462506177381045 Training loss: 3.5860109329223633
2025-12-09 12:07:50.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0025345856659171567 Training loss: 3.698561668395996
2025-12-09 12:07:50.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002522800133894418 Training loss: 3.6210241317749023
2025-12-09 12:07:50.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0025108953952650164 Training loss: 3.671583414077759
2025-12-09 12:07:50.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0024988728375175216 Training loss: 3.8112857341766357
2025-12-09 12:07:50.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.002486733861872236 Training loss: 3.6273434162139893
2025-12-09 12:07:50.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0024744798831178817 Training loss: 3.6662676334381104
2025-12-09 12:07:50.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0024621123294467097 Training loss: 3.6455135345458984
2025-12-09 12:07:50.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.002449632642288045 Training loss: 4.05660343170166
2025-12-09 12:07:50.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002437042276140287 Training loss: 3.6306304931640625
2025-12-09 12:07:50.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0024243426984013913 Training loss: 3.6869640350341797
2025-12-09 12:07:50.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0024115353891978435 Training loss: 3.7590556144714355
2025-12-09 12:07:50.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.002398621841212154 Training loss: 3.801476001739502
2025-12-09 12:07:50.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.002385603559508884 Training loss: 3.657881498336792
2025-12-09 12:07:50.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.002372482061359234 Training loss: 3.627887487411499
2025-12-09 12:07:50.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0023592588760642046 Training loss: 3.8806252479553223
2025-12-09 12:07:50.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00234593554477636 Training loss: 3.7899057865142822
2025-12-09 12:07:50.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.002332513620320205 Training loss: 3.545201539993286
2025-12-09 12:07:50.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.002318994667011207 Training loss: 3.51725435256958
2025-12-09 12:07:50.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.002305380260473476 Training loss: 3.5522685050964355
2025-12-09 12:07:50.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002291671987456123 Training loss: 3.3618643283843994
2025-12-09 12:07:50.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0022778714456483324 Training loss: 3.755150079727173
2025-12-09 12:07:50.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0022639802434931446 Training loss: 3.6214025020599365
2025-12-09 12:07:50.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0022500000000000003 Training loss: 3.8280460834503174
2025-12-09 12:07:50.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0022359323445560408 Training loss: 3.423414468765259
2025-12-09 12:07:50.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0022217789167362076 Training loss: 3.43143630027771
2025-12-09 12:07:50.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.002207541366112149 Training loss: 3.6678214073181152
2025-12-09 12:07:50.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0021932213520599654 Training loss: 3.6339993476867676
2025-12-09 12:07:50.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0021788205435668085 Training loss: 3.649966239929199
2025-12-09 12:07:50.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0021643406190363624 Training loss: 3.6059305667877197
2025-12-09 12:07:50.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0021497832660932296 Training loss: 3.5922510623931885
2025-12-09 12:07:50.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0021351501813862356 Training loss: 3.4891762733459473
2025-12-09 12:07:50.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0021204430703906873 Training loss: 3.531851053237915
2025-12-09 12:07:50.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0021056636472096026 Training loss: 3.716019630432129
2025-12-09 12:07:50.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002090813634373931 Training loss: 3.477907180786133
2025-12-09 12:07:50.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0020758947626417943 Training loss: 3.973571300506592
2025-12-09 12:07:50.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.002060908770796769 Training loss: 3.3784096240997314
2025-12-09 12:07:50.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0020458574054452315 Training loss: 3.5443763732910156
2025-12-09 12:07:50.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.002030742420812791 Training loss: 3.5410265922546387
2025-12-09 12:07:50.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0020155655785398397 Training loss: 3.608571767807007
2025-12-09 12:07:50.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.002000328647476231 Training loss: 3.3663411140441895
2025-12-09 12:07:50.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.001985033403475123 Training loss: 3.589420795440674
2025-12-09 12:07:50.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.001969681629186004 Training loss: 3.4118247032165527
2025-12-09 12:07:50.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.001954275113846926 Training loss: 3.4628167152404785
2025-12-09 12:07:50.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0019388156530759713 Training loss: 3.3815696239471436
2025-12-09 12:07:50.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0019233050486619715 Training loss: 3.5092644691467285
2025-12-09 12:07:50.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0019077451083545144 Training loss: 3.6392972469329834
2025-12-09 12:07:50.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0018921376456532484 Training loss: 3.2417285442352295
2025-12-09 12:07:50.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0018764844795965232 Training loss: 3.7119765281677246
2025-12-09 12:07:50.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0018607874345493807 Training loss: 3.5939512252807617
2025-12-09 12:07:50.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0018450483399909264 Training loss: 3.4400367736816406
2025-12-09 12:07:50.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0018292690303011077 Training loss: 3.6355886459350586
2025-12-09 12:07:50.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.001813451344546913 Training loss: 3.464183807373047
2025-12-09 12:07:50.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0017975971262680348 Training loss: 3.3029751777648926
2025-12-09 12:07:50.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0017817082232620054 Training loss: 3.3171534538269043
2025-12-09 12:07:50.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0017657864873688344 Training loss: 3.404851198196411
2025-12-09 12:07:50.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0017498337742551818 Training loss: 3.351013422012329
2025-12-09 12:07:50.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0017338519431980798 Training loss: 3.7146453857421875
2025-12-09 12:07:50.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0017178428568682357 Training loss: 3.5499842166900635
2025-12-09 12:07:50.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.001701808381112938 Training loss: 3.5463411808013916
2025-12-09 12:07:50.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0016857503847385955 Training loss: 3.6728577613830566
2025-12-09 12:07:50.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0016696707392929266 Training loss: 3.2027735710144043
2025-12-09 12:07:50.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.001653571318846834 Training loss: 3.4992568492889404
2025-12-09 12:07:50.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0016374539997759824 Training loss: 3.324655294418335
2025-12-09 12:07:50.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0016213206605421066 Training loss: 3.1080174446105957
2025-12-09 12:07:50.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.001605173181474081 Training loss: 3.526928186416626
2025-12-09 12:07:50.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0015890134445487678 Training loss: 3.363774061203003
2025-12-09 12:07:50.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0015728433331716725 Training loss: 3.6196000576019287
2025-12-09 12:07:50.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0015566647319574351 Training loss: 3.003631114959717
2025-12-09 12:07:50.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0015404795265101807 Training loss: 3.5254693031311035
2025-12-09 12:07:50.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0015242896032037524 Training loss: 3.2831826210021973
2025-12-09 12:07:50.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0015080968489618568 Training loss: 3.3955256938934326
2025-12-09 12:07:50.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0014919031510381437 Training loss: 3.441046714782715
2025-12-09 12:07:50.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0014757103967962479 Training loss: 3.4102895259857178
2025-12-09 12:07:50.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0014595204734898198 Training loss: 3.1476733684539795
2025-12-09 12:07:50.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0014433352680425654 Training loss: 3.4704833030700684
2025-12-09 12:07:50.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0014271566668283282 Training loss: 3.168666124343872
2025-12-09 12:07:50.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.001410986555451232 Training loss: 3.262883424758911
2025-12-09 12:07:50.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0013948268185259188 Training loss: 3.401083469390869
2025-12-09 12:07:50.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.001378679339457894 Training loss: 3.1510417461395264
2025-12-09 12:07:50.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0013625460002240181 Training loss: 3.463719129562378
2025-12-09 12:07:50.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0013464286811531662 Training loss: 3.3561384677886963
2025-12-09 12:07:50.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0013303292607070737 Training loss: 3.2139787673950195
2025-12-09 12:07:50.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.001314249615261405 Training loss: 3.454230785369873
2025-12-09 12:07:50.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0012981916188870622 Training loss: 3.3416213989257812
2025-12-09 12:07:50.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.001282157143131765 Training loss: 3.2024433612823486
2025-12-09 12:07:50.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00126614805680192 Training loss: 3.20147967338562
2025-12-09 12:07:50.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0012501662257448183 Training loss: 3.5318820476531982
2025-12-09 12:07:50.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.001234213512631166 Training loss: 3.2932732105255127
2025-12-09 12:07:50.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.001218291776737995 Training loss: 3.3624932765960693
2025-12-09 12:07:50.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0012024028737319652 Training loss: 3.1245036125183105
2025-12-09 12:07:51.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0011865486554530874 Training loss: 3.193692207336426
2025-12-09 12:07:51.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.001170730969698893 Training loss: 3.4519474506378174
2025-12-09 12:07:51.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0011549516600090739 Training loss: 3.3076059818267822
2025-12-09 12:07:51.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00113921256545062 Training loss: 3.2571864128112793
2025-12-09 12:07:51.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0011235155204034769 Training loss: 3.245718479156494
2025-12-09 12:07:51.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0011078623543467519 Training loss: 3.3094482421875
2025-12-09 12:07:51.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0010922548916454857 Training loss: 3.1434876918792725
2025-12-09 12:07:51.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0010766949513380285 Training loss: 3.627774953842163
2025-12-09 12:07:51.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.001061184346924029 Training loss: 3.2959580421447754
2025-12-09 12:07:51.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0010457248861530741 Training loss: 3.240569829940796
2025-12-09 12:07:51.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0010303183708139964 Training loss: 3.1962645053863525
2025-12-09 12:07:51.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0010149665965248776 Training loss: 3.3213586807250977
2025-12-09 12:07:51.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009996713525237694 Training loss: 3.196148633956909
2025-12-09 12:07:51.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0009844344214601601 Training loss: 3.107743501663208
2025-12-09 12:07:51.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.000969257579187209 Training loss: 3.2953882217407227
2025-12-09 12:07:51.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009541425945547689 Training loss: 3.0771117210388184
2025-12-09 12:07:51.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.000939091229203231 Training loss: 3.1785495281219482
2025-12-09 12:07:51.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0009241052373582058 Training loss: 3.2789463996887207
2025-12-09 12:07:51.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009091863656260695 Training loss: 2.9500136375427246
2025-12-09 12:07:51.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0008943363527903977 Training loss: 3.362035036087036
2025-12-09 12:07:51.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0008795569296093132 Training loss: 3.1423139572143555
2025-12-09 12:07:51.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0008648498186137653 Training loss: 3.394465446472168
2025-12-09 12:07:51.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0008502167339067705 Training loss: 3.0915184020996094
2025-12-09 12:07:51.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0008356593809636371 Training loss: 3.09486985206604
2025-12-09 12:07:51.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0008211794564331918 Training loss: 3.1626739501953125
2025-12-09 12:07:51.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0008067786479400346 Training loss: 3.14481782913208
2025-12-09 12:07:51.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0007924586338878512 Training loss: 3.104214668273926
2025-12-09 12:07:51.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0007782210832637924 Training loss: 3.460597515106201
2025-12-09 12:07:51.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0007640676554439594 Training loss: 3.1590278148651123
2025-12-09 12:07:51.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0007500000000000003 Training loss: 3.2370123863220215
2025-12-09 12:07:51.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.000736019756506856 Training loss: 3.2514142990112305
2025-12-09 12:07:51.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.000722128554351668 Training loss: 3.0821373462677
2025-12-09 12:07:51.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0007083280125438767 Training loss: 2.979484796524048
2025-12-09 12:07:51.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0006946197395265243 Training loss: 3.1978814601898193
2025-12-09 12:07:51.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0006810053329887928 Training loss: 3.174119710922241
2025-12-09 12:07:51.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0006674863796797954 Training loss: 3.323975086212158
2025-12-09 12:07:51.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0006540644552236401 Training loss: 3.172330856323242
2025-12-09 12:07:51.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0006407411239357954 Training loss: 3.1237635612487793
2025-12-09 12:07:51.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0006275179386407663 Training loss: 2.9941179752349854
2025-12-09 12:07:51.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0006143964404911165 Training loss: 3.011157274246216
2025-12-09 12:07:51.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0006013781587878463 Training loss: 3.115248918533325
2025-12-09 12:07:51.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0005884646108021563 Training loss: 2.976708173751831
2025-12-09 12:07:51.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.000575657301598609 Training loss: 3.0971598625183105
2025-12-09 12:07:51.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0005629577238597133 Training loss: 3.074589729309082
2025-12-09 12:07:51.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0005503673577119553 Training loss: 3.3040366172790527
2025-12-09 12:07:51.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0005378876705532904 Training loss: 2.780287742614746
2025-12-09 12:07:51.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0005255201168821184 Training loss: 3.1740546226501465
2025-12-09 12:07:51.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0005132661381277644 Training loss: 3.069525718688965
2025-12-09 12:07:51.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0005011271624824787 Training loss: 3.189584255218506
2025-12-09 12:07:51.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0004891046047349837 Training loss: 2.6287760734558105
2025-12-09 12:07:51.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00047719986610558236 Training loss: 3.047875165939331
2025-12-09 12:07:51.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00046541433408284357 Training loss: 2.810731887817383
2025-12-09 12:07:51.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00045374938226189584 Training loss: 3.128237009048462
2025-12-09 12:07:51.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.00044220637018433165 Training loss: 3.0452709197998047
2025-12-09 12:07:51.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00043078664317975653 Training loss: 3.083146572113037
2025-12-09 12:07:51.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0004194915322089899 Training loss: 2.72196102142334
2025-12-09 12:07:51.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00040832235370894606 Training loss: 3.1087398529052734
2025-12-09 12:07:51.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0003972804094391998 Training loss: 3.0713963508605957
2025-12-09 12:07:51.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0003863669863302698 Training loss: 3.055866241455078
2025-12-09 12:07:51.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00037558335633362935 Training loss: 3.103181838989258
2025-12-09 12:07:51.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.000364930776273457 Training loss: 2.802208185195923
2025-12-09 12:07:51.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0003544104877001596 Training loss: 3.0502665042877197
2025-12-09 12:07:51.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0003440237167456663 Training loss: 3.0144670009613037
2025-12-09 12:07:51.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0003337716739805264 Training loss: 2.902317762374878
2025-12-09 12:07:51.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00032365555427281634 Training loss: 2.9626381397247314
2025-12-09 12:07:51.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00031367653664888173 Training loss: 2.7841696739196777
2025-12-09 12:07:51.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0003038357841559191 Training loss: 2.9107303619384766
2025-12-09 12:07:51.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00029413444372642495 Training loss: 3.1840641498565674
2025-12-09 12:07:51.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.00028457364604452375 Training loss: 3.001225233078003
2025-12-09 12:07:51.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.00027515450541418343 Training loss: 3.0896966457366943
2025-12-09 12:07:51.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.0002658781196293482 Training loss: 2.853977918624878
2025-12-09 12:07:51.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0002567455698459882 Training loss: 2.9120564460754395
2025-12-09 12:07:51.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00024775792045609354 Training loss: 2.9984946250915527
2025-12-09 12:07:51.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.00023891621896361882 Training loss: 3.031127691268921
2025-12-09 12:07:51.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.00023022149586239971 Training loss: 3.0089004039764404
2025-12-09 12:07:51.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.00022167476451604625 Training loss: 2.8707549571990967
2025-12-09 12:07:51.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.00021327702103983865 Training loss: 3.1487033367156982
2025-12-09 12:07:51.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.00020502924418463014 Training loss: 3.3257360458374023
2025-12-09 12:07:51.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0001969323952227733 Training loss: 2.7326877117156982
2025-12-09 12:07:51.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.00018898741783608642 Training loss: 2.9054930210113525
2025-12-09 12:07:51.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00018119523800586568 Training loss: 2.6401567459106445
2025-12-09 12:07:51.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.00017355676390496484 Training loss: 3.1470718383789062
2025-12-09 12:07:51.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0001660728857919464 Training loss: 3.0091798305511475
2025-12-09 12:07:51.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00015874447590732537 Training loss: 2.843773365020752
2025-12-09 12:07:51.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0001515723883719072 Training loss: 2.922555446624756
2025-12-09 12:07:51.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00014455745908724228 Training loss: 2.998190402984619
2025-12-09 12:07:51.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0001377005056382018 Training loss: 3.0436513423919678
2025-12-09 12:07:51.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00013100232719768994 Training loss: 2.9133145809173584
2025-12-09 12:07:51.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00012446370443349862 Training loss: 3.0233964920043945
2025-12-09 12:07:51.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0001180853994173236 Training loss: 2.886996030807495
2025-12-09 12:07:51.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.00011186815553594382 Training loss: 2.9829773902893066
2025-12-09 12:07:51.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0001058126974045811 Training loss: 3.1250338554382324
2025-12-09 12:07:51.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.991973078244637e-05 Training loss: 2.5960426330566406
2025-12-09 12:07:51.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.418994249048473e-05 Training loss: 3.032918930053711
2025-12-09 12:07:51.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 8.862400033132573e-05 Training loss: 2.785698890686035
2025-12-09 12:07:51.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 8.322255301145204e-05 Training loss: 2.8610129356384277
2025-12-09 12:07:51.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 7.798623006559435e-05 Training loss: 2.834228038787842
2025-12-09 12:07:51.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 7.291564178335719e-05 Training loss: 2.9522299766540527
2025-12-09 12:07:51.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 6.801137913809213e-05 Training loss: 2.781555652618408
2025-12-09 12:07:51.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 6.327401371801944e-05 Training loss: 3.062258243560791
2025-12-09 12:07:51.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 5.870409765960966e-05 Training loss: 2.7055156230926514
2025-12-09 12:07:51.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 5.430216358323309e-05 Training loss: 3.1717772483825684
2025-12-09 12:07:51.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 5.00687245310833e-05 Training loss: 2.979220390319824
2025-12-09 12:07:51.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 4.60042739073816e-05 Training loss: 3.0882859230041504
2025-12-09 12:07:51.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 4.2109285420872056e-05 Training loss: 3.0551676750183105
2025-12-09 12:07:51.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 3.8384213029610984e-05 Training loss: 2.861394166946411
2025-12-09 12:07:51.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 3.4829490888057425e-05 Training loss: 3.3885231018066406
2025-12-09 12:07:51.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 3.144553329647448e-05 Training loss: 2.745482921600342
2025-12-09 12:07:52.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 2.823273465264142e-05 Training loss: 3.061140537261963
2025-12-09 12:07:52.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 2.5191469405887624e-05 Training loss: 2.8795626163482666
2025-12-09 12:07:52.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 2.2322092013450313e-05 Training loss: 2.911536931991577
2025-12-09 12:07:52.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 1.9624936899163947e-05 Training loss: 2.901172161102295
2025-12-09 12:07:52.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 1.7100318414482063e-05 Training loss: 2.9538931846618652
2025-12-09 12:07:52.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 1.4748530801840077e-05 Training loss: 2.716857671737671
2025-12-09 12:07:52.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 1.2569848160362384e-05 Training loss: 2.918416738510132
2025-12-09 12:07:52.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 1.0564524413915422e-05 Training loss: 2.8739700317382812
2025-12-09 12:07:52.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 8.732793281513663e-06 Training loss: 3.027895212173462
2025-12-09 12:07:52.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 7.07486825007908e-06 Training loss: 2.9787421226501465
2025-12-09 12:07:52.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 5.590942549560052e-06 Training loss: 2.757899045944214
2025-12-09 12:07:52.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 4.281189130410535e-06 Training loss: 2.829373836517334
2025-12-09 12:07:52.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 3.145760643432527e-06 Training loss: 3.0382795333862305
2025-12-09 12:07:52.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 2.184789421984634e-06 Training loss: 2.9295363426208496
2025-12-09 12:07:52.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 1.3983874665589036e-06 Training loss: 3.085521936416626
2025-12-09 12:07:52.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 7.866464317276001e-07 Training loss: 2.836246967315674
2025-12-09 12:07:52.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 3.496376154604186e-07 Training loss: 3.048431396484375
2025-12-09 12:07:52.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 8.741195081479747e-08 Training loss: 2.848733901977539
2025-12-09 12:07:52.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 2.9003043174743652
