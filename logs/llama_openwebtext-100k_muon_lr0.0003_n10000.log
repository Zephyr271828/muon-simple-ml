2025-12-09 12:20:30.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 12.188782691955566
2025-12-09 12:20:30.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 12.174444198608398
2025-12-09 12:20:31.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 12.144030570983887
2025-12-09 12:20:31.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 12.17935848236084
2025-12-09 12:20:32.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 12.161232948303223
2025-12-09 12:20:32.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 12.14616870880127
2025-12-09 12:20:33.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 12.160811424255371
2025-12-09 12:20:33.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 12.133784294128418
2025-12-09 12:20:34.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 12.11612319946289
2025-12-09 12:20:34.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 12.146688461303711
2025-12-09 12:20:35.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 12.12402629852295
2025-12-09 12:20:35.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 12.071239471435547
2025-12-09 12:20:36.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 12.09512710571289
2025-12-09 12:20:36.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 12.062712669372559
2025-12-09 12:20:37.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 12.04948616027832
2025-12-09 12:20:37.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 12.121193885803223
2025-12-09 12:20:38.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 11.954026222229004
2025-12-09 12:20:38.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 11.993003845214844
2025-12-09 12:20:39.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 11.976103782653809
2025-12-09 12:20:39.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 11.967857360839844
2025-12-09 12:20:40.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 11.919740676879883
2025-12-09 12:20:40.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 11.95875358581543
2025-12-09 12:20:41.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 11.947139739990234
2025-12-09 12:20:41.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 11.659972190856934
2025-12-09 12:20:42.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 11.6832857131958
2025-12-09 12:20:42.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 11.737570762634277
2025-12-09 12:20:43.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 11.498892784118652
2025-12-09 12:20:43.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 11.83128547668457
2025-12-09 12:20:44.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 11.568790435791016
2025-12-09 12:20:44.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 11.460875511169434
2025-12-09 12:20:45.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 11.426647186279297
2025-12-09 12:20:45.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 11.222281455993652
2025-12-09 12:20:46.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 11.130450248718262
2025-12-09 12:20:46.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 10.968555450439453
2025-12-09 12:20:47.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 10.90891170501709
2025-12-09 12:20:47.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 11.058290481567383
2025-12-09 12:20:48.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 10.615336418151855
2025-12-09 12:20:48.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 10.950026512145996
2025-12-09 12:20:49.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 10.875797271728516
2025-12-09 12:20:49.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 10.365668296813965
2025-12-09 12:20:50.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 10.011601448059082
2025-12-09 12:20:50.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 10.17039680480957
2025-12-09 12:20:51.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 9.812211990356445
2025-12-09 12:20:51.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 9.948208808898926
2025-12-09 12:20:52.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 9.826486587524414
2025-12-09 12:20:52.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 9.541305541992188
2025-12-09 12:20:53.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 9.334948539733887
2025-12-09 12:20:53.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 9.57007122039795
2025-12-09 12:20:54.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 9.086962699890137
2025-12-09 12:20:54.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 9.032302856445312
2025-12-09 12:20:55.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 9.31021499633789
2025-12-09 12:20:55.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 8.96768856048584
2025-12-09 12:20:56.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 8.994295120239258
2025-12-09 12:20:56.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 8.65422534942627
2025-12-09 12:20:57.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 8.462798118591309
2025-12-09 12:20:57.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 8.510784149169922
2025-12-09 12:20:58.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 8.38158893585205
2025-12-09 12:20:58.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 8.249397277832031
2025-12-09 12:20:59.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 8.169649124145508
2025-12-09 12:20:59.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 8.474555015563965
2025-12-09 12:21:00.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 8.34271240234375
2025-12-09 12:21:00.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 8.339387893676758
2025-12-09 12:21:01.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 8.313979148864746
2025-12-09 12:21:01.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 8.255382537841797
2025-12-09 12:21:02.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 8.290842056274414
2025-12-09 12:21:02.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 7.870612144470215
2025-12-09 12:21:03.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 7.746570587158203
2025-12-09 12:21:03.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 8.215826988220215
2025-12-09 12:21:04.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 7.874950408935547
2025-12-09 12:21:04.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 8.550620079040527
2025-12-09 12:21:05.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 7.81047248840332
2025-12-09 12:21:05.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 8.405535697937012
2025-12-09 12:21:06.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 8.072908401489258
2025-12-09 12:21:06.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 8.14838695526123
2025-12-09 12:21:07.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 8.215425491333008
2025-12-09 12:21:07.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 7.812533378601074
2025-12-09 12:21:08.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 8.137750625610352
2025-12-09 12:21:08.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 8.43032169342041
2025-12-09 12:21:09.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 8.259703636169434
2025-12-09 12:21:09.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 7.899823188781738
2025-12-09 12:21:10.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 8.175036430358887
2025-12-09 12:21:10.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 7.636178493499756
2025-12-09 12:21:11.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 7.744719982147217
2025-12-09 12:21:11.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 8.34550666809082
2025-12-09 12:21:12.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 7.865810394287109
2025-12-09 12:21:12.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 8.995126724243164
2025-12-09 12:21:13.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 7.747104167938232
2025-12-09 12:21:13.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 7.7078375816345215
2025-12-09 12:21:14.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 7.974325656890869
2025-12-09 12:21:14.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 8.22165298461914
2025-12-09 12:21:15.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 7.810426712036133
2025-12-09 12:21:15.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 7.643614768981934
2025-12-09 12:21:16.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 7.649393081665039
2025-12-09 12:21:16.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 7.665740966796875
2025-12-09 12:21:17.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 7.602553367614746
2025-12-09 12:21:17.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 7.798756122589111
2025-12-09 12:21:18.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 7.855906963348389
2025-12-09 12:21:18.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 7.481542587280273
2025-12-09 12:21:19.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 7.696374416351318
2025-12-09 12:21:19.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 7.769283771514893
2025-12-09 12:21:20.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999997089396425 Training loss: 7.492011547088623
2025-12-09 12:21:20.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029999988357586825 Training loss: 7.733391284942627
2025-12-09 12:21:21.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000299999738045746 Training loss: 7.775341987609863
2025-12-09 12:21:21.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0002999995343036539 Training loss: 7.672926425933838
2025-12-09 12:21:22.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00029999927234967104 Training loss: 8.083788871765137
2025-12-09 12:21:22.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.00029999895218389905 Training loss: 7.608232021331787
2025-12-09 12:21:23.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0002999985738064622 Training loss: 7.624826431274414
2025-12-09 12:21:23.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00029999813721750737 Training loss: 8.131349563598633
2025-12-09 12:21:24.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.00029999764241720394 Training loss: 7.6111249923706055
2025-12-09 12:21:24.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0002999970894057439 Training loss: 8.011396408081055
2025-12-09 12:21:25.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029999647818334195 Training loss: 7.6698317527771
2025-12-09 12:21:25.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0002999958087502352 Training loss: 7.763233184814453
2025-12-09 12:21:26.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.00029999508110668355 Training loss: 7.711025238037109
2025-12-09 12:21:26.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0002999942952529693 Training loss: 7.667514324188232
2025-12-09 12:21:27.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00029999345118939745 Training loss: 7.746524810791016
2025-12-09 12:21:27.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0002999925489162956 Training loss: 7.365662097930908
2025-12-09 12:21:28.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029999158843401386 Training loss: 7.650025844573975
2025-12-09 12:21:28.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.000299990569742925 Training loss: 7.20205020904541
2025-12-09 12:21:29.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002999894928434243 Training loss: 7.5733747482299805
2025-12-09 12:21:29.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.00029998835773592975 Training loss: 8.310541152954102
2025-12-09 12:21:30.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.00029998716442088184 Training loss: 7.255277633666992
2025-12-09 12:21:30.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0002999859128987437 Training loss: 7.652935028076172
2025-12-09 12:21:31.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00029998460317000097 Training loss: 7.266719341278076
2025-12-09 12:21:31.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00029998323523516195 Training loss: 7.651053428649902
2025-12-09 12:21:32.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002999818090947575 Training loss: 7.519716739654541
2025-12-09 12:21:32.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00029998032474934106 Training loss: 7.485326290130615
2025-12-09 12:21:33.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0002999787821994887 Training loss: 7.5593719482421875
2025-12-09 12:21:33.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00029997718144579913 Training loss: 7.322648525238037
2025-12-09 12:21:34.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0002999755224888935 Training loss: 7.601352214813232
2025-12-09 12:21:34.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029997380532941555 Training loss: 7.204314231872559
2025-12-09 12:21:35.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00029997202996803177 Training loss: 7.237954616546631
2025-12-09 12:21:35.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0002999701964054312 Training loss: 7.756885051727295
2025-12-09 12:21:36.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0002999683046423252 Training loss: 7.41697883605957
2025-12-09 12:21:36.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0002999663546794481 Training loss: 7.290273189544678
2025-12-09 12:21:37.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.00029996434651755657 Training loss: 8.753403663635254
2025-12-09 12:21:37.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.00029996228015743 Training loss: 7.487626075744629
2025-12-09 12:21:38.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002999601555998703 Training loss: 7.215329647064209
2025-12-09 12:21:38.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0002999579728457019 Training loss: 7.4393768310546875
2025-12-09 12:21:39.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0002999557318957719 Training loss: 7.354517459869385
2025-12-09 12:21:39.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.00029995343275095003 Training loss: 7.699073314666748
2025-12-09 12:21:40.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.00029995107541212843 Training loss: 7.394265651702881
2025-12-09 12:21:40.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00029994865988022205 Training loss: 7.407994747161865
2025-12-09 12:21:41.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0002999461861561683 Training loss: 7.671948432922363
2025-12-09 12:21:41.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0002999436542409271 Training loss: 7.211996078491211
2025-12-09 12:21:42.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0002999410641354812 Training loss: 7.839519023895264
2025-12-09 12:21:42.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00029993841584083553 Training loss: 7.147340774536133
2025-12-09 12:21:43.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00029993570935801805 Training loss: 7.351314067840576
2025-12-09 12:21:43.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.000299932944688079 Training loss: 7.175620079040527
2025-12-09 12:21:44.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.00029993012183209135 Training loss: 7.541374206542969
2025-12-09 12:21:44.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002999272407911505 Training loss: 8.197352409362793
2025-12-09 12:21:45.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.00029992430156637454 Training loss: 7.574273586273193
2025-12-09 12:21:45.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00029992130415890426 Training loss: 7.266273021697998
2025-12-09 12:21:46.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.00029991824856990276 Training loss: 7.713816165924072
2025-12-09 12:21:46.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0002999151348005559 Training loss: 7.088119029998779
2025-12-09 12:21:47.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0002999119628520721 Training loss: 7.051084518432617
2025-12-09 12:21:47.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.00029990873272568226 Training loss: 7.461830139160156
2025-12-09 12:21:48.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.00029990544442263996 Training loss: 7.369263172149658
2025-12-09 12:21:48.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002999020979442214 Training loss: 7.286739826202393
2025-12-09 12:21:49.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0002998986932917252 Training loss: 7.565497398376465
2025-12-09 12:21:49.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.00029989523046647257 Training loss: 6.915327548980713
2025-12-09 12:21:50.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00029989170946980755 Training loss: 7.389556884765625
2025-12-09 12:21:50.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00029988813030309644 Training loss: 7.65376091003418
2025-12-09 12:21:51.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0002998844929677283 Training loss: 7.5735554695129395
2025-12-09 12:21:51.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00029988079746511465 Training loss: 8.860498428344727
2025-12-09 12:21:52.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.00029987704379668973 Training loss: 7.323353290557861
2025-12-09 12:21:52.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0002998732319639102 Training loss: 7.4631195068359375
2025-12-09 12:21:53.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.00029986936196825536 Training loss: 7.850420951843262
2025-12-09 12:21:53.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0002998654338112271 Training loss: 7.050968170166016
2025-12-09 12:21:54.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00029986144749434985 Training loss: 7.617872714996338
2025-12-09 12:21:54.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0002998574030191706 Training loss: 7.600150108337402
2025-12-09 12:21:55.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.000299853300387259 Training loss: 7.859177589416504
2025-12-09 12:21:55.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00029984913960020714 Training loss: 7.662201404571533
2025-12-09 12:21:56.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.00029984492065962976 Training loss: 7.375117301940918
2025-12-09 12:21:56.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00029984064356716414 Training loss: 8.295684814453125
2025-12-09 12:21:57.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0002998363083244701 Training loss: 7.287317276000977
2025-12-09 12:21:57.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00029983191493323017 Training loss: 7.476119518280029
2025-12-09 12:21:58.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0002998274633951493 Training loss: 7.312543869018555
2025-12-09 12:21:58.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00029982295371195494 Training loss: 7.5114827156066895
2025-12-09 12:21:59.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00029981838588539735 Training loss: 7.493231296539307
2025-12-09 12:21:59.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.00029981375991724915 Training loss: 7.183638572692871
2025-12-09 12:22:00.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0002998090758093056 Training loss: 7.199141025543213
2025-12-09 12:22:00.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00029980433356338447 Training loss: 7.159573554992676
2025-12-09 12:22:01.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0002997995331813262 Training loss: 7.318190097808838
2025-12-09 12:22:01.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.00029979467466499367 Training loss: 7.307717323303223
2025-12-09 12:22:02.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00029978975801627243 Training loss: 7.024955749511719
2025-12-09 12:22:02.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0002997847832370704 Training loss: 7.159353733062744
2025-12-09 12:22:03.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0002997797503293184 Training loss: 6.921091079711914
2025-12-09 12:22:03.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00029977465929496947 Training loss: 8.05911636352539
2025-12-09 12:22:04.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0002997695101359994 Training loss: 7.249794006347656
2025-12-09 12:22:04.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0002997643028544064 Training loss: 6.971057891845703
2025-12-09 12:22:05.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0002997590374522114 Training loss: 7.224427223205566
2025-12-09 12:22:05.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0002997537139314577 Training loss: 7.470342636108398
2025-12-09 12:22:06.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0002997483322942114 Training loss: 7.102756023406982
2025-12-09 12:22:06.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0002997428925425609 Training loss: 7.320789337158203
2025-12-09 12:22:07.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0002997373946786173 Training loss: 7.782594680786133
2025-12-09 12:22:07.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.00029973183870451417 Training loss: 8.1922025680542
2025-12-09 12:22:08.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0002997262246224077 Training loss: 6.775104522705078
2025-12-09 12:22:08.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00029972055243447665 Training loss: 7.5176215171813965
2025-12-09 12:22:09.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.00029971482214292223 Training loss: 7.297370433807373
2025-12-09 12:22:09.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.00029970903374996826 Training loss: 6.87294340133667
2025-12-09 12:22:10.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0002997031872578611 Training loss: 7.258229732513428
2025-12-09 12:22:10.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00029969728266886973 Training loss: 7.026768684387207
2025-12-09 12:22:11.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00029969131998528554 Training loss: 7.2957611083984375
2025-12-09 12:22:11.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0002996852992094225 Training loss: 7.046902656555176
2025-12-09 12:22:12.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.00029967922034361723 Training loss: 7.565504550933838
2025-12-09 12:22:12.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0002996730833902287 Training loss: 7.195613384246826
2025-12-09 12:22:13.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00029966688835163875 Training loss: 7.540211200714111
2025-12-09 12:22:13.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00029966063523025136 Training loss: 7.19476842880249
2025-12-09 12:22:14.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00029965432402849333 Training loss: 6.811445713043213
2025-12-09 12:22:14.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0002996479547488139 Training loss: 7.265832424163818
2025-12-09 12:22:15.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0002996415273936849 Training loss: 7.1617865562438965
2025-12-09 12:22:15.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00029963504196560056 Training loss: 7.714066505432129
2025-12-09 12:22:16.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.00029962849846707786 Training loss: 7.266205787658691
2025-12-09 12:22:16.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0002996218969006561 Training loss: 6.905048847198486
2025-12-09 12:22:17.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00029961523726889733 Training loss: 7.434126853942871
2025-12-09 12:22:17.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00029960851957438594 Training loss: 7.147252559661865
2025-12-09 12:22:18.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.000299601743819729 Training loss: 7.281554698944092
2025-12-09 12:22:18.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00029959491000755594 Training loss: 7.077820301055908
2025-12-09 12:22:19.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00029958801814051897 Training loss: 7.159626483917236
2025-12-09 12:22:19.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0002995810682212926 Training loss: 7.2383713722229
2025-12-09 12:22:20.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0002995740602525739 Training loss: 7.258257865905762
2025-12-09 12:22:20.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0002995669942370827 Training loss: 7.370626449584961
2025-12-09 12:22:21.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.000299559870177561 Training loss: 7.2535481452941895
2025-12-09 12:22:21.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0002995526880767737 Training loss: 7.034478664398193
2025-12-09 12:22:22.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.00029954544793750785 Training loss: 7.21697473526001
2025-12-09 12:22:22.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00029953814976257335 Training loss: 8.11747932434082
2025-12-09 12:22:23.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0002995307935548024 Training loss: 7.008217811584473
2025-12-09 12:22:23.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0002995233793170498 Training loss: 6.864396095275879
2025-12-09 12:22:24.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.00029951590705219283 Training loss: 7.181149482727051
2025-12-09 12:22:24.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0002995083767631314 Training loss: 7.07420015335083
2025-12-09 12:22:25.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0002995007884527879 Training loss: 7.5258402824401855
2025-12-09 12:22:25.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.00029949314212410715 Training loss: 7.24080753326416
2025-12-09 12:22:26.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.00029948543778005656 Training loss: 7.264304161071777
2025-12-09 12:22:26.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00029947767542362597 Training loss: 7.225785732269287
2025-12-09 12:22:27.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0002994698550578279 Training loss: 7.036707878112793
2025-12-09 12:22:27.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0002994619766856972 Training loss: 6.826416492462158
2025-12-09 12:22:28.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00029945404031029134 Training loss: 7.312766075134277
2025-12-09 12:22:28.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.00029944604593469033 Training loss: 7.128983974456787
2025-12-09 12:22:29.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00029943799356199656 Training loss: 7.279306411743164
2025-12-09 12:22:29.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.00029942988319533504 Training loss: 6.77372932434082
2025-12-09 12:22:30.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0002994217148378532 Training loss: 6.941922664642334
2025-12-09 12:22:30.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00029941348849272105 Training loss: 7.204277515411377
2025-12-09 12:22:31.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0002994052041631311 Training loss: 6.624100685119629
2025-12-09 12:22:31.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0002993968618522982 Training loss: 7.0743231773376465
2025-12-09 12:22:32.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0002993884615634601 Training loss: 7.385269641876221
2025-12-09 12:22:32.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.00029938000329987645 Training loss: 7.124320983886719
2025-12-09 12:22:33.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00029937148706483003 Training loss: 7.324812412261963
2025-12-09 12:22:33.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.00029936291286162577 Training loss: 6.756494045257568
2025-12-09 12:22:34.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.000299354280693591 Training loss: 7.181506156921387
2025-12-09 12:22:34.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0002993455905640758 Training loss: 7.4439897537231445
2025-12-09 12:22:35.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0002993368424764526 Training loss: 6.924532413482666
2025-12-09 12:22:35.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0002993280364341165 Training loss: 7.098599910736084
2025-12-09 12:22:36.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00029931917244048473 Training loss: 7.1860151290893555
2025-12-09 12:22:36.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0002993102504989974 Training loss: 7.206268310546875
2025-12-09 12:22:37.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00029930127061311685 Training loss: 6.898766994476318
2025-12-09 12:22:37.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0002992922327863281 Training loss: 7.2686052322387695
2025-12-09 12:22:38.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00029928313702213844 Training loss: 6.843591690063477
2025-12-09 12:22:38.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.00029927398332407784 Training loss: 6.905237197875977
2025-12-09 12:22:39.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00029926477169569865 Training loss: 7.951791286468506
2025-12-09 12:22:39.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00029925550214057565 Training loss: 7.091726303100586
2025-12-09 12:22:40.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.00029924617466230624 Training loss: 7.359088897705078
2025-12-09 12:22:40.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.00029923678926451034 Training loss: 7.206450462341309
2025-12-09 12:22:41.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00029922734595083005 Training loss: 6.928484916687012
2025-12-09 12:22:41.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0002992178447249302 Training loss: 6.9301300048828125
2025-12-09 12:22:42.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.00029920828559049805 Training loss: 6.241344928741455
2025-12-09 12:22:42.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0002991986685512433 Training loss: 6.614350318908691
2025-12-09 12:22:43.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0002991889936108982 Training loss: 7.093343257904053
2025-12-09 12:22:43.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0002991792607732173 Training loss: 6.948230266571045
2025-12-09 12:22:44.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0002991694700419778 Training loss: 7.789380073547363
2025-12-09 12:22:44.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00029915962142097925 Training loss: 7.268045425415039
2025-12-09 12:22:45.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00029914971491404373 Training loss: 6.7175517082214355
2025-12-09 12:22:45.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00029913975052501575 Training loss: 6.853254795074463
2025-12-09 12:22:46.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0002991297282577623 Training loss: 7.085665225982666
2025-12-09 12:22:46.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00029911964811617285 Training loss: 7.502862930297852
2025-12-09 12:22:47.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00029910951010415926 Training loss: 6.977637767791748
2025-12-09 12:22:47.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0002990993142256559 Training loss: 7.126972198486328
2025-12-09 12:22:48.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0002990890604846196 Training loss: 6.5964250564575195
2025-12-09 12:22:48.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00029907874888502966 Training loss: 6.741867542266846
2025-12-09 12:22:49.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.00029906837943088785 Training loss: 7.25412130355835
2025-12-09 12:22:49.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.00029905795212621823 Training loss: 7.055274963378906
2025-12-09 12:22:50.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.00029904746697506754 Training loss: 7.168618202209473
2025-12-09 12:22:50.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0002990369239815048 Training loss: 6.616494655609131
2025-12-09 12:22:51.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.00029902632314962157 Training loss: 7.215983867645264
2025-12-09 12:22:51.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002990156644835318 Training loss: 7.140087127685547
2025-12-09 12:22:52.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.00029900494798737194 Training loss: 7.021543502807617
2025-12-09 12:22:52.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00029899417366530085 Training loss: 6.700840950012207
2025-12-09 12:22:53.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.00029898334152149984 Training loss: 7.0353217124938965
2025-12-09 12:22:53.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0002989724515601726 Training loss: 7.369460105895996
2025-12-09 12:22:54.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0002989615037855454 Training loss: 7.0872015953063965
2025-12-09 12:22:54.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0002989504982018668 Training loss: 6.8650970458984375
2025-12-09 12:22:55.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.00029893943481340785 Training loss: 7.907649517059326
2025-12-09 12:22:55.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.000298928313624462 Training loss: 7.0981364250183105
2025-12-09 12:22:56.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0002989171346393453 Training loss: 6.429078578948975
2025-12-09 12:22:56.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00029890589786239595 Training loss: 7.264065265655518
2025-12-09 12:22:57.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0002988946032979748 Training loss: 6.864343643188477
2025-12-09 12:22:57.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.000298883250950465 Training loss: 6.750553607940674
2025-12-09 12:22:58.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0002988718408242722 Training loss: 6.996353626251221
2025-12-09 12:22:58.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.00029886037292382455 Training loss: 6.645390510559082
2025-12-09 12:22:59.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00029884884725357236 Training loss: 6.593333721160889
2025-12-09 12:22:59.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0002988372638179886 Training loss: 6.545547962188721
2025-12-09 12:23:00.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002988256226215685 Training loss: 7.263132095336914
2025-12-09 12:23:00.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0002988139236688299 Training loss: 6.799861431121826
2025-12-09 12:23:01.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.00029880216696431285 Training loss: 7.2041826248168945
2025-12-09 12:23:01.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0002987903525125799 Training loss: 6.927257061004639
2025-12-09 12:23:02.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.000298778480318216 Training loss: 6.6665544509887695
2025-12-09 12:23:02.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0002987665503858286 Training loss: 7.207803726196289
2025-12-09 12:23:03.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0002987545627200474 Training loss: 7.5673370361328125
2025-12-09 12:23:03.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0002987425173255246 Training loss: 6.686209678649902
2025-12-09 12:23:04.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0002987304142069348 Training loss: 7.081945419311523
2025-12-09 12:23:04.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0002987182533689749 Training loss: 7.512413024902344
2025-12-09 12:23:05.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0002987060348163644 Training loss: 6.709562301635742
2025-12-09 12:23:05.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.000298693758553845 Training loss: 6.627894401550293
2025-12-09 12:23:06.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.00029868142458618096 Training loss: 7.06186056137085
2025-12-09 12:23:06.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0002986690329181587 Training loss: 6.961027145385742
2025-12-09 12:23:07.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00029865658355458736 Training loss: 7.1782732009887695
2025-12-09 12:23:07.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0002986440765002982 Training loss: 7.742118835449219
2025-12-09 12:23:08.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.000298631511760145 Training loss: 7.1069135665893555
2025-12-09 12:23:08.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0002986188893390038 Training loss: 6.814944744110107
2025-12-09 12:23:09.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0002986062092417733 Training loss: 7.217160224914551
2025-12-09 12:23:09.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.00029859347147337417 Training loss: 6.985872268676758
2025-12-09 12:23:10.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0002985806760387499 Training loss: 7.592942714691162
2025-12-09 12:23:10.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00029856782294286594 Training loss: 6.4228315353393555
2025-12-09 12:23:11.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00029855491219071053 Training loss: 6.933017730712891
2025-12-09 12:23:11.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.000298541943787294 Training loss: 6.963496208190918
2025-12-09 12:23:12.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00029852891773764906 Training loss: 6.65458869934082
2025-12-09 12:23:12.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00029851583404683096 Training loss: 6.928836345672607
2025-12-09 12:23:13.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0002985026927199172 Training loss: 6.718790531158447
2025-12-09 12:23:13.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.00029848949376200766 Training loss: 6.995616436004639
2025-12-09 12:23:14.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0002984762371782246 Training loss: 7.295806884765625
2025-12-09 12:23:14.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00029846292297371264 Training loss: 7.112307548522949
2025-12-09 12:23:15.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0002984495511536388 Training loss: 6.818396091461182
2025-12-09 12:23:15.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0002984361217231923 Training loss: 7.1696271896362305
2025-12-09 12:23:16.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.00029842263468758505 Training loss: 6.7975640296936035
2025-12-09 12:23:16.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0002984090900520509 Training loss: 6.865509510040283
2025-12-09 12:23:17.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00029839548782184636 Training loss: 7.030539035797119
2025-12-09 12:23:17.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.00029838182800225017 Training loss: 7.406294345855713
2025-12-09 12:23:18.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00029836811059856354 Training loss: 6.830258846282959
2025-12-09 12:23:18.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.00029835433561610974 Training loss: 6.634660720825195
2025-12-09 12:23:19.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0002983405030602346 Training loss: 6.863336086273193
2025-12-09 12:23:19.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.00029832661293630644 Training loss: 7.474697589874268
2025-12-09 12:23:20.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.00029831266524971557 Training loss: 7.009188175201416
2025-12-09 12:23:20.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0002982986600058749 Training loss: 7.418682098388672
2025-12-09 12:23:21.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0002982845972102196 Training loss: 7.135552883148193
2025-12-09 12:23:21.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0002982704768682071 Training loss: 6.952149868011475
2025-12-09 12:23:22.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00029825629898531724 Training loss: 6.7174530029296875
2025-12-09 12:23:22.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0002982420635670523 Training loss: 6.866261005401611
2025-12-09 12:23:23.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.00029822777061893653 Training loss: 7.085995197296143
2025-12-09 12:23:23.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00029821342014651694 Training loss: 6.891943454742432
2025-12-09 12:23:24.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0002981990121553627 Training loss: 7.256250381469727
2025-12-09 12:23:24.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0002981845466510651 Training loss: 6.67479133605957
2025-12-09 12:23:25.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00029817002363923803 Training loss: 6.789850234985352
2025-12-09 12:23:25.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00029815544312551754 Training loss: 6.900368690490723
2025-12-09 12:23:26.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00029814080511556207 Training loss: 7.262360095977783
2025-12-09 12:23:26.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00029812610961505234 Training loss: 6.747485160827637
2025-12-09 12:23:27.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.00029811135662969143 Training loss: 6.788418769836426
2025-12-09 12:23:27.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.00029809654616520456 Training loss: 6.807806491851807
2025-12-09 12:23:28.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00029808167822733953 Training loss: 7.129422187805176
2025-12-09 12:23:28.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0002980667528218662 Training loss: 7.063125133514404
2025-12-09 12:23:29.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002980517699545769 Training loss: 6.538910865783691
2025-12-09 12:23:29.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0002980367296312861 Training loss: 6.8812055587768555
2025-12-09 12:23:30.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00029802163185783074 Training loss: 6.820864677429199
2025-12-09 12:23:30.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.00029800647664006993 Training loss: 6.6293768882751465
2025-12-09 12:23:31.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0002979912639838851 Training loss: 6.854920864105225
2025-12-09 12:23:31.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.00029797599389518 Training loss: 7.584568023681641
2025-12-09 12:23:32.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0002979606663798807 Training loss: 6.790043830871582
2025-12-09 12:23:32.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0002979452814439354 Training loss: 7.191828727722168
2025-12-09 12:23:33.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.00029792983909331485 Training loss: 7.133690357208252
2025-12-09 12:23:33.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0002979143393340117 Training loss: 6.781800270080566
2025-12-09 12:23:34.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.00029789878217204133 Training loss: 6.495738506317139
2025-12-09 12:23:34.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00029788316761344106 Training loss: 6.8415141105651855
2025-12-09 12:23:35.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00029786749566427064 Training loss: 7.789327621459961
2025-12-09 12:23:35.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.000297851766330612 Training loss: 7.039257526397705
2025-12-09 12:23:36.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.00029783597961856946 Training loss: 7.360894203186035
2025-12-09 12:23:36.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00029782013553426937 Training loss: 6.700859546661377
2025-12-09 12:23:37.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.00029780423408386073 Training loss: 6.66072416305542
2025-12-09 12:23:37.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.00029778827527351443 Training loss: 6.6598358154296875
2025-12-09 12:23:38.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0002977722591094238 Training loss: 6.167579650878906
2025-12-09 12:23:38.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.00029775618559780447 Training loss: 6.901973724365234
2025-12-09 12:23:39.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.00029774005474489417 Training loss: 6.7240424156188965
2025-12-09 12:23:39.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.00029772386655695305 Training loss: 7.03380823135376
2025-12-09 12:23:40.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0002977076210402633 Training loss: 6.9681878089904785
2025-12-09 12:23:40.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.00029769131820112966 Training loss: 6.784424781799316
2025-12-09 12:23:41.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.00029767495804587885 Training loss: 6.762856960296631
2025-12-09 12:23:41.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.0002976585405808599 Training loss: 6.71380615234375
2025-12-09 12:23:42.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0002976420658124441 Training loss: 7.010233402252197
2025-12-09 12:23:42.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0002976255337470251 Training loss: 6.460738658905029
2025-12-09 12:23:43.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.00029760894439101855 Training loss: 7.141519546508789
2025-12-09 12:23:43.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0002975922977508625 Training loss: 6.870368957519531
2025-12-09 12:23:44.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0002975755938330172 Training loss: 6.822725772857666
2025-12-09 12:23:44.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.00029755883264396513 Training loss: 7.0195841789245605
2025-12-09 12:23:45.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00029754201419021094 Training loss: 6.845125675201416
2025-12-09 12:23:45.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0002975251384782816 Training loss: 6.65463399887085
2025-12-09 12:23:46.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.00029750820551472615 Training loss: 7.059190273284912
2025-12-09 12:23:46.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.00029749121530611597 Training loss: 6.83862829208374
2025-12-09 12:23:47.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0002974741678590447 Training loss: 7.066915035247803
2025-12-09 12:23:47.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.00029745706318012806 Training loss: 7.21701717376709
2025-12-09 12:23:48.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.00029743990127600406 Training loss: 6.728062629699707
2025-12-09 12:23:48.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0002974226821533329 Training loss: 7.026810169219971
2025-12-09 12:23:49.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.000297405405818797 Training loss: 6.632516860961914
2025-12-09 12:23:49.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0002973880722791009 Training loss: 7.051370143890381
2025-12-09 12:23:50.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0002973706815409715 Training loss: 7.005805492401123
2025-12-09 12:23:50.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0002973532336111577 Training loss: 7.089400291442871
2025-12-09 12:23:51.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00029733572849643085 Training loss: 6.631850242614746
2025-12-09 12:23:51.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.00029731816620358424 Training loss: 7.2124199867248535
2025-12-09 12:23:52.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0002973005467394334 Training loss: 6.417664527893066
2025-12-09 12:23:52.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.00029728287011081625 Training loss: 6.7356414794921875
2025-12-09 12:23:53.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0002972651363245927 Training loss: 6.609213352203369
2025-12-09 12:23:53.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00029724734538764475 Training loss: 6.817221164703369
2025-12-09 12:23:54.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0002972294973068768 Training loss: 6.905410289764404
2025-12-09 12:23:54.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.00029721159208921546 Training loss: 6.21352481842041
2025-12-09 12:23:55.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.00029719362974160924 Training loss: 6.953610897064209
2025-12-09 12:23:55.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.000297175610271029 Training loss: 6.389537811279297
2025-12-09 12:23:56.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.00029715753368446786 Training loss: 6.897193431854248
2025-12-09 12:23:56.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00029713939998894087 Training loss: 6.907784938812256
2025-12-09 12:23:57.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0002971212091914854 Training loss: 6.6767449378967285
2025-12-09 12:23:57.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0002971029612991609 Training loss: 6.638968467712402
2025-12-09 12:23:58.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0002970846563190491 Training loss: 6.488215923309326
2025-12-09 12:23:58.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00029706629425825374 Training loss: 6.777031898498535
2025-12-09 12:23:59.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.00029704787512390085 Training loss: 7.06546688079834
2025-12-09 12:23:59.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0002970293989231385 Training loss: 6.899211406707764
2025-12-09 12:24:00.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.0002970108656631369 Training loss: 6.63032341003418
2025-12-09 12:24:00.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0002969922753510885 Training loss: 6.630556583404541
2025-12-09 12:24:01.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00029697362799420776 Training loss: 6.988895416259766
2025-12-09 12:24:01.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0002969549235997315 Training loss: 6.776453018188477
2025-12-09 12:24:02.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0002969361621749184 Training loss: 6.453487873077393
2025-12-09 12:24:02.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.00029691734372704943 Training loss: 7.183653831481934
2025-12-09 12:24:03.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0002968984682634277 Training loss: 6.816318511962891
2025-12-09 12:24:03.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0002968795357913784 Training loss: 7.13561487197876
2025-12-09 12:24:04.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0002968605463182488 Training loss: 7.108542442321777
2025-12-09 12:24:04.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0002968414998514085 Training loss: 6.877376079559326
2025-12-09 12:24:05.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0002968223963982488 Training loss: 6.808412551879883
2025-12-09 12:24:05.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.00029680323596618355 Training loss: 6.65167760848999
2025-12-09 12:24:06.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00029678401856264857 Training loss: 6.361397743225098
2025-12-09 12:24:06.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0002967647441951017 Training loss: 6.639608860015869
2025-12-09 12:24:07.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0002967454128710229 Training loss: 6.117376804351807
2025-12-09 12:24:07.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.00029672602459791434 Training loss: 6.692080020904541
2025-12-09 12:24:08.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0002967065793833002 Training loss: 7.228443145751953
2025-12-09 12:24:08.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0002966870772347269 Training loss: 6.625426292419434
2025-12-09 12:24:09.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0002966675181597627 Training loss: 7.333661079406738
2025-12-09 12:24:09.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0002966479021659981 Training loss: 6.6580657958984375
2025-12-09 12:24:10.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.00029662822926104576 Training loss: 6.96909761428833
2025-12-09 12:24:10.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0002966084994525403 Training loss: 7.2629008293151855
2025-12-09 12:24:11.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.00029658871274813853 Training loss: 7.144023895263672
2025-12-09 12:24:11.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.00029656886915551924 Training loss: 6.836947441101074
2025-12-09 12:24:12.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0002965489686823833 Training loss: 7.232394695281982
2025-12-09 12:24:12.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.00029652901133645377 Training loss: 6.87493371963501
2025-12-09 12:24:13.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0002965089971254757 Training loss: 6.596698760986328
2025-12-09 12:24:13.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0002964889260572162 Training loss: 6.71883487701416
2025-12-09 12:24:14.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0002964687981394644 Training loss: 6.509883403778076
2025-12-09 12:24:14.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.00029644861338003165 Training loss: 6.36197566986084
2025-12-09 12:24:15.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0002964283717867512 Training loss: 5.831789970397949
2025-12-09 12:24:15.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0002964080733674784 Training loss: 7.04181432723999
2025-12-09 12:24:16.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0002963877181300907 Training loss: 6.470491886138916
2025-12-09 12:24:16.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00029636730608248766 Training loss: 6.561017990112305
2025-12-09 12:24:17.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0002963468372325906 Training loss: 6.655717849731445
2025-12-09 12:24:17.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.00029632631158834326 Training loss: 6.528258800506592
2025-12-09 12:24:18.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.00029630572915771117 Training loss: 6.090678691864014
2025-12-09 12:24:18.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.000296285089948682 Training loss: 6.5820841789245605
2025-12-09 12:24:19.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.00029626439396926533 Training loss: 6.715540409088135
2025-12-09 12:24:19.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.00029624364122749294 Training loss: 6.7396039962768555
2025-12-09 12:24:20.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0002962228317314186 Training loss: 6.757885456085205
2025-12-09 12:24:20.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00029620196548911797 Training loss: 7.060742378234863
2025-12-09 12:24:21.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.0002961810425086889 Training loss: 6.839778900146484
2025-12-09 12:24:21.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00029616006279825126 Training loss: 7.053164482116699
2025-12-09 12:24:22.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0002961390263659467 Training loss: 6.141213893890381
2025-12-09 12:24:22.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0002961179332199391 Training loss: 6.857758522033691
2025-12-09 12:24:23.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00029609678336841444 Training loss: 6.785346508026123
2025-12-09 12:24:23.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.00029607557681958035 Training loss: 7.040229320526123
2025-12-09 12:24:24.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.00029605431358166684 Training loss: 6.987583160400391
2025-12-09 12:24:24.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.00029603299366292565 Training loss: 6.310708045959473
2025-12-09 12:24:25.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00029601161707163077 Training loss: 7.016108989715576
2025-12-09 12:24:25.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.00029599018381607785 Training loss: 7.318703651428223
2025-12-09 12:24:26.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0002959686939045848 Training loss: 6.860852241516113
2025-12-09 12:24:26.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.00029594714734549146 Training loss: 6.423022747039795
2025-12-09 12:24:27.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0002959255441471597 Training loss: 6.406862258911133
2025-12-09 12:24:27.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0002959038843179731 Training loss: 6.9554290771484375
2025-12-09 12:24:28.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0002958821678663376 Training loss: 6.726370811462402
2025-12-09 12:24:28.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.00029586039480068087 Training loss: 6.685351371765137
2025-12-09 12:24:29.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0002958385651294525 Training loss: 6.534305572509766
2025-12-09 12:24:29.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00029581667886112434 Training loss: 6.513578414916992
2025-12-09 12:24:30.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.00029579473600418993 Training loss: 6.586243629455566
2025-12-09 12:24:30.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0002957727365671649 Training loss: 6.845244884490967
2025-12-09 12:24:31.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0002957506805585867 Training loss: 7.507894992828369
2025-12-09 12:24:31.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00029572856798701504 Training loss: 6.705533981323242
2025-12-09 12:24:32.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0002957063988610312 Training loss: 6.670415878295898
2025-12-09 12:24:32.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0002956841731892386 Training loss: 6.5213799476623535
2025-12-09 12:24:33.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0002956618909802627 Training loss: 6.649342060089111
2025-12-09 12:24:33.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.00029563955224275065 Training loss: 6.650908946990967
2025-12-09 12:24:34.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.00029561715698537183 Training loss: 6.844213962554932
2025-12-09 12:24:34.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0002955947052168172 Training loss: 6.415319442749023
2025-12-09 12:24:35.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0002955721969458001 Training loss: 6.537859916687012
2025-12-09 12:24:35.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0002955496321810553 Training loss: 6.816422939300537
2025-12-09 12:24:36.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.00029552701093133994 Training loss: 6.961129188537598
2025-12-09 12:24:36.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.00029550433320543284 Training loss: 6.728102684020996
2025-12-09 12:24:37.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0002954815990121347 Training loss: 6.845798492431641
2025-12-09 12:24:37.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00029545880836026833 Training loss: 6.7213454246521
2025-12-09 12:24:38.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0002954359612586782 Training loss: 6.419521331787109
2025-12-09 12:24:38.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00029541305771623095 Training loss: 6.695436477661133
2025-12-09 12:24:39.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.00029539009774181494 Training loss: 7.039922714233398
2025-12-09 12:24:39.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.00029536708134434054 Training loss: 6.537868976593018
2025-12-09 12:24:40.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.00029534400853273985 Training loss: 6.787029266357422
2025-12-09 12:24:40.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0002953208793159671 Training loss: 6.543159484863281
2025-12-09 12:24:41.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.00029529769370299823 Training loss: 6.6168060302734375
2025-12-09 12:24:41.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0002952744517028311 Training loss: 6.370161533355713
2025-12-09 12:24:42.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00029525115332448555 Training loss: 6.903572082519531
2025-12-09 12:24:42.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0002952277985770032 Training loss: 6.799563407897949
2025-12-09 12:24:43.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0002952043874694475 Training loss: 6.674545764923096
2025-12-09 12:24:43.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00029518092001090397 Training loss: 6.950685501098633
2025-12-09 12:24:44.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00029515739621047973 Training loss: 6.430619239807129
2025-12-09 12:24:44.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.000295133816077304 Training loss: 6.366275310516357
2025-12-09 12:24:45.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.0002951101796205278 Training loss: 6.733737945556641
2025-12-09 12:24:45.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0002950864868493239 Training loss: 6.341740608215332
2025-12-09 12:24:46.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.00029506273777288696 Training loss: 6.586526870727539
2025-12-09 12:24:46.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0002950389324004337 Training loss: 6.681351184844971
2025-12-09 12:24:47.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.00029501507074120237 Training loss: 6.443542957305908
2025-12-09 12:24:47.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.00029499115280445326 Training loss: 6.483232498168945
2025-12-09 12:24:48.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0002949671785994685 Training loss: 6.318404197692871
2025-12-09 12:24:48.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.00029494314813555193 Training loss: 6.239515781402588
2025-12-09 12:24:49.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.00029491906142202934 Training loss: 7.149618625640869
2025-12-09 12:24:49.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.00029489491846824837 Training loss: 6.821633338928223
2025-12-09 12:24:50.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0002948707192835783 Training loss: 6.043242454528809
2025-12-09 12:24:50.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0002948464638774105 Training loss: 6.716642379760742
2025-12-09 12:24:51.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.00029482215225915795 Training loss: 6.580132007598877
2025-12-09 12:24:51.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0002947977844382555 Training loss: 6.566055774688721
2025-12-09 12:24:52.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0002947733604241599 Training loss: 6.493320465087891
2025-12-09 12:24:52.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.00029474888022634955 Training loss: 6.4293084144592285
2025-12-09 12:24:53.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.00029472434385432474 Training loss: 6.667917251586914
2025-12-09 12:24:53.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0002946997513176076 Training loss: 6.34693717956543
2025-12-09 12:24:54.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.000294675102625742 Training loss: 6.6396565437316895
2025-12-09 12:24:54.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.00029465039778829366 Training loss: 6.590345859527588
2025-12-09 12:24:55.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0002946256368148499 Training loss: 6.6911749839782715
2025-12-09 12:24:55.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.00029460081971502015 Training loss: 6.511083602905273
2025-12-09 12:24:56.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.00029457594649843534 Training loss: 6.385643005371094
2025-12-09 12:24:56.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0002945510171747483 Training loss: 6.670419692993164
2025-12-09 12:24:57.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0002945260317536336 Training loss: 6.896018028259277
2025-12-09 12:24:57.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0002945009902447876 Training loss: 6.89819860458374
2025-12-09 12:24:58.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.00029447589265792847 Training loss: 6.503737926483154
2025-12-09 12:24:58.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.00029445073900279605 Training loss: 6.731751441955566
2025-12-09 12:24:59.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.000294425529289152 Training loss: 7.011893272399902
2025-12-09 12:24:59.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.00029440026352677966 Training loss: 8.231888771057129
2025-12-09 12:25:00.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.00029437494172548424 Training loss: 6.310595989227295
2025-12-09 12:25:00.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.00029434956389509263 Training loss: 6.788820743560791
2025-12-09 12:25:01.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0002943241300454534 Training loss: 7.905280113220215
2025-12-09 12:25:01.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.0002942986401864371 Training loss: 5.833343505859375
2025-12-09 12:25:02.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0002942730943279357 Training loss: 6.407827854156494
2025-12-09 12:25:02.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.000294247492479863 Training loss: 6.3989410400390625
2025-12-09 12:25:03.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.00029422183465215474 Training loss: 6.42039680480957
2025-12-09 12:25:03.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00029419612085476813 Training loss: 6.897237777709961
2025-12-09 12:25:04.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0002941703510976822 Training loss: 6.557707786560059
2025-12-09 12:25:04.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.00029414452539089776 Training loss: 6.663332462310791
2025-12-09 12:25:05.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00029411864374443716 Training loss: 6.184370517730713
2025-12-09 12:25:05.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0002940927061683446 Training loss: 6.626238822937012
2025-12-09 12:25:06.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.0002940667126726859 Training loss: 6.378687858581543
2025-12-09 12:25:06.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0002940406632675487 Training loss: 6.402095794677734
2025-12-09 12:25:07.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0002940145579630423 Training loss: 6.530222415924072
2025-12-09 12:25:07.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.00029398839676929756 Training loss: 6.235146522521973
2025-12-09 12:25:08.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.00029396217969646717 Training loss: 6.569255828857422
2025-12-09 12:25:08.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00029393590675472545 Training loss: 6.547133445739746
2025-12-09 12:25:09.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00029390957795426845 Training loss: 6.361693382263184
2025-12-09 12:25:09.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0002938831933053138 Training loss: 6.823136329650879
2025-12-09 12:25:10.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.000293856752818101 Training loss: 5.988265514373779
2025-12-09 12:25:10.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.00029383025650289095 Training loss: 6.697545528411865
2025-12-09 12:25:11.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0002938037043699664 Training loss: 6.612017631530762
2025-12-09 12:25:11.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0002937770964296317 Training loss: 6.7618632316589355
2025-12-09 12:25:12.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0002937504326922129 Training loss: 6.907101154327393
2025-12-09 12:25:12.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.00029372371316805767 Training loss: 6.647493839263916
2025-12-09 12:25:13.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.00029369693786753534 Training loss: 6.64600944519043
2025-12-09 12:25:13.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0002936701068010368 Training loss: 6.787437438964844
2025-12-09 12:25:14.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0002936432199789748 Training loss: 6.894039630889893
2025-12-09 12:25:14.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.00029361627741178356 Training loss: 7.05488920211792
2025-12-09 12:25:15.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.00029358927910991885 Training loss: 6.720664978027344
2025-12-09 12:25:15.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.00029356222508385827 Training loss: 6.231743812561035
2025-12-09 12:25:16.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.000293535115344101 Training loss: 6.491024017333984
2025-12-09 12:25:16.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0002935079499011677 Training loss: 6.2545695304870605
2025-12-09 12:25:17.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0002934807287656008 Training loss: 6.341223239898682
2025-12-09 12:25:17.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.00029345345194796435 Training loss: 6.541720390319824
2025-12-09 12:25:18.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0002934261194588438 Training loss: 6.632734775543213
2025-12-09 12:25:18.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.00029339873130884654 Training loss: 6.498826026916504
2025-12-09 12:25:19.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.00029337128750860124 Training loss: 6.60556697845459
2025-12-09 12:25:19.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.00029334378806875836 Training loss: 6.455318927764893
2025-12-09 12:25:20.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.00029331623299998986 Training loss: 6.389649868011475
2025-12-09 12:25:20.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0002932886223129894 Training loss: 7.18873405456543
2025-12-09 12:25:21.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.000293260956018472 Training loss: 6.155402183532715
2025-12-09 12:25:21.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0002932332341271746 Training loss: 6.994700908660889
2025-12-09 12:25:22.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00029320545664985535 Training loss: 7.468626499176025
2025-12-09 12:25:22.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.00029317762359729423 Training loss: 6.5482587814331055
2025-12-09 12:25:23.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.00029314973498029275 Training loss: 6.535046100616455
2025-12-09 12:25:23.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0002931217908096739 Training loss: 6.835795879364014
2025-12-09 12:25:24.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0002930937910962822 Training loss: 6.115656852722168
2025-12-09 12:25:24.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.00029306573585098384 Training loss: 6.839248180389404
2025-12-09 12:25:25.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.00029303762508466654 Training loss: 6.748213291168213
2025-12-09 12:25:25.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00029300945880823956 Training loss: 6.764068126678467
2025-12-09 12:25:26.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0002929812370326336 Training loss: 7.071047782897949
2025-12-09 12:25:26.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.000292952959768801 Training loss: 6.72433614730835
2025-12-09 12:25:27.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0002929246270277157 Training loss: 7.206926345825195
2025-12-09 12:25:27.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.000292896238820373 Training loss: 6.24918270111084
2025-12-09 12:25:28.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0002928677951577898 Training loss: 6.561212062835693
2025-12-09 12:25:28.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.00029283929605100455 Training loss: 6.501022815704346
2025-12-09 12:25:29.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0002928107415110772 Training loss: 6.467002868652344
2025-12-09 12:25:29.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0002927821315490893 Training loss: 6.727989673614502
2025-12-09 12:25:30.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0002927534661761436 Training loss: 6.8573808670043945
2025-12-09 12:25:30.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.00029272474540336475 Training loss: 6.451075553894043
2025-12-09 12:25:31.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.00029269596924189875 Training loss: 6.342863082885742
2025-12-09 12:25:31.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0002926671377029129 Training loss: 6.810486316680908
2025-12-09 12:25:32.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.00029263825079759635 Training loss: 6.62162971496582
2025-12-09 12:25:32.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.00029260930853715935 Training loss: 6.710310935974121
2025-12-09 12:25:33.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0002925803109328339 Training loss: 6.541512966156006
2025-12-09 12:25:33.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0002925512579958735 Training loss: 6.502023220062256
2025-12-09 12:25:34.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0002925221497375529 Training loss: 6.2329888343811035
2025-12-09 12:25:34.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.00029249298616916856 Training loss: 6.546230792999268
2025-12-09 12:25:35.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.00029246376730203817 Training loss: 6.330800533294678
2025-12-09 12:25:35.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0002924344931475011 Training loss: 6.363105773925781
2025-12-09 12:25:36.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.000292405163716918 Training loss: 6.468930721282959
2025-12-09 12:25:36.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0002923757790216711 Training loss: 6.245622158050537
2025-12-09 12:25:37.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.000292346339073164 Training loss: 6.45366907119751
2025-12-09 12:25:37.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.00029231684388282184 Training loss: 6.577494144439697
2025-12-09 12:25:38.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.000292287293462091 Training loss: 6.737673759460449
2025-12-09 12:25:38.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0002922576878224395 Training loss: 6.617131233215332
2025-12-09 12:25:39.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.00029222802697535674 Training loss: 6.8037943840026855
2025-12-09 12:25:39.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0002921983109323535 Training loss: 6.614975452423096
2025-12-09 12:25:40.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0002921685397049619 Training loss: 6.324967384338379
2025-12-09 12:25:40.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0002921387133047357 Training loss: 6.637189865112305
2025-12-09 12:25:41.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0002921088317432499 Training loss: 6.927946090698242
2025-12-09 12:25:41.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0002920788950321009 Training loss: 6.80064058303833
2025-12-09 12:25:42.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.00029204890318290666 Training loss: 6.436922073364258
2025-12-09 12:25:42.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0002920188562073063 Training loss: 6.91193962097168
2025-12-09 12:25:43.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0002919887541169605 Training loss: 6.4444355964660645
2025-12-09 12:25:43.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0002919585969235514 Training loss: 6.593950271606445
2025-12-09 12:25:44.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.00029192838463878236 Training loss: 6.732170104980469
2025-12-09 12:25:44.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0002918981172743781 Training loss: 6.283050060272217
2025-12-09 12:25:45.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.00029186779484208485 Training loss: 6.319036483764648
2025-12-09 12:25:45.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0002918374173536702 Training loss: 6.547009468078613
2025-12-09 12:25:46.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.000291806984820923 Training loss: 6.509109973907471
2025-12-09 12:25:46.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.00029177649725565353 Training loss: 6.5076422691345215
2025-12-09 12:25:47.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.00029174595466969344 Training loss: 6.920194625854492
2025-12-09 12:25:47.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.00029171535707489565 Training loss: 6.805829048156738
2025-12-09 12:25:48.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0002916847044831346 Training loss: 6.374271392822266
2025-12-09 12:25:48.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0002916539969063059 Training loss: 6.813254356384277
2025-12-09 12:25:49.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0002916232343563265 Training loss: 6.339118003845215
2025-12-09 12:25:49.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0002915924168451349 Training loss: 6.510984897613525
2025-12-09 12:25:50.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0002915615443846906 Training loss: 6.774764537811279
2025-12-09 12:25:50.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0002915306169869747 Training loss: 6.626612186431885
2025-12-09 12:25:51.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0002914996346639895 Training loss: 7.692838668823242
2025-12-09 12:25:51.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.00029146859742775865 Training loss: 6.820408821105957
2025-12-09 12:25:52.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00029143750529032707 Training loss: 6.449343681335449
2025-12-09 12:25:52.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0002914063582637611 Training loss: 5.967576503753662
2025-12-09 12:25:53.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0002913751563601481 Training loss: 6.447951316833496
2025-12-09 12:25:53.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0002913438995915971 Training loss: 6.678717136383057
2025-12-09 12:25:54.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0002913125879702381 Training loss: 6.413311958312988
2025-12-09 12:25:54.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.00029128122150822263 Training loss: 6.368016719818115
2025-12-09 12:25:55.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0002912498002177234 Training loss: 6.571152210235596
2025-12-09 12:25:55.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0002912183241109344 Training loss: 6.6538848876953125
2025-12-09 12:25:56.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.00029118679320007087 Training loss: 6.784142017364502
2025-12-09 12:25:56.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0002911552074973693 Training loss: 6.10978889465332
2025-12-09 12:25:57.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0002911235670150875 Training loss: 6.524747371673584
2025-12-09 12:25:57.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0002910918717655046 Training loss: 6.493642330169678
2025-12-09 12:25:58.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00029106012176092084 Training loss: 6.702610492706299
2025-12-09 12:25:58.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0002910283170136578 Training loss: 6.77406120300293
2025-12-09 12:25:59.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00029099645753605827 Training loss: 6.782020092010498
2025-12-09 12:25:59.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.00029096454334048627 Training loss: 6.53703498840332
2025-12-09 12:26:00.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0002909325744393271 Training loss: 6.477913856506348
2025-12-09 12:26:00.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0002909005508449873 Training loss: 6.651147842407227
2025-12-09 12:26:01.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0002908684725698946 Training loss: 6.3358001708984375
2025-12-09 12:26:01.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0002908363396264978 Training loss: 6.68843936920166
2025-12-09 12:26:02.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.00029080415202726727 Training loss: 6.662461280822754
2025-12-09 12:26:02.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0002907719097846943 Training loss: 6.528373718261719
2025-12-09 12:26:03.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0002907396129112915 Training loss: 6.370907783508301
2025-12-09 12:26:03.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.00029070726141959265 Training loss: 6.46120023727417
2025-12-09 12:26:04.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.00029067485532215267 Training loss: 6.371310710906982
2025-12-09 12:26:04.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0002906423946315478 Training loss: 6.080550670623779
2025-12-09 12:26:05.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.00029060987936037536 Training loss: 6.388772964477539
2025-12-09 12:26:05.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.00029057730952125393 Training loss: 6.59747838973999
2025-12-09 12:26:06.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0002905446851268233 Training loss: 6.778031826019287
2025-12-09 12:26:06.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0002905120061897441 Training loss: 6.8816938400268555
2025-12-09 12:26:07.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0002904792727226987 Training loss: 6.707498550415039
2025-12-09 12:26:07.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.00029044648473839014 Training loss: 6.824956893920898
2025-12-09 12:26:08.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0002904136422495429 Training loss: 6.630346298217773
2025-12-09 12:26:08.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0002903807452689024 Training loss: 6.573080539703369
2025-12-09 12:26:09.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00029034779380923535 Training loss: 6.338686943054199
2025-12-09 12:26:09.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.00029031478788332955 Training loss: 6.774095058441162
2025-12-09 12:26:10.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0002902817275039941 Training loss: 6.5289130210876465
2025-12-09 12:26:10.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.00029024861268405887 Training loss: 6.474934101104736
2025-12-09 12:26:11.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.00029021544343637526 Training loss: 6.605628967285156
2025-12-09 12:26:11.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.00029018221977381546 Training loss: 6.41925573348999
2025-12-09 12:26:12.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.00029014894170927306 Training loss: 6.515495777130127
2025-12-09 12:26:12.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0002901156092556625 Training loss: 6.286942958831787
2025-12-09 12:26:13.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0002900822224259195 Training loss: 6.887917995452881
2025-12-09 12:26:13.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0002900487812330009 Training loss: 6.3709845542907715
2025-12-09 12:26:14.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.00029001528568988453 Training loss: 6.309436321258545
2025-12-09 12:26:14.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.00028998173580956934 Training loss: 6.363369941711426
2025-12-09 12:26:15.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00028994813160507536 Training loss: 6.637786388397217
2025-12-09 12:26:15.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0002899144730894438 Training loss: 6.551698207855225
2025-12-09 12:26:16.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00028988076027573685 Training loss: 6.680747985839844
2025-12-09 12:26:16.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.00028984699317703775 Training loss: 6.420063018798828
2025-12-09 12:26:17.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0002898131718064509 Training loss: 6.549717903137207
2025-12-09 12:26:17.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.00028977929617710166 Training loss: 6.442731857299805
2025-12-09 12:26:18.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.00028974536630213657 Training loss: 6.690125942230225
2025-12-09 12:26:18.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.00028971138219472303 Training loss: 6.7402663230896
2025-12-09 12:26:19.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.00028967734386804977 Training loss: 6.358127593994141
2025-12-09 12:26:19.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.0002896432513353264 Training loss: 6.568753719329834
2025-12-09 12:26:20.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.00028960910460978337 Training loss: 6.474996566772461
2025-12-09 12:26:20.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0002895749037046725 Training loss: 6.354576110839844
2025-12-09 12:26:21.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0002895406486332665 Training loss: 6.4392409324646
2025-12-09 12:26:21.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.00028950633940885907 Training loss: 6.435054779052734
2025-12-09 12:26:22.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.00028947197604476494 Training loss: 6.454556465148926
2025-12-09 12:26:22.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.00028943755855431985 Training loss: 5.793407440185547
2025-12-09 12:26:23.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0002894030869508806 Training loss: 6.646829605102539
2025-12-09 12:26:23.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.0002893685612478249 Training loss: 6.805997848510742
2025-12-09 12:26:24.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.00028933398145855154 Training loss: 6.92694091796875
2025-12-09 12:26:24.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0002892993475964802 Training loss: 6.282048225402832
2025-12-09 12:26:25.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0002892646596750517 Training loss: 6.617573261260986
2025-12-09 12:26:25.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0002892299177077277 Training loss: 6.241048812866211
2025-12-09 12:26:26.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0002891951217079908 Training loss: 6.356611251831055
2025-12-09 12:26:26.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0002891602716893448 Training loss: 6.55258846282959
2025-12-09 12:26:27.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0002891253676653142 Training loss: 6.70468807220459
2025-12-09 12:26:27.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.00028909040964944456 Training loss: 6.568999767303467
2025-12-09 12:26:28.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.0002890553976553025 Training loss: 6.431135654449463
2025-12-09 12:26:28.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.00028902033169647543 Training loss: 6.680624961853027
2025-12-09 12:26:29.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.00028898521178657174 Training loss: 6.883256435394287
2025-12-09 12:26:29.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0002889500379392209 Training loss: 6.671437740325928
2025-12-09 12:26:30.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.000288914810168073 Training loss: 6.029820442199707
2025-12-09 12:26:30.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.00028887952848679943 Training loss: 6.350650310516357
2025-12-09 12:26:31.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.0002888441929090922 Training loss: 6.396564483642578
2025-12-09 12:26:31.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.00028880880344866447 Training loss: 6.374082565307617
2025-12-09 12:26:32.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.00028877336011925005 Training loss: 5.998293399810791
2025-12-09 12:26:32.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.000288737862934604 Training loss: 6.176441192626953
2025-12-09 12:26:33.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.00028870231190850185 Training loss: 6.5088653564453125
2025-12-09 12:26:33.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.00028866670705474047 Training loss: 6.549511432647705
2025-12-09 12:26:34.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.00028863104838713726 Training loss: 6.48929500579834
2025-12-09 12:26:34.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.00028859533591953074 Training loss: 6.718307018280029
2025-12-09 12:26:35.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.00028855956966578023 Training loss: 6.359255790710449
2025-12-09 12:26:35.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.00028852374963976585 Training loss: 6.761305809020996
2025-12-09 12:26:36.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.0002884878758553887 Training loss: 6.366689682006836
2025-12-09 12:26:36.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.00028845194832657065 Training loss: 6.513833045959473
2025-12-09 12:26:37.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.0002884159670672545 Training loss: 6.843564510345459
2025-12-09 12:26:37.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.00028837993209140385 Training loss: 6.956070899963379
