2025-12-09 10:06:29.795 | INFO     | __main__:train:24 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 4.785327434539795
2025-12-09 10:06:29.813 | INFO     | __main__:train:24 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 4.819818496704102
2025-12-09 10:06:29.823 | INFO     | __main__:train:24 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 4.804565906524658
2025-12-09 10:06:29.832 | INFO     | __main__:train:24 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 4.749790668487549
2025-12-09 10:06:29.842 | INFO     | __main__:train:24 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 4.81915283203125
2025-12-09 10:06:29.851 | INFO     | __main__:train:24 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 4.861029624938965
2025-12-09 10:06:29.860 | INFO     | __main__:train:24 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 4.799532413482666
2025-12-09 10:06:29.869 | INFO     | __main__:train:24 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 4.771493434906006
2025-12-09 10:06:29.878 | INFO     | __main__:train:24 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 4.964752197265625
2025-12-09 10:06:29.886 | INFO     | __main__:train:24 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 4.750262260437012
2025-12-09 10:06:29.896 | INFO     | __main__:train:24 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 4.794868469238281
2025-12-09 10:06:29.904 | INFO     | __main__:train:24 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 4.771642684936523
2025-12-09 10:06:29.913 | INFO     | __main__:train:24 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 4.733961582183838
2025-12-09 10:06:29.922 | INFO     | __main__:train:24 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 4.6485700607299805
2025-12-09 10:06:29.931 | INFO     | __main__:train:24 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 4.7442708015441895
2025-12-09 10:06:29.940 | INFO     | __main__:train:24 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 4.58980655670166
2025-12-09 10:06:29.949 | INFO     | __main__:train:24 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 4.754114151000977
2025-12-09 10:06:29.958 | INFO     | __main__:train:24 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 4.624231815338135
2025-12-09 10:06:29.967 | INFO     | __main__:train:24 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 4.50446081161499
2025-12-09 10:06:29.976 | INFO     | __main__:train:24 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 4.557643890380859
2025-12-09 10:06:29.984 | INFO     | __main__:train:24 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 4.641833782196045
2025-12-09 10:06:29.994 | INFO     | __main__:train:24 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 4.574362277984619
2025-12-09 10:06:30.002 | INFO     | __main__:train:24 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 4.533324718475342
2025-12-09 10:06:30.011 | INFO     | __main__:train:24 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 4.58717155456543
2025-12-09 10:06:30.020 | INFO     | __main__:train:24 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 4.513736248016357
2025-12-09 10:06:30.028 | INFO     | __main__:train:24 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 4.552007675170898
2025-12-09 10:06:30.037 | INFO     | __main__:train:24 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 4.40024471282959
2025-12-09 10:06:30.046 | INFO     | __main__:train:24 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 4.473487377166748
2025-12-09 10:06:30.054 | INFO     | __main__:train:24 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 4.41727352142334
2025-12-09 10:06:30.063 | INFO     | __main__:train:24 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 4.391679763793945
2025-12-09 10:06:30.072 | INFO     | __main__:train:24 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 4.3071770668029785
2025-12-09 10:06:30.080 | INFO     | __main__:train:24 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 4.383708953857422
2025-12-09 10:06:30.089 | INFO     | __main__:train:24 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 4.272345066070557
2025-12-09 10:06:30.098 | INFO     | __main__:train:24 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 4.260380744934082
2025-12-09 10:06:30.107 | INFO     | __main__:train:24 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 4.251530647277832
2025-12-09 10:06:30.118 | INFO     | __main__:train:24 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 4.161488056182861
2025-12-09 10:06:30.126 | INFO     | __main__:train:24 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 4.331816673278809
2025-12-09 10:06:30.135 | INFO     | __main__:train:24 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 4.2743821144104
2025-12-09 10:06:30.144 | INFO     | __main__:train:24 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 4.257899284362793
2025-12-09 10:06:30.152 | INFO     | __main__:train:24 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 4.315646171569824
2025-12-09 10:06:30.161 | INFO     | __main__:train:24 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 4.071743011474609
2025-12-09 10:06:30.170 | INFO     | __main__:train:24 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 4.109609127044678
2025-12-09 10:06:30.178 | INFO     | __main__:train:24 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 4.308043003082275
2025-12-09 10:06:30.187 | INFO     | __main__:train:24 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 4.209845542907715
2025-12-09 10:06:30.196 | INFO     | __main__:train:24 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 4.426250457763672
2025-12-09 10:06:30.209 | INFO     | __main__:train:24 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 4.194648742675781
2025-12-09 10:06:30.217 | INFO     | __main__:train:24 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 4.197292327880859
2025-12-09 10:06:30.226 | INFO     | __main__:train:24 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 4.100912094116211
2025-12-09 10:06:30.236 | INFO     | __main__:train:24 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 3.9879307746887207
2025-12-09 10:06:30.245 | INFO     | __main__:train:24 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 4.023214340209961
2025-12-09 10:06:30.253 | INFO     | __main__:train:24 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 4.064911365509033
2025-12-09 10:06:30.263 | INFO     | __main__:train:24 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 4.077817916870117
2025-12-09 10:06:30.271 | INFO     | __main__:train:24 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 4.285807132720947
2025-12-09 10:06:30.280 | INFO     | __main__:train:24 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 4.191381454467773
2025-12-09 10:06:30.289 | INFO     | __main__:train:24 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 4.140826225280762
2025-12-09 10:06:30.298 | INFO     | __main__:train:24 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 4.145630836486816
2025-12-09 10:06:30.306 | INFO     | __main__:train:24 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 4.021300315856934
2025-12-09 10:06:30.315 | INFO     | __main__:train:24 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 4.0287065505981445
2025-12-09 10:06:30.323 | INFO     | __main__:train:24 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 4.14361572265625
2025-12-09 10:06:30.332 | INFO     | __main__:train:24 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 4.060873508453369
2025-12-09 10:06:30.341 | INFO     | __main__:train:24 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 3.9914350509643555
2025-12-09 10:06:30.349 | INFO     | __main__:train:24 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 4.053844451904297
2025-12-09 10:06:30.358 | INFO     | __main__:train:24 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 4.139436721801758
2025-12-09 10:06:30.367 | INFO     | __main__:train:24 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 3.999554395675659
2025-12-09 10:06:30.376 | INFO     | __main__:train:24 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 3.8944098949432373
2025-12-09 10:06:30.384 | INFO     | __main__:train:24 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 4.234257221221924
2025-12-09 10:06:30.393 | INFO     | __main__:train:24 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 3.9814608097076416
2025-12-09 10:06:30.402 | INFO     | __main__:train:24 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 3.8452281951904297
2025-12-09 10:06:30.411 | INFO     | __main__:train:24 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 4.0507612228393555
2025-12-09 10:06:30.419 | INFO     | __main__:train:24 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 4.167977333068848
2025-12-09 10:06:30.429 | INFO     | __main__:train:24 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 3.9934773445129395
2025-12-09 10:06:30.438 | INFO     | __main__:train:24 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 3.8579037189483643
2025-12-09 10:06:30.446 | INFO     | __main__:train:24 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 4.215763092041016
2025-12-09 10:06:30.454 | INFO     | __main__:train:24 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 3.947359085083008
2025-12-09 10:06:30.464 | INFO     | __main__:train:24 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 4.187653064727783
2025-12-09 10:06:30.472 | INFO     | __main__:train:24 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 4.069011688232422
2025-12-09 10:06:30.481 | INFO     | __main__:train:24 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 3.819049596786499
2025-12-09 10:06:30.489 | INFO     | __main__:train:24 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 4.201991081237793
2025-12-09 10:06:30.554 | INFO     | __main__:train:24 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 4.00170373916626
