Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 50/10000 [00:00<00:20, 492.95it/s]Tokenizing texts:   1%|▏         | 135/10000 [00:00<00:14, 700.69it/s]Tokenizing texts:   2%|▏         | 206/10000 [00:00<00:15, 626.12it/s]Tokenizing texts:   3%|▎         | 274/10000 [00:00<00:15, 644.46it/s]Tokenizing texts:   3%|▎         | 340/10000 [00:00<00:15, 615.94it/s]Tokenizing texts:   4%|▍         | 403/10000 [00:00<00:15, 607.81it/s]Tokenizing texts:   5%|▍         | 465/10000 [00:00<00:16, 578.46it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 580.03it/s]Tokenizing texts:   6%|▌         | 597/10000 [00:00<00:15, 605.42it/s]Tokenizing texts:   7%|▋         | 658/10000 [00:01<00:15, 598.96it/s]Tokenizing texts:   7%|▋         | 719/10000 [00:01<00:15, 583.69it/s]Tokenizing texts:   8%|▊         | 780/10000 [00:01<00:15, 590.93it/s]Tokenizing texts:   8%|▊         | 840/10000 [00:01<00:15, 579.06it/s]Tokenizing texts:   9%|▉         | 906/10000 [00:01<00:15, 601.32it/s]Tokenizing texts:  10%|▉         | 984/10000 [00:01<00:13, 652.08it/s]Tokenizing texts:  11%|█         | 1051/10000 [00:01<00:13, 640.15it/s]Tokenizing texts:  11%|█         | 1116/10000 [00:01<00:13, 638.02it/s]Tokenizing texts:  12%|█▏        | 1180/10000 [00:01<00:14, 604.11it/s]Tokenizing texts:  12%|█▏        | 1241/10000 [00:02<00:14, 596.65it/s]Tokenizing texts:  13%|█▎        | 1308/10000 [00:02<00:14, 617.30it/s]Tokenizing texts:  14%|█▎        | 1371/10000 [00:02<00:14, 590.35it/s]Tokenizing texts:  14%|█▍        | 1442/10000 [00:02<00:13, 623.02it/s]Tokenizing texts:  15%|█▌        | 1518/10000 [00:02<00:12, 656.38it/s]Tokenizing texts:  16%|█▌        | 1594/10000 [00:02<00:12, 685.46it/s]Tokenizing texts:  17%|█▋        | 1663/10000 [00:02<00:13, 618.02it/s]Tokenizing texts:  17%|█▋        | 1727/10000 [00:02<00:13, 618.29it/s]Tokenizing texts:  18%|█▊        | 1794/10000 [00:02<00:12, 631.97it/s]Tokenizing texts:  19%|█▊        | 1859/10000 [00:03<00:13, 613.50it/s]Tokenizing texts:  19%|█▉        | 1921/10000 [00:03<00:13, 603.16it/s]Tokenizing texts:  20%|█▉        | 1982/10000 [00:03<00:13, 582.95it/s]Tokenizing texts:  21%|██        | 2052/10000 [00:03<00:12, 614.70it/s]Tokenizing texts:  21%|██        | 2114/10000 [00:03<00:14, 562.52it/s]Tokenizing texts:  22%|██▏       | 2172/10000 [00:03<00:13, 562.93it/s]Tokenizing texts:  22%|██▏       | 2249/10000 [00:03<00:12, 619.61it/s]Tokenizing texts:  23%|██▎       | 2312/10000 [00:03<00:12, 602.46it/s]Tokenizing texts:  24%|██▍       | 2381/10000 [00:03<00:12, 627.05it/s]Tokenizing texts:  24%|██▍       | 2446/10000 [00:03<00:11, 630.22it/s]Tokenizing texts:  25%|██▌       | 2510/10000 [00:04<00:11, 628.27it/s]Tokenizing texts:  26%|██▌       | 2574/10000 [00:04<00:12, 615.54it/s]Tokenizing texts:  26%|██▋       | 2636/10000 [00:04<00:12, 580.74it/s]Tokenizing texts:  27%|██▋       | 2695/10000 [00:04<00:13, 553.91it/s]Tokenizing texts:  28%|██▊       | 2769/10000 [00:04<00:11, 603.42it/s]Tokenizing texts:  28%|██▊       | 2831/10000 [00:04<00:12, 560.16it/s]Tokenizing texts:  29%|██▉       | 2889/10000 [00:04<00:12, 563.57it/s]Tokenizing texts:  30%|██▉       | 2952/10000 [00:04<00:12, 580.97it/s]Tokenizing texts:  30%|███       | 3024/10000 [00:04<00:11, 602.94it/s]Tokenizing texts:  31%|███       | 3086/10000 [00:05<00:11, 601.57it/s]Tokenizing texts:  32%|███▏      | 3153/10000 [00:05<00:11, 613.56it/s]Tokenizing texts:  32%|███▏      | 3227/10000 [00:05<00:10, 649.25it/s]Tokenizing texts:  33%|███▎      | 3294/10000 [00:05<00:10, 649.15it/s]Tokenizing texts:  34%|███▎      | 3361/10000 [00:05<00:10, 650.90it/s]Tokenizing texts:  34%|███▍      | 3427/10000 [00:05<00:10, 652.20it/s]Tokenizing texts:  35%|███▍      | 3497/10000 [00:05<00:09, 653.45it/s]Tokenizing texts:  36%|███▌      | 3566/10000 [00:05<00:09, 659.89it/s]Tokenizing texts:  36%|███▋      | 3633/10000 [00:05<00:09, 662.28it/s]Tokenizing texts:  37%|███▋      | 3700/10000 [00:06<00:09, 637.96it/s]Tokenizing texts:  38%|███▊      | 3765/10000 [00:06<00:10, 617.72it/s]Tokenizing texts:  38%|███▊      | 3828/10000 [00:06<00:10, 588.59it/s]Tokenizing texts:  39%|███▉      | 3896/10000 [00:06<00:10, 603.98it/s]Tokenizing texts:  40%|███▉      | 3959/10000 [00:06<00:09, 607.77it/s]Tokenizing texts:  40%|████      | 4021/10000 [00:06<00:10, 597.37it/s]Tokenizing texts:  41%|████      | 4096/10000 [00:06<00:09, 640.17it/s]Tokenizing texts:  42%|████▏     | 4169/10000 [00:06<00:08, 665.36it/s]Tokenizing texts:  42%|████▏     | 4236/10000 [00:06<00:08, 664.14it/s]Tokenizing texts:  43%|████▎     | 4303/10000 [00:06<00:09, 632.36it/s]Tokenizing texts:  44%|████▍     | 4382/10000 [00:07<00:08, 676.61it/s]Tokenizing texts:  45%|████▍     | 4451/10000 [00:07<00:08, 672.63it/s]Tokenizing texts:  45%|████▌     | 4519/10000 [00:07<00:08, 642.88it/s]Tokenizing texts:  46%|████▌     | 4585/10000 [00:07<00:08, 643.89it/s]Tokenizing texts:  47%|████▋     | 4660/10000 [00:07<00:07, 671.15it/s]Tokenizing texts:  47%|████▋     | 4739/10000 [00:07<00:07, 703.57it/s]Tokenizing texts:  48%|████▊     | 4810/10000 [00:07<00:07, 662.47it/s]Tokenizing texts:  49%|████▉     | 4882/10000 [00:07<00:07, 678.10it/s]Tokenizing texts:  50%|████▉     | 4953/10000 [00:07<00:07, 684.16it/s]Tokenizing texts:  50%|█████     | 5029/10000 [00:08<00:07, 701.74it/s]Tokenizing texts:  51%|█████     | 5100/10000 [00:08<00:07, 651.55it/s]Tokenizing texts:  52%|█████▏    | 5167/10000 [00:08<00:07, 634.80it/s]Tokenizing texts:  52%|█████▏    | 5232/10000 [00:08<00:07, 629.04it/s]Tokenizing texts:  53%|█████▎    | 5304/10000 [00:08<00:07, 651.65it/s]Tokenizing texts:  54%|█████▍    | 5388/10000 [00:08<00:06, 703.75it/s]Tokenizing texts:  55%|█████▍    | 5459/10000 [00:08<00:06, 691.04it/s]Tokenizing texts:  55%|█████▌    | 5529/10000 [00:08<00:07, 634.58it/s]Tokenizing texts:  56%|█████▌    | 5596/10000 [00:08<00:06, 642.95it/s]Tokenizing texts:  57%|█████▋    | 5683/10000 [00:09<00:06, 706.31it/s]Tokenizing texts:  58%|█████▊    | 5755/10000 [00:09<00:06, 674.65it/s]Tokenizing texts:  58%|█████▊    | 5824/10000 [00:09<00:06, 602.45it/s]Tokenizing texts:  59%|█████▉    | 5893/10000 [00:09<00:06, 624.37it/s]Tokenizing texts:  60%|█████▉    | 5958/10000 [00:09<00:06, 625.53it/s]Tokenizing texts:  60%|██████    | 6022/10000 [00:09<00:06, 614.34it/s]Tokenizing texts:  61%|██████    | 6093/10000 [00:09<00:06, 638.46it/s]Tokenizing texts:  62%|██████▏   | 6167/10000 [00:09<00:05, 667.19it/s]Tokenizing texts:  62%|██████▏   | 6235/10000 [00:09<00:05, 650.09it/s]Tokenizing texts:  63%|██████▎   | 6301/10000 [00:10<00:05, 631.89it/s]Tokenizing texts:  64%|██████▎   | 6365/10000 [00:10<00:05, 614.93it/s]Tokenizing texts:  64%|██████▍   | 6427/10000 [00:10<00:05, 616.02it/s]Tokenizing texts:  65%|██████▍   | 6492/10000 [00:10<00:05, 624.95it/s]Tokenizing texts:  66%|██████▌   | 6557/10000 [00:10<00:05, 632.16it/s]Tokenizing texts:  66%|██████▋   | 6628/10000 [00:10<00:05, 653.55it/s]Tokenizing texts:  67%|██████▋   | 6694/10000 [00:10<00:05, 619.81it/s]Tokenizing texts:  68%|██████▊   | 6769/10000 [00:10<00:04, 655.53it/s]Tokenizing texts:  69%|██████▊   | 6851/10000 [00:10<00:04, 700.14it/s]Tokenizing texts:  69%|██████▉   | 6922/10000 [00:11<00:04, 619.38it/s]Tokenizing texts:  70%|██████▉   | 6987/10000 [00:11<00:04, 627.00it/s]Tokenizing texts:  71%|███████   | 7052/10000 [00:11<00:04, 608.27it/s]Tokenizing texts:  71%|███████   | 7114/10000 [00:11<00:04, 582.61it/s]Tokenizing texts:  72%|███████▏  | 7177/10000 [00:11<00:04, 594.76it/s]Tokenizing texts:  72%|███████▏  | 7244/10000 [00:11<00:04, 611.46it/s]Tokenizing texts:  73%|███████▎  | 7330/10000 [00:11<00:03, 681.13it/s]Tokenizing texts:  74%|███████▍  | 7399/10000 [00:11<00:04, 636.14it/s]Tokenizing texts:  75%|███████▍  | 7464/10000 [00:11<00:04, 583.02it/s]Tokenizing texts:  75%|███████▌  | 7543/10000 [00:12<00:03, 637.48it/s]Tokenizing texts:  76%|███████▌  | 7617/10000 [00:12<00:03, 665.10it/s]Tokenizing texts:  77%|███████▋  | 7689/10000 [00:12<00:03, 678.60it/s]Tokenizing texts:  78%|███████▊  | 7758/10000 [00:12<00:03, 624.10it/s]Tokenizing texts:  78%|███████▊  | 7822/10000 [00:12<00:03, 621.50it/s]Tokenizing texts:  79%|███████▉  | 7886/10000 [00:12<00:03, 616.68it/s]Tokenizing texts:  80%|███████▉  | 7966/10000 [00:12<00:03, 666.23it/s]Tokenizing texts:  80%|████████  | 8036/10000 [00:12<00:02, 675.31it/s]Tokenizing texts:  81%|████████  | 8105/10000 [00:12<00:03, 581.26it/s]Tokenizing texts:  82%|████████▏ | 8169/10000 [00:13<00:03, 595.58it/s]Tokenizing texts:  82%|████████▏ | 8231/10000 [00:13<00:02, 599.12it/s]Tokenizing texts:  83%|████████▎ | 8293/10000 [00:13<00:02, 593.53it/s]Tokenizing texts:  84%|████████▎ | 8358/10000 [00:13<00:02, 601.90it/s]Tokenizing texts:  84%|████████▍ | 8419/10000 [00:13<00:02, 587.88it/s]Tokenizing texts:  85%|████████▍ | 8497/10000 [00:13<00:02, 639.11it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:13<00:02, 640.22it/s]Tokenizing texts:  86%|████████▋ | 8636/10000 [00:13<00:02, 663.19it/s]Tokenizing texts:  87%|████████▋ | 8714/10000 [00:13<00:01, 696.08it/s]Tokenizing texts:  88%|████████▊ | 8784/10000 [00:13<00:01, 695.60it/s]Tokenizing texts:  89%|████████▊ | 8854/10000 [00:14<00:01, 601.30it/s]Tokenizing texts:  89%|████████▉ | 8936/10000 [00:14<00:01, 658.30it/s]Tokenizing texts:  90%|█████████ | 9005/10000 [00:14<00:01, 622.48it/s]Tokenizing texts:  91%|█████████ | 9074/10000 [00:14<00:01, 638.50it/s]Tokenizing texts:  91%|█████████▏| 9144/10000 [00:14<00:01, 654.86it/s]Tokenizing texts:  92%|█████████▏| 9220/10000 [00:14<00:01, 682.84it/s]Tokenizing texts:  93%|█████████▎| 9290/10000 [00:14<00:01, 641.21it/s]Tokenizing texts:  94%|█████████▎| 9356/10000 [00:14<00:01, 617.74it/s]Tokenizing texts:  94%|█████████▍| 9419/10000 [00:14<00:00, 597.94it/s]Tokenizing texts:  95%|█████████▍| 9481/10000 [00:15<00:00, 552.03it/s]Tokenizing texts:  96%|█████████▌| 9555/10000 [00:15<00:00, 601.54it/s]Tokenizing texts:  96%|█████████▌| 9617/10000 [00:15<00:00, 581.18it/s]Tokenizing texts:  97%|█████████▋| 9679/10000 [00:15<00:00, 590.65it/s]Tokenizing texts:  97%|█████████▋| 9739/10000 [00:15<00:00, 583.62it/s]Tokenizing texts:  98%|█████████▊| 9798/10000 [00:15<00:00, 580.71it/s]Tokenizing texts:  99%|█████████▊| 9868/10000 [00:15<00:00, 614.76it/s]Tokenizing texts:  99%|█████████▉| 9941/10000 [00:15<00:00, 643.98it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:15<00:00, 627.90it/s]
2025-12-09 11:47:08.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 12.145103454589844
2025-12-09 11:47:09.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 12.176996231079102
2025-12-09 11:47:09.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 12.145171165466309
2025-12-09 11:47:09.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 12.13653564453125
2025-12-09 11:47:10.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 12.163667678833008
2025-12-09 11:47:10.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 12.136428833007812
2025-12-09 11:47:11.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 12.098267555236816
2025-12-09 11:47:11.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 12.043051719665527
2025-12-09 11:47:11.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 12.114592552185059
2025-12-09 11:47:12.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 11.996326446533203
2025-12-09 11:47:12.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 11.960759162902832
2025-12-09 11:47:12.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 11.895442008972168
2025-12-09 11:47:13.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 11.87214469909668
2025-12-09 11:47:13.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 11.827155113220215
2025-12-09 11:47:14.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 11.558284759521484
2025-12-09 11:47:14.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 11.68525505065918
2025-12-09 11:47:14.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 11.31070327758789
2025-12-09 11:47:15.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 11.184494972229004
2025-12-09 11:47:15.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 10.998677253723145
2025-12-09 11:47:16.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 10.854060173034668
2025-12-09 11:47:16.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 10.740215301513672
2025-12-09 11:47:16.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 10.76933479309082
2025-12-09 11:47:17.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 10.602653503417969
2025-12-09 11:47:17.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 10.235729217529297
2025-12-09 11:47:18.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 10.116432189941406
2025-12-09 11:47:18.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 10.39419174194336
2025-12-09 11:47:18.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 10.380270957946777
2025-12-09 11:47:19.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 10.161845207214355
2025-12-09 11:47:19.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 9.958392143249512
2025-12-09 11:47:19.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 9.568705558776855
2025-12-09 11:47:20.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 9.786627769470215
2025-12-09 11:47:20.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 9.62728214263916
2025-12-09 11:47:21.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 9.540205001831055
2025-12-09 11:47:21.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 9.423073768615723
2025-12-09 11:47:21.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 9.586657524108887
2025-12-09 11:47:22.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 9.336458206176758
2025-12-09 11:47:22.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 9.885966300964355
2025-12-09 11:47:23.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 9.267749786376953
2025-12-09 11:47:23.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 9.294771194458008
2025-12-09 11:47:23.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 8.978522300720215
2025-12-09 11:47:24.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 9.261975288391113
2025-12-09 11:47:24.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 9.256571769714355
2025-12-09 11:47:24.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 9.023041725158691
2025-12-09 11:47:25.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 9.1301908493042
2025-12-09 11:47:25.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 8.83480453491211
2025-12-09 11:47:26.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 9.077311515808105
2025-12-09 11:47:26.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 8.607157707214355
2025-12-09 11:47:26.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 8.761695861816406
2025-12-09 11:47:27.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 8.93116283416748
2025-12-09 11:47:27.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 8.942452430725098
2025-12-09 11:47:28.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 8.428116798400879
2025-12-09 11:47:28.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 8.799565315246582
2025-12-09 11:47:28.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 8.709877014160156
2025-12-09 11:47:29.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 8.549063682556152
2025-12-09 11:47:29.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 8.728723526000977
2025-12-09 11:47:29.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 8.682965278625488
2025-12-09 11:47:30.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 8.859166145324707
2025-12-09 11:47:30.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 8.542308807373047
2025-12-09 11:47:31.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 8.16097640991211
2025-12-09 11:47:31.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 8.49697494506836
2025-12-09 11:47:31.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 8.658379554748535
2025-12-09 11:47:32.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 8.84610652923584
2025-12-09 11:47:32.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 8.814373970031738
2025-12-09 11:47:33.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 8.165688514709473
2025-12-09 11:47:33.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 8.040759086608887
2025-12-09 11:47:33.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 8.185894012451172
2025-12-09 11:47:34.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 8.439620971679688
2025-12-09 11:47:34.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 8.350719451904297
2025-12-09 11:47:34.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 8.508283615112305
2025-12-09 11:47:35.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 8.072927474975586
2025-12-09 11:47:35.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 8.299278259277344
2025-12-09 11:47:36.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 8.146361351013184
2025-12-09 11:47:36.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 8.029796600341797
2025-12-09 11:47:36.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 8.219923973083496
2025-12-09 11:47:37.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 8.442108154296875
2025-12-09 11:47:37.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 8.116660118103027
2025-12-09 11:47:38.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 7.952785968780518
2025-12-09 11:47:38.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 8.357368469238281
2025-12-09 11:47:38.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 8.084672927856445
2025-12-09 11:47:39.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 8.449590682983398
2025-12-09 11:47:39.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 8.24378490447998
2025-12-09 11:47:39.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 8.198479652404785
2025-12-09 11:47:40.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 8.030122756958008
2025-12-09 11:47:40.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 8.17776870727539
2025-12-09 11:47:41.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 8.134732246398926
2025-12-09 11:47:41.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 8.518845558166504
2025-12-09 11:47:41.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 8.330638885498047
2025-12-09 11:47:42.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 7.68256139755249
2025-12-09 11:47:42.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 7.784154415130615
2025-12-09 11:47:43.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 8.628256797790527
2025-12-09 11:47:43.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 7.738435745239258
2025-12-09 11:47:43.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 7.834094047546387
2025-12-09 11:47:44.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 7.788382053375244
2025-12-09 11:47:44.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 8.027979850769043
2025-12-09 11:47:44.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 7.85819149017334
2025-12-09 11:47:45.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 8.636942863464355
2025-12-09 11:47:45.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 8.137662887573242
2025-12-09 11:47:46.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 8.04203987121582
2025-12-09 11:47:46.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 7.925161361694336
2025-12-09 11:47:46.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 8.528118133544922
2025-12-09 11:47:47.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.999999029798809e-05 Training loss: 8.08510971069336
2025-12-09 11:47:47.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 9.999996119195611e-05 Training loss: 7.772557735443115
2025-12-09 11:47:48.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 9.999991268191536e-05 Training loss: 8.40029239654541
2025-12-09 11:47:48.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 9.999984476788465e-05 Training loss: 7.798978328704834
2025-12-09 11:47:48.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 9.999975744989037e-05 Training loss: 7.673266887664795
2025-12-09 11:47:49.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 9.999965072796636e-05 Training loss: 8.110108375549316
2025-12-09 11:47:49.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 9.999952460215408e-05 Training loss: 8.358715057373047
2025-12-09 11:47:50.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.999937907250246e-05 Training loss: 7.84780740737915
2025-12-09 11:47:50.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 9.999921413906798e-05 Training loss: 8.00473403930664
2025-12-09 11:47:50.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 9.999902980191464e-05 Training loss: 7.821457386016846
2025-12-09 11:47:51.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 9.999882606111399e-05 Training loss: 7.807631492614746
2025-12-09 11:47:51.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 9.999860291674508e-05 Training loss: 8.09992790222168
2025-12-09 11:47:51.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 9.999836036889453e-05 Training loss: 7.9720778465271
2025-12-09 11:47:52.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 9.999809841765644e-05 Training loss: 8.154169082641602
2025-12-09 11:47:52.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 9.999781706313251e-05 Training loss: 7.857701301574707
2025-12-09 11:47:53.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 9.999751630543188e-05 Training loss: 7.739567279815674
2025-12-09 11:47:53.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 9.99971961446713e-05 Training loss: 7.83629846572876
2025-12-09 11:47:53.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 9.999685658097502e-05 Training loss: 8.03139591217041
2025-12-09 11:47:54.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 9.999649761447478e-05 Training loss: 7.811523914337158
2025-12-09 11:47:54.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 9.999611924530994e-05 Training loss: 7.8334126472473145
2025-12-09 11:47:55.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 9.999572147362731e-05 Training loss: 8.113614082336426
2025-12-09 11:47:55.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 9.999530429958124e-05 Training loss: 8.453021049499512
2025-12-09 11:47:55.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 9.999486772333366e-05 Training loss: 7.78454065322876
2025-12-09 11:47:56.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 9.999441174505399e-05 Training loss: 7.672814846038818
2025-12-09 11:47:56.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 9.999393636491918e-05 Training loss: 7.839169502258301
2025-12-09 11:47:56.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 9.99934415831137e-05 Training loss: 8.00721549987793
2025-12-09 11:47:57.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 9.99929273998296e-05 Training loss: 7.81346321105957
2025-12-09 11:47:57.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 9.99923938152664e-05 Training loss: 7.790741920471191
2025-12-09 11:47:58.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 9.999184082963118e-05 Training loss: 8.008183479309082
2025-12-09 11:47:58.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 9.999126844313853e-05 Training loss: 8.185885429382324
2025-12-09 11:47:58.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 9.999067665601061e-05 Training loss: 8.227400779724121
2025-12-09 11:47:59.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 9.999006546847707e-05 Training loss: 7.901724338531494
2025-12-09 11:47:59.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 9.998943488077508e-05 Training loss: 8.421446800231934
2025-12-09 11:48:00.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 9.998878489314938e-05 Training loss: 7.886033058166504
2025-12-09 11:48:00.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 9.99881155058522e-05 Training loss: 7.850650310516357
2025-12-09 11:48:00.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 9.998742671914335e-05 Training loss: 7.740858554840088
2025-12-09 11:48:01.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 9.99867185332901e-05 Training loss: 8.094813346862793
2025-12-09 11:48:01.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 9.998599094856732e-05 Training loss: 8.009747505187988
2025-12-09 11:48:01.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 9.99852439652573e-05 Training loss: 8.028159141540527
2025-12-09 11:48:02.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 9.998447758365002e-05 Training loss: 7.9801740646362305
2025-12-09 11:48:02.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 9.998369180404283e-05 Training loss: 7.898463726043701
2025-12-09 11:48:03.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 9.99828866267407e-05 Training loss: 7.922643184661865
2025-12-09 11:48:03.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 9.998206205205611e-05 Training loss: 8.147125244140625
2025-12-09 11:48:03.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 9.998121808030906e-05 Training loss: 8.163900375366211
2025-12-09 11:48:04.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 9.998035471182708e-05 Training loss: 7.428576469421387
2025-12-09 11:48:04.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 9.997947194694519e-05 Training loss: 7.723243713378906
2025-12-09 11:48:05.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 9.997856978600604e-05 Training loss: 7.922468185424805
2025-12-09 11:48:05.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 9.997764822935967e-05 Training loss: 8.341830253601074
2025-12-09 11:48:05.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 9.997670727736378e-05 Training loss: 7.5742387771606445
2025-12-09 11:48:06.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 9.99757469303835e-05 Training loss: 7.456731796264648
2025-12-09 11:48:06.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 9.997476718879153e-05 Training loss: 7.90753698348999
2025-12-09 11:48:07.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 9.99737680529681e-05 Training loss: 7.594541549682617
2025-12-09 11:48:07.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 9.997274952330094e-05 Training loss: 7.418360233306885
2025-12-09 11:48:07.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 9.997171160018531e-05 Training loss: 7.8998284339904785
2025-12-09 11:48:08.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 9.997065428402403e-05 Training loss: 7.448208332061768
2025-12-09 11:48:08.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 9.996957757522742e-05 Training loss: 7.978304386138916
2025-12-09 11:48:08.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 9.996848147421334e-05 Training loss: 8.025504112243652
2025-12-09 11:48:09.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 9.996736598140714e-05 Training loss: 7.877490520477295
2025-12-09 11:48:09.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 9.996623109724174e-05 Training loss: 7.579190254211426
2025-12-09 11:48:10.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 9.996507682215754e-05 Training loss: 7.638476848602295
2025-12-09 11:48:10.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 9.996390315660253e-05 Training loss: 7.909550189971924
2025-12-09 11:48:10.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 9.996271010103216e-05 Training loss: 7.994347095489502
2025-12-09 11:48:11.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 9.996149765590946e-05 Training loss: 7.711395263671875
2025-12-09 11:48:11.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 9.99602658217049e-05 Training loss: 7.531627178192139
2025-12-09 11:48:12.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 9.995901459889658e-05 Training loss: 7.756680011749268
2025-12-09 11:48:12.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 9.995774398797007e-05 Training loss: 7.904731750488281
2025-12-09 11:48:12.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 9.995645398941846e-05 Training loss: 7.638429164886475
2025-12-09 11:48:13.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 9.995514460374238e-05 Training loss: 7.522543430328369
2025-12-09 11:48:13.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 9.995381583144996e-05 Training loss: 8.254206657409668
2025-12-09 11:48:13.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 9.995246767305688e-05 Training loss: 7.609104633331299
2025-12-09 11:48:14.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 9.995110012908634e-05 Training loss: 7.655765533447266
2025-12-09 11:48:14.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 9.994971320006905e-05 Training loss: 7.9198079109191895
2025-12-09 11:48:15.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 9.994830688654326e-05 Training loss: 7.372212886810303
2025-12-09 11:48:15.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 9.994688118905472e-05 Training loss: 7.547969818115234
2025-12-09 11:48:15.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 9.994543610815671e-05 Training loss: 7.6139349937438965
2025-12-09 11:48:16.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 9.994397164441007e-05 Training loss: 7.527105808258057
2025-12-09 11:48:16.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 9.994248779838311e-05 Training loss: 7.312665939331055
2025-12-09 11:48:17.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 9.994098457065166e-05 Training loss: 7.398762226104736
2025-12-09 11:48:17.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 9.993946196179913e-05 Training loss: 7.725341320037842
2025-12-09 11:48:17.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 9.993791997241639e-05 Training loss: 7.731175899505615
2025-12-09 11:48:18.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 9.993635860310187e-05 Training loss: 7.705629348754883
2025-12-09 11:48:18.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 9.99347778544615e-05 Training loss: 7.41988468170166
2025-12-09 11:48:19.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 9.993317772710874e-05 Training loss: 7.58156156539917
2025-12-09 11:48:19.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 9.993155822166457e-05 Training loss: 8.819605827331543
2025-12-09 11:48:19.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 9.992991933875748e-05 Training loss: 7.416795253753662
2025-12-09 11:48:20.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 9.99282610790235e-05 Training loss: 7.567806243896484
2025-12-09 11:48:20.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 9.992658344310614e-05 Training loss: 7.696567058563232
2025-12-09 11:48:20.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 9.992488643165651e-05 Training loss: 7.639448642730713
2025-12-09 11:48:21.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 9.992317004533313e-05 Training loss: 7.434476375579834
2025-12-09 11:48:21.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 9.992143428480214e-05 Training loss: 7.397044658660889
2025-12-09 11:48:22.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 9.991967915073714e-05 Training loss: 7.3311309814453125
2025-12-09 11:48:22.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 9.991790464381926e-05 Training loss: 7.2498579025268555
2025-12-09 11:48:22.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 9.991611076473714e-05 Training loss: 7.438169479370117
2025-12-09 11:48:23.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 9.991429751418697e-05 Training loss: 7.436784267425537
2025-12-09 11:48:23.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 9.991246489287245e-05 Training loss: 7.2839813232421875
2025-12-09 11:48:24.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 9.991061290150475e-05 Training loss: 8.275676727294922
2025-12-09 11:48:24.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 9.990874154080259e-05 Training loss: 8.676860809326172
2025-12-09 11:48:24.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 9.990685081149222e-05 Training loss: 7.48333740234375
2025-12-09 11:48:25.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 9.990494071430742e-05 Training loss: 7.744640827178955
2025-12-09 11:48:25.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 9.990301124998945e-05 Training loss: 7.497315883636475
2025-12-09 11:48:25.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 9.990106241928706e-05 Training loss: 7.6768364906311035
2025-12-09 11:48:26.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 9.989909422295659e-05 Training loss: 7.527669906616211
2025-12-09 11:48:26.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 9.989710666176186e-05 Training loss: 7.640161037445068
2025-12-09 11:48:27.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 9.989509973647417e-05 Training loss: 7.434682369232178
2025-12-09 11:48:27.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 9.989307344787242e-05 Training loss: 8.302355766296387
2025-12-09 11:48:27.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 9.989102779674293e-05 Training loss: 7.614448547363281
2025-12-09 11:48:28.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 9.98889627838796e-05 Training loss: 7.161438941955566
2025-12-09 11:48:28.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 9.98868784100838e-05 Training loss: 7.43381929397583
2025-12-09 11:48:29.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 9.988477467616447e-05 Training loss: 8.221230506896973
2025-12-09 11:48:29.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 9.988265158293799e-05 Training loss: 7.626047134399414
2025-12-09 11:48:29.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 9.98805091312283e-05 Training loss: 7.715358734130859
2025-12-09 11:48:30.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 9.987834732186687e-05 Training loss: 7.847178936004639
2025-12-09 11:48:30.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 9.987616615569263e-05 Training loss: 7.457144737243652
2025-12-09 11:48:31.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 9.987396563355205e-05 Training loss: 7.559576988220215
2025-12-09 11:48:31.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 9.987174575629911e-05 Training loss: 7.543483734130859
2025-12-09 11:48:31.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 9.986950652479532e-05 Training loss: 7.1467108726501465
2025-12-09 11:48:32.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 9.986724793990966e-05 Training loss: 8.083647727966309
2025-12-09 11:48:32.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 9.986497000251866e-05 Training loss: 8.035528182983398
2025-12-09 11:48:32.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 9.986267271350633e-05 Training loss: 7.622312545776367
2025-12-09 11:48:33.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 9.98603560737642e-05 Training loss: 7.248511791229248
2025-12-09 11:48:33.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 9.985802008419131e-05 Training loss: 7.402602672576904
2025-12-09 11:48:34.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 9.985566474569424e-05 Training loss: 7.221917629241943
2025-12-09 11:48:34.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 9.985329005918702e-05 Training loss: 7.166650772094727
2025-12-09 11:48:34.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 9.985089602559125e-05 Training loss: 7.768223762512207
2025-12-09 11:48:35.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 9.984848264583597e-05 Training loss: 7.374870300292969
2025-12-09 11:48:35.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 9.98460499208578e-05 Training loss: 7.250491142272949
2025-12-09 11:48:36.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 9.98435978516008e-05 Training loss: 7.34370231628418
2025-12-09 11:48:36.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 9.98411264390166e-05 Training loss: 7.145953178405762
2025-12-09 11:48:36.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 9.983863568406428e-05 Training loss: 7.33269739151001
2025-12-09 11:48:37.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 9.983612558771049e-05 Training loss: 7.694819927215576
2025-12-09 11:48:37.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 9.983359615092931e-05 Training loss: 7.677737236022949
2025-12-09 11:48:37.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 9.983104737470239e-05 Training loss: 8.05290412902832
2025-12-09 11:48:38.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 9.982847926001886e-05 Training loss: 7.4027557373046875
2025-12-09 11:48:38.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 9.982589180787534e-05 Training loss: 7.453749179840088
2025-12-09 11:48:39.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 9.982328501927599e-05 Training loss: 7.220217704772949
2025-12-09 11:48:39.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 9.982065889523242e-05 Training loss: 7.592085838317871
2025-12-09 11:48:39.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 9.98180134367638e-05 Training loss: 7.485268592834473
2025-12-09 11:48:40.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 9.981534864489679e-05 Training loss: 7.414720058441162
2025-12-09 11:48:40.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 9.981266452066553e-05 Training loss: 7.267216682434082
2025-12-09 11:48:41.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 9.98099610651117e-05 Training loss: 7.376457691192627
2025-12-09 11:48:41.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 9.980723827928441e-05 Training loss: 7.125808238983154
2025-12-09 11:48:41.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 9.980449616424037e-05 Training loss: 7.077689170837402
2025-12-09 11:48:42.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 9.98017347210437e-05 Training loss: 7.9965691566467285
2025-12-09 11:48:42.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 9.979895395076609e-05 Training loss: 8.030732154846191
2025-12-09 11:48:43.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 9.979615385448669e-05 Training loss: 7.653727054595947
2025-12-09 11:48:43.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 9.979333443329217e-05 Training loss: 7.254208087921143
2025-12-09 11:48:43.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 9.97904956882767e-05 Training loss: 7.755012035369873
2025-12-09 11:48:44.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 9.978763762054194e-05 Training loss: 7.3413472175598145
2025-12-09 11:48:44.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 9.978476023119701e-05 Training loss: 7.24556827545166
2025-12-09 11:48:44.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 9.978186352135861e-05 Training loss: 7.915196895599365
2025-12-09 11:48:45.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 9.977894749215089e-05 Training loss: 7.548702239990234
2025-12-09 11:48:45.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 9.97760121447055e-05 Training loss: 7.573354721069336
2025-12-09 11:48:46.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 9.977305748016159e-05 Training loss: 7.190713405609131
2025-12-09 11:48:46.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 9.977008349966582e-05 Training loss: 7.067152500152588
2025-12-09 11:48:46.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 9.976709020437229e-05 Training loss: 7.215725898742676
2025-12-09 11:48:47.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 9.97640775954427e-05 Training loss: 7.430906295776367
2025-12-09 11:48:47.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 9.976104567404617e-05 Training loss: 7.325526714324951
2025-12-09 11:48:48.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 9.97579944413593e-05 Training loss: 7.468432903289795
2025-12-09 11:48:48.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 9.975492389856622e-05 Training loss: 7.1791672706604
2025-12-09 11:48:48.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 9.975183404685856e-05 Training loss: 7.394776821136475
2025-12-09 11:48:49.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 9.974872488743543e-05 Training loss: 7.532541751861572
2025-12-09 11:48:49.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 9.974559642150345e-05 Training loss: 7.562143325805664
2025-12-09 11:48:50.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 9.974244865027669e-05 Training loss: 7.146413803100586
2025-12-09 11:48:50.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 9.973928157497674e-05 Training loss: 7.274050235748291
2025-12-09 11:48:50.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 9.973609519683268e-05 Training loss: 7.147783279418945
2025-12-09 11:48:51.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 9.973288951708111e-05 Training loss: 7.315310001373291
2025-12-09 11:48:51.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 9.972966453696608e-05 Training loss: 7.324540138244629
2025-12-09 11:48:51.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 9.972642025773912e-05 Training loss: 7.000606060028076
2025-12-09 11:48:52.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 9.972315668065929e-05 Training loss: 6.98887825012207
2025-12-09 11:48:52.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 9.97198738069931e-05 Training loss: 7.278721809387207
2025-12-09 11:48:53.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 9.971657163801458e-05 Training loss: 7.338680267333984
2025-12-09 11:48:53.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 9.971325017500526e-05 Training loss: 6.79439115524292
2025-12-09 11:48:53.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 9.970990941925411e-05 Training loss: 7.395223617553711
2025-12-09 11:48:54.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 9.970654937205762e-05 Training loss: 7.412186145782471
2025-12-09 11:48:54.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 9.970317003471976e-05 Training loss: 7.47524356842041
2025-12-09 11:48:55.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 9.969977140855198e-05 Training loss: 6.990269660949707
2025-12-09 11:48:55.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 9.969635349487321e-05 Training loss: 7.417163372039795
2025-12-09 11:48:55.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 9.969291629500991e-05 Training loss: 7.028880596160889
2025-12-09 11:48:56.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 9.968945981029596e-05 Training loss: 6.950544834136963
2025-12-09 11:48:56.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 9.968598404207275e-05 Training loss: 7.2215986251831055
2025-12-09 11:48:56.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 9.96824889916892e-05 Training loss: 6.949516773223877
2025-12-09 11:48:57.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 9.96789746605016e-05 Training loss: 7.122054100036621
2025-12-09 11:48:57.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 9.967544104987387e-05 Training loss: 7.130945205688477
2025-12-09 11:48:58.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 9.967188816117727e-05 Training loss: 7.165381908416748
2025-12-09 11:48:58.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 9.966831599579066e-05 Training loss: 7.171438217163086
2025-12-09 11:48:58.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 9.96647245551003e-05 Training loss: 6.993332386016846
2025-12-09 11:48:59.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 9.966111384049997e-05 Training loss: 7.026773452758789
2025-12-09 11:48:59.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 9.965748385339089e-05 Training loss: 6.915960311889648
2025-12-09 11:49:00.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 9.96538345951818e-05 Training loss: 7.4057183265686035
2025-12-09 11:49:00.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 9.965016606728894e-05 Training loss: 6.6158037185668945
2025-12-09 11:49:00.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 9.964647827113595e-05 Training loss: 7.042331218719482
2025-12-09 11:49:01.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 9.964277120815401e-05 Training loss: 7.067566394805908
2025-12-09 11:49:01.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 9.963904487978177e-05 Training loss: 7.420267105102539
2025-12-09 11:49:02.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 9.963529928746534e-05 Training loss: 7.239110469818115
2025-12-09 11:49:02.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 9.963153443265828e-05 Training loss: 7.016342639923096
2025-12-09 11:49:02.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 9.96277503168217e-05 Training loss: 7.30832052230835
2025-12-09 11:49:03.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 9.96239469414241e-05 Training loss: 7.486076831817627
2025-12-09 11:49:03.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 9.962012430794153e-05 Training loss: 6.996462821960449
2025-12-09 11:49:03.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 9.961628241785747e-05 Training loss: 7.275088787078857
2025-12-09 11:49:04.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 9.961242127266288e-05 Training loss: 7.036124229431152
2025-12-09 11:49:04.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 9.960854087385619e-05 Training loss: 7.138568878173828
2025-12-09 11:49:05.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 9.96046412229433e-05 Training loss: 7.27836275100708
2025-12-09 11:49:05.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 9.960072232143762e-05 Training loss: 7.222910404205322
2025-12-09 11:49:05.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 9.959678417085997e-05 Training loss: 7.376580238342285
2025-12-09 11:49:06.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 9.95928267727387e-05 Training loss: 7.027856826782227
2025-12-09 11:49:06.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 9.958885012860954e-05 Training loss: 6.891918659210205
2025-12-09 11:49:07.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 9.958485424001583e-05 Training loss: 7.220438003540039
2025-12-09 11:49:07.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 9.958083910850821e-05 Training loss: 7.412985324859619
2025-12-09 11:49:07.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 9.957680473564495e-05 Training loss: 6.984160900115967
2025-12-09 11:49:08.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 9.957275112299165e-05 Training loss: 6.969766139984131
2025-12-09 11:49:08.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 9.956867827212148e-05 Training loss: 7.236194133758545
2025-12-09 11:49:09.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 9.956458618461502e-05 Training loss: 7.263391971588135
2025-12-09 11:49:09.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 9.956047486206032e-05 Training loss: 6.808859825134277
2025-12-09 11:49:09.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 9.955634430605291e-05 Training loss: 7.241218090057373
2025-12-09 11:49:10.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 9.955219451819579e-05 Training loss: 7.194659233093262
2025-12-09 11:49:10.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 9.954802550009942e-05 Training loss: 7.172647476196289
2025-12-09 11:49:10.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 9.954383725338167e-05 Training loss: 6.904300212860107
2025-12-09 11:49:11.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 9.953962977966795e-05 Training loss: 7.35193395614624
2025-12-09 11:49:11.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 9.953540308059111e-05 Training loss: 7.136175632476807
2025-12-09 11:49:12.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 9.953115715779141e-05 Training loss: 6.810515880584717
2025-12-09 11:49:12.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 9.952689201291664e-05 Training loss: 7.4253950119018555
2025-12-09 11:49:12.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 9.9522607647622e-05 Training loss: 7.113484859466553
2025-12-09 11:49:13.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 9.951830406357019e-05 Training loss: 6.839158058166504
2025-12-09 11:49:13.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 9.951398126243134e-05 Training loss: 6.8677825927734375
2025-12-09 11:49:14.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 9.950963924588303e-05 Training loss: 7.254299163818359
2025-12-09 11:49:14.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 9.950527801561033e-05 Training loss: 7.482301712036133
2025-12-09 11:49:14.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 9.950089757330574e-05 Training loss: 7.237466812133789
2025-12-09 11:49:15.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 9.949649792066922e-05 Training loss: 7.319311618804932
2025-12-09 11:49:15.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 9.94920790594082e-05 Training loss: 7.123021125793457
2025-12-09 11:49:15.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 9.948764099123755e-05 Training loss: 6.791804790496826
2025-12-09 11:49:16.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 9.94831837178796e-05 Training loss: 7.2132158279418945
2025-12-09 11:49:16.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.947870724106412e-05 Training loss: 6.904420375823975
2025-12-09 11:49:17.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.947421156252836e-05 Training loss: 6.888669013977051
2025-12-09 11:49:17.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.946969668401697e-05 Training loss: 7.128454208374023
2025-12-09 11:49:17.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 9.946516260728214e-05 Training loss: 6.722099781036377
2025-12-09 11:49:18.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 9.946060933408341e-05 Training loss: 7.524021148681641
2025-12-09 11:49:18.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 9.945603686618785e-05 Training loss: 7.416651725769043
2025-12-09 11:49:19.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 9.945144520536992e-05 Training loss: 7.200228691101074
2025-12-09 11:49:19.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 9.944683435341155e-05 Training loss: 7.398989200592041
2025-12-09 11:49:19.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 9.944220431210216e-05 Training loss: 7.004769325256348
2025-12-09 11:49:20.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 9.943755508323855e-05 Training loss: 7.647043228149414
2025-12-09 11:49:20.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 9.943288666862498e-05 Training loss: 6.783438682556152
2025-12-09 11:49:21.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 9.942819907007321e-05 Training loss: 6.895878314971924
2025-12-09 11:49:21.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 9.942349228940237e-05 Training loss: 7.13479471206665
2025-12-09 11:49:21.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 9.941876632843909e-05 Training loss: 7.133122444152832
2025-12-09 11:49:22.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 9.941402118901744e-05 Training loss: 7.188307762145996
2025-12-09 11:49:22.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 9.940925687297886e-05 Training loss: 7.728641033172607
2025-12-09 11:49:22.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 9.940447338217234e-05 Training loss: 6.750404357910156
2025-12-09 11:49:23.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 9.939967071845423e-05 Training loss: 7.283710956573486
2025-12-09 11:49:23.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 9.939484888368838e-05 Training loss: 7.092391014099121
2025-12-09 11:49:24.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 9.939000787974602e-05 Training loss: 6.59874153137207
2025-12-09 11:49:24.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 9.938514770850587e-05 Training loss: 6.900854110717773
2025-12-09 11:49:24.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 9.938026837185404e-05 Training loss: 7.216620922088623
2025-12-09 11:49:25.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 9.937536987168413e-05 Training loss: 8.951762199401855
2025-12-09 11:49:25.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 9.937045220989715e-05 Training loss: 6.992628574371338
2025-12-09 11:49:26.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 9.936551538840155e-05 Training loss: 7.195681571960449
2025-12-09 11:49:26.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.93605594091132e-05 Training loss: 8.083684921264648
2025-12-09 11:49:26.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.935558427395542e-05 Training loss: 6.768604278564453
2025-12-09 11:49:27.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 9.935058998485897e-05 Training loss: 6.998510360717773
2025-12-09 11:49:27.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 9.934557654376205e-05 Training loss: 7.720041751861572
2025-12-09 11:49:28.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 9.934054395261026e-05 Training loss: 7.123677730560303
2025-12-09 11:49:28.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 9.933549221335664e-05 Training loss: 7.08010196685791
2025-12-09 11:49:28.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 9.933042132796171e-05 Training loss: 7.247946739196777
2025-12-09 11:49:29.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 9.932533129839334e-05 Training loss: 6.9896368980407715
2025-12-09 11:49:29.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 9.932022212662691e-05 Training loss: 6.6833648681640625
2025-12-09 11:49:29.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 9.931509381464515e-05 Training loss: 6.903198719024658
2025-12-09 11:49:30.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 9.930994636443829e-05 Training loss: 6.911981105804443
2025-12-09 11:49:30.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 9.930477977800392e-05 Training loss: 7.544430732727051
2025-12-09 11:49:31.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 9.929959405734712e-05 Training loss: 7.058236598968506
2025-12-09 11:49:31.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 9.929438920448037e-05 Training loss: 7.559010028839111
2025-12-09 11:49:31.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 9.928916522142357e-05 Training loss: 7.448326110839844
2025-12-09 11:49:32.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 9.928392211020401e-05 Training loss: 7.11624002456665
2025-12-09 11:49:32.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.927865987285649e-05 Training loss: 6.817259788513184
2025-12-09 11:49:33.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 9.927337851142314e-05 Training loss: 6.706788063049316
2025-12-09 11:49:33.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 9.926807802795359e-05 Training loss: 7.270288944244385
2025-12-09 11:49:33.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 9.926275842450483e-05 Training loss: 6.917986869812012
2025-12-09 11:49:34.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 9.925741970314129e-05 Training loss: 7.114480972290039
2025-12-09 11:49:34.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 9.925206186593484e-05 Training loss: 7.157128810882568
2025-12-09 11:49:35.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 9.924668491496474e-05 Training loss: 7.290952682495117
2025-12-09 11:49:35.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 9.92412888523177e-05 Training loss: 6.966279983520508
2025-12-09 11:49:35.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 9.923587368008778e-05 Training loss: 6.8722991943359375
2025-12-09 11:49:36.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 9.923043940037657e-05 Training loss: 6.847870349884033
2025-12-09 11:49:36.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 9.922498601529296e-05 Training loss: 7.559738636016846
2025-12-09 11:49:36.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 9.92195135269533e-05 Training loss: 7.03010368347168
2025-12-09 11:49:37.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 9.921402193748139e-05 Training loss: 7.360805511474609
2025-12-09 11:49:37.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 9.920851124900837e-05 Training loss: 6.97221040725708
2025-12-09 11:49:38.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 9.920298146367286e-05 Training loss: 7.1341166496276855
2025-12-09 11:49:38.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 9.919743258362085e-05 Training loss: 7.413518905639648
2025-12-09 11:49:38.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 9.919186461100576e-05 Training loss: 7.363177299499512
2025-12-09 11:49:39.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 9.91862775479884e-05 Training loss: 7.320401668548584
2025-12-09 11:49:39.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 9.9180671396737e-05 Training loss: 6.877528190612793
2025-12-09 11:49:40.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 9.91750461594272e-05 Training loss: 6.596088886260986
2025-12-09 11:49:40.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 9.916940183824206e-05 Training loss: 7.335231304168701
2025-12-09 11:49:40.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 9.916373843537201e-05 Training loss: 7.011961460113525
2025-12-09 11:49:41.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 9.915805595301491e-05 Training loss: 6.793820381164551
2025-12-09 11:49:41.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 9.915235439337603e-05 Training loss: 6.965399742126465
2025-12-09 11:49:41.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 9.914663375866804e-05 Training loss: 7.1608052253723145
2025-12-09 11:49:42.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 9.914089405111098e-05 Training loss: 7.205203533172607
2025-12-09 11:49:42.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 9.913513527293235e-05 Training loss: 7.483813285827637
2025-12-09 11:49:43.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 9.912935742636698e-05 Training loss: 7.048422336578369
2025-12-09 11:49:43.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 9.912356051365718e-05 Training loss: 7.017841339111328
2025-12-09 11:49:43.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 9.911774453705258e-05 Training loss: 6.965842247009277
2025-12-09 11:49:44.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 9.91119094988103e-05 Training loss: 7.142075061798096
2025-12-09 11:49:44.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 9.910605540119475e-05 Training loss: 7.5451459884643555
2025-12-09 11:49:45.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 9.91001822464778e-05 Training loss: 6.9043498039245605
2025-12-09 11:49:45.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 9.909429003693876e-05 Training loss: 6.639228820800781
2025-12-09 11:49:45.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 9.908837877486423e-05 Training loss: 6.866599082946777
2025-12-09 11:49:46.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 9.908244846254826e-05 Training loss: 7.022936820983887
2025-12-09 11:49:46.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 9.907649910229229e-05 Training loss: 6.858850479125977
2025-12-09 11:49:47.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 9.907053069640517e-05 Training loss: 6.916691303253174
2025-12-09 11:49:47.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 9.90645432472031e-05 Training loss: 7.217833042144775
2025-12-09 11:49:47.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 9.905853675700969e-05 Training loss: 6.9660725593566895
2025-12-09 11:49:48.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 9.905251122815596e-05 Training loss: 7.267037391662598
2025-12-09 11:49:48.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 9.90464666629803e-05 Training loss: 6.958216667175293
2025-12-09 11:49:48.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 9.904040306382846e-05 Training loss: 7.296041965484619
2025-12-09 11:49:49.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 9.903432043305365e-05 Training loss: 7.03193998336792
2025-12-09 11:49:49.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 9.902821877301637e-05 Training loss: 6.818295955657959
2025-12-09 11:49:50.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 9.90220980860846e-05 Training loss: 7.238439083099365
2025-12-09 11:49:50.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 9.901595837463363e-05 Training loss: 7.068245887756348
2025-12-09 11:49:50.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 9.900979964104617e-05 Training loss: 6.842331409454346
2025-12-09 11:49:51.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 9.900362188771231e-05 Training loss: 7.43743896484375
2025-12-09 11:49:51.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 9.899742511702951e-05 Training loss: 7.243923187255859
2025-12-09 11:49:52.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 9.89912093314026e-05 Training loss: 7.2780985832214355
2025-12-09 11:49:52.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 9.898497453324384e-05 Training loss: 6.957734107971191
2025-12-09 11:49:52.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 9.897872072497281e-05 Training loss: 6.964870452880859
2025-12-09 11:49:53.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 9.897244790901649e-05 Training loss: 7.59340238571167
2025-12-09 11:49:53.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 9.896615608780925e-05 Training loss: 6.9806976318359375
2025-12-09 11:49:54.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 9.895984526379281e-05 Training loss: 7.290073871612549
2025-12-09 11:49:54.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 9.895351543941629e-05 Training loss: 7.168740749359131
2025-12-09 11:49:54.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 9.894716661713617e-05 Training loss: 7.410833835601807
2025-12-09 11:49:55.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 9.894079879941627e-05 Training loss: 7.270066738128662
2025-12-09 11:49:55.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 9.893441198872787e-05 Training loss: 7.008913993835449
2025-12-09 11:49:55.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 9.892800618754954e-05 Training loss: 6.6544928550720215
2025-12-09 11:49:56.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 9.892158139836725e-05 Training loss: 6.891770362854004
2025-12-09 11:49:56.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 9.891513762367431e-05 Training loss: 7.0645036697387695
2025-12-09 11:49:57.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 9.890867486597146e-05 Training loss: 6.773699760437012
2025-12-09 11:49:57.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 9.890219312776676e-05 Training loss: 6.650774002075195
2025-12-09 11:49:57.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 9.889569241157563e-05 Training loss: 6.947473049163818
2025-12-09 11:49:58.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 9.888917271992091e-05 Training loss: 6.8173346519470215
2025-12-09 11:49:58.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 9.888263405533271e-05 Training loss: 6.951380252838135
2025-12-09 11:49:59.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 9.88760764203486e-05 Training loss: 6.903751373291016
2025-12-09 11:49:59.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 9.886949981751346e-05 Training loss: 6.989677906036377
2025-12-09 11:49:59.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 9.886290424937952e-05 Training loss: 7.047475337982178
2025-12-09 11:50:00.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 9.885628971850642e-05 Training loss: 7.344315528869629
2025-12-09 11:50:00.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 9.884965622746111e-05 Training loss: 6.912552833557129
2025-12-09 11:50:00.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 9.884300377881795e-05 Training loss: 7.069790363311768
2025-12-09 11:50:01.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 9.883633237515858e-05 Training loss: 6.594280242919922
2025-12-09 11:50:01.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 9.882964201907207e-05 Training loss: 7.141423225402832
2025-12-09 11:50:02.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 9.882293271315481e-05 Training loss: 6.95171594619751
2025-12-09 11:50:02.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 9.881620446001056e-05 Training loss: 6.7073774337768555
2025-12-09 11:50:02.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 9.88094572622504e-05 Training loss: 6.972420692443848
2025-12-09 11:50:03.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 9.88026911224928e-05 Training loss: 6.703334331512451
2025-12-09 11:50:03.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 9.879590604336359e-05 Training loss: 7.602982521057129
2025-12-09 11:50:04.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 9.87891020274959e-05 Training loss: 6.853431224822998
2025-12-09 11:50:04.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 9.878227907753021e-05 Training loss: 7.530290126800537
2025-12-09 11:50:04.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 9.877543719611444e-05 Training loss: 7.657013893127441
2025-12-09 11:50:05.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 9.876857638590373e-05 Training loss: 6.9029316902160645
2025-12-09 11:50:05.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 9.876169664956067e-05 Training loss: 6.999636650085449
2025-12-09 11:50:06.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 9.875479798975512e-05 Training loss: 6.503961563110352
2025-12-09 11:50:06.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 9.874788040916432e-05 Training loss: 7.23370885848999
2025-12-09 11:50:06.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 9.874094391047289e-05 Training loss: 6.834693431854248
2025-12-09 11:50:07.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 9.873398849637268e-05 Training loss: 7.012709140777588
2025-12-09 11:50:07.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 9.872701416956299e-05 Training loss: 6.815609455108643
2025-12-09 11:50:07.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 9.872002093275042e-05 Training loss: 7.250544548034668
2025-12-09 11:50:08.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 9.871300878864891e-05 Training loss: 7.012099742889404
2025-12-09 11:50:08.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 9.870597773997972e-05 Training loss: 6.85342264175415
2025-12-09 11:50:09.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 9.869892778947148e-05 Training loss: 6.695843696594238
2025-12-09 11:50:09.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 9.869185893986012e-05 Training loss: 7.000168800354004
2025-12-09 11:50:09.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 9.868477119388896e-05 Training loss: 7.115825176239014
2025-12-09 11:50:10.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 9.867766455430857e-05 Training loss: 6.559372901916504
2025-12-09 11:50:10.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 9.867053902387693e-05 Training loss: 6.422725677490234
2025-12-09 11:50:11.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 9.86633946053593e-05 Training loss: 7.381414890289307
2025-12-09 11:50:11.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 9.865623130152828e-05 Training loss: 6.428493976593018
2025-12-09 11:50:11.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 9.864904911516384e-05 Training loss: 6.901484489440918
2025-12-09 11:50:12.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 9.864184804905323e-05 Training loss: 6.926694393157959
2025-12-09 11:50:12.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 9.863462810599105e-05 Training loss: 7.690785884857178
2025-12-09 11:50:13.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 9.862738928877922e-05 Training loss: 7.333956241607666
2025-12-09 11:50:13.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 9.862013160022696e-05 Training loss: 6.818979263305664
2025-12-09 11:50:13.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 9.861285504315085e-05 Training loss: 6.5373430252075195
2025-12-09 11:50:14.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 9.860555962037479e-05 Training loss: 7.1257123947143555
2025-12-09 11:50:14.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 9.859824533472998e-05 Training loss: 6.78510856628418
2025-12-09 11:50:14.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 9.859091218905498e-05 Training loss: 6.864948749542236
2025-12-09 11:50:15.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 9.85835601861956e-05 Training loss: 7.237854957580566
2025-12-09 11:50:15.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 9.857618932900503e-05 Training loss: 6.98051643371582
2025-12-09 11:50:16.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 9.856879962034374e-05 Training loss: 7.083234786987305
2025-12-09 11:50:16.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 9.856139106307955e-05 Training loss: 7.141849040985107
2025-12-09 11:50:16.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 9.855396366008758e-05 Training loss: 7.1941399574279785
2025-12-09 11:50:17.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 9.854651741425023e-05 Training loss: 7.011606693267822
2025-12-09 11:50:17.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 9.853905232845728e-05 Training loss: 6.917628765106201
2025-12-09 11:50:18.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 9.853156840560575e-05 Training loss: 6.988471031188965
2025-12-09 11:50:18.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 9.852406564860003e-05 Training loss: 7.088883399963379
2025-12-09 11:50:18.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 9.851654406035179e-05 Training loss: 7.138969421386719
2025-12-09 11:50:19.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 9.850900364378e-05 Training loss: 6.862949371337891
2025-12-09 11:50:19.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 9.850144440181096e-05 Training loss: 6.818655967712402
2025-12-09 11:50:20.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 9.849386633737825e-05 Training loss: 6.813723564147949
2025-12-09 11:50:20.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 9.848626945342278e-05 Training loss: 7.61210823059082
2025-12-09 11:50:20.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 9.847865375289275e-05 Training loss: 6.931758880615234
2025-12-09 11:50:21.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 9.847101923874367e-05 Training loss: 6.965424537658691
2025-12-09 11:50:21.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 9.846336591393833e-05 Training loss: 6.209145545959473
2025-12-09 11:50:21.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 9.845569378144686e-05 Training loss: 7.304688930511475
2025-12-09 11:50:22.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 9.844800284424664e-05 Training loss: 7.044193744659424
2025-12-09 11:50:22.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 9.844029310532239e-05 Training loss: 6.9135966300964355
2025-12-09 11:50:23.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 9.843256456766609e-05 Training loss: 7.650106430053711
2025-12-09 11:50:23.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 9.842481723427705e-05 Training loss: 6.587161064147949
2025-12-09 11:50:23.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 9.841705110816187e-05 Training loss: 7.765134334564209
2025-12-09 11:50:24.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 9.840926619233441e-05 Training loss: 7.1059722900390625
2025-12-09 11:50:24.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 9.840146248981585e-05 Training loss: 6.972935199737549
2025-12-09 11:50:25.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 9.839364000363467e-05 Training loss: 7.158896446228027
2025-12-09 11:50:25.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 9.83857987368266e-05 Training loss: 6.910734176635742
2025-12-09 11:50:25.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 9.837793869243468e-05 Training loss: 7.00017786026001
2025-12-09 11:50:26.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 9.837005987350926e-05 Training loss: 7.106931209564209
2025-12-09 11:50:26.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 9.836216228310798e-05 Training loss: 6.689429759979248
2025-12-09 11:50:26.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 9.835424592429567e-05 Training loss: 6.858794212341309
2025-12-09 11:50:27.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 9.834631080014457e-05 Training loss: 7.0745086669921875
2025-12-09 11:50:27.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 9.833835691373413e-05 Training loss: 6.774682521820068
2025-12-09 11:50:28.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 9.83303842681511e-05 Training loss: 7.2738423347473145
2025-12-09 11:50:28.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 9.83223928664895e-05 Training loss: 7.125803470611572
2025-12-09 11:50:28.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 9.831438271185065e-05 Training loss: 7.303215026855469
2025-12-09 11:50:29.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 9.830635380734313e-05 Training loss: 7.088649749755859
2025-12-09 11:50:29.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 9.82983061560828e-05 Training loss: 7.275162696838379
2025-12-09 11:50:30.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 9.829023976119279e-05 Training loss: 7.131931781768799
2025-12-09 11:50:30.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 9.828215462580353e-05 Training loss: 6.668101787567139
2025-12-09 11:50:30.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 9.827405075305267e-05 Training loss: 6.6733808517456055
2025-12-09 11:50:31.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 9.826592814608518e-05 Training loss: 6.775439739227295
2025-12-09 11:50:31.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 9.825778680805331e-05 Training loss: 6.7551798820495605
2025-12-09 11:50:32.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 9.824962674211653e-05 Training loss: 7.314277648925781
2025-12-09 11:50:32.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 9.824144795144159e-05 Training loss: 6.978423118591309
2025-12-09 11:50:32.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 9.823325043920254e-05 Training loss: 6.975545883178711
2025-12-09 11:50:33.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 9.822503420858069e-05 Training loss: 7.1154561042785645
2025-12-09 11:50:33.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 9.821679926276456e-05 Training loss: 7.022782802581787
2025-12-09 11:50:33.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 9.820854560494999e-05 Training loss: 6.947037220001221
2025-12-09 11:50:34.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 9.820027323834006e-05 Training loss: 6.716480255126953
2025-12-09 11:50:34.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 9.819198216614512e-05 Training loss: 6.743873119354248
2025-12-09 11:50:35.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 9.818367239158278e-05 Training loss: 6.926332473754883
2025-12-09 11:50:35.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 9.817534391787789e-05 Training loss: 7.252163410186768
2025-12-09 11:50:35.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 9.816699674826255e-05 Training loss: 6.840665817260742
2025-12-09 11:50:36.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 9.815863088597618e-05 Training loss: 7.723925590515137
2025-12-09 11:50:36.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 9.815024633426538e-05 Training loss: 6.687836647033691
2025-12-09 11:50:37.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 9.814184309638402e-05 Training loss: 6.5613813400268555
2025-12-09 11:50:37.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 9.813342117559323e-05 Training loss: 6.768777847290039
2025-12-09 11:50:37.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 9.812498057516143e-05 Training loss: 7.644130706787109
2025-12-09 11:50:38.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 9.811652129836421e-05 Training loss: 7.134343147277832
2025-12-09 11:50:38.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 9.810804334848449e-05 Training loss: 7.164360523223877
2025-12-09 11:50:39.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 9.809954672881238e-05 Training loss: 6.7673468589782715
2025-12-09 11:50:39.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 9.809103144264525e-05 Training loss: 7.103536128997803
2025-12-09 11:50:39.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 9.808249749328768e-05 Training loss: 7.1449875831604
2025-12-09 11:50:40.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 9.80739448840516e-05 Training loss: 6.494079113006592
2025-12-09 11:50:40.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 9.806537361825606e-05 Training loss: 6.57370138168335
2025-12-09 11:50:40.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 9.805678369922742e-05 Training loss: 6.983403205871582
2025-12-09 11:50:41.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 9.804817513029927e-05 Training loss: 6.9228386878967285
2025-12-09 11:50:41.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 9.803954791481239e-05 Training loss: 6.778756618499756
2025-12-09 11:50:42.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 9.803090205611487e-05 Training loss: 6.856255054473877
2025-12-09 11:50:42.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 9.802223755756198e-05 Training loss: 6.72905158996582
2025-12-09 11:50:42.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 9.801355442251625e-05 Training loss: 6.850779056549072
2025-12-09 11:50:43.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 9.800485265434744e-05 Training loss: 6.868256092071533
2025-12-09 11:50:43.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 9.799613225643253e-05 Training loss: 6.7737202644348145
2025-12-09 11:50:44.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 9.798739323215574e-05 Training loss: 6.774923801422119
2025-12-09 11:50:44.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 9.797863558490849e-05 Training loss: 7.0140180587768555
2025-12-09 11:50:44.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 9.79698593180895e-05 Training loss: 6.976245403289795
2025-12-09 11:50:45.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 9.796106443510462e-05 Training loss: 6.829812049865723
2025-12-09 11:50:45.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 9.795225093936702e-05 Training loss: 6.906742572784424
2025-12-09 11:50:46.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 9.794341883429699e-05 Training loss: 6.930849075317383
2025-12-09 11:50:46.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 9.793456812332215e-05 Training loss: 7.001607418060303
2025-12-09 11:50:46.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 9.792569880987726e-05 Training loss: 6.682878494262695
2025-12-09 11:50:47.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 9.791681089740432e-05 Training loss: 6.848859786987305
2025-12-09 11:50:47.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 9.790790438935256e-05 Training loss: 6.484069347381592
2025-12-09 11:50:47.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 9.789897928917847e-05 Training loss: 6.7772932052612305
2025-12-09 11:50:48.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 9.789003560034561e-05 Training loss: 6.866575241088867
2025-12-09 11:50:48.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 9.788107332632495e-05 Training loss: 6.780346393585205
2025-12-09 11:50:49.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 9.787209247059452e-05 Training loss: 6.858809471130371
2025-12-09 11:50:49.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 9.786309303663963e-05 Training loss: 6.768187999725342
2025-12-09 11:50:49.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 9.785407502795278e-05 Training loss: 6.763391017913818
2025-12-09 11:50:50.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 9.784503844803368e-05 Training loss: 6.827047348022461
2025-12-09 11:50:50.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 9.783598330038925e-05 Training loss: 6.56864070892334
2025-12-09 11:50:51.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 9.782690958853362e-05 Training loss: 7.004622936248779
2025-12-09 11:50:51.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 9.781781731598812e-05 Training loss: 6.896975040435791
2025-12-09 11:50:51.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 9.780870648628128e-05 Training loss: 6.5168633460998535
2025-12-09 11:50:52.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 9.779957710294886e-05 Training loss: 6.931194305419922
2025-12-09 11:50:52.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 9.779042916953376e-05 Training loss: 7.912079811096191
2025-12-09 11:50:53.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 9.778126268958613e-05 Training loss: 6.902949810028076
2025-12-09 11:50:53.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 9.77720776666633e-05 Training loss: 6.834272861480713
2025-12-09 11:50:53.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 9.77628741043298e-05 Training loss: 6.90239143371582
2025-12-09 11:50:54.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 9.775365200615735e-05 Training loss: 6.978382587432861
2025-12-09 11:50:54.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 9.774441137572487e-05 Training loss: 6.79665470123291
2025-12-09 11:50:54.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 9.773515221661846e-05 Training loss: 7.056282997131348
2025-12-09 11:50:55.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 9.772587453243143e-05 Training loss: 6.698540687561035
2025-12-09 11:50:55.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 9.771657832676427e-05 Training loss: 7.082544803619385
2025-12-09 11:50:56.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 9.770726360322463e-05 Training loss: 7.505589962005615
2025-12-09 11:50:56.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 9.769793036542741e-05 Training loss: 6.792557716369629
2025-12-09 11:50:56.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 9.768857861699463e-05 Training loss: 6.664902687072754
2025-12-09 11:50:57.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 9.767920836155553e-05 Training loss: 6.79031229019165
2025-12-09 11:50:57.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 9.766981960274653e-05 Training loss: 6.4748992919921875
2025-12-09 11:50:58.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 9.766041234421122e-05 Training loss: 6.8009209632873535
2025-12-09 11:50:58.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 9.765098658960036e-05 Training loss: 6.563292980194092
2025-12-09 11:50:58.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 9.764154234257192e-05 Training loss: 6.8018293380737305
2025-12-09 11:50:59.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 9.763207960679101e-05 Training loss: 6.766617774963379
2025-12-09 11:50:59.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 9.762259838592994e-05 Training loss: 6.633728981018066
2025-12-09 11:51:00.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 9.761309868366819e-05 Training loss: 7.0273590087890625
2025-12-09 11:51:00.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 9.760358050369243e-05 Training loss: 6.586808681488037
2025-12-09 11:51:00.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 9.759404384969643e-05 Training loss: 6.4387593269348145
2025-12-09 11:51:01.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 9.758448872538122e-05 Training loss: 6.521852970123291
2025-12-09 11:51:01.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 9.757491513445493e-05 Training loss: 7.733351230621338
2025-12-09 11:51:01.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 9.756532308063293e-05 Training loss: 7.135603904724121
2025-12-09 11:51:02.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 9.755571256763765e-05 Training loss: 6.748829364776611
2025-12-09 11:51:02.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 9.754608359919879e-05 Training loss: 7.082398414611816
2025-12-09 11:51:03.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 9.753643617905313e-05 Training loss: 6.831174373626709
2025-12-09 11:51:03.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 9.752677031094466e-05 Training loss: 6.830504894256592
2025-12-09 11:51:03.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 9.751708599862452e-05 Training loss: 6.883012294769287
2025-12-09 11:51:04.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 9.750738324585098e-05 Training loss: 6.5276570320129395
2025-12-09 11:51:04.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 9.749766205638952e-05 Training loss: 6.715087413787842
2025-12-09 11:51:05.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 9.748792243401273e-05 Training loss: 7.134062767028809
2025-12-09 11:51:05.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 9.747816438250037e-05 Training loss: 6.580774784088135
2025-12-09 11:51:05.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 9.746838790563934e-05 Training loss: 6.9306182861328125
2025-12-09 11:51:06.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 9.74585930072237e-05 Training loss: 6.236834526062012
2025-12-09 11:51:06.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 9.744877969105469e-05 Training loss: 6.927964210510254
2025-12-09 11:51:07.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 9.743894796094062e-05 Training loss: 6.955658912658691
2025-12-09 11:51:07.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 9.742909782069701e-05 Training loss: 7.390912055969238
2025-12-09 11:51:07.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 9.741922927414651e-05 Training loss: 6.817129611968994
2025-12-09 11:51:08.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 9.740934232511894e-05 Training loss: 7.0487518310546875
2025-12-09 11:51:08.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 9.739943697745118e-05 Training loss: 6.792307376861572
2025-12-09 11:51:08.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 9.738951323498732e-05 Training loss: 6.597773551940918
2025-12-09 11:51:09.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 9.737957110157858e-05 Training loss: 7.194668769836426
2025-12-09 11:51:09.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 9.736961058108332e-05 Training loss: 6.721730709075928
2025-12-09 11:51:10.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 9.735963167736698e-05 Training loss: 6.7273125648498535
2025-12-09 11:51:10.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 9.734963439430222e-05 Training loss: 6.827000617980957
2025-12-09 11:51:10.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 9.733961873576878e-05 Training loss: 6.58355188369751
2025-12-09 11:51:11.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 9.732958470565353e-05 Training loss: 6.988039016723633
2025-12-09 11:51:11.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 9.731953230785049e-05 Training loss: 6.86458158493042
2025-12-09 11:51:12.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 9.730946154626079e-05 Training loss: 7.11777400970459
2025-12-09 11:51:12.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 9.729937242479271e-05 Training loss: 6.7841386795043945
2025-12-09 11:51:12.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 9.728926494736164e-05 Training loss: 6.570395469665527
2025-12-09 11:51:13.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 9.727913911789009e-05 Training loss: 6.612871170043945
2025-12-09 11:51:13.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 9.726899494030768e-05 Training loss: 6.600027561187744
2025-12-09 11:51:13.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 9.725883241855119e-05 Training loss: 6.914009094238281
2025-12-09 11:51:14.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 9.724865155656448e-05 Training loss: 6.708413600921631
2025-12-09 11:51:14.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 9.723845235829857e-05 Training loss: 6.590179920196533
2025-12-09 11:51:15.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 9.722823482771155e-05 Training loss: 6.409059524536133
2025-12-09 11:51:15.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 9.721799896876864e-05 Training loss: 6.420908451080322
2025-12-09 11:51:15.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 9.720774478544219e-05 Training loss: 6.9892578125
2025-12-09 11:51:16.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 9.719747228171163e-05 Training loss: 7.300526142120361
2025-12-09 11:51:16.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 9.718718146156355e-05 Training loss: 6.618362903594971
2025-12-09 11:51:17.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 9.717687232899159e-05 Training loss: 6.496706485748291
2025-12-09 11:51:17.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 9.716654488799652e-05 Training loss: 6.8204755783081055
2025-12-09 11:51:17.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 9.715619914258624e-05 Training loss: 6.8043622970581055
2025-12-09 11:51:18.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 9.71458350967757e-05 Training loss: 6.805016040802002
2025-12-09 11:51:18.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 9.713545275458703e-05 Training loss: 6.730920791625977
2025-12-09 11:51:19.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 9.712505212004938e-05 Training loss: 6.537230968475342
2025-12-09 11:51:19.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 9.711463319719904e-05 Training loss: 7.005712985992432
2025-12-09 11:51:19.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 9.710419599007939e-05 Training loss: 6.810000419616699
2025-12-09 11:51:20.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 9.70937405027409e-05 Training loss: 6.942133903503418
2025-12-09 11:51:20.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 9.708326673924115e-05 Training loss: 6.716490745544434
2025-12-09 11:51:20.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 9.707277470364482e-05 Training loss: 6.940902233123779
2025-12-09 11:51:21.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 9.706226440002363e-05 Training loss: 6.607178688049316
2025-12-09 11:51:21.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 9.705173583245645e-05 Training loss: 6.307256698608398
2025-12-09 11:51:22.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 9.704118900502919e-05 Training loss: 6.515537261962891
2025-12-09 11:51:22.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 9.703062392183489e-05 Training loss: 6.657983303070068
2025-12-09 11:51:22.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 9.702004058697363e-05 Training loss: 6.7905683517456055
2025-12-09 11:51:23.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 9.700943900455262e-05 Training loss: 6.807360649108887
2025-12-09 11:51:23.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 9.69988191786861e-05 Training loss: 6.6713547706604
2025-12-09 11:51:24.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 9.698818111349543e-05 Training loss: 7.139111042022705
2025-12-09 11:51:24.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 9.697752481310904e-05 Training loss: 6.529998779296875
2025-12-09 11:51:24.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 9.696685028166244e-05 Training loss: 6.291393756866455
2025-12-09 11:51:25.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 9.69561575232982e-05 Training loss: 7.700027942657471
2025-12-09 11:51:25.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 9.694544654216596e-05 Training loss: 6.585041522979736
2025-12-09 11:51:26.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 9.693471734242243e-05 Training loss: 6.357657432556152
2025-12-09 11:51:26.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 9.692396992823145e-05 Training loss: 6.971815586090088
2025-12-09 11:51:26.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 9.691320430376385e-05 Training loss: 6.779513835906982
2025-12-09 11:51:27.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 9.690242047319755e-05 Training loss: 7.00014591217041
2025-12-09 11:51:27.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 9.689161844071757e-05 Training loss: 6.531767845153809
2025-12-09 11:51:27.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 9.688079821051595e-05 Training loss: 7.906946182250977
2025-12-09 11:51:28.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 9.68699597867918e-05 Training loss: 7.001306056976318
2025-12-09 11:51:28.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 9.685910317375134e-05 Training loss: 6.83883810043335
2025-12-09 11:51:29.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 9.684822837560776e-05 Training loss: 6.697249889373779
2025-12-09 11:51:29.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 9.683733539658139e-05 Training loss: 6.743311882019043
2025-12-09 11:51:29.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 9.682642424089958e-05 Training loss: 6.579504013061523
2025-12-09 11:51:30.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 9.681549491279674e-05 Training loss: 6.569587707519531
2025-12-09 11:51:30.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 9.68045474165143e-05 Training loss: 6.7995405197143555
2025-12-09 11:51:31.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 9.679358175630081e-05 Training loss: 6.597778797149658
2025-12-09 11:51:31.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 9.67825979364118e-05 Training loss: 6.746580600738525
2025-12-09 11:51:31.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 9.677159596110987e-05 Training loss: 5.439647197723389
2025-12-09 11:51:32.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 9.676057583466472e-05 Training loss: 6.774657249450684
2025-12-09 11:51:32.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 9.674953756135298e-05 Training loss: 6.686281681060791
2025-12-09 11:51:33.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 9.673848114545843e-05 Training loss: 6.770918846130371
2025-12-09 11:51:33.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 9.672740659127184e-05 Training loss: 6.451181411743164
2025-12-09 11:51:33.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 9.671631390309102e-05 Training loss: 6.942211627960205
2025-12-09 11:51:34.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 9.670520308522084e-05 Training loss: 6.735526084899902
2025-12-09 11:51:34.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 9.66940741419732e-05 Training loss: 6.830439567565918
2025-12-09 11:51:34.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 9.668292707766699e-05 Training loss: 7.0077924728393555
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 3.45 GiB is free. Including non-PyTorch memory, this process has 90.93 GiB memory in use. Of the allocated memory 89.96 GiB is allocated by PyTorch, and 216.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 48/10000 [00:00<00:20, 477.55it/s]Tokenizing texts:   1%|▏         | 130/10000 [00:00<00:14, 672.69it/s]Tokenizing texts:   2%|▏         | 198/10000 [00:00<00:15, 616.98it/s]Tokenizing texts:   3%|▎         | 261/10000 [00:00<00:15, 619.29it/s]Tokenizing texts:   3%|▎         | 328/10000 [00:00<00:15, 633.80it/s]Tokenizing texts:   4%|▍         | 392/10000 [00:00<00:16, 586.63it/s]Tokenizing texts:   5%|▍         | 452/10000 [00:00<00:16, 566.12it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:00<00:15, 613.33it/s]Tokenizing texts:   6%|▌         | 588/10000 [00:00<00:16, 573.26it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:01<00:16, 572.90it/s]Tokenizing texts:   7%|▋         | 709/10000 [00:01<00:16, 560.34it/s]Tokenizing texts:   8%|▊         | 770/10000 [00:01<00:16, 573.98it/s]Tokenizing texts:   8%|▊         | 828/10000 [00:01<00:16, 566.50it/s]Tokenizing texts:   9%|▉         | 889/10000 [00:01<00:15, 575.06it/s]Tokenizing texts:  10%|▉         | 973/10000 [00:01<00:13, 651.02it/s]Tokenizing texts:  10%|█         | 1046/10000 [00:01<00:13, 673.85it/s]Tokenizing texts:  11%|█         | 1114/10000 [00:01<00:14, 613.41it/s]Tokenizing texts:  12%|█▏        | 1177/10000 [00:01<00:15, 581.62it/s]Tokenizing texts:  12%|█▏        | 1237/10000 [00:02<00:14, 584.94it/s]Tokenizing texts:  13%|█▎        | 1303/10000 [00:02<00:14, 601.42it/s]Tokenizing texts:  14%|█▎        | 1364/10000 [00:02<00:14, 582.96it/s]Tokenizing texts:  14%|█▍        | 1425/10000 [00:02<00:14, 586.22it/s]Tokenizing texts:  15%|█▌        | 1502/10000 [00:02<00:13, 636.58it/s]Tokenizing texts:  16%|█▌        | 1576/10000 [00:02<00:12, 663.64it/s]Tokenizing texts:  16%|█▋        | 1643/10000 [00:02<00:13, 612.08it/s]Tokenizing texts:  17%|█▋        | 1706/10000 [00:02<00:14, 591.60it/s]Tokenizing texts:  18%|█▊        | 1781/10000 [00:02<00:12, 633.14it/s]Tokenizing texts:  18%|█▊        | 1846/10000 [00:03<00:13, 595.83it/s]Tokenizing texts:  19%|█▉        | 1910/10000 [00:03<00:13, 605.71it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:03<00:14, 561.22it/s]Tokenizing texts:  20%|██        | 2041/10000 [00:03<00:13, 594.78it/s]Tokenizing texts:  21%|██        | 2102/10000 [00:03<00:14, 550.81it/s]Tokenizing texts:  22%|██▏       | 2159/10000 [00:03<00:14, 555.86it/s]Tokenizing texts:  22%|██▏       | 2232/10000 [00:03<00:12, 603.27it/s]Tokenizing texts:  23%|██▎       | 2298/10000 [00:03<00:12, 618.77it/s]Tokenizing texts:  24%|██▎       | 2361/10000 [00:03<00:12, 600.49it/s]Tokenizing texts:  24%|██▍       | 2429/10000 [00:04<00:12, 614.23it/s]Tokenizing texts:  25%|██▍       | 2493/10000 [00:04<00:12, 620.59it/s]Tokenizing texts:  26%|██▌       | 2561/10000 [00:04<00:12, 606.90it/s]Tokenizing texts:  26%|██▌       | 2623/10000 [00:04<00:12, 574.44it/s]Tokenizing texts:  27%|██▋       | 2681/10000 [00:04<00:12, 571.08it/s]Tokenizing texts:  27%|██▋       | 2739/10000 [00:04<00:12, 571.15it/s]Tokenizing texts:  28%|██▊       | 2797/10000 [00:04<00:13, 532.83it/s]Tokenizing texts:  29%|██▊       | 2854/10000 [00:04<00:13, 540.28it/s]Tokenizing texts:  29%|██▉       | 2931/10000 [00:04<00:11, 602.88it/s]Tokenizing texts:  30%|██▉       | 2992/10000 [00:05<00:11, 595.93it/s]Tokenizing texts:  31%|███       | 3053/10000 [00:05<00:11, 592.08it/s]Tokenizing texts:  31%|███       | 3124/10000 [00:05<00:11, 623.69it/s]Tokenizing texts:  32%|███▏      | 3187/10000 [00:05<00:11, 614.71it/s]Tokenizing texts:  33%|███▎      | 3254/10000 [00:05<00:10, 629.06it/s]Tokenizing texts:  33%|███▎      | 3319/10000 [00:05<00:10, 627.95it/s]Tokenizing texts:  34%|███▍      | 3384/10000 [00:05<00:10, 633.27it/s]Tokenizing texts:  35%|███▍      | 3460/10000 [00:05<00:09, 669.66it/s]Tokenizing texts:  35%|███▌      | 3528/10000 [00:05<00:10, 622.91it/s]Tokenizing texts:  36%|███▌      | 3592/10000 [00:05<00:10, 620.41it/s]Tokenizing texts:  37%|███▋      | 3655/10000 [00:06<00:10, 609.60it/s]Tokenizing texts:  37%|███▋      | 3717/10000 [00:06<00:10, 610.08it/s]Tokenizing texts:  38%|███▊      | 3779/10000 [00:06<00:10, 589.04it/s]Tokenizing texts:  38%|███▊      | 3839/10000 [00:06<00:10, 569.59it/s]Tokenizing texts:  39%|███▉      | 3897/10000 [00:06<00:10, 570.60it/s]Tokenizing texts:  40%|███▉      | 3960/10000 [00:06<00:10, 587.30it/s]Tokenizing texts:  40%|████      | 4019/10000 [00:06<00:10, 574.22it/s]Tokenizing texts:  41%|████      | 4095/10000 [00:06<00:09, 626.30it/s]Tokenizing texts:  42%|████▏     | 4166/10000 [00:06<00:08, 649.29it/s]Tokenizing texts:  42%|████▏     | 4232/10000 [00:07<00:08, 646.79it/s]Tokenizing texts:  43%|████▎     | 4297/10000 [00:07<00:09, 615.49it/s]Tokenizing texts:  44%|████▎     | 4373/10000 [00:07<00:08, 650.44it/s]Tokenizing texts:  44%|████▍     | 4445/10000 [00:07<00:08, 656.32it/s]Tokenizing texts:  45%|████▌     | 4511/10000 [00:07<00:08, 646.80it/s]Tokenizing texts:  46%|████▌     | 4576/10000 [00:07<00:08, 646.50it/s]Tokenizing texts:  46%|████▋     | 4641/10000 [00:07<00:08, 642.45it/s]Tokenizing texts:  47%|████▋     | 4728/10000 [00:07<00:07, 705.76it/s]Tokenizing texts:  48%|████▊     | 4799/10000 [00:07<00:08, 636.56it/s]Tokenizing texts:  49%|████▉     | 4875/10000 [00:08<00:07, 660.79it/s]Tokenizing texts:  49%|████▉     | 4946/10000 [00:08<00:07, 669.85it/s]Tokenizing texts:  50%|█████     | 5024/10000 [00:08<00:07, 699.52it/s]Tokenizing texts:  51%|█████     | 5095/10000 [00:08<00:07, 640.59it/s]Tokenizing texts:  52%|█████▏    | 5161/10000 [00:08<00:07, 633.50it/s]Tokenizing texts:  52%|█████▏    | 5226/10000 [00:08<00:07, 622.39it/s]Tokenizing texts:  53%|█████▎    | 5292/10000 [00:08<00:07, 632.23it/s]Tokenizing texts:  54%|█████▍    | 5379/10000 [00:08<00:06, 693.26it/s]Tokenizing texts:  54%|█████▍    | 5449/10000 [00:08<00:06, 680.06it/s]Tokenizing texts:  55%|█████▌    | 5518/10000 [00:09<00:07, 614.07it/s]Tokenizing texts:  56%|█████▌    | 5588/10000 [00:09<00:06, 635.86it/s]Tokenizing texts:  57%|█████▋    | 5672/10000 [00:09<00:06, 691.49it/s]Tokenizing texts:  57%|█████▋    | 5743/10000 [00:09<00:06, 671.65it/s]Tokenizing texts:  58%|█████▊    | 5812/10000 [00:09<00:07, 596.05it/s]Tokenizing texts:  59%|█████▉    | 5881/10000 [00:09<00:06, 618.28it/s]Tokenizing texts:  59%|█████▉    | 5945/10000 [00:09<00:06, 606.98it/s]Tokenizing texts:  60%|██████    | 6010/10000 [00:09<00:06, 595.66it/s]Tokenizing texts:  61%|██████    | 6087/10000 [00:09<00:06, 641.82it/s]Tokenizing texts:  62%|██████▏   | 6157/10000 [00:10<00:05, 652.70it/s]Tokenizing texts:  62%|██████▏   | 6224/10000 [00:10<00:05, 637.45it/s]Tokenizing texts:  63%|██████▎   | 6289/10000 [00:10<00:05, 637.32it/s]Tokenizing texts:  64%|██████▎   | 6354/10000 [00:10<00:06, 600.97it/s]Tokenizing texts:  64%|██████▍   | 6423/10000 [00:10<00:05, 617.70it/s]Tokenizing texts:  65%|██████▍   | 6486/10000 [00:10<00:05, 615.48it/s]Tokenizing texts:  65%|██████▌   | 6548/10000 [00:10<00:05, 613.10it/s]Tokenizing texts:  66%|██████▌   | 6621/10000 [00:10<00:05, 645.96it/s]Tokenizing texts:  67%|██████▋   | 6686/10000 [00:10<00:05, 639.35it/s]Tokenizing texts:  68%|██████▊   | 6751/10000 [00:10<00:05, 622.11it/s]Tokenizing texts:  68%|██████▊   | 6834/10000 [00:11<00:04, 679.36it/s]Tokenizing texts:  69%|██████▉   | 6903/10000 [00:11<00:05, 613.14it/s]Tokenizing texts:  70%|██████▉   | 6966/10000 [00:11<00:04, 612.33it/s]Tokenizing texts:  70%|███████   | 7029/10000 [00:11<00:04, 614.91it/s]Tokenizing texts:  71%|███████   | 7092/10000 [00:11<00:05, 574.69it/s]Tokenizing texts:  72%|███████▏  | 7153/10000 [00:11<00:04, 583.31it/s]Tokenizing texts:  72%|███████▏  | 7217/10000 [00:11<00:04, 598.37it/s]Tokenizing texts:  73%|███████▎  | 7296/10000 [00:11<00:04, 652.52it/s]Tokenizing texts:  74%|███████▎  | 7362/10000 [00:11<00:04, 648.82it/s]Tokenizing texts:  74%|███████▍  | 7428/10000 [00:12<00:04, 603.67it/s]Tokenizing texts:  75%|███████▍  | 7490/10000 [00:12<00:04, 593.96it/s]Tokenizing texts:  76%|███████▌  | 7561/10000 [00:12<00:03, 615.19it/s]Tokenizing texts:  76%|███████▋  | 7635/10000 [00:12<00:03, 649.93it/s]Tokenizing texts:  77%|███████▋  | 7709/10000 [00:12<00:03, 674.79it/s]Tokenizing texts:  78%|███████▊  | 7777/10000 [00:12<00:03, 623.04it/s]Tokenizing texts:  78%|███████▊  | 7841/10000 [00:12<00:03, 618.09it/s]Tokenizing texts:  79%|███████▉  | 7904/10000 [00:12<00:03, 612.08it/s]Tokenizing texts:  80%|███████▉  | 7982/10000 [00:12<00:03, 659.35it/s]Tokenizing texts:  80%|████████  | 8049/10000 [00:13<00:03, 628.96it/s]Tokenizing texts:  81%|████████  | 8113/10000 [00:13<00:03, 567.89it/s]Tokenizing texts:  82%|████████▏ | 8180/10000 [00:13<00:03, 594.38it/s]Tokenizing texts:  82%|████████▏ | 8241/10000 [00:13<00:03, 561.02it/s]Tokenizing texts:  83%|████████▎ | 8313/10000 [00:13<00:02, 602.18it/s]Tokenizing texts:  84%|████████▍ | 8376/10000 [00:13<00:02, 608.39it/s]Tokenizing texts:  84%|████████▍ | 8438/10000 [00:13<00:02, 594.71it/s]Tokenizing texts:  85%|████████▌ | 8511/10000 [00:13<00:02, 627.28it/s]Tokenizing texts:  86%|████████▌ | 8575/10000 [00:13<00:02, 623.98it/s]Tokenizing texts:  86%|████████▋ | 8650/10000 [00:14<00:02, 659.78it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:14<00:01, 667.36it/s]Tokenizing texts:  88%|████████▊ | 8793/10000 [00:14<00:01, 662.82it/s]Tokenizing texts:  89%|████████▊ | 8860/10000 [00:14<00:01, 595.68it/s]Tokenizing texts:  89%|████████▉ | 8943/10000 [00:14<00:01, 657.69it/s]Tokenizing texts:  90%|█████████ | 9011/10000 [00:14<00:01, 618.87it/s]Tokenizing texts:  91%|█████████ | 9080/10000 [00:14<00:01, 625.59it/s]Tokenizing texts:  92%|█████████▏| 9151/10000 [00:14<00:01, 646.58it/s]Tokenizing texts:  92%|█████████▏| 9225/10000 [00:14<00:01, 653.45it/s]Tokenizing texts:  93%|█████████▎| 9291/10000 [00:15<00:01, 635.36it/s]Tokenizing texts:  94%|█████████▎| 9355/10000 [00:15<00:01, 618.53it/s]Tokenizing texts:  94%|█████████▍| 9418/10000 [00:15<00:00, 596.04it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:15<00:00, 599.18it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:15<00:00, 563.26it/s]Tokenizing texts:  96%|█████████▌| 9599/10000 [00:15<00:00, 567.93it/s]Tokenizing texts:  97%|█████████▋| 9659/10000 [00:15<00:00, 575.80it/s]Tokenizing texts:  97%|█████████▋| 9717/10000 [00:15<00:00, 570.56it/s]Tokenizing texts:  98%|█████████▊| 9775/10000 [00:15<00:00, 544.49it/s]Tokenizing texts:  99%|█████████▊| 9852/10000 [00:16<00:00, 605.74it/s]Tokenizing texts:  99%|█████████▉| 9923/10000 [00:16<00:00, 635.44it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:16<00:00, 616.26it/s]
2025-12-09 11:52:30.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 12.195647239685059
2025-12-09 11:52:30.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 12.210142135620117
2025-12-09 11:52:31.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 12.209911346435547
2025-12-09 11:52:31.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 12.047470092773438
2025-12-09 11:52:32.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 12.087320327758789
2025-12-09 11:52:32.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 12.045024871826172
2025-12-09 11:52:32.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 11.848505020141602
2025-12-09 11:52:33.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 11.786792755126953
2025-12-09 11:52:33.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 11.418610572814941
2025-12-09 11:52:33.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 11.632242202758789
2025-12-09 11:52:34.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 11.154977798461914
2025-12-09 11:52:34.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 11.309325218200684
2025-12-09 11:52:35.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 10.759820938110352
2025-12-09 11:52:35.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 10.69898509979248
2025-12-09 11:52:35.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 10.538938522338867
2025-12-09 11:52:36.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 10.37386417388916
2025-12-09 11:52:36.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 10.151605606079102
2025-12-09 11:52:37.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 10.043848991394043
2025-12-09 11:52:37.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 9.884604454040527
2025-12-09 11:52:37.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 9.698040962219238
2025-12-09 11:52:38.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 10.032525062561035
2025-12-09 11:52:38.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 9.76235580444336
2025-12-09 11:52:38.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 9.516702651977539
2025-12-09 11:52:39.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 9.517524719238281
2025-12-09 11:52:39.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 9.093731880187988
2025-12-09 11:52:40.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 9.345085144042969
2025-12-09 11:52:40.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 9.043966293334961
2025-12-09 11:52:40.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 8.905014038085938
2025-12-09 11:52:41.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 8.844878196716309
2025-12-09 11:52:41.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 8.68779468536377
2025-12-09 11:52:42.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 9.051750183105469
2025-12-09 11:52:42.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 8.652824401855469
2025-12-09 11:52:42.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 8.584383010864258
2025-12-09 11:52:43.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 8.822124481201172
2025-12-09 11:52:43.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 8.160841941833496
2025-12-09 11:52:43.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 8.335931777954102
2025-12-09 11:52:44.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 8.440910339355469
2025-12-09 11:52:44.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 8.458935737609863
2025-12-09 11:52:45.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 8.36422348022461
2025-12-09 11:52:45.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 8.24206829071045
2025-12-09 11:52:45.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 8.326736450195312
2025-12-09 11:52:46.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 8.578356742858887
2025-12-09 11:52:46.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 8.958684921264648
2025-12-09 11:52:47.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 8.57981014251709
2025-12-09 11:52:47.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 8.057558059692383
2025-12-09 11:52:47.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 8.08301067352295
2025-12-09 11:52:48.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 8.192325592041016
2025-12-09 11:52:48.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 8.264022827148438
2025-12-09 11:52:48.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 8.394608497619629
2025-12-09 11:52:49.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 8.031991004943848
2025-12-09 11:52:49.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 8.237576484680176
2025-12-09 11:52:50.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 7.996508598327637
2025-12-09 11:52:50.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 8.10703182220459
2025-12-09 11:52:50.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 7.972955226898193
2025-12-09 11:52:51.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 8.124898910522461
2025-12-09 11:52:51.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 8.069774627685547
2025-12-09 11:52:52.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 8.467214584350586
2025-12-09 11:52:52.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 8.084531784057617
2025-12-09 11:52:52.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 8.294401168823242
2025-12-09 11:52:53.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 8.344572067260742
2025-12-09 11:52:53.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 9.257359504699707
2025-12-09 11:52:54.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 8.463911056518555
2025-12-09 11:52:54.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 8.2970609664917
2025-12-09 11:52:54.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 7.9953460693359375
2025-12-09 11:52:55.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 9.164457321166992
2025-12-09 11:52:55.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 8.06363582611084
2025-12-09 11:52:55.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 8.085485458374023
2025-12-09 11:52:56.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 8.127724647521973
2025-12-09 11:52:56.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 7.9769697189331055
2025-12-09 11:52:57.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 8.089627265930176
2025-12-09 11:52:57.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 8.531134605407715
2025-12-09 11:52:57.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 8.155579566955566
2025-12-09 11:52:58.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 8.024641036987305
2025-12-09 11:52:58.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 8.28935718536377
2025-12-09 11:52:59.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 8.272858619689941
2025-12-09 11:52:59.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 7.763786315917969
2025-12-09 11:52:59.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 8.34685230255127
2025-12-09 11:53:00.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 8.44885540008545
2025-12-09 11:53:00.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 7.8034515380859375
2025-12-09 11:53:00.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 7.961642265319824
2025-12-09 11:53:01.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 7.820781230926514
2025-12-09 11:53:01.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 7.862807273864746
2025-12-09 11:53:02.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 8.350871086120605
2025-12-09 11:53:02.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 8.191899299621582
2025-12-09 11:53:02.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 8.106264114379883
2025-12-09 11:53:03.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 8.036953926086426
2025-12-09 11:53:03.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 7.682372093200684
2025-12-09 11:53:04.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 7.972251892089844
2025-12-09 11:53:04.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 8.544795989990234
2025-12-09 11:53:04.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 8.358997344970703
2025-12-09 11:53:05.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 8.467023849487305
2025-12-09 11:53:05.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 7.680288791656494
2025-12-09 11:53:05.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 8.244388580322266
2025-12-09 11:53:06.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 7.989962577819824
2025-12-09 11:53:06.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 7.862560272216797
2025-12-09 11:53:07.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 7.945010185241699
2025-12-09 11:53:07.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 7.59691047668457
2025-12-09 11:53:07.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 7.854055404663086
2025-12-09 11:53:08.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 8.045104026794434
2025-12-09 11:53:08.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 8.22482967376709
2025-12-09 11:53:09.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999997089396425 Training loss: 8.65170669555664
2025-12-09 11:53:09.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029999988357586825 Training loss: 8.318739891052246
2025-12-09 11:53:09.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000299999738045746 Training loss: 7.779054641723633
2025-12-09 11:53:10.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0002999995343036539 Training loss: 7.773654937744141
2025-12-09 11:53:10.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00029999927234967104 Training loss: 8.369976997375488
2025-12-09 11:53:10.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.00029999895218389905 Training loss: 8.044071197509766
2025-12-09 11:53:11.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0002999985738064622 Training loss: 8.034329414367676
2025-12-09 11:53:11.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00029999813721750737 Training loss: 7.924981594085693
2025-12-09 11:53:12.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.00029999764241720394 Training loss: 8.248053550720215
2025-12-09 11:53:12.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0002999970894057439 Training loss: 7.9607110023498535
2025-12-09 11:53:12.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029999647818334195 Training loss: 7.834194660186768
2025-12-09 11:53:13.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0002999958087502352 Training loss: 7.589869499206543
2025-12-09 11:53:13.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.00029999508110668355 Training loss: 7.7927632331848145
2025-12-09 11:53:14.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0002999942952529693 Training loss: 7.85597562789917
2025-12-09 11:53:14.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00029999345118939745 Training loss: 7.892955780029297
2025-12-09 11:53:14.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0002999925489162956 Training loss: 8.02617359161377
2025-12-09 11:53:15.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029999158843401386 Training loss: 8.134603500366211
2025-12-09 11:53:15.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.000299990569742925 Training loss: 7.912607669830322
2025-12-09 11:53:16.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002999894928434243 Training loss: 7.783679485321045
2025-12-09 11:53:16.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.00029998835773592975 Training loss: 8.083732604980469
2025-12-09 11:53:16.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.00029998716442088184 Training loss: 7.949980735778809
2025-12-09 11:53:17.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0002999859128987437 Training loss: 8.045610427856445
2025-12-09 11:53:17.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00029998460317000097 Training loss: 7.695173263549805
2025-12-09 11:53:17.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00029998323523516195 Training loss: 7.75277853012085
2025-12-09 11:53:18.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002999818090947575 Training loss: 7.398220539093018
2025-12-09 11:53:18.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00029998032474934106 Training loss: 8.039436340332031
2025-12-09 11:53:19.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0002999787821994887 Training loss: 8.095203399658203
2025-12-09 11:53:19.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00029997718144579913 Training loss: 7.930168151855469
2025-12-09 11:53:19.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0002999755224888935 Training loss: 7.917242527008057
2025-12-09 11:53:20.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029997380532941555 Training loss: 7.789340496063232
2025-12-09 11:53:20.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00029997202996803177 Training loss: 7.939821720123291
2025-12-09 11:53:21.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0002999701964054312 Training loss: 7.78075647354126
2025-12-09 11:53:21.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0002999683046423252 Training loss: 7.913728713989258
2025-12-09 11:53:21.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0002999663546794481 Training loss: 7.516773700714111
2025-12-09 11:53:22.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.00029996434651755657 Training loss: 8.30136489868164
2025-12-09 11:53:22.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.00029996228015743 Training loss: 7.126419544219971
2025-12-09 11:53:22.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002999601555998703 Training loss: 7.667633056640625
2025-12-09 11:53:23.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0002999579728457019 Training loss: 7.4263715744018555
2025-12-09 11:53:23.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0002999557318957719 Training loss: 7.385196208953857
2025-12-09 11:53:24.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.00029995343275095003 Training loss: 7.674373626708984
2025-12-09 11:53:24.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.00029995107541212843 Training loss: 7.678907871246338
2025-12-09 11:53:24.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00029994865988022205 Training loss: 7.8711724281311035
2025-12-09 11:53:25.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0002999461861561683 Training loss: 8.061915397644043
2025-12-09 11:53:25.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0002999436542409271 Training loss: 7.710690021514893
2025-12-09 11:53:26.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0002999410641354812 Training loss: 7.909791946411133
2025-12-09 11:53:26.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00029993841584083553 Training loss: 7.75919246673584
2025-12-09 11:53:26.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00029993570935801805 Training loss: 7.4135355949401855
2025-12-09 11:53:27.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.000299932944688079 Training loss: 7.671759128570557
2025-12-09 11:53:27.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.00029993012183209135 Training loss: 7.536664009094238
2025-12-09 11:53:28.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002999272407911505 Training loss: 7.7433624267578125
2025-12-09 11:53:28.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.00029992430156637454 Training loss: 7.48207426071167
2025-12-09 11:53:28.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00029992130415890426 Training loss: 7.560174465179443
2025-12-09 11:53:29.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.00029991824856990276 Training loss: 8.239033699035645
2025-12-09 11:53:29.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0002999151348005559 Training loss: 8.125737190246582
2025-12-09 11:53:29.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0002999119628520721 Training loss: 7.751916408538818
2025-12-09 11:53:30.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.00029990873272568226 Training loss: 7.496130466461182
2025-12-09 11:53:30.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.00029990544442263996 Training loss: 7.509068965911865
2025-12-09 11:53:31.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002999020979442214 Training loss: 7.813720226287842
2025-12-09 11:53:31.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0002998986932917252 Training loss: 7.479601860046387
2025-12-09 11:53:31.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.00029989523046647257 Training loss: 7.674168586730957
2025-12-09 11:53:32.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00029989170946980755 Training loss: 7.649210453033447
2025-12-09 11:53:32.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00029988813030309644 Training loss: 7.418205261230469
2025-12-09 11:53:33.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0002998844929677283 Training loss: 7.441422462463379
2025-12-09 11:53:33.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00029988079746511465 Training loss: 7.348363876342773
2025-12-09 11:53:33.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.00029987704379668973 Training loss: 7.898510456085205
2025-12-09 11:53:34.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0002998732319639102 Training loss: 7.562131404876709
2025-12-09 11:53:34.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.00029986936196825536 Training loss: 7.892760753631592
2025-12-09 11:53:34.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0002998654338112271 Training loss: 7.546709060668945
2025-12-09 11:53:35.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00029986144749434985 Training loss: 7.525168418884277
2025-12-09 11:53:35.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0002998574030191706 Training loss: 7.459802627563477
2025-12-09 11:53:36.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.000299853300387259 Training loss: 7.401913166046143
2025-12-09 11:53:36.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00029984913960020714 Training loss: 7.4280924797058105
2025-12-09 11:53:36.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.00029984492065962976 Training loss: 7.715527534484863
2025-12-09 11:53:37.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00029984064356716414 Training loss: 7.3319220542907715
2025-12-09 11:53:37.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0002998363083244701 Training loss: 7.413334369659424
2025-12-09 11:53:38.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00029983191493323017 Training loss: 7.629110336303711
2025-12-09 11:53:38.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0002998274633951493 Training loss: 8.234021186828613
2025-12-09 11:53:38.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00029982295371195494 Training loss: 8.259940147399902
2025-12-09 11:53:39.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00029981838588539735 Training loss: 7.390384197235107
2025-12-09 11:53:39.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.00029981375991724915 Training loss: 7.560504913330078
2025-12-09 11:53:40.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0002998090758093056 Training loss: 7.733277320861816
2025-12-09 11:53:40.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00029980433356338447 Training loss: 7.515452861785889
2025-12-09 11:53:40.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0002997995331813262 Training loss: 8.130315780639648
2025-12-09 11:53:41.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.00029979467466499367 Training loss: 7.275527000427246
2025-12-09 11:53:41.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00029978975801627243 Training loss: 7.114192008972168
2025-12-09 11:53:41.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0002997847832370704 Training loss: 7.586554050445557
2025-12-09 11:53:42.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0002997797503293184 Training loss: 7.384702205657959
2025-12-09 11:53:42.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00029977465929496947 Training loss: 7.184194087982178
2025-12-09 11:53:43.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0002997695101359994 Training loss: 7.781519412994385
2025-12-09 11:53:43.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0002997643028544064 Training loss: 7.775705337524414
2025-12-09 11:53:43.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0002997590374522114 Training loss: 7.496984004974365
2025-12-09 11:53:44.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0002997537139314577 Training loss: 7.415854454040527
2025-12-09 11:53:44.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0002997483322942114 Training loss: 7.238037586212158
2025-12-09 11:53:45.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0002997428925425609 Training loss: 7.7661027908325195
2025-12-09 11:53:45.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0002997373946786173 Training loss: 7.47092342376709
2025-12-09 11:53:45.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.00029973183870451417 Training loss: 7.472369194030762
2025-12-09 11:53:46.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0002997262246224077 Training loss: 7.2180256843566895
2025-12-09 11:53:46.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00029972055243447665 Training loss: 7.600348472595215
2025-12-09 11:53:47.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.00029971482214292223 Training loss: 7.562334060668945
2025-12-09 11:53:47.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.00029970903374996826 Training loss: 7.181590557098389
2025-12-09 11:53:47.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0002997031872578611 Training loss: 7.419060707092285
2025-12-09 11:53:48.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00029969728266886973 Training loss: 7.353801250457764
2025-12-09 11:53:48.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00029969131998528554 Training loss: 7.227292060852051
2025-12-09 11:53:48.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0002996852992094225 Training loss: 7.485999584197998
2025-12-09 11:53:49.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.00029967922034361723 Training loss: 7.368610382080078
2025-12-09 11:53:49.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0002996730833902287 Training loss: 7.6003336906433105
2025-12-09 11:53:50.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00029966688835163875 Training loss: 7.408330917358398
2025-12-09 11:53:50.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00029966063523025136 Training loss: 7.876013278961182
2025-12-09 11:53:50.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00029965432402849333 Training loss: 7.452951908111572
2025-12-09 11:53:51.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0002996479547488139 Training loss: 7.333792209625244
2025-12-09 11:53:51.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0002996415273936849 Training loss: 7.533891201019287
2025-12-09 11:53:52.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00029963504196560056 Training loss: 7.620802879333496
2025-12-09 11:53:52.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.00029962849846707786 Training loss: 7.336310863494873
2025-12-09 11:53:52.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0002996218969006561 Training loss: 7.351088523864746
2025-12-09 11:53:53.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00029961523726889733 Training loss: 7.250522136688232
2025-12-09 11:53:53.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00029960851957438594 Training loss: 7.282327175140381
2025-12-09 11:53:54.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.000299601743819729 Training loss: 7.448734283447266
2025-12-09 11:53:54.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00029959491000755594 Training loss: 7.4393720626831055
2025-12-09 11:53:54.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00029958801814051897 Training loss: 7.1354594230651855
2025-12-09 11:53:55.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0002995810682212926 Training loss: 7.19436502456665
2025-12-09 11:53:55.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0002995740602525739 Training loss: 7.450198173522949
2025-12-09 11:53:55.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0002995669942370827 Training loss: 7.5184197425842285
2025-12-09 11:53:56.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.000299559870177561 Training loss: 7.31287956237793
2025-12-09 11:53:56.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0002995526880767737 Training loss: 7.470338344573975
2025-12-09 11:53:57.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.00029954544793750785 Training loss: 7.125987529754639
2025-12-09 11:53:57.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00029953814976257335 Training loss: 7.126150608062744
2025-12-09 11:53:57.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0002995307935548024 Training loss: 7.1201019287109375
2025-12-09 11:53:58.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0002995233793170498 Training loss: 7.307815074920654
2025-12-09 11:53:58.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.00029951590705219283 Training loss: 7.288769721984863
2025-12-09 11:53:59.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0002995083767631314 Training loss: 7.018692493438721
2025-12-09 11:53:59.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0002995007884527879 Training loss: 6.80881404876709
2025-12-09 11:53:59.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.00029949314212410715 Training loss: 7.283051013946533
2025-12-09 11:54:00.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.00029948543778005656 Training loss: 7.136895656585693
2025-12-09 11:54:00.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00029947767542362597 Training loss: 7.234604358673096
2025-12-09 11:54:00.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0002994698550578279 Training loss: 7.766472816467285
2025-12-09 11:54:01.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0002994619766856972 Training loss: 7.3972859382629395
2025-12-09 11:54:01.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00029945404031029134 Training loss: 7.464795112609863
2025-12-09 11:54:02.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.00029944604593469033 Training loss: 7.16055154800415
2025-12-09 11:54:02.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00029943799356199656 Training loss: 7.044769763946533
2025-12-09 11:54:02.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.00029942988319533504 Training loss: 6.984248638153076
2025-12-09 11:54:03.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0002994217148378532 Training loss: 6.999874114990234
2025-12-09 11:54:03.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00029941348849272105 Training loss: 7.513742446899414
2025-12-09 11:54:04.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0002994052041631311 Training loss: 7.274445533752441
2025-12-09 11:54:04.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0002993968618522982 Training loss: 6.974969863891602
2025-12-09 11:54:04.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0002993884615634601 Training loss: 7.233747959136963
2025-12-09 11:54:05.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.00029938000329987645 Training loss: 7.3128743171691895
2025-12-09 11:54:05.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00029937148706483003 Training loss: 7.699583053588867
2025-12-09 11:54:06.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.00029936291286162577 Training loss: 7.516881465911865
2025-12-09 11:54:06.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.000299354280693591 Training loss: 7.096187591552734
2025-12-09 11:54:06.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0002993455905640758 Training loss: 7.286556720733643
2025-12-09 11:54:07.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0002993368424764526 Training loss: 7.0550360679626465
2025-12-09 11:54:07.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0002993280364341165 Training loss: 6.8728837966918945
2025-12-09 11:54:08.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00029931917244048473 Training loss: 8.113113403320312
2025-12-09 11:54:08.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0002993102504989974 Training loss: 7.236594200134277
2025-12-09 11:54:08.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00029930127061311685 Training loss: 7.375564098358154
2025-12-09 11:54:09.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0002992922327863281 Training loss: 7.372352123260498
2025-12-09 11:54:09.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00029928313702213844 Training loss: 7.289633750915527
2025-12-09 11:54:09.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.00029927398332407784 Training loss: 6.9264750480651855
2025-12-09 11:54:10.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00029926477169569865 Training loss: 7.135375499725342
2025-12-09 11:54:10.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00029925550214057565 Training loss: 7.485382556915283
2025-12-09 11:54:11.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.00029924617466230624 Training loss: 6.92376708984375
2025-12-09 11:54:11.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.00029923678926451034 Training loss: 7.09110164642334
2025-12-09 11:54:11.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00029922734595083005 Training loss: 7.117799758911133
2025-12-09 11:54:12.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0002992178447249302 Training loss: 7.291856288909912
2025-12-09 11:54:12.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.00029920828559049805 Training loss: 7.019102573394775
2025-12-09 11:54:13.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0002991986685512433 Training loss: 7.304093360900879
2025-12-09 11:54:13.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0002991889936108982 Training loss: 6.951170921325684
2025-12-09 11:54:13.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0002991792607732173 Training loss: 7.742063999176025
2025-12-09 11:54:14.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0002991694700419778 Training loss: 7.199301719665527
2025-12-09 11:54:14.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00029915962142097925 Training loss: 7.588949203491211
2025-12-09 11:54:14.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00029914971491404373 Training loss: 7.131102561950684
2025-12-09 11:54:15.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00029913975052501575 Training loss: 7.146050453186035
2025-12-09 11:54:15.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0002991297282577623 Training loss: 7.319965839385986
2025-12-09 11:54:16.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00029911964811617285 Training loss: 7.095100402832031
2025-12-09 11:54:16.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00029910951010415926 Training loss: 7.228006839752197
2025-12-09 11:54:16.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0002990993142256559 Training loss: 7.242134094238281
2025-12-09 11:54:17.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0002990890604846196 Training loss: 7.1615071296691895
2025-12-09 11:54:17.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00029907874888502966 Training loss: 6.974894046783447
2025-12-09 11:54:18.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.00029906837943088785 Training loss: 7.575570583343506
2025-12-09 11:54:18.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.00029905795212621823 Training loss: 6.9730658531188965
2025-12-09 11:54:18.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.00029904746697506754 Training loss: 6.8733649253845215
2025-12-09 11:54:19.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0002990369239815048 Training loss: 7.149638652801514
2025-12-09 11:54:19.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.00029902632314962157 Training loss: 7.344451427459717
2025-12-09 11:54:20.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002990156644835318 Training loss: 7.2489447593688965
2025-12-09 11:54:20.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.00029900494798737194 Training loss: 7.185527801513672
2025-12-09 11:54:20.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00029899417366530085 Training loss: 7.304479122161865
2025-12-09 11:54:21.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.00029898334152149984 Training loss: 7.226280212402344
2025-12-09 11:54:21.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0002989724515601726 Training loss: 7.696072101593018
2025-12-09 11:54:21.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0002989615037855454 Training loss: 7.279362678527832
2025-12-09 11:54:22.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0002989504982018668 Training loss: 7.50941276550293
2025-12-09 11:54:22.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.00029893943481340785 Training loss: 6.83786153793335
2025-12-09 11:54:23.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.000298928313624462 Training loss: 6.576047420501709
2025-12-09 11:54:23.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0002989171346393453 Training loss: 7.252228736877441
2025-12-09 11:54:23.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00029890589786239595 Training loss: 7.305447101593018
2025-12-09 11:54:24.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0002988946032979748 Training loss: 7.00565767288208
2025-12-09 11:54:24.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.000298883250950465 Training loss: 7.329619407653809
2025-12-09 11:54:25.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0002988718408242722 Training loss: 7.0783371925354
2025-12-09 11:54:25.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.00029886037292382455 Training loss: 7.148462295532227
2025-12-09 11:54:25.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00029884884725357236 Training loss: 7.31771183013916
2025-12-09 11:54:26.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0002988372638179886 Training loss: 6.9070820808410645
2025-12-09 11:54:26.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002988256226215685 Training loss: 7.201871395111084
2025-12-09 11:54:27.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0002988139236688299 Training loss: 7.253376007080078
2025-12-09 11:54:27.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.00029880216696431285 Training loss: 7.003986835479736
2025-12-09 11:54:27.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0002987903525125799 Training loss: 6.877485275268555
2025-12-09 11:54:28.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.000298778480318216 Training loss: 7.875543117523193
2025-12-09 11:54:28.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0002987665503858286 Training loss: 7.368495941162109
2025-12-09 11:54:28.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0002987545627200474 Training loss: 6.946967601776123
2025-12-09 11:54:29.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0002987425173255246 Training loss: 6.936091423034668
2025-12-09 11:54:29.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0002987304142069348 Training loss: 6.9634904861450195
2025-12-09 11:54:30.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0002987182533689749 Training loss: 6.959895133972168
2025-12-09 11:54:30.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0002987060348163644 Training loss: 7.93217658996582
2025-12-09 11:54:30.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.000298693758553845 Training loss: 6.433786869049072
2025-12-09 11:54:31.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.00029868142458618096 Training loss: 7.170147895812988
2025-12-09 11:54:31.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0002986690329181587 Training loss: 7.205742359161377
2025-12-09 11:54:32.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00029865658355458736 Training loss: 6.899666786193848
2025-12-09 11:54:32.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0002986440765002982 Training loss: 8.135766983032227
2025-12-09 11:54:32.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.000298631511760145 Training loss: 7.174765110015869
2025-12-09 11:54:33.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0002986188893390038 Training loss: 7.2752604484558105
2025-12-09 11:54:33.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0002986062092417733 Training loss: 6.988500118255615
2025-12-09 11:54:34.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.00029859347147337417 Training loss: 7.48091459274292
2025-12-09 11:54:34.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0002985806760387499 Training loss: 7.788593769073486
2025-12-09 11:54:34.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00029856782294286594 Training loss: 7.074017524719238
2025-12-09 11:54:35.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00029855491219071053 Training loss: 7.728113174438477
2025-12-09 11:54:35.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.000298541943787294 Training loss: 6.880221843719482
2025-12-09 11:54:35.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00029852891773764906 Training loss: 7.5837483406066895
2025-12-09 11:54:36.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00029851583404683096 Training loss: 7.6195526123046875
2025-12-09 11:54:36.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0002985026927199172 Training loss: 7.59470272064209
2025-12-09 11:54:37.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.00029848949376200766 Training loss: 7.275839805603027
2025-12-09 11:54:37.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0002984762371782246 Training loss: 7.094937801361084
2025-12-09 11:54:37.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00029846292297371264 Training loss: 6.799649715423584
2025-12-09 11:54:38.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0002984495511536388 Training loss: 7.507339954376221
2025-12-09 11:54:38.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0002984361217231923 Training loss: 7.298641204833984
2025-12-09 11:54:39.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.00029842263468758505 Training loss: 6.9161834716796875
2025-12-09 11:54:39.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0002984090900520509 Training loss: 7.264341354370117
2025-12-09 11:54:39.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00029839548782184636 Training loss: 7.118619918823242
2025-12-09 11:54:40.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.00029838182800225017 Training loss: 7.042575836181641
2025-12-09 11:54:40.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00029836811059856354 Training loss: 7.821866035461426
2025-12-09 11:54:40.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.00029835433561610974 Training loss: 7.3389811515808105
2025-12-09 11:54:41.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0002983405030602346 Training loss: 7.238269805908203
2025-12-09 11:54:41.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.00029832661293630644 Training loss: 7.000759601593018
2025-12-09 11:54:42.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.00029831266524971557 Training loss: 7.077417850494385
2025-12-09 11:54:42.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0002982986600058749 Training loss: 7.063013076782227
2025-12-09 11:54:42.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0002982845972102196 Training loss: 7.0888543128967285
2025-12-09 11:54:43.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0002982704768682071 Training loss: 6.7171454429626465
2025-12-09 11:54:43.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00029825629898531724 Training loss: 7.399361610412598
2025-12-09 11:54:44.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0002982420635670523 Training loss: 7.119822025299072
2025-12-09 11:54:44.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.00029822777061893653 Training loss: 7.1462626457214355
2025-12-09 11:54:44.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00029821342014651694 Training loss: 7.373182773590088
2025-12-09 11:54:45.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0002981990121553627 Training loss: 7.073023796081543
2025-12-09 11:54:45.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0002981845466510651 Training loss: 6.896833419799805
2025-12-09 11:54:46.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00029817002363923803 Training loss: 6.845884799957275
2025-12-09 11:54:46.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00029815544312551754 Training loss: 7.687742233276367
2025-12-09 11:54:46.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00029814080511556207 Training loss: 7.585714817047119
2025-12-09 11:54:47.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00029812610961505234 Training loss: 6.997318267822266
2025-12-09 11:54:47.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.00029811135662969143 Training loss: 7.311058521270752
2025-12-09 11:54:47.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.00029809654616520456 Training loss: 6.88217306137085
2025-12-09 11:54:48.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00029808167822733953 Training loss: 6.837698936462402
2025-12-09 11:54:48.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0002980667528218662 Training loss: 6.6077799797058105
2025-12-09 11:54:49.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002980517699545769 Training loss: 6.683467864990234
2025-12-09 11:54:49.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0002980367296312861 Training loss: 7.091337203979492
2025-12-09 11:54:49.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00029802163185783074 Training loss: 7.9117913246154785
2025-12-09 11:54:50.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.00029800647664006993 Training loss: 6.852082252502441
2025-12-09 11:54:50.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0002979912639838851 Training loss: 6.827653408050537
2025-12-09 11:54:51.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.00029797599389518 Training loss: 7.049091815948486
2025-12-09 11:54:51.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0002979606663798807 Training loss: 7.242882251739502
2025-12-09 11:54:51.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0002979452814439354 Training loss: 6.870440483093262
2025-12-09 11:54:52.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.00029792983909331485 Training loss: 6.8034348487854
2025-12-09 11:54:52.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0002979143393340117 Training loss: 7.15391731262207
2025-12-09 11:54:53.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.00029789878217204133 Training loss: 7.083178997039795
2025-12-09 11:54:53.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00029788316761344106 Training loss: 6.989114761352539
2025-12-09 11:54:53.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00029786749566427064 Training loss: 6.842757701873779
2025-12-09 11:54:54.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.000297851766330612 Training loss: 6.896291255950928
2025-12-09 11:54:54.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.00029783597961856946 Training loss: 7.316189765930176
2025-12-09 11:54:54.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00029782013553426937 Training loss: 7.263115406036377
2025-12-09 11:54:55.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.00029780423408386073 Training loss: 6.829526424407959
2025-12-09 11:54:55.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.00029778827527351443 Training loss: 7.199077606201172
2025-12-09 11:54:56.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0002977722591094238 Training loss: 7.304202079772949
2025-12-09 11:54:56.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.00029775618559780447 Training loss: 7.60340690612793
2025-12-09 11:54:56.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.00029774005474489417 Training loss: 6.671960353851318
2025-12-09 11:54:57.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.00029772386655695305 Training loss: 6.793115615844727
2025-12-09 11:54:57.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0002977076210402633 Training loss: 6.9647955894470215
2025-12-09 11:54:58.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.00029769131820112966 Training loss: 6.891860008239746
2025-12-09 11:54:58.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.00029767495804587885 Training loss: 6.86324405670166
2025-12-09 11:54:58.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.0002976585405808599 Training loss: 7.3610453605651855
2025-12-09 11:54:59.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0002976420658124441 Training loss: 7.125548362731934
2025-12-09 11:54:59.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0002976255337470251 Training loss: 6.97001314163208
2025-12-09 11:55:00.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.00029760894439101855 Training loss: 7.375765800476074
2025-12-09 11:55:00.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0002975922977508625 Training loss: 7.361415863037109
2025-12-09 11:55:00.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0002975755938330172 Training loss: 6.806918621063232
2025-12-09 11:55:01.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.00029755883264396513 Training loss: 7.1979265213012695
2025-12-09 11:55:01.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00029754201419021094 Training loss: 7.105180740356445
2025-12-09 11:55:01.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0002975251384782816 Training loss: 7.237445831298828
2025-12-09 11:55:02.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.00029750820551472615 Training loss: 6.6619391441345215
2025-12-09 11:55:02.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.00029749121530611597 Training loss: 7.684256553649902
2025-12-09 11:55:03.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0002974741678590447 Training loss: 6.646976470947266
2025-12-09 11:55:03.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.00029745706318012806 Training loss: 7.024657726287842
2025-12-09 11:55:03.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.00029743990127600406 Training loss: 7.148715496063232
2025-12-09 11:55:04.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0002974226821533329 Training loss: 7.347435474395752
2025-12-09 11:55:04.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.000297405405818797 Training loss: 7.2233476638793945
2025-12-09 11:55:05.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0002973880722791009 Training loss: 7.044113636016846
2025-12-09 11:55:05.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0002973706815409715 Training loss: 7.061619281768799
2025-12-09 11:55:05.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0002973532336111577 Training loss: 6.73193883895874
2025-12-09 11:55:06.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00029733572849643085 Training loss: 7.574886798858643
2025-12-09 11:55:06.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.00029731816620358424 Training loss: 7.151193618774414
2025-12-09 11:55:07.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0002973005467394334 Training loss: 7.718868255615234
2025-12-09 11:55:07.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.00029728287011081625 Training loss: 7.595010757446289
2025-12-09 11:55:07.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0002972651363245927 Training loss: 6.591740131378174
2025-12-09 11:55:08.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00029724734538764475 Training loss: 6.6961822509765625
2025-12-09 11:55:08.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0002972294973068768 Training loss: 6.9801154136657715
2025-12-09 11:55:08.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.00029721159208921546 Training loss: 7.065634250640869
2025-12-09 11:55:09.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.00029719362974160924 Training loss: 7.116903305053711
2025-12-09 11:55:09.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.000297175610271029 Training loss: 6.952546119689941
2025-12-09 11:55:10.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.00029715753368446786 Training loss: 7.134626865386963
2025-12-09 11:55:10.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00029713939998894087 Training loss: 7.083832263946533
2025-12-09 11:55:10.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0002971212091914854 Training loss: 7.420498371124268
2025-12-09 11:55:11.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0002971029612991609 Training loss: 7.248666286468506
2025-12-09 11:55:11.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0002970846563190491 Training loss: 6.987319469451904
2025-12-09 11:55:12.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00029706629425825374 Training loss: 7.043112754821777
2025-12-09 11:55:12.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.00029704787512390085 Training loss: 6.980787754058838
2025-12-09 11:55:12.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0002970293989231385 Training loss: 6.539548397064209
2025-12-09 11:55:13.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.0002970108656631369 Training loss: 7.198231220245361
2025-12-09 11:55:13.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0002969922753510885 Training loss: 7.159551620483398
2025-12-09 11:55:13.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00029697362799420776 Training loss: 7.487451076507568
2025-12-09 11:55:14.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0002969549235997315 Training loss: 7.052190780639648
2025-12-09 11:55:14.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0002969361621749184 Training loss: 7.762582302093506
2025-12-09 11:55:15.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.00029691734372704943 Training loss: 6.976710796356201
2025-12-09 11:55:15.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0002968984682634277 Training loss: 6.559445858001709
2025-12-09 11:55:15.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0002968795357913784 Training loss: 6.614408016204834
2025-12-09 11:55:16.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0002968605463182488 Training loss: 7.37146520614624
2025-12-09 11:55:16.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0002968414998514085 Training loss: 7.363891124725342
2025-12-09 11:55:17.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0002968223963982488 Training loss: 7.134531021118164
2025-12-09 11:55:17.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.00029680323596618355 Training loss: 6.9343342781066895
2025-12-09 11:55:17.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00029678401856264857 Training loss: 6.618861675262451
2025-12-09 11:55:18.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0002967647441951017 Training loss: 7.036452293395996
2025-12-09 11:55:18.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0002967454128710229 Training loss: 6.209503173828125
2025-12-09 11:55:19.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.00029672602459791434 Training loss: 6.746883869171143
2025-12-09 11:55:19.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0002967065793833002 Training loss: 7.075491428375244
2025-12-09 11:55:19.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0002966870772347269 Training loss: 6.591805458068848
2025-12-09 11:55:20.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0002966675181597627 Training loss: 7.0756402015686035
2025-12-09 11:55:20.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0002966479021659981 Training loss: 7.444648265838623
2025-12-09 11:55:20.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.00029662822926104576 Training loss: 6.824526309967041
2025-12-09 11:55:21.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0002966084994525403 Training loss: 7.126953601837158
2025-12-09 11:55:21.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.00029658871274813853 Training loss: 6.74571418762207
2025-12-09 11:55:22.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.00029656886915551924 Training loss: 6.55606746673584
2025-12-09 11:55:22.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0002965489686823833 Training loss: 6.85092830657959
2025-12-09 11:55:22.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.00029652901133645377 Training loss: 6.6263604164123535
2025-12-09 11:55:23.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0002965089971254757 Training loss: 6.890448570251465
2025-12-09 11:55:23.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0002964889260572162 Training loss: 6.878946781158447
2025-12-09 11:55:24.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0002964687981394644 Training loss: 7.194085121154785
2025-12-09 11:55:24.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.00029644861338003165 Training loss: 6.866166591644287
2025-12-09 11:55:24.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0002964283717867512 Training loss: 6.843255996704102
2025-12-09 11:55:25.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0002964080733674784 Training loss: 6.96978759765625
2025-12-09 11:55:25.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0002963877181300907 Training loss: 6.847822666168213
2025-12-09 11:55:26.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00029636730608248766 Training loss: 6.901224613189697
2025-12-09 11:55:26.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0002963468372325906 Training loss: 6.81647253036499
2025-12-09 11:55:26.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.00029632631158834326 Training loss: 6.949038982391357
2025-12-09 11:55:27.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.00029630572915771117 Training loss: 7.763039588928223
2025-12-09 11:55:27.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.000296285089948682 Training loss: 6.977027893066406
2025-12-09 11:55:27.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.00029626439396926533 Training loss: 6.598813533782959
2025-12-09 11:55:28.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.00029624364122749294 Training loss: 6.763906002044678
2025-12-09 11:55:28.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0002962228317314186 Training loss: 7.543667316436768
2025-12-09 11:55:29.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00029620196548911797 Training loss: 7.168282985687256
2025-12-09 11:55:29.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.0002961810425086889 Training loss: 7.107926368713379
2025-12-09 11:55:29.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00029616006279825126 Training loss: 7.28717041015625
2025-12-09 11:55:30.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0002961390263659467 Training loss: 7.271278381347656
2025-12-09 11:55:30.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0002961179332199391 Training loss: 6.741591930389404
2025-12-09 11:55:31.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00029609678336841444 Training loss: 6.648401737213135
2025-12-09 11:55:31.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.00029607557681958035 Training loss: 8.256195068359375
2025-12-09 11:55:31.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.00029605431358166684 Training loss: 6.810608863830566
2025-12-09 11:55:32.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.00029603299366292565 Training loss: 6.966558933258057
2025-12-09 11:55:32.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00029601161707163077 Training loss: 8.152031898498535
2025-12-09 11:55:33.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.00029599018381607785 Training loss: 6.796609878540039
2025-12-09 11:55:33.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0002959686939045848 Training loss: 7.142396450042725
2025-12-09 11:55:33.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.00029594714734549146 Training loss: 7.198575973510742
2025-12-09 11:55:34.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0002959255441471597 Training loss: 7.215454578399658
2025-12-09 11:55:34.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0002959038843179731 Training loss: 6.591285228729248
2025-12-09 11:55:34.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0002958821678663376 Training loss: 7.332264423370361
2025-12-09 11:55:35.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.00029586039480068087 Training loss: 7.066429615020752
2025-12-09 11:55:35.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0002958385651294525 Training loss: 6.906401634216309
2025-12-09 11:55:36.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00029581667886112434 Training loss: 6.518440246582031
2025-12-09 11:55:36.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.00029579473600418993 Training loss: 6.7340264320373535
2025-12-09 11:55:36.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0002957727365671649 Training loss: 6.856858730316162
2025-12-09 11:55:37.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0002957506805585867 Training loss: 6.723696231842041
2025-12-09 11:55:37.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00029572856798701504 Training loss: 6.975331783294678
2025-12-09 11:55:38.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0002957063988610312 Training loss: 7.777789115905762
2025-12-09 11:55:38.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0002956841731892386 Training loss: 7.276788711547852
2025-12-09 11:55:38.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0002956618909802627 Training loss: 7.2566633224487305
2025-12-09 11:55:39.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.00029563955224275065 Training loss: 7.044919013977051
2025-12-09 11:55:39.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.00029561715698537183 Training loss: 7.115651607513428
2025-12-09 11:55:40.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0002955947052168172 Training loss: 7.207121849060059
2025-12-09 11:55:40.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0002955721969458001 Training loss: 7.002445220947266
2025-12-09 11:55:40.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0002955496321810553 Training loss: 6.63572359085083
2025-12-09 11:55:41.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.00029552701093133994 Training loss: 7.31461763381958
2025-12-09 11:55:41.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.00029550433320543284 Training loss: 7.107491970062256
2025-12-09 11:55:41.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0002954815990121347 Training loss: 6.898401260375977
2025-12-09 11:55:42.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00029545880836026833 Training loss: 6.694540500640869
2025-12-09 11:55:42.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0002954359612586782 Training loss: 6.934370040893555
2025-12-09 11:55:43.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00029541305771623095 Training loss: 7.917392253875732
2025-12-09 11:55:43.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.00029539009774181494 Training loss: 7.324766635894775
2025-12-09 11:55:43.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.00029536708134434054 Training loss: 7.6818413734436035
2025-12-09 11:55:44.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.00029534400853273985 Training loss: 6.63301944732666
2025-12-09 11:55:44.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0002953208793159671 Training loss: 7.0661821365356445
2025-12-09 11:55:45.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.00029529769370299823 Training loss: 6.7913079261779785
2025-12-09 11:55:45.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0002952744517028311 Training loss: 6.790693759918213
2025-12-09 11:55:45.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00029525115332448555 Training loss: 6.852602481842041
2025-12-09 11:55:46.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0002952277985770032 Training loss: 6.788434982299805
2025-12-09 11:55:46.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0002952043874694475 Training loss: 7.528634548187256
2025-12-09 11:55:47.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00029518092001090397 Training loss: 6.592212677001953
2025-12-09 11:55:47.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00029515739621047973 Training loss: 7.170669078826904
2025-12-09 11:55:47.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.000295133816077304 Training loss: 7.242835521697998
2025-12-09 11:55:48.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.0002951101796205278 Training loss: 6.829017162322998
2025-12-09 11:55:48.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0002950864868493239 Training loss: 6.952441692352295
2025-12-09 11:55:48.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.00029506273777288696 Training loss: 6.785737037658691
2025-12-09 11:55:49.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0002950389324004337 Training loss: 6.67531156539917
2025-12-09 11:55:49.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.00029501507074120237 Training loss: 6.839783668518066
2025-12-09 11:55:50.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.00029499115280445326 Training loss: 7.002652645111084
2025-12-09 11:55:50.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0002949671785994685 Training loss: 7.393033504486084
2025-12-09 11:55:50.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.00029494314813555193 Training loss: 7.090306758880615
2025-12-09 11:55:51.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.00029491906142202934 Training loss: 6.92723274230957
2025-12-09 11:55:51.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.00029489491846824837 Training loss: 6.841902256011963
2025-12-09 11:55:52.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0002948707192835783 Training loss: 6.773946762084961
2025-12-09 11:55:52.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0002948464638774105 Training loss: 7.188416957855225
2025-12-09 11:55:52.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.00029482215225915795 Training loss: 6.714441776275635
2025-12-09 11:55:53.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0002947977844382555 Training loss: 6.711036682128906
2025-12-09 11:55:53.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0002947733604241599 Training loss: 6.9096856117248535
2025-12-09 11:55:54.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.00029474888022634955 Training loss: 6.783294677734375
2025-12-09 11:55:54.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.00029472434385432474 Training loss: 6.6444411277771
2025-12-09 11:55:54.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0002946997513176076 Training loss: 6.908658027648926
2025-12-09 11:55:55.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.000294675102625742 Training loss: 7.159716606140137
2025-12-09 11:55:55.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.00029465039778829366 Training loss: 6.684567928314209
2025-12-09 11:55:55.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0002946256368148499 Training loss: 6.767345905303955
2025-12-09 11:55:56.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.00029460081971502015 Training loss: 6.310988903045654
2025-12-09 11:55:56.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.00029457594649843534 Training loss: 7.036595344543457
2025-12-09 11:55:57.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0002945510171747483 Training loss: 6.94980525970459
2025-12-09 11:55:57.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0002945260317536336 Training loss: 6.607010364532471
2025-12-09 11:55:57.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0002945009902447876 Training loss: 6.758669376373291
2025-12-09 11:55:58.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.00029447589265792847 Training loss: 7.242245197296143
2025-12-09 11:55:58.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.00029445073900279605 Training loss: 6.916139602661133
2025-12-09 11:55:59.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.000294425529289152 Training loss: 7.554798603057861
2025-12-09 11:55:59.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.00029440026352677966 Training loss: 6.71931266784668
2025-12-09 11:55:59.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.00029437494172548424 Training loss: 6.304664611816406
2025-12-09 11:56:00.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.00029434956389509263 Training loss: 6.476449489593506
2025-12-09 11:56:00.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0002943241300454534 Training loss: 6.967843532562256
2025-12-09 11:56:01.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.0002942986401864371 Training loss: 6.211884498596191
2025-12-09 11:56:01.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0002942730943279357 Training loss: 7.030936241149902
2025-12-09 11:56:01.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.000294247492479863 Training loss: 6.6605143547058105
2025-12-09 11:56:02.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.00029422183465215474 Training loss: 7.864816188812256
2025-12-09 11:56:02.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00029419612085476813 Training loss: 6.859327793121338
2025-12-09 11:56:02.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0002941703510976822 Training loss: 6.943105220794678
2025-12-09 11:56:03.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.00029414452539089776 Training loss: 6.740509986877441
2025-12-09 11:56:03.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00029411864374443716 Training loss: 7.004462718963623
2025-12-09 11:56:04.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0002940927061683446 Training loss: 6.916112899780273
2025-12-09 11:56:04.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.0002940667126726859 Training loss: 7.255512237548828
2025-12-09 11:56:04.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0002940406632675487 Training loss: 6.696594715118408
2025-12-09 11:56:05.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0002940145579630423 Training loss: 7.277486801147461
2025-12-09 11:56:05.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.00029398839676929756 Training loss: 7.448177337646484
2025-12-09 11:56:06.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.00029396217969646717 Training loss: 6.908821105957031
2025-12-09 11:56:06.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00029393590675472545 Training loss: 7.343203067779541
2025-12-09 11:56:06.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00029390957795426845 Training loss: 6.864994049072266
2025-12-09 11:56:07.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0002938831933053138 Training loss: 7.097275257110596
2025-12-09 11:56:07.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.000293856752818101 Training loss: 6.9997453689575195
2025-12-09 11:56:08.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.00029383025650289095 Training loss: 6.802065372467041
2025-12-09 11:56:08.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0002938037043699664 Training loss: 6.697347640991211
2025-12-09 11:56:08.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0002937770964296317 Training loss: 7.066537380218506
2025-12-09 11:56:09.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0002937504326922129 Training loss: 7.726354122161865
2025-12-09 11:56:09.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.00029372371316805767 Training loss: 7.348162651062012
2025-12-09 11:56:09.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.00029369693786753534 Training loss: 7.218082427978516
2025-12-09 11:56:10.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0002936701068010368 Training loss: 7.056727409362793
2025-12-09 11:56:10.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0002936432199789748 Training loss: 6.955348014831543
2025-12-09 11:56:11.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.00029361627741178356 Training loss: 7.224085807800293
2025-12-09 11:56:11.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.00029358927910991885 Training loss: 6.798649787902832
2025-12-09 11:56:11.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.00029356222508385827 Training loss: 7.757386207580566
2025-12-09 11:56:12.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.000293535115344101 Training loss: 6.844369411468506
2025-12-09 11:56:12.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0002935079499011677 Training loss: 6.6421308517456055
2025-12-09 11:56:13.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0002934807287656008 Training loss: 6.872237682342529
2025-12-09 11:56:13.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.00029345345194796435 Training loss: 6.136438369750977
2025-12-09 11:56:13.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0002934261194588438 Training loss: 6.493834495544434
2025-12-09 11:56:14.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.00029339873130884654 Training loss: 6.425230026245117
2025-12-09 11:56:14.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.00029337128750860124 Training loss: 6.605944633483887
2025-12-09 11:56:14.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.00029334378806875836 Training loss: 6.592581272125244
2025-12-09 11:56:15.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.00029331623299998986 Training loss: 6.884833335876465
2025-12-09 11:56:15.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0002932886223129894 Training loss: 8.05169677734375
2025-12-09 11:56:16.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.000293260956018472 Training loss: 6.961767196655273
2025-12-09 11:56:16.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0002932332341271746 Training loss: 6.772994518280029
2025-12-09 11:56:16.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00029320545664985535 Training loss: 6.947725772857666
2025-12-09 11:56:17.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.00029317762359729423 Training loss: 6.601851463317871
2025-12-09 11:56:17.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.00029314973498029275 Training loss: 6.551305770874023
2025-12-09 11:56:18.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0002931217908096739 Training loss: 6.838006496429443
2025-12-09 11:56:18.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0002930937910962822 Training loss: 6.8493828773498535
2025-12-09 11:56:18.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.00029306573585098384 Training loss: 6.894174575805664
2025-12-09 11:56:19.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.00029303762508466654 Training loss: 7.50305700302124
2025-12-09 11:56:19.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00029300945880823956 Training loss: 6.591477870941162
2025-12-09 11:56:20.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0002929812370326336 Training loss: 7.590169906616211
2025-12-09 11:56:20.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.000292952959768801 Training loss: 6.9849629402160645
2025-12-09 11:56:20.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0002929246270277157 Training loss: 6.729162693023682
2025-12-09 11:56:21.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.000292896238820373 Training loss: 6.5081706047058105
2025-12-09 11:56:21.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0002928677951577898 Training loss: 7.07421350479126
2025-12-09 11:56:21.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.00029283929605100455 Training loss: 7.203721046447754
2025-12-09 11:56:22.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0002928107415110772 Training loss: 6.953268527984619
2025-12-09 11:56:22.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0002927821315490893 Training loss: 6.819796085357666
2025-12-09 11:56:23.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0002927534661761436 Training loss: 7.048008918762207
2025-12-09 11:56:23.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.00029272474540336475 Training loss: 7.061766147613525
2025-12-09 11:56:23.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.00029269596924189875 Training loss: 6.967432022094727
2025-12-09 11:56:24.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0002926671377029129 Training loss: 6.731805801391602
2025-12-09 11:56:24.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.00029263825079759635 Training loss: 6.5012640953063965
2025-12-09 11:56:25.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.00029260930853715935 Training loss: 6.740126132965088
2025-12-09 11:56:25.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0002925803109328339 Training loss: 6.172352313995361
2025-12-09 11:56:25.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0002925512579958735 Training loss: 6.933715343475342
2025-12-09 11:56:26.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0002925221497375529 Training loss: 6.889273166656494
2025-12-09 11:56:26.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.00029249298616916856 Training loss: 6.673946857452393
2025-12-09 11:56:27.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.00029246376730203817 Training loss: 6.749152183532715
2025-12-09 11:56:27.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0002924344931475011 Training loss: 6.700533866882324
2025-12-09 11:56:27.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.000292405163716918 Training loss: 6.410143852233887
2025-12-09 11:56:28.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0002923757790216711 Training loss: 6.712266445159912
2025-12-09 11:56:28.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.000292346339073164 Training loss: 6.796163082122803
2025-12-09 11:56:28.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.00029231684388282184 Training loss: 7.32967472076416
2025-12-09 11:56:29.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.000292287293462091 Training loss: 6.622994899749756
2025-12-09 11:56:29.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0002922576878224395 Training loss: 6.438577651977539
2025-12-09 11:56:30.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.00029222802697535674 Training loss: 7.111904621124268
2025-12-09 11:56:30.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0002921983109323535 Training loss: 7.343465805053711
2025-12-09 11:56:30.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0002921685397049619 Training loss: 6.123937129974365
2025-12-09 11:56:31.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0002921387133047357 Training loss: 6.8111395835876465
2025-12-09 11:56:31.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0002921088317432499 Training loss: 7.123471736907959
2025-12-09 11:56:32.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0002920788950321009 Training loss: 7.217993259429932
2025-12-09 11:56:32.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.00029204890318290666 Training loss: 6.682277202606201
2025-12-09 11:56:32.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0002920188562073063 Training loss: 7.332352638244629
2025-12-09 11:56:33.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0002919887541169605 Training loss: 6.393797397613525
2025-12-09 11:56:33.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0002919585969235514 Training loss: 7.331409454345703
2025-12-09 11:56:34.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.00029192838463878236 Training loss: 7.065472602844238
2025-12-09 11:56:34.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0002918981172743781 Training loss: 7.198538303375244
2025-12-09 11:56:34.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.00029186779484208485 Training loss: 6.794921875
2025-12-09 11:56:35.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0002918374173536702 Training loss: 7.326994895935059
2025-12-09 11:56:35.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.000291806984820923 Training loss: 6.60535192489624
2025-12-09 11:56:35.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.00029177649725565353 Training loss: 6.61992883682251
2025-12-09 11:56:36.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.00029174595466969344 Training loss: 6.6432785987854
2025-12-09 11:56:36.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.00029171535707489565 Training loss: 6.95419454574585
2025-12-09 11:56:37.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0002916847044831346 Training loss: 7.160248279571533
2025-12-09 11:56:37.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0002916539969063059 Training loss: 6.71013879776001
2025-12-09 11:56:37.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0002916232343563265 Training loss: 6.905999660491943
2025-12-09 11:56:38.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0002915924168451349 Training loss: 6.98860502243042
2025-12-09 11:56:38.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0002915615443846906 Training loss: 6.276131629943848
2025-12-09 11:56:39.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0002915306169869747 Training loss: 7.065808296203613
2025-12-09 11:56:39.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0002914996346639895 Training loss: 6.955865859985352
2025-12-09 11:56:39.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.00029146859742775865 Training loss: 6.930798530578613
2025-12-09 11:56:40.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00029143750529032707 Training loss: 6.4257049560546875
2025-12-09 11:56:40.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0002914063582637611 Training loss: 7.156493663787842
2025-12-09 11:56:41.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0002913751563601481 Training loss: 7.296015739440918
2025-12-09 11:56:41.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0002913438995915971 Training loss: 6.752323627471924
2025-12-09 11:56:41.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0002913125879702381 Training loss: 6.642221927642822
2025-12-09 11:56:42.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.00029128122150822263 Training loss: 6.650130271911621
2025-12-09 11:56:42.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0002912498002177234 Training loss: 6.6374101638793945
2025-12-09 11:56:42.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0002912183241109344 Training loss: 6.9699387550354
2025-12-09 11:56:43.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.00029118679320007087 Training loss: 6.647225379943848
2025-12-09 11:56:43.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0002911552074973693 Training loss: 7.062304973602295
2025-12-09 11:56:44.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0002911235670150875 Training loss: 6.289029121398926
2025-12-09 11:56:44.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0002910918717655046 Training loss: 6.671955585479736
2025-12-09 11:56:44.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00029106012176092084 Training loss: 7.213873863220215
2025-12-09 11:56:45.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0002910283170136578 Training loss: 6.799468994140625
2025-12-09 11:56:45.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00029099645753605827 Training loss: 6.955523490905762
2025-12-09 11:56:46.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.00029096454334048627 Training loss: 7.159491062164307
2025-12-09 11:56:46.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0002909325744393271 Training loss: 6.622128009796143
2025-12-09 11:56:46.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0002909005508449873 Training loss: 7.331874847412109
2025-12-09 11:56:47.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0002908684725698946 Training loss: 6.441458702087402
2025-12-09 11:56:47.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0002908363396264978 Training loss: 7.276391506195068
2025-12-09 11:56:48.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.00029080415202726727 Training loss: 6.693237781524658
2025-12-09 11:56:48.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0002907719097846943 Training loss: 6.720526695251465
2025-12-09 11:56:48.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0002907396129112915 Training loss: 6.830204486846924
2025-12-09 11:56:49.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.00029070726141959265 Training loss: 6.4827961921691895
2025-12-09 11:56:49.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.00029067485532215267 Training loss: 6.967102527618408
2025-12-09 11:56:49.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0002906423946315478 Training loss: 6.832348823547363
2025-12-09 11:56:50.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.00029060987936037536 Training loss: 6.824953079223633
2025-12-09 11:56:50.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.00029057730952125393 Training loss: 6.756855487823486
2025-12-09 11:56:51.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0002905446851268233 Training loss: 6.802938461303711
2025-12-09 11:56:51.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0002905120061897441 Training loss: 6.33267879486084
2025-12-09 11:56:51.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0002904792727226987 Training loss: 6.657593727111816
2025-12-09 11:56:52.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.00029044648473839014 Training loss: 6.6889848709106445
2025-12-09 11:56:52.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0002904136422495429 Training loss: 6.630516052246094
2025-12-09 11:56:53.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0002903807452689024 Training loss: 6.390352249145508
2025-12-09 11:56:53.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00029034779380923535 Training loss: 7.108647346496582
2025-12-09 11:56:53.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.00029031478788332955 Training loss: 6.749696254730225
2025-12-09 11:56:54.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0002902817275039941 Training loss: 6.6192426681518555
2025-12-09 11:56:54.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.00029024861268405887 Training loss: 6.893702507019043
2025-12-09 11:56:55.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.00029021544343637526 Training loss: 6.53472900390625
2025-12-09 11:56:55.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.00029018221977381546 Training loss: 6.485311985015869
2025-12-09 11:56:55.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.00029014894170927306 Training loss: 6.856256008148193
2025-12-09 11:56:56.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0002901156092556625 Training loss: 7.32341194152832
2025-12-09 11:56:56.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0002900822224259195 Training loss: 6.595483779907227
2025-12-09 11:56:56.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0002900487812330009 Training loss: 6.684064865112305
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 3.29 GiB is free. Including non-PyTorch memory, this process has 90.93 GiB memory in use. Of the allocated memory 89.96 GiB is allocated by PyTorch, and 216.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   1%|          | 53/10000 [00:00<00:18, 525.41it/s]Tokenizing texts:   1%|▏         | 136/10000 [00:00<00:15, 634.71it/s]Tokenizing texts:   2%|▏         | 205/10000 [00:00<00:15, 651.57it/s]Tokenizing texts:   3%|▎         | 273/10000 [00:00<00:14, 661.79it/s]Tokenizing texts:   3%|▎         | 340/10000 [00:00<00:15, 626.58it/s]Tokenizing texts:   4%|▍         | 403/10000 [00:00<00:15, 616.90it/s]Tokenizing texts:   5%|▍         | 465/10000 [00:00<00:16, 585.30it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 585.53it/s]Tokenizing texts:   6%|▌         | 599/10000 [00:00<00:15, 614.77it/s]Tokenizing texts:   7%|▋         | 661/10000 [00:01<00:15, 600.47it/s]Tokenizing texts:   7%|▋         | 722/10000 [00:01<00:16, 578.94it/s]Tokenizing texts:   8%|▊         | 781/10000 [00:01<00:15, 577.24it/s]Tokenizing texts:   8%|▊         | 843/10000 [00:01<00:15, 586.54it/s]Tokenizing texts:   9%|▉         | 911/10000 [00:01<00:14, 610.78it/s]Tokenizing texts:  10%|▉         | 993/10000 [00:01<00:13, 671.22it/s]Tokenizing texts:  11%|█         | 1061/10000 [00:01<00:13, 649.27it/s]Tokenizing texts:  11%|█▏        | 1127/10000 [00:01<00:13, 638.68it/s]Tokenizing texts:  12%|█▏        | 1192/10000 [00:01<00:14, 617.29it/s]Tokenizing texts:  13%|█▎        | 1254/10000 [00:02<00:14, 583.98it/s]Tokenizing texts:  13%|█▎        | 1322/10000 [00:02<00:14, 609.12it/s]Tokenizing texts:  14%|█▍        | 1384/10000 [00:02<00:14, 609.52it/s]Tokenizing texts:  15%|█▍        | 1453/10000 [00:02<00:13, 632.35it/s]Tokenizing texts:  15%|█▌        | 1528/10000 [00:02<00:12, 658.86it/s]Tokenizing texts:  16%|█▌        | 1603/10000 [00:02<00:12, 673.03it/s]Tokenizing texts:  17%|█▋        | 1671/10000 [00:02<00:13, 618.37it/s]Tokenizing texts:  17%|█▋        | 1743/10000 [00:02<00:12, 646.21it/s]Tokenizing texts:  18%|█▊        | 1810/10000 [00:02<00:12, 650.50it/s]Tokenizing texts:  19%|█▉        | 1876/10000 [00:03<00:13, 599.89it/s]Tokenizing texts:  19%|█▉        | 1941/10000 [00:03<00:13, 612.55it/s]Tokenizing texts:  20%|██        | 2004/10000 [00:03<00:13, 584.53it/s]Tokenizing texts:  21%|██        | 2072/10000 [00:03<00:13, 603.53it/s]Tokenizing texts:  21%|██▏       | 2134/10000 [00:03<00:13, 566.15it/s]Tokenizing texts:  22%|██▏       | 2193/10000 [00:03<00:13, 572.35it/s]Tokenizing texts:  23%|██▎       | 2272/10000 [00:03<00:12, 631.08it/s]Tokenizing texts:  23%|██▎       | 2336/10000 [00:03<00:13, 586.28it/s]Tokenizing texts:  24%|██▍       | 2419/10000 [00:03<00:11, 651.35it/s]Tokenizing texts:  25%|██▍       | 2486/10000 [00:04<00:11, 632.94it/s]Tokenizing texts:  26%|██▌       | 2553/10000 [00:04<00:11, 642.52it/s]Tokenizing texts:  26%|██▌       | 2619/10000 [00:04<00:12, 577.68it/s]Tokenizing texts:  27%|██▋       | 2679/10000 [00:04<00:12, 579.97it/s]Tokenizing texts:  27%|██▋       | 2739/10000 [00:04<00:12, 580.30it/s]Tokenizing texts:  28%|██▊       | 2798/10000 [00:04<00:13, 543.72it/s]Tokenizing texts:  29%|██▊       | 2856/10000 [00:04<00:12, 553.49it/s]Tokenizing texts:  29%|██▉       | 2934/10000 [00:04<00:11, 616.08it/s]Tokenizing texts:  30%|██▉       | 2997/10000 [00:04<00:11, 611.11it/s]Tokenizing texts:  31%|███       | 3059/10000 [00:05<00:11, 608.67it/s]Tokenizing texts:  31%|███▏      | 3128/10000 [00:05<00:10, 630.46it/s]Tokenizing texts:  32%|███▏      | 3195/10000 [00:05<00:10, 639.94it/s]Tokenizing texts:  33%|███▎      | 3260/10000 [00:05<00:10, 634.16it/s]Tokenizing texts:  33%|███▎      | 3331/10000 [00:05<00:10, 656.16it/s]Tokenizing texts:  34%|███▍      | 3397/10000 [00:05<00:10, 648.90it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:05<00:09, 702.81it/s]Tokenizing texts:  36%|███▌      | 3558/10000 [00:05<00:09, 649.62it/s]Tokenizing texts:  36%|███▌      | 3624/10000 [00:05<00:09, 650.19it/s]Tokenizing texts:  37%|███▋      | 3690/10000 [00:05<00:09, 644.11it/s]Tokenizing texts:  38%|███▊      | 3756/10000 [00:06<00:10, 617.54it/s]Tokenizing texts:  38%|███▊      | 3819/10000 [00:06<00:10, 599.61it/s]Tokenizing texts:  39%|███▉      | 3887/10000 [00:06<00:09, 620.88it/s]Tokenizing texts:  40%|███▉      | 3950/10000 [00:06<00:10, 601.27it/s]Tokenizing texts:  40%|████      | 4011/10000 [00:06<00:10, 596.66it/s]Tokenizing texts:  41%|████      | 4089/10000 [00:06<00:09, 640.58it/s]Tokenizing texts:  42%|████▏     | 4160/10000 [00:06<00:08, 660.06it/s]Tokenizing texts:  42%|████▏     | 4228/10000 [00:06<00:08, 665.54it/s]Tokenizing texts:  43%|████▎     | 4295/10000 [00:06<00:08, 634.31it/s]Tokenizing texts:  44%|████▎     | 4373/10000 [00:07<00:08, 667.51it/s]Tokenizing texts:  44%|████▍     | 4445/10000 [00:07<00:08, 673.44it/s]Tokenizing texts:  45%|████▌     | 4513/10000 [00:07<00:08, 658.45it/s]Tokenizing texts:  46%|████▌     | 4580/10000 [00:07<00:08, 658.34it/s]Tokenizing texts:  46%|████▋     | 4649/10000 [00:07<00:08, 666.91it/s]Tokenizing texts:  47%|████▋     | 4733/10000 [00:07<00:07, 713.64it/s]Tokenizing texts:  48%|████▊     | 4805/10000 [00:07<00:07, 661.85it/s]Tokenizing texts:  49%|████▉     | 4877/10000 [00:07<00:07, 677.57it/s]Tokenizing texts:  50%|████▉     | 4950/10000 [00:07<00:07, 691.91it/s]Tokenizing texts:  50%|█████     | 5028/10000 [00:07<00:06, 716.73it/s]Tokenizing texts:  51%|█████     | 5101/10000 [00:08<00:07, 656.92it/s]Tokenizing texts:  52%|█████▏    | 5168/10000 [00:08<00:07, 639.68it/s]Tokenizing texts:  52%|█████▏    | 5233/10000 [00:08<00:07, 625.64it/s]Tokenizing texts:  53%|█████▎    | 5304/10000 [00:08<00:07, 646.76it/s]Tokenizing texts:  54%|█████▍    | 5388/10000 [00:08<00:06, 700.34it/s]Tokenizing texts:  55%|█████▍    | 5459/10000 [00:08<00:06, 688.47it/s]Tokenizing texts:  55%|█████▌    | 5529/10000 [00:08<00:07, 633.10it/s]Tokenizing texts:  56%|█████▌    | 5596/10000 [00:08<00:06, 641.59it/s]Tokenizing texts:  57%|█████▋    | 5684/10000 [00:08<00:06, 704.14it/s]Tokenizing texts:  58%|█████▊    | 5756/10000 [00:09<00:06, 670.10it/s]Tokenizing texts:  58%|█████▊    | 5824/10000 [00:09<00:06, 605.48it/s]Tokenizing texts:  59%|█████▉    | 5894/10000 [00:09<00:06, 628.99it/s]Tokenizing texts:  60%|█████▉    | 5959/10000 [00:09<00:06, 626.57it/s]Tokenizing texts:  60%|██████    | 6023/10000 [00:09<00:06, 619.32it/s]Tokenizing texts:  61%|██████    | 6094/10000 [00:09<00:06, 639.37it/s]Tokenizing texts:  62%|██████▏   | 6169/10000 [00:09<00:05, 670.39it/s]Tokenizing texts:  62%|██████▏   | 6237/10000 [00:09<00:05, 652.93it/s]Tokenizing texts:  63%|██████▎   | 6303/10000 [00:09<00:05, 638.18it/s]Tokenizing texts:  64%|██████▎   | 6368/10000 [00:10<00:05, 620.59it/s]Tokenizing texts:  64%|██████▍   | 6434/10000 [00:10<00:05, 631.52it/s]Tokenizing texts:  65%|██████▍   | 6498/10000 [00:10<00:05, 632.51it/s]Tokenizing texts:  66%|██████▌   | 6564/10000 [00:10<00:05, 639.22it/s]Tokenizing texts:  66%|██████▋   | 6630/10000 [00:10<00:05, 645.17it/s]Tokenizing texts:  67%|██████▋   | 6695/10000 [00:10<00:05, 620.45it/s]Tokenizing texts:  68%|██████▊   | 6772/10000 [00:10<00:04, 663.22it/s]Tokenizing texts:  69%|██████▊   | 6852/10000 [00:10<00:04, 699.71it/s]Tokenizing texts:  69%|██████▉   | 6923/10000 [00:10<00:04, 624.46it/s]Tokenizing texts:  70%|██████▉   | 6989/10000 [00:11<00:04, 631.51it/s]Tokenizing texts:  71%|███████   | 7054/10000 [00:11<00:04, 615.14it/s]Tokenizing texts:  71%|███████   | 7117/10000 [00:11<00:04, 580.05it/s]Tokenizing texts:  72%|███████▏  | 7186/10000 [00:11<00:04, 608.89it/s]Tokenizing texts:  72%|███████▏  | 7248/10000 [00:11<00:04, 599.18it/s]Tokenizing texts:  73%|███████▎  | 7336/10000 [00:11<00:04, 664.47it/s]Tokenizing texts:  74%|███████▍  | 7403/10000 [00:11<00:03, 651.86it/s]Tokenizing texts:  75%|███████▍  | 7469/10000 [00:11<00:04, 593.64it/s]Tokenizing texts:  75%|███████▌  | 7545/10000 [00:11<00:03, 635.70it/s]Tokenizing texts:  76%|███████▌  | 7618/10000 [00:12<00:03, 656.71it/s]Tokenizing texts:  77%|███████▋  | 7696/10000 [00:12<00:03, 684.51it/s]Tokenizing texts:  78%|███████▊  | 7766/10000 [00:12<00:03, 638.43it/s]Tokenizing texts:  78%|███████▊  | 7831/10000 [00:12<00:03, 630.86it/s]Tokenizing texts:  79%|███████▉  | 7895/10000 [00:12<00:03, 623.52it/s]Tokenizing texts:  80%|███████▉  | 7975/10000 [00:12<00:03, 669.80it/s]Tokenizing texts:  80%|████████  | 8043/10000 [00:12<00:03, 643.85it/s]Tokenizing texts:  81%|████████  | 8108/10000 [00:12<00:03, 592.02it/s]Tokenizing texts:  82%|████████▏ | 8171/10000 [00:12<00:03, 599.12it/s]Tokenizing texts:  82%|████████▏ | 8232/10000 [00:13<00:03, 586.47it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:13<00:02, 613.79it/s]Tokenizing texts:  84%|████████▎ | 8363/10000 [00:13<00:02, 613.57it/s]Tokenizing texts:  84%|████████▍ | 8425/10000 [00:13<00:02, 596.49it/s]Tokenizing texts:  85%|████████▌ | 8503/10000 [00:13<00:02, 647.44it/s]Tokenizing texts:  86%|████████▌ | 8569/10000 [00:13<00:02, 634.98it/s]Tokenizing texts:  86%|████████▋ | 8647/10000 [00:13<00:01, 676.54it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:13<00:01, 679.87it/s]Tokenizing texts:  88%|████████▊ | 8794/10000 [00:13<00:01, 672.69it/s]Tokenizing texts:  89%|████████▊ | 8862/10000 [00:14<00:01, 610.91it/s]Tokenizing texts:  89%|████████▉ | 8945/10000 [00:14<00:01, 669.36it/s]Tokenizing texts:  90%|█████████ | 9014/10000 [00:14<00:01, 635.78it/s]Tokenizing texts:  91%|█████████ | 9080/10000 [00:14<00:01, 636.08it/s]Tokenizing texts:  92%|█████████▏| 9152/10000 [00:14<00:01, 659.18it/s]Tokenizing texts:  92%|█████████▏| 9225/10000 [00:14<00:01, 664.44it/s]Tokenizing texts:  93%|█████████▎| 9292/10000 [00:14<00:01, 648.93it/s]Tokenizing texts:  94%|█████████▎| 9358/10000 [00:14<00:01, 598.94it/s]Tokenizing texts:  94%|█████████▍| 9422/10000 [00:14<00:00, 606.82it/s]Tokenizing texts:  95%|█████████▍| 9484/10000 [00:15<00:00, 558.48it/s]Tokenizing texts:  96%|█████████▌| 9557/10000 [00:15<00:00, 603.07it/s]Tokenizing texts:  96%|█████████▌| 9619/10000 [00:15<00:00, 593.21it/s]Tokenizing texts:  97%|█████████▋| 9682/10000 [00:15<00:00, 602.12it/s]Tokenizing texts:  97%|█████████▋| 9743/10000 [00:15<00:00, 588.81it/s]Tokenizing texts:  98%|█████████▊| 9803/10000 [00:15<00:00, 588.19it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:15<00:00, 613.17it/s]Tokenizing texts:  99%|█████████▉| 9948/10000 [00:15<00:00, 656.35it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:15<00:00, 631.26it/s]
2025-12-09 11:57:45.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 12.163034439086914
2025-12-09 11:57:45.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 12.192876815795898
2025-12-09 11:57:46.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 12.115283012390137
2025-12-09 11:57:46.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 12.006651878356934
2025-12-09 11:57:46.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 11.896477699279785
2025-12-09 11:57:47.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 11.848509788513184
2025-12-09 11:57:47.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 11.625916481018066
2025-12-09 11:57:48.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 10.940481185913086
2025-12-09 11:57:48.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 10.654305458068848
2025-12-09 11:57:48.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 10.120201110839844
2025-12-09 11:57:49.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 10.105287551879883
2025-12-09 11:57:49.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 10.0431489944458
2025-12-09 11:57:50.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 9.925274848937988
2025-12-09 11:57:50.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 9.616560935974121
2025-12-09 11:57:50.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 9.630809783935547
2025-12-09 11:57:51.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 9.524812698364258
2025-12-09 11:57:51.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 9.505146980285645
2025-12-09 11:57:51.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 9.479969024658203
2025-12-09 11:57:52.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 9.4948091506958
2025-12-09 11:57:52.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 9.019283294677734
2025-12-09 11:57:53.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 8.642292976379395
2025-12-09 11:57:53.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 8.313167572021484
2025-12-09 11:57:53.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 8.410198211669922
2025-12-09 11:57:54.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 9.24057674407959
2025-12-09 11:57:54.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 8.439962387084961
2025-12-09 11:57:55.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 8.341361045837402
2025-12-09 11:57:55.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 8.373710632324219
2025-12-09 11:57:55.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 8.285319328308105
2025-12-09 11:57:56.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 8.289020538330078
2025-12-09 11:57:56.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 8.648266792297363
2025-12-09 11:57:56.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 8.23114013671875
2025-12-09 11:57:57.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 8.380739212036133
2025-12-09 11:57:57.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 8.346726417541504
2025-12-09 11:57:58.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 8.669692039489746
2025-12-09 11:57:58.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 8.451268196105957
2025-12-09 11:57:58.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 8.5569429397583
2025-12-09 11:57:59.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 9.469520568847656
2025-12-09 11:57:59.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 8.565106391906738
2025-12-09 11:58:00.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 9.349461555480957
2025-12-09 11:58:00.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 9.467888832092285
2025-12-09 11:58:00.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 8.263838768005371
2025-12-09 11:58:01.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 8.597699165344238
2025-12-09 11:58:01.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 8.544163703918457
2025-12-09 11:58:01.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 8.28820514678955
2025-12-09 11:58:02.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 8.36696720123291
2025-12-09 11:58:02.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 8.537219047546387
2025-12-09 11:58:03.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 8.30577564239502
2025-12-09 11:58:03.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 8.20102310180664
2025-12-09 11:58:03.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 8.299720764160156
2025-12-09 11:58:04.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 8.156744956970215
2025-12-09 11:58:04.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 8.90001392364502
2025-12-09 11:58:05.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 8.046710014343262
2025-12-09 11:58:05.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 8.279866218566895
2025-12-09 11:58:05.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 8.356385231018066
2025-12-09 11:58:06.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 8.702637672424316
2025-12-09 11:58:06.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 8.164571762084961
2025-12-09 11:58:06.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 8.10025405883789
2025-12-09 11:58:07.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 8.21685791015625
2025-12-09 11:58:07.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 8.049637794494629
2025-12-09 11:58:08.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 8.016632080078125
2025-12-09 11:58:08.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 8.2543363571167
2025-12-09 11:58:08.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 8.327221870422363
2025-12-09 11:58:09.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 8.154609680175781
2025-12-09 11:58:09.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 8.338589668273926
2025-12-09 11:58:10.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 8.063992500305176
2025-12-09 11:58:10.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 8.108779907226562
2025-12-09 11:58:10.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 8.495901107788086
2025-12-09 11:58:11.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 8.354949951171875
2025-12-09 11:58:11.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 8.161706924438477
2025-12-09 11:58:11.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 8.251791954040527
2025-12-09 11:58:12.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 8.161210060119629
2025-12-09 11:58:12.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 8.312765121459961
2025-12-09 11:58:13.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 8.131490707397461
2025-12-09 11:58:13.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 8.185748100280762
2025-12-09 11:58:13.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 8.133986473083496
2025-12-09 11:58:14.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 8.101377487182617
2025-12-09 11:58:14.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 8.3768949508667
2025-12-09 11:58:15.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 8.370002746582031
2025-12-09 11:58:15.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 7.904540061950684
2025-12-09 11:58:15.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 7.847740173339844
2025-12-09 11:58:16.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 8.1318359375
2025-12-09 11:58:16.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 7.850449562072754
2025-12-09 11:58:16.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 8.310515403747559
2025-12-09 11:58:17.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 8.275249481201172
2025-12-09 11:58:17.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 8.10615062713623
2025-12-09 11:58:18.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 8.242816925048828
2025-12-09 11:58:18.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 8.471940040588379
2025-12-09 11:58:18.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 8.261445999145508
2025-12-09 11:58:19.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 8.055402755737305
2025-12-09 11:58:19.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 7.751707077026367
2025-12-09 11:58:20.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 8.502073287963867
2025-12-09 11:58:20.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 7.941368103027344
2025-12-09 11:58:20.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 8.875041961669922
2025-12-09 11:58:21.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 7.858638763427734
2025-12-09 11:58:21.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 8.494112014770508
2025-12-09 11:58:21.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 8.347163200378418
2025-12-09 11:58:22.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 8.320319175720215
2025-12-09 11:58:22.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 7.937711238861084
2025-12-09 11:58:23.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 8.316486358642578
2025-12-09 11:58:23.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 7.849207401275635
2025-12-09 11:58:23.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009999999029798809 Training loss: 7.753601551055908
2025-12-09 11:58:24.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.000999999611919561 Training loss: 8.444405555725098
2025-12-09 11:58:24.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0009999991268191536 Training loss: 8.232436180114746
2025-12-09 11:58:25.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0009999984476788465 Training loss: 8.237497329711914
2025-12-09 11:58:25.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0009999975744989036 Training loss: 7.78886079788208
2025-12-09 11:58:25.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0009999965072796635 Training loss: 8.559237480163574
2025-12-09 11:58:26.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0009999952460215409 Training loss: 7.953524112701416
2025-12-09 11:58:26.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0009999937907250245 Training loss: 8.032050132751465
2025-12-09 11:58:27.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0009999921413906799 Training loss: 7.724843502044678
2025-12-09 11:58:27.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0009999902980191463 Training loss: 8.04150104522705
2025-12-09 11:58:27.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00099998826061114 Training loss: 8.024497985839844
2025-12-09 11:58:28.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0009999860291674508 Training loss: 7.766926288604736
2025-12-09 11:58:28.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0009999836036889453 Training loss: 7.75118350982666
2025-12-09 11:58:28.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0009999809841765644 Training loss: 7.9802632331848145
2025-12-09 11:58:29.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.000999978170631325 Training loss: 7.821259021759033
2025-12-09 11:58:29.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0009999751630543187 Training loss: 8.05175495147705
2025-12-09 11:58:30.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.000999971961446713 Training loss: 7.599653720855713
2025-12-09 11:58:30.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0009999685658097501 Training loss: 7.7950334548950195
2025-12-09 11:58:30.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0009999649761447478 Training loss: 7.887042045593262
2025-12-09 11:58:31.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0009999611924530994 Training loss: 7.714775562286377
2025-12-09 11:58:31.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.000999957214736273 Training loss: 8.02319049835205
2025-12-09 11:58:32.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0009999530429958125 Training loss: 7.516730785369873
2025-12-09 11:58:32.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0009999486772333365 Training loss: 7.27385950088501
2025-12-09 11:58:32.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00099994411745054 Training loss: 7.796440601348877
2025-12-09 11:58:33.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0009999393636491917 Training loss: 7.917876720428467
2025-12-09 11:58:33.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.000999934415831137 Training loss: 8.071786880493164
2025-12-09 11:58:33.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.000999929273998296 Training loss: 7.588228225708008
2025-12-09 11:58:34.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0009999239381526638 Training loss: 7.84539270401001
2025-12-09 11:58:34.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0009999184082963117 Training loss: 7.82369327545166
2025-12-09 11:58:35.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0009999126844313852 Training loss: 7.635559558868408
2025-12-09 11:58:35.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.000999906766560106 Training loss: 7.905280590057373
2025-12-09 11:58:35.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0009999006546847706 Training loss: 7.9369401931762695
2025-12-09 11:58:36.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0009998943488077508 Training loss: 7.861617565155029
2025-12-09 11:58:36.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0009998878489314938 Training loss: 7.7497992515563965
2025-12-09 11:58:37.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.000999881155058522 Training loss: 7.809140682220459
2025-12-09 11:58:37.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0009998742671914335 Training loss: 8.016775131225586
2025-12-09 11:58:37.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.000999867185332901 Training loss: 7.390947341918945
2025-12-09 11:58:38.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.000999859909485673 Training loss: 7.819207668304443
2025-12-09 11:58:38.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.000999852439652573 Training loss: 7.26800537109375
2025-12-09 11:58:39.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0009998447758365 Training loss: 9.05301570892334
2025-12-09 11:58:39.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0009998369180404282 Training loss: 7.5699896812438965
2025-12-09 11:58:39.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.000999828866267407 Training loss: 7.682825565338135
2025-12-09 11:58:40.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0009998206205205612 Training loss: 8.364670753479004
2025-12-09 11:58:40.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0009998121808030905 Training loss: 7.608509540557861
2025-12-09 11:58:40.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0009998035471182707 Training loss: 8.624787330627441
2025-12-09 11:58:41.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.000999794719469452 Training loss: 7.566892623901367
2025-12-09 11:58:41.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0009997856978600603 Training loss: 7.4581990242004395
2025-12-09 11:58:42.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0009997764822935967 Training loss: 7.883506774902344
2025-12-09 11:58:42.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.000999767072773638 Training loss: 7.897951126098633
2025-12-09 11:58:42.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0009997574693038351 Training loss: 7.483532428741455
2025-12-09 11:58:43.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0009997476718879154 Training loss: 7.793302536010742
2025-12-09 11:58:43.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.000999737680529681 Training loss: 7.882689476013184
2025-12-09 11:58:44.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0009997274952330093 Training loss: 8.05925178527832
2025-12-09 11:58:44.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.000999717116001853 Training loss: 7.769495487213135
2025-12-09 11:58:44.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0009997065428402404 Training loss: 8.038328170776367
2025-12-09 11:58:45.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0009996957757522741 Training loss: 7.924686908721924
2025-12-09 11:58:45.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0009996848147421334 Training loss: 8.05998420715332
2025-12-09 11:58:45.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0009996736598140714 Training loss: 8.372714042663574
2025-12-09 11:58:46.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0009996623109724174 Training loss: 7.215878963470459
2025-12-09 11:58:46.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0009996507682215755 Training loss: 7.742047309875488
2025-12-09 11:58:47.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0009996390315660253 Training loss: 7.463719844818115
2025-12-09 11:58:47.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0009996271010103215 Training loss: 7.225977420806885
2025-12-09 11:58:47.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0009996149765590945 Training loss: 7.520662307739258
2025-12-09 11:58:48.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.000999602658217049 Training loss: 7.788507461547852
2025-12-09 11:58:48.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0009995901459889658 Training loss: 7.923064231872559
2025-12-09 11:58:49.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0009995774398797008 Training loss: 7.398204803466797
2025-12-09 11:58:49.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0009995645398941846 Training loss: 7.521371841430664
2025-12-09 11:58:49.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0009995514460374238 Training loss: 7.733614921569824
2025-12-09 11:58:50.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0009995381583144996 Training loss: 7.3826584815979
2025-12-09 11:58:50.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0009995246767305688 Training loss: 7.6645331382751465
2025-12-09 11:58:51.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0009995110012908633 Training loss: 7.416742324829102
2025-12-09 11:58:51.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0009994971320006906 Training loss: 7.5225934982299805
2025-12-09 11:58:51.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0009994830688654327 Training loss: 7.380815029144287
2025-12-09 11:58:52.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.000999468811890547 Training loss: 8.220470428466797
2025-12-09 11:58:52.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.000999454361081567 Training loss: 8.058857917785645
2025-12-09 11:58:52.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.0009994397164441006 Training loss: 7.23861026763916
2025-12-09 11:58:53.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.000999424877983831 Training loss: 7.4743971824646
2025-12-09 11:58:53.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0009994098457065167 Training loss: 9.13571834564209
2025-12-09 11:58:54.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0009993946196179913 Training loss: 7.412985801696777
2025-12-09 11:58:54.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.000999379199724164 Training loss: 7.318389892578125
2025-12-09 11:58:54.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0009993635860310187 Training loss: 7.8754353523254395
2025-12-09 11:58:55.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.000999347778544615 Training loss: 7.420847415924072
2025-12-09 11:58:55.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0009993317772710873 Training loss: 7.981661319732666
2025-12-09 11:58:56.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0009993155822166457 Training loss: 7.364670276641846
2025-12-09 11:58:56.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0009992991933875748 Training loss: 7.386404037475586
2025-12-09 11:58:56.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0009992826107902348 Training loss: 7.53912878036499
2025-12-09 11:58:57.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0009992658344310614 Training loss: 7.806736469268799
2025-12-09 11:58:57.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.000999248864316565 Training loss: 7.477468013763428
2025-12-09 11:58:57.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0009992317004533314 Training loss: 7.438858509063721
2025-12-09 11:58:58.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0009992143428480215 Training loss: 7.536301612854004
2025-12-09 11:58:58.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0009991967915073715 Training loss: 7.82763147354126
2025-12-09 11:58:59.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0009991790464381925 Training loss: 7.181128978729248
2025-12-09 11:58:59.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0009991611076473714 Training loss: 7.682668209075928
2025-12-09 11:58:59.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0009991429751418698 Training loss: 7.601749420166016
2025-12-09 11:59:00.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0009991246489287244 Training loss: 7.780276298522949
2025-12-09 11:59:00.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0009991061290150474 Training loss: 7.524214267730713
2025-12-09 11:59:01.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0009990874154080258 Training loss: 7.517573833465576
2025-12-09 11:59:01.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0009990685081149222 Training loss: 7.239368438720703
2025-12-09 11:59:01.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0009990494071430741 Training loss: 7.946127414703369
2025-12-09 11:59:02.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0009990301124998943 Training loss: 7.380575180053711
2025-12-09 11:59:02.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0009990106241928704 Training loss: 7.094995498657227
2025-12-09 11:59:03.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0009989909422295658 Training loss: 7.573654651641846
2025-12-09 11:59:03.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0009989710666176185 Training loss: 7.790853500366211
2025-12-09 11:59:03.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0009989509973647418 Training loss: 7.1911211013793945
2025-12-09 11:59:04.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0009989307344787242 Training loss: 7.020416736602783
2025-12-09 11:59:04.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0009989102779674292 Training loss: 7.548094272613525
2025-12-09 11:59:04.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.000998889627838796 Training loss: 7.545131206512451
2025-12-09 11:59:05.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.000998868784100838 Training loss: 8.026115417480469
2025-12-09 11:59:05.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0009988477467616447 Training loss: 7.722711086273193
2025-12-09 11:59:06.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0009988265158293798 Training loss: 7.825374126434326
2025-12-09 11:59:06.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.000998805091312283 Training loss: 7.288910865783691
2025-12-09 11:59:06.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0009987834732186687 Training loss: 7.170194149017334
2025-12-09 11:59:07.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0009987616615569263 Training loss: 7.62982177734375
2025-12-09 11:59:07.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0009987396563355204 Training loss: 7.361668109893799
2025-12-09 11:59:08.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.000998717457562991 Training loss: 7.611928939819336
2025-12-09 11:59:08.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0009986950652479533 Training loss: 7.295044422149658
2025-12-09 11:59:08.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0009986724793990967 Training loss: 7.363790512084961
2025-12-09 11:59:09.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0009986497000251866 Training loss: 7.04460334777832
2025-12-09 11:59:09.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0009986267271350634 Training loss: 7.254483222961426
2025-12-09 11:59:10.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0009986035607376421 Training loss: 7.720302581787109
2025-12-09 11:59:10.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0009985802008419132 Training loss: 7.4596381187438965
2025-12-09 11:59:10.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0009985566474569425 Training loss: 7.347620487213135
2025-12-09 11:59:11.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0009985329005918703 Training loss: 7.676898002624512
2025-12-09 11:59:11.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0009985089602559125 Training loss: 7.891622066497803
2025-12-09 11:59:11.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0009984848264583597 Training loss: 7.235136032104492
2025-12-09 11:59:12.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.000998460499208578 Training loss: 7.283065319061279
2025-12-09 11:59:12.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.000998435978516008 Training loss: 7.417981147766113
2025-12-09 11:59:13.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0009984112643901658 Training loss: 7.706059455871582
2025-12-09 11:59:13.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0009983863568406427 Training loss: 7.255741119384766
2025-12-09 11:59:13.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0009983612558771048 Training loss: 6.638543128967285
2025-12-09 11:59:14.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0009983359615092931 Training loss: 7.247200965881348
2025-12-09 11:59:14.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.000998310473747024 Training loss: 7.739259243011475
2025-12-09 11:59:15.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0009982847926001885 Training loss: 7.439174175262451
2025-12-09 11:59:15.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0009982589180787533 Training loss: 6.943292140960693
2025-12-09 11:59:15.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0009982328501927599 Training loss: 7.6303253173828125
2025-12-09 11:59:16.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0009982065889523242 Training loss: 7.287990093231201
2025-12-09 11:59:16.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.000998180134367638 Training loss: 7.565235614776611
2025-12-09 11:59:16.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0009981534864489678 Training loss: 7.509146213531494
2025-12-09 11:59:17.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0009981266452066553 Training loss: 7.788050651550293
2025-12-09 11:59:17.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0009980996106511168 Training loss: 7.547353267669678
2025-12-09 11:59:18.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.000998072382792844 Training loss: 7.3979034423828125
2025-12-09 11:59:18.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0009980449616424037 Training loss: 7.114386081695557
2025-12-09 11:59:18.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.000998017347210437 Training loss: 7.291896820068359
2025-12-09 11:59:19.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.000997989539507661 Training loss: 7.138538837432861
2025-12-09 11:59:19.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.000997961538544867 Training loss: 8.311306953430176
2025-12-09 11:59:20.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0009979333443329217 Training loss: 7.507253170013428
2025-12-09 11:59:20.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.000997904956882767 Training loss: 7.302073001861572
2025-12-09 11:59:20.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0009978763762054192 Training loss: 7.215590000152588
2025-12-09 11:59:21.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00099784760231197 Training loss: 7.553249359130859
2025-12-09 11:59:21.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.000997818635213586 Training loss: 7.653748512268066
2025-12-09 11:59:22.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0009977894749215088 Training loss: 7.066263198852539
2025-12-09 11:59:22.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.000997760121447055 Training loss: 7.475822925567627
2025-12-09 11:59:22.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0009977305748016159 Training loss: 7.294275760650635
2025-12-09 11:59:23.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.000997700834996658 Training loss: 7.7484002113342285
2025-12-09 11:59:23.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.000997670902043723 Training loss: 7.082922458648682
2025-12-09 11:59:23.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.000997640775954427 Training loss: 7.153512954711914
2025-12-09 11:59:24.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.0009976104567404615 Training loss: 7.431005954742432
2025-12-09 11:59:24.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0009975799444135929 Training loss: 6.960775852203369
2025-12-09 11:59:25.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0009975492389856621 Training loss: 7.041862487792969
2025-12-09 11:59:25.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0009975183404685856 Training loss: 7.192813396453857
2025-12-09 11:59:25.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0009974872488743543 Training loss: 7.382180690765381
2025-12-09 11:59:26.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0009974559642150344 Training loss: 7.2810211181640625
2025-12-09 11:59:26.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.000997424486502767 Training loss: 7.699681282043457
2025-12-09 11:59:27.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0009973928157497674 Training loss: 7.002138614654541
2025-12-09 11:59:27.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.000997360951968327 Training loss: 7.654074192047119
2025-12-09 11:59:27.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0009973288951708112 Training loss: 7.332629203796387
2025-12-09 11:59:28.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0009972966453696609 Training loss: 7.802114486694336
2025-12-09 11:59:28.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0009972642025773912 Training loss: 7.3677239418029785
2025-12-09 11:59:29.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0009972315668065929 Training loss: 7.3008928298950195
2025-12-09 11:59:29.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.000997198738069931 Training loss: 7.337677955627441
2025-12-09 11:59:29.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.000997165716380146 Training loss: 7.52299690246582
2025-12-09 11:59:30.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0009971325017500525 Training loss: 7.15311861038208
2025-12-09 11:59:30.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0009970990941925411 Training loss: 6.942046165466309
2025-12-09 11:59:30.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0009970654937205762 Training loss: 7.427358150482178
2025-12-09 11:59:31.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0009970317003471976 Training loss: 7.423610210418701
2025-12-09 11:59:31.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0009969977140855198 Training loss: 7.419850826263428
2025-12-09 11:59:32.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009969635349487322 Training loss: 7.275195598602295
2025-12-09 11:59:32.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.000996929162950099 Training loss: 7.1010236740112305
2025-12-09 11:59:32.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0009968945981029596 Training loss: 7.342330455780029
2025-12-09 11:59:33.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009968598404207275 Training loss: 7.4640631675720215
2025-12-09 11:59:33.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0009968248899168918 Training loss: 7.682768821716309
2025-12-09 11:59:34.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.000996789746605016 Training loss: 8.277417182922363
2025-12-09 11:59:34.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009967544104987386 Training loss: 7.126711368560791
2025-12-09 11:59:34.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0009967188816117727 Training loss: 7.016894817352295
2025-12-09 11:59:35.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0009966831599579067 Training loss: 7.3811564445495605
2025-12-09 11:59:35.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.000996647245551003 Training loss: 7.549909591674805
2025-12-09 11:59:36.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0009966111384049996 Training loss: 7.5264434814453125
2025-12-09 11:59:36.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0009965748385339088 Training loss: 7.177248954772949
2025-12-09 11:59:36.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0009965383459518181 Training loss: 6.799937725067139
2025-12-09 11:59:37.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0009965016606728893 Training loss: 7.231451511383057
2025-12-09 11:59:37.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0009964647827113596 Training loss: 7.112030506134033
2025-12-09 11:59:37.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0009964277120815403 Training loss: 7.670905590057373
2025-12-09 11:59:38.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0009963904487978177 Training loss: 7.452422618865967
2025-12-09 11:59:38.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0009963529928746534 Training loss: 8.010001182556152
2025-12-09 11:59:39.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0009963153443265829 Training loss: 6.833184242248535
2025-12-09 11:59:39.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0009962775031682168 Training loss: 7.185210704803467
2025-12-09 11:59:39.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0009962394694142409 Training loss: 7.383999824523926
2025-12-09 11:59:40.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0009962012430794153 Training loss: 7.348850250244141
2025-12-09 11:59:40.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0009961628241785747 Training loss: 7.350939750671387
2025-12-09 11:59:41.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0009961242127266288 Training loss: 7.746349811553955
2025-12-09 11:59:41.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0009960854087385617 Training loss: 7.540901184082031
2025-12-09 11:59:41.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.000996046412229433 Training loss: 7.237720489501953
2025-12-09 11:59:42.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0009960072232143762 Training loss: 7.315936088562012
2025-12-09 11:59:42.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0009959678417085997 Training loss: 7.554211616516113
2025-12-09 11:59:42.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0009959282677273868 Training loss: 6.745067119598389
2025-12-09 11:59:43.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0009958885012860954 Training loss: 9.16088581085205
2025-12-09 11:59:43.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0009958485424001581 Training loss: 7.062124252319336
2025-12-09 11:59:44.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0009958083910850822 Training loss: 7.218323230743408
2025-12-09 11:59:44.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0009957680473564494 Training loss: 6.831634998321533
2025-12-09 11:59:44.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0009957275112299165 Training loss: 7.180285930633545
2025-12-09 11:59:45.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0009956867827212148 Training loss: 7.314449310302734
2025-12-09 11:59:45.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.00099564586184615 Training loss: 7.625958442687988
2025-12-09 11:59:46.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0009956047486206032 Training loss: 7.350064277648926
2025-12-09 11:59:46.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0009955634430605291 Training loss: 7.018315315246582
2025-12-09 11:59:46.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.000995521945181958 Training loss: 7.666355609893799
2025-12-09 11:59:47.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0009954802550009943 Training loss: 7.419686794281006
2025-12-09 11:59:47.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.0009954383725338167 Training loss: 7.380845069885254
2025-12-09 11:59:48.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0009953962977966794 Training loss: 7.10598611831665
2025-12-09 11:59:48.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.000995354030805911 Training loss: 7.823146820068359
2025-12-09 11:59:48.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0009953115715779141 Training loss: 7.237271785736084
2025-12-09 11:59:49.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0009952689201291663 Training loss: 7.436704158782959
2025-12-09 11:59:49.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00099522607647622 Training loss: 7.380693435668945
2025-12-09 11:59:49.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0009951830406357018 Training loss: 6.993168830871582
2025-12-09 11:59:50.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0009951398126243135 Training loss: 7.320809364318848
2025-12-09 11:59:50.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0009950963924588304 Training loss: 6.998902797698975
2025-12-09 11:59:51.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0009950527801561033 Training loss: 7.027320384979248
2025-12-09 11:59:51.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0009950089757330574 Training loss: 7.433650970458984
2025-12-09 11:59:51.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0009949649792066922 Training loss: 7.151974201202393
2025-12-09 11:59:52.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.000994920790594082 Training loss: 7.655622482299805
2025-12-09 11:59:52.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0009948764099123755 Training loss: 8.255760192871094
2025-12-09 11:59:53.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.000994831837178796 Training loss: 7.303532123565674
2025-12-09 11:59:53.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.000994787072410641 Training loss: 7.289849281311035
2025-12-09 11:59:53.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0009947421156252835 Training loss: 6.972695350646973
2025-12-09 11:59:54.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0009946969668401698 Training loss: 7.394505023956299
2025-12-09 11:59:54.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.0009946516260728212 Training loss: 7.32027006149292
2025-12-09 11:59:55.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.000994606093340834 Training loss: 7.156782150268555
2025-12-09 11:59:55.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0009945603686618784 Training loss: 7.21410608291626
2025-12-09 11:59:55.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.000994514452053699 Training loss: 7.798482418060303
2025-12-09 11:59:56.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0009944683435341155 Training loss: 6.861510753631592
2025-12-09 11:59:56.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0009944220431210215 Training loss: 7.836240768432617
2025-12-09 11:59:56.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0009943755508323854 Training loss: 7.229801177978516
2025-12-09 11:59:57.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0009943288666862497 Training loss: 7.560859203338623
2025-12-09 11:59:57.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.000994281990700732 Training loss: 7.680565357208252
2025-12-09 11:59:58.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0009942349228940237 Training loss: 7.067097187042236
2025-12-09 11:59:58.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0009941876632843908 Training loss: 7.155828475952148
2025-12-09 11:59:58.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0009941402118901744 Training loss: 7.082911968231201
2025-12-09 11:59:59.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0009940925687297885 Training loss: 7.640539169311523
2025-12-09 11:59:59.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0009940447338217234 Training loss: 6.95618200302124
2025-12-09 12:00:00.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0009939967071845423 Training loss: 7.002398490905762
2025-12-09 12:00:00.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0009939484888368837 Training loss: 7.048412322998047
2025-12-09 12:00:00.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0009939000787974601 Training loss: 6.910360813140869
2025-12-09 12:00:01.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.0009938514770850585 Training loss: 7.272913932800293
2025-12-09 12:00:01.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.0009938026837185403 Training loss: 7.191866397857666
2025-12-09 12:00:01.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0009937536987168413 Training loss: 7.0006256103515625
2025-12-09 12:00:02.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0009937045220989715 Training loss: 7.324591636657715
2025-12-09 12:00:02.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0009936551538840153 Training loss: 6.961051940917969
2025-12-09 12:00:03.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.000993605594091132 Training loss: 6.8459978103637695
2025-12-09 12:00:03.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.000993555842739554 Training loss: 7.205642223358154
2025-12-09 12:00:03.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0009935058998485897 Training loss: 7.320609092712402
2025-12-09 12:00:04.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0009934557654376205 Training loss: 7.515509128570557
2025-12-09 12:00:04.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0009934054395261025 Training loss: 7.694757461547852
2025-12-09 12:00:05.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0009933549221335664 Training loss: 7.244123458862305
2025-12-09 12:00:05.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.000993304213279617 Training loss: 7.22634744644165
2025-12-09 12:00:05.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0009932533129839334 Training loss: 7.409102916717529
2025-12-09 12:00:06.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.000993202221266269 Training loss: 6.894642353057861
2025-12-09 12:00:06.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0009931509381464515 Training loss: 7.4337592124938965
2025-12-09 12:00:07.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0009930994636443828 Training loss: 7.00593900680542
2025-12-09 12:00:07.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0009930477977800392 Training loss: 6.900500297546387
2025-12-09 12:00:07.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.000992995940573471 Training loss: 7.10638427734375
2025-12-09 12:00:08.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0009929438920448037 Training loss: 7.113773345947266
2025-12-09 12:00:08.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0009928916522142356 Training loss: 6.996668815612793
2025-12-09 12:00:08.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00099283922110204 Training loss: 6.936768531799316
2025-12-09 12:00:09.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.0009927865987285648 Training loss: 7.068575382232666
2025-12-09 12:00:09.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0009927337851142314 Training loss: 7.274648666381836
2025-12-09 12:00:10.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.000992680780279536 Training loss: 7.096066474914551
2025-12-09 12:00:10.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0009926275842450482 Training loss: 7.098959922790527
2025-12-09 12:00:10.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0009925741970314129 Training loss: 7.039676666259766
2025-12-09 12:00:11.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.0009925206186593484 Training loss: 7.350255012512207
2025-12-09 12:00:11.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.0009924668491496473 Training loss: 7.162681579589844
2025-12-09 12:00:12.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.000992412888523177 Training loss: 7.813267230987549
2025-12-09 12:00:12.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.000992358736800878 Training loss: 7.218216896057129
2025-12-09 12:00:12.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0009923043940037657 Training loss: 6.977889537811279
2025-12-09 12:00:13.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0009922498601529295 Training loss: 6.923476696014404
2025-12-09 12:00:13.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.000992195135269533 Training loss: 6.98903226852417
2025-12-09 12:00:14.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0009921402193748137 Training loss: 6.633384704589844
2025-12-09 12:00:14.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0009920851124900836 Training loss: 7.040977954864502
2025-12-09 12:00:14.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.0009920298146367287 Training loss: 7.090988636016846
2025-12-09 12:00:15.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0009919743258362086 Training loss: 7.515102863311768
2025-12-09 12:00:15.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0009919186461100577 Training loss: 6.893669605255127
2025-12-09 12:00:15.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.000991862775479884 Training loss: 7.4726080894470215
2025-12-09 12:00:16.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00099180671396737 Training loss: 6.749088287353516
2025-12-09 12:00:16.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0009917504615942721 Training loss: 7.269937992095947
2025-12-09 12:00:17.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0009916940183824206 Training loss: 7.365699768066406
2025-12-09 12:00:17.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0009916373843537201 Training loss: 7.038516044616699
2025-12-09 12:00:17.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0009915805595301491 Training loss: 7.183223247528076
2025-12-09 12:00:18.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.0009915235439337602 Training loss: 7.7750396728515625
2025-12-09 12:00:18.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0009914663375866803 Training loss: 7.272334098815918
2025-12-09 12:00:19.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0009914089405111098 Training loss: 7.291191101074219
2025-12-09 12:00:19.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0009913513527293235 Training loss: 7.472557067871094
2025-12-09 12:00:19.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0009912935742636697 Training loss: 7.436251640319824
2025-12-09 12:00:20.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0009912356051365717 Training loss: 6.88408899307251
2025-12-09 12:00:20.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0009911774453705258 Training loss: 7.3224945068359375
2025-12-09 12:00:21.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.0009911190949881028 Training loss: 6.550334930419922
2025-12-09 12:00:21.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0009910605540119474 Training loss: 7.256500244140625
2025-12-09 12:00:21.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0009910018224647782 Training loss: 6.791090965270996
2025-12-09 12:00:22.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0009909429003693876 Training loss: 7.1515960693359375
2025-12-09 12:00:22.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0009908837877486423 Training loss: 7.38965368270874
2025-12-09 12:00:22.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.0009908244846254825 Training loss: 6.869956016540527
2025-12-09 12:00:23.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.000990764991022923 Training loss: 6.8617095947265625
2025-12-09 12:00:23.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0009907053069640515 Training loss: 7.148702144622803
2025-12-09 12:00:24.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.000990645432472031 Training loss: 6.970191478729248
2025-12-09 12:00:24.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.000990585367570097 Training loss: 7.064856052398682
2025-12-09 12:00:24.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.0009905251122815596 Training loss: 7.4463419914245605
2025-12-09 12:00:25.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.000990464666629803 Training loss: 7.1266303062438965
2025-12-09 12:00:25.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0009904040306382847 Training loss: 7.483878135681152
2025-12-09 12:00:26.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0009903432043305365 Training loss: 7.075639724731445
2025-12-09 12:00:26.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0009902821877301638 Training loss: 7.363073825836182
2025-12-09 12:00:26.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.000990220980860846 Training loss: 7.344825744628906
2025-12-09 12:00:27.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.0009901595837463362 Training loss: 7.2913594245910645
2025-12-09 12:00:27.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0009900979964104616 Training loss: 6.97576379776001
2025-12-09 12:00:27.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.000990036218877123 Training loss: 7.092408657073975
2025-12-09 12:00:28.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.000989974251170295 Training loss: 6.814452648162842
2025-12-09 12:00:28.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.000989912093314026 Training loss: 7.228304386138916
2025-12-09 12:00:29.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0009898497453324385 Training loss: 7.372936248779297
2025-12-09 12:00:29.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.000989787207249728 Training loss: 7.430624008178711
2025-12-09 12:00:29.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0009897244790901649 Training loss: 6.971487998962402
2025-12-09 12:00:30.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0009896615608780924 Training loss: 7.721053123474121
2025-12-09 12:00:30.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.000989598452637928 Training loss: 7.277741432189941
2025-12-09 12:00:31.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0009895351543941628 Training loss: 7.0581536293029785
2025-12-09 12:00:31.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0009894716661713616 Training loss: 6.651464939117432
2025-12-09 12:00:31.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0009894079879941627 Training loss: 7.964301109313965
2025-12-09 12:00:32.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0009893441198872788 Training loss: 6.87536096572876
2025-12-09 12:00:32.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.0009892800618754953 Training loss: 7.185734748840332
2025-12-09 12:00:33.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0009892158139836725 Training loss: 7.20966100692749
2025-12-09 12:00:33.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0009891513762367431 Training loss: 7.4270501136779785
2025-12-09 12:00:33.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.0009890867486597146 Training loss: 7.040834426879883
2025-12-09 12:00:34.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0009890219312776677 Training loss: 7.737486839294434
2025-12-09 12:00:34.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0009889569241157564 Training loss: 6.838325023651123
2025-12-09 12:00:34.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.000988891727199209 Training loss: 6.997594356536865
2025-12-09 12:00:35.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.000988826340553327 Training loss: 6.187948226928711
2025-12-09 12:00:35.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0009887607642034859 Training loss: 7.795039176940918
2025-12-09 12:00:36.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0009886949981751346 Training loss: 6.588823318481445
2025-12-09 12:00:36.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0009886290424937951 Training loss: 7.040180206298828
2025-12-09 12:00:36.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0009885628971850642 Training loss: 6.942149639129639
2025-12-09 12:00:37.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0009884965622746112 Training loss: 7.128566741943359
2025-12-09 12:00:37.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0009884300377881795 Training loss: 7.176023006439209
2025-12-09 12:00:38.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0009883633237515858 Training loss: 6.969916820526123
2025-12-09 12:00:38.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0009882964201907208 Training loss: 7.137135028839111
2025-12-09 12:00:38.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0009882293271315482 Training loss: 6.9552154541015625
2025-12-09 12:00:39.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0009881620446001056 Training loss: 7.525629997253418
2025-12-09 12:00:39.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.000988094572622504 Training loss: 7.420582294464111
2025-12-09 12:00:40.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0009880269112249281 Training loss: 7.744894504547119
2025-12-09 12:00:40.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.000987959060433636 Training loss: 6.758722305297852
2025-12-09 12:00:40.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.0009878910202749589 Training loss: 7.098827362060547
2025-12-09 12:00:41.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0009878227907753022 Training loss: 6.900410175323486
2025-12-09 12:00:41.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0009877543719611444 Training loss: 6.896575927734375
2025-12-09 12:00:41.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0009876857638590373 Training loss: 7.248706817626953
2025-12-09 12:00:42.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.0009876169664956068 Training loss: 6.99046516418457
2025-12-09 12:00:42.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0009875479798975512 Training loss: 7.125633239746094
2025-12-09 12:00:43.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0009874788040916433 Training loss: 7.234933376312256
2025-12-09 12:00:43.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0009874094391047288 Training loss: 7.0180768966674805
2025-12-09 12:00:43.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.0009873398849637267 Training loss: 6.750127792358398
2025-12-09 12:00:44.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00098727014169563 Training loss: 7.281252384185791
2025-12-09 12:00:44.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.0009872002093275043 Training loss: 7.365250110626221
2025-12-09 12:00:45.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.000987130087886489 Training loss: 6.879098892211914
2025-12-09 12:00:45.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.000987059777399797 Training loss: 7.098609924316406
2025-12-09 12:00:45.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.0009869892778947148 Training loss: 8.070328712463379
2025-12-09 12:00:46.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0009869185893986011 Training loss: 7.370302677154541
2025-12-09 12:00:46.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0009868477119388895 Training loss: 7.289418697357178
2025-12-09 12:00:47.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.0009867766455430858 Training loss: 6.8578667640686035
2025-12-09 12:00:47.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.0009867053902387693 Training loss: 7.15665340423584
2025-12-09 12:00:47.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0009866339460535929 Training loss: 7.144228935241699
2025-12-09 12:00:48.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0009865623130152828 Training loss: 6.896724224090576
2025-12-09 12:00:48.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.0009864904911516383 Training loss: 7.371510982513428
2025-12-09 12:00:48.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0009864184804905323 Training loss: 7.324470043182373
2025-12-09 12:00:49.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0009863462810599105 Training loss: 6.9808549880981445
2025-12-09 12:00:49.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.000986273892887792 Training loss: 7.886877059936523
2025-12-09 12:00:50.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0009862013160022696 Training loss: 7.036606311798096
2025-12-09 12:00:50.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0009861285504315085 Training loss: 7.489642143249512
2025-12-09 12:00:50.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0009860555962037478 Training loss: 7.3589582443237305
2025-12-09 12:00:51.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0009859824533472999 Training loss: 7.16518497467041
2025-12-09 12:00:51.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0009859091218905498 Training loss: 6.9221510887146
2025-12-09 12:00:52.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.000985835601861956 Training loss: 7.727482795715332
2025-12-09 12:00:52.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.0009857618932900504 Training loss: 6.989415645599365
2025-12-09 12:00:52.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0009856879962034375 Training loss: 7.072854042053223
2025-12-09 12:00:53.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0009856139106307956 Training loss: 6.928814888000488
2025-12-09 12:00:53.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0009855396366008756 Training loss: 7.376487731933594
2025-12-09 12:00:54.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0009854651741425023 Training loss: 8.121116638183594
2025-12-09 12:00:54.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0009853905232845728 Training loss: 7.06154203414917
2025-12-09 12:00:54.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0009853156840560575 Training loss: 6.945557594299316
2025-12-09 12:00:55.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0009852406564860004 Training loss: 6.937526226043701
2025-12-09 12:00:55.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.000985165440603518 Training loss: 7.1234235763549805
2025-12-09 12:00:55.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0009850900364378 Training loss: 6.96568489074707
2025-12-09 12:00:56.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0009850144440181096 Training loss: 7.875219821929932
2025-12-09 12:00:56.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0009849386633737824 Training loss: 6.983744144439697
2025-12-09 12:00:57.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.0009848626945342278 Training loss: 7.2204766273498535
2025-12-09 12:00:57.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0009847865375289275 Training loss: 7.324361801147461
2025-12-09 12:00:57.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0009847101923874367 Training loss: 7.168854713439941
2025-12-09 12:00:58.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.0009846336591393832 Training loss: 7.137368202209473
2025-12-09 12:00:58.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0009845569378144686 Training loss: 6.971034526824951
2025-12-09 12:00:59.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.0009844800284424663 Training loss: 6.750809192657471
2025-12-09 12:00:59.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0009844029310532238 Training loss: 7.145030975341797
2025-12-09 12:00:59.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0009843256456766609 Training loss: 6.949342727661133
2025-12-09 12:01:00.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0009842481723427705 Training loss: 6.664475440979004
2025-12-09 12:01:00.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.0009841705110816186 Training loss: 7.1126275062561035
2025-12-09 12:01:00.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.000984092661923344 Training loss: 6.597280025482178
2025-12-09 12:01:01.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0009840146248981585 Training loss: 7.15090799331665
2025-12-09 12:01:01.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.0009839364000363466 Training loss: 6.865684986114502
2025-12-09 12:01:02.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.000983857987368266 Training loss: 6.840259075164795
2025-12-09 12:01:02.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0009837793869243467 Training loss: 7.082773208618164
2025-12-09 12:01:02.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.0009837005987350927 Training loss: 6.90855598449707
2025-12-09 12:01:03.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0009836216228310797 Training loss: 7.685556411743164
2025-12-09 12:01:03.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0009835424592429566 Training loss: 7.978285789489746
2025-12-09 12:01:04.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0009834631080014456 Training loss: 7.333579063415527
2025-12-09 12:01:04.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0009833835691373412 Training loss: 7.283302307128906
2025-12-09 12:01:04.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.000983303842681511 Training loss: 7.231754302978516
2025-12-09 12:01:05.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.000983223928664895 Training loss: 7.185320854187012
2025-12-09 12:01:05.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0009831438271185064 Training loss: 6.503413200378418
2025-12-09 12:01:06.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.0009830635380734312 Training loss: 6.908692359924316
2025-12-09 12:01:06.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.000982983061560828 Training loss: 7.226515293121338
2025-12-09 12:01:06.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0009829023976119279 Training loss: 7.076889514923096
2025-12-09 12:01:07.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0009828215462580352 Training loss: 6.844151496887207
2025-12-09 12:01:07.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0009827405075305267 Training loss: 6.9417548179626465
2025-12-09 12:01:07.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.000982659281460852 Training loss: 7.472141265869141
2025-12-09 12:01:08.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0009825778680805331 Training loss: 7.042254447937012
2025-12-09 12:01:08.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.0009824962674211653 Training loss: 6.87209415435791
2025-12-09 12:01:09.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0009824144795144158 Training loss: 6.945627689361572
2025-12-09 12:01:09.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0009823325043920256 Training loss: 6.633835315704346
2025-12-09 12:01:09.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0009822503420858068 Training loss: 7.266514301300049
2025-12-09 12:01:10.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0009821679926276456 Training loss: 7.132686138153076
2025-12-09 12:01:10.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0009820854560494998 Training loss: 7.290655136108398
2025-12-09 12:01:11.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0009820027323834007 Training loss: 7.116331100463867
2025-12-09 12:01:11.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0009819198216614513 Training loss: 6.6910552978515625
2025-12-09 12:01:11.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0009818367239158277 Training loss: 7.328753471374512
2025-12-09 12:01:12.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0009817534391787788 Training loss: 6.809731960296631
2025-12-09 12:01:12.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0009816699674826256 Training loss: 7.105456352233887
2025-12-09 12:01:13.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.0009815863088597618 Training loss: 6.885411739349365
2025-12-09 12:01:13.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.0009815024633426537 Training loss: 6.988394737243652
2025-12-09 12:01:13.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.00098141843096384 Training loss: 7.0749125480651855
2025-12-09 12:01:14.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.0009813342117559324 Training loss: 6.919492244720459
2025-12-09 12:01:14.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.0009812498057516143 Training loss: 7.156891822814941
2025-12-09 12:01:14.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0009811652129836422 Training loss: 7.329735279083252
2025-12-09 12:01:15.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0009810804334848449 Training loss: 6.455320835113525
2025-12-09 12:01:15.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.0009809954672881237 Training loss: 7.52678108215332
2025-12-09 12:01:16.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0009809103144264523 Training loss: 6.89572286605835
2025-12-09 12:01:16.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0009808249749328768 Training loss: 7.019077301025391
2025-12-09 12:01:16.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.000980739448840516 Training loss: 7.239087104797363
2025-12-09 12:01:17.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.0009806537361825606 Training loss: 7.232363224029541
2025-12-09 12:01:17.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0009805678369922742 Training loss: 7.244472026824951
2025-12-09 12:01:18.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.0009804817513029928 Training loss: 6.9605584144592285
2025-12-09 12:01:18.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.000980395479148124 Training loss: 6.969308376312256
2025-12-09 12:01:18.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0009803090205611487 Training loss: 7.30450439453125
2025-12-09 12:01:19.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.0009802223755756199 Training loss: 7.050876140594482
2025-12-09 12:01:19.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0009801355442251626 Training loss: 7.195952415466309
2025-12-09 12:01:20.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0009800485265434745 Training loss: 7.055855751037598
2025-12-09 12:01:20.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0009799613225643252 Training loss: 7.0311360359191895
2025-12-09 12:01:20.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.0009798739323215572 Training loss: 7.0095391273498535
2025-12-09 12:01:21.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.0009797863558490849 Training loss: 7.212560653686523
2025-12-09 12:01:21.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.000979698593180895 Training loss: 8.2991304397583
2025-12-09 12:01:21.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0009796106443510462 Training loss: 6.8370819091796875
2025-12-09 12:01:22.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.00097952250939367 Training loss: 7.3613996505737305
2025-12-09 12:01:22.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.0009794341883429699 Training loss: 7.2772088050842285
2025-12-09 12:01:23.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0009793456812332215 Training loss: 7.103468894958496
2025-12-09 12:01:23.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0009792569880987725 Training loss: 6.745631694793701
2025-12-09 12:01:23.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0009791681089740432 Training loss: 6.981790065765381
2025-12-09 12:01:24.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.0009790790438935256 Training loss: 6.991612434387207
2025-12-09 12:01:24.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.0009789897928917846 Training loss: 7.0652546882629395
2025-12-09 12:01:25.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.000978900356003456 Training loss: 6.940522193908691
2025-12-09 12:01:25.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0009788107332632495 Training loss: 7.073658466339111
2025-12-09 12:01:25.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0009787209247059453 Training loss: 7.064385414123535
2025-12-09 12:01:26.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.0009786309303663962 Training loss: 7.652308464050293
2025-12-09 12:01:26.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0009785407502795277 Training loss: 7.05230188369751
2025-12-09 12:01:27.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0009784503844803367 Training loss: 6.800323009490967
2025-12-09 12:01:27.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0009783598330038925 Training loss: 7.291532516479492
2025-12-09 12:01:27.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0009782690958853363 Training loss: 7.258736610412598
2025-12-09 12:01:28.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0009781781731598813 Training loss: 6.846775531768799
2025-12-09 12:01:28.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.000978087064862813 Training loss: 7.431147575378418
2025-12-09 12:01:28.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0009779957710294885 Training loss: 6.948266983032227
2025-12-09 12:01:29.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0009779042916953375 Training loss: 7.012070655822754
2025-12-09 12:01:29.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0009778126268958612 Training loss: 7.227962970733643
2025-12-09 12:01:30.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.000977720776666633 Training loss: 6.643306255340576
2025-12-09 12:01:30.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.000977628741043298 Training loss: 7.046093463897705
2025-12-09 12:01:30.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0009775365200615734 Training loss: 6.764304161071777
2025-12-09 12:01:31.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0009774441137572487 Training loss: 6.9599995613098145
2025-12-09 12:01:31.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0009773515221661846 Training loss: 6.991168022155762
2025-12-09 12:01:32.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0009772587453243141 Training loss: 7.240253925323486
2025-12-09 12:01:32.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.0009771657832676427 Training loss: 6.730522632598877
2025-12-09 12:01:32.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0009770726360322465 Training loss: 6.937039852142334
2025-12-09 12:01:33.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.000976979303654274 Training loss: 7.542394161224365
2025-12-09 12:01:33.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0009768857861699462 Training loss: 6.676952838897705
2025-12-09 12:01:34.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0009767920836155552 Training loss: 6.81767463684082
2025-12-09 12:01:34.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0009766981960274653 Training loss: 6.74185848236084
2025-12-09 12:01:34.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.000976604123442112 Training loss: 7.465503215789795
2025-12-09 12:01:35.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.0009765098658960035 Training loss: 6.692911624908447
2025-12-09 12:01:35.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0009764154234257191 Training loss: 6.8784098625183105
2025-12-09 12:01:35.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.00097632079606791 Training loss: 7.180807590484619
2025-12-09 12:01:36.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0009762259838592994 Training loss: 7.050564765930176
2025-12-09 12:01:36.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0009761309868366819 Training loss: 7.313096046447754
2025-12-09 12:01:37.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0009760358050369243 Training loss: 6.838676929473877
2025-12-09 12:01:37.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0009759404384969643 Training loss: 6.948596000671387
2025-12-09 12:01:37.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0009758448872538121 Training loss: 6.8691086769104
2025-12-09 12:01:38.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0009757491513445493 Training loss: 6.880108833312988
2025-12-09 12:01:38.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0009756532308063293 Training loss: 7.222430229187012
2025-12-09 12:01:39.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0009755571256763765 Training loss: 6.901295185089111
2025-12-09 12:01:39.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0009754608359919879 Training loss: 7.180896759033203
2025-12-09 12:01:39.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0009753643617905312 Training loss: 7.1715545654296875
2025-12-09 12:01:40.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0009752677031094466 Training loss: 6.590025424957275
2025-12-09 12:01:40.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0009751708599862451 Training loss: 6.752316474914551
2025-12-09 12:01:40.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0009750738324585098 Training loss: 6.7444329261779785
2025-12-09 12:01:41.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0009749766205638952 Training loss: 7.822662353515625
2025-12-09 12:01:41.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.0009748792243401273 Training loss: 7.005153656005859
2025-12-09 12:01:42.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0009747816438250037 Training loss: 7.184478282928467
2025-12-09 12:01:42.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0009746838790563935 Training loss: 7.12422513961792
2025-12-09 12:01:42.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.000974585930072237 Training loss: 6.999091625213623
2025-12-09 12:01:43.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0009744877969105468 Training loss: 6.7892069816589355
2025-12-09 12:01:43.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0009743894796094062 Training loss: 7.163353443145752
2025-12-09 12:01:44.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.0009742909782069701 Training loss: 6.972917079925537
2025-12-09 12:01:44.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0009741922927414651 Training loss: 7.169806480407715
2025-12-09 12:01:44.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0009740934232511893 Training loss: 6.991377830505371
2025-12-09 12:01:45.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0009739943697745117 Training loss: 6.731271266937256
2025-12-09 12:01:45.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0009738951323498732 Training loss: 7.369096279144287
2025-12-09 12:01:46.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0009737957110157858 Training loss: 6.808143615722656
2025-12-09 12:01:46.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0009736961058108331 Training loss: 7.792073726654053
2025-12-09 12:01:46.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0009735963167736698 Training loss: 7.203647136688232
2025-12-09 12:01:47.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.0009734963439430222 Training loss: 6.86553430557251
2025-12-09 12:01:47.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0009733961873576877 Training loss: 6.59531307220459
2025-12-09 12:01:47.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0009732958470565352 Training loss: 6.700313568115234
2025-12-09 12:01:48.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0009731953230785049 Training loss: 6.996798515319824
2025-12-09 12:01:48.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0009730946154626079 Training loss: 7.446941375732422
2025-12-09 12:01:49.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.000972993724247927 Training loss: 7.21003532409668
2025-12-09 12:01:49.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0009728926494736163 Training loss: 6.767492771148682
2025-12-09 12:01:49.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0009727913911789008 Training loss: 7.021949291229248
2025-12-09 12:01:50.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0009726899494030768 Training loss: 7.030025005340576
2025-12-09 12:01:50.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0009725883241855118 Training loss: 7.205124855041504
2025-12-09 12:01:51.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0009724865155656448 Training loss: 7.5990190505981445
2025-12-09 12:01:51.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0009723845235829856 Training loss: 7.254859447479248
2025-12-09 12:01:51.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0009722823482771155 Training loss: 6.989003658294678
2025-12-09 12:01:52.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0009721799896876864 Training loss: 6.755059719085693
2025-12-09 12:01:52.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0009720774478544219 Training loss: 7.709157943725586
2025-12-09 12:01:53.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0009719747228171163 Training loss: 6.866214275360107
2025-12-09 12:01:53.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0009718718146156355 Training loss: 7.214068412780762
2025-12-09 12:01:53.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0009717687232899158 Training loss: 7.0659308433532715
2025-12-09 12:01:54.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0009716654488799652 Training loss: 7.096471309661865
2025-12-09 12:01:54.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0009715619914258623 Training loss: 6.842726707458496
2025-12-09 12:01:54.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.000971458350967757 Training loss: 6.961872100830078
2025-12-09 12:01:55.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0009713545275458703 Training loss: 7.184791564941406
2025-12-09 12:01:55.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0009712505212004937 Training loss: 6.8991570472717285
2025-12-09 12:01:56.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0009711463319719904 Training loss: 7.100872039794922
2025-12-09 12:01:56.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0009710419599007938 Training loss: 6.693134307861328
2025-12-09 12:01:56.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0009709374050274089 Training loss: 7.292740345001221
2025-12-09 12:01:57.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0009708326673924114 Training loss: 7.193123817443848
2025-12-09 12:01:57.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0009707277470364482 Training loss: 6.777451992034912
2025-12-09 12:01:58.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0009706226440002363 Training loss: 7.426961421966553
2025-12-09 12:01:58.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0009705173583245644 Training loss: 6.705275535583496
2025-12-09 12:01:58.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0009704118900502918 Training loss: 7.0695600509643555
2025-12-09 12:01:59.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0009703062392183488 Training loss: 6.947417736053467
2025-12-09 12:01:59.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0009702004058697362 Training loss: 6.59307861328125
2025-12-09 12:02:00.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0009700943900455262 Training loss: 7.171966075897217
2025-12-09 12:02:00.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0009699881917868609 Training loss: 7.217903137207031
2025-12-09 12:02:00.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.0009698818111349544 Training loss: 6.7108893394470215
2025-12-09 12:02:01.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0009697752481310904 Training loss: 6.992037296295166
2025-12-09 12:02:01.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0009696685028166244 Training loss: 6.880644798278809
2025-12-09 12:02:01.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.000969561575232982 Training loss: 6.998692512512207
2025-12-09 12:02:02.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0009694544654216595 Training loss: 6.927560329437256
2025-12-09 12:02:02.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0009693471734242243 Training loss: 6.726859092712402
2025-12-09 12:02:03.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0009692396992823144 Training loss: 7.38444185256958
2025-12-09 12:02:03.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0009691320430376385 Training loss: 6.681159019470215
2025-12-09 12:02:03.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0009690242047319755 Training loss: 6.802889823913574
2025-12-09 12:02:04.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.0009689161844071756 Training loss: 7.2704668045043945
2025-12-09 12:02:04.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0009688079821051594 Training loss: 6.648741722106934
2025-12-09 12:02:05.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.0009686995978679181 Training loss: 6.502857208251953
2025-12-09 12:02:05.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0009685910317375133 Training loss: 6.9080305099487305
2025-12-09 12:02:05.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0009684822837560776 Training loss: 7.173154354095459
2025-12-09 12:02:06.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0009683733539658139 Training loss: 6.9802045822143555
2025-12-09 12:02:06.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0009682642424089958 Training loss: 7.405025959014893
2025-12-09 12:02:07.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.0009681549491279673 Training loss: 7.415944576263428
2025-12-09 12:02:07.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.000968045474165143 Training loss: 7.042191982269287
2025-12-09 12:02:07.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0009679358175630081 Training loss: 7.11625337600708
2025-12-09 12:02:08.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.000967825979364118 Training loss: 6.093024730682373
2025-12-09 12:02:08.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.0009677159596110987 Training loss: 6.850123405456543
2025-12-09 12:02:08.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.000967605758346647 Training loss: 7.003256320953369
2025-12-09 12:02:09.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0009674953756135297 Training loss: 6.875373363494873
2025-12-09 12:02:09.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0009673848114545843 Training loss: 7.018247127532959
2025-12-09 12:02:10.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0009672740659127184 Training loss: 7.981795310974121
2025-12-09 12:02:10.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0009671631390309102 Training loss: 7.9251017570495605
2025-12-09 12:02:10.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0009670520308522084 Training loss: 7.019673824310303
2025-12-09 12:02:11.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0009669407414197318 Training loss: 7.003682613372803
2025-12-09 12:02:11.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0009668292707766699 Training loss: 6.900816917419434
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 3.27 GiB is free. Including non-PyTorch memory, this process has 90.93 GiB memory in use. Of the allocated memory 89.96 GiB is allocated by PyTorch, and 216.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   1%|          | 51/10000 [00:00<00:19, 509.30it/s]Tokenizing texts:   1%|▏         | 136/10000 [00:00<00:15, 633.47it/s]Tokenizing texts:   2%|▏         | 205/10000 [00:00<00:15, 649.59it/s]Tokenizing texts:   3%|▎         | 273/10000 [00:00<00:14, 660.49it/s]Tokenizing texts:   3%|▎         | 339/10000 [00:00<00:15, 626.15it/s]Tokenizing texts:   4%|▍         | 402/10000 [00:00<00:15, 615.04it/s]Tokenizing texts:   5%|▍         | 464/10000 [00:00<00:16, 585.33it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 584.57it/s]Tokenizing texts:   6%|▌         | 598/10000 [00:00<00:15, 611.64it/s]Tokenizing texts:   7%|▋         | 660/10000 [00:01<00:15, 601.72it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:01<00:16, 576.15it/s]Tokenizing texts:   8%|▊         | 781/10000 [00:01<00:15, 576.72it/s]Tokenizing texts:   8%|▊         | 843/10000 [00:01<00:15, 585.78it/s]Tokenizing texts:   9%|▉         | 911/10000 [00:01<00:14, 610.13it/s]Tokenizing texts:  10%|▉         | 994/10000 [00:01<00:13, 673.99it/s]Tokenizing texts:  11%|█         | 1062/10000 [00:01<00:13, 645.40it/s]Tokenizing texts:  11%|█▏        | 1128/10000 [00:01<00:13, 634.77it/s]Tokenizing texts:  12%|█▏        | 1192/10000 [00:01<00:14, 617.89it/s]Tokenizing texts:  13%|█▎        | 1255/10000 [00:02<00:14, 585.77it/s]Tokenizing texts:  13%|█▎        | 1322/10000 [00:02<00:14, 609.04it/s]Tokenizing texts:  14%|█▍        | 1384/10000 [00:02<00:14, 608.52it/s]Tokenizing texts:  15%|█▍        | 1453/10000 [00:02<00:13, 630.80it/s]Tokenizing texts:  15%|█▌        | 1528/10000 [00:02<00:12, 656.64it/s]Tokenizing texts:  16%|█▌        | 1603/10000 [00:02<00:12, 671.76it/s]Tokenizing texts:  17%|█▋        | 1671/10000 [00:02<00:13, 617.75it/s]Tokenizing texts:  17%|█▋        | 1743/10000 [00:02<00:12, 645.81it/s]Tokenizing texts:  18%|█▊        | 1810/10000 [00:02<00:12, 650.14it/s]Tokenizing texts:  19%|█▉        | 1876/10000 [00:03<00:13, 599.55it/s]Tokenizing texts:  19%|█▉        | 1941/10000 [00:03<00:13, 612.84it/s]Tokenizing texts:  20%|██        | 2004/10000 [00:03<00:13, 585.09it/s]Tokenizing texts:  21%|██        | 2072/10000 [00:03<00:13, 604.33it/s]Tokenizing texts:  21%|██▏       | 2134/10000 [00:03<00:13, 567.14it/s]Tokenizing texts:  22%|██▏       | 2193/10000 [00:03<00:13, 573.05it/s]Tokenizing texts:  23%|██▎       | 2272/10000 [00:03<00:12, 632.06it/s]Tokenizing texts:  23%|██▎       | 2337/10000 [00:03<00:13, 589.42it/s]Tokenizing texts:  24%|██▍       | 2420/10000 [00:03<00:11, 648.82it/s]Tokenizing texts:  25%|██▍       | 2487/10000 [00:04<00:11, 637.27it/s]Tokenizing texts:  26%|██▌       | 2554/10000 [00:04<00:11, 645.78it/s]Tokenizing texts:  26%|██▌       | 2620/10000 [00:04<00:12, 578.66it/s]Tokenizing texts:  27%|██▋       | 2680/10000 [00:04<00:12, 581.08it/s]Tokenizing texts:  27%|██▋       | 2740/10000 [00:04<00:12, 583.32it/s]Tokenizing texts:  28%|██▊       | 2800/10000 [00:04<00:13, 536.56it/s]Tokenizing texts:  29%|██▊       | 2861/10000 [00:04<00:12, 554.42it/s]Tokenizing texts:  29%|██▉       | 2936/10000 [00:04<00:11, 606.05it/s]Tokenizing texts:  30%|███       | 3000/10000 [00:04<00:11, 614.48it/s]Tokenizing texts:  31%|███       | 3063/10000 [00:05<00:11, 606.07it/s]Tokenizing texts:  31%|███▏      | 3134/10000 [00:05<00:10, 635.26it/s]Tokenizing texts:  32%|███▏      | 3199/10000 [00:05<00:10, 629.31it/s]Tokenizing texts:  33%|███▎      | 3267/10000 [00:05<00:10, 643.93it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:05<00:10, 644.64it/s]Tokenizing texts:  34%|███▍      | 3398/10000 [00:05<00:10, 643.27it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:05<00:09, 702.91it/s]Tokenizing texts:  36%|███▌      | 3558/10000 [00:05<00:09, 648.60it/s]Tokenizing texts:  36%|███▌      | 3624/10000 [00:05<00:09, 649.00it/s]Tokenizing texts:  37%|███▋      | 3690/10000 [00:05<00:09, 642.86it/s]Tokenizing texts:  38%|███▊      | 3756/10000 [00:06<00:10, 616.25it/s]Tokenizing texts:  38%|███▊      | 3819/10000 [00:06<00:10, 598.67it/s]Tokenizing texts:  39%|███▉      | 3887/10000 [00:06<00:09, 620.09it/s]Tokenizing texts:  40%|███▉      | 3950/10000 [00:06<00:10, 601.72it/s]Tokenizing texts:  40%|████      | 4011/10000 [00:06<00:10, 597.71it/s]Tokenizing texts:  41%|████      | 4089/10000 [00:06<00:09, 642.20it/s]Tokenizing texts:  42%|████▏     | 4159/10000 [00:06<00:08, 658.70it/s]Tokenizing texts:  42%|████▏     | 4227/10000 [00:06<00:08, 663.40it/s]Tokenizing texts:  43%|████▎     | 4294/10000 [00:06<00:08, 640.98it/s]Tokenizing texts:  44%|████▎     | 4371/10000 [00:07<00:08, 677.47it/s]Tokenizing texts:  44%|████▍     | 4445/10000 [00:07<00:08, 671.00it/s]Tokenizing texts:  45%|████▌     | 4513/10000 [00:07<00:08, 657.00it/s]Tokenizing texts:  46%|████▌     | 4579/10000 [00:07<00:08, 656.19it/s]Tokenizing texts:  46%|████▋     | 4648/10000 [00:07<00:08, 665.01it/s]Tokenizing texts:  47%|████▋     | 4733/10000 [00:07<00:07, 713.02it/s]Tokenizing texts:  48%|████▊     | 4805/10000 [00:07<00:07, 661.57it/s]Tokenizing texts:  49%|████▉     | 4877/10000 [00:07<00:07, 677.18it/s]Tokenizing texts:  50%|████▉     | 4950/10000 [00:07<00:07, 692.07it/s]Tokenizing texts:  50%|█████     | 5028/10000 [00:07<00:06, 716.98it/s]Tokenizing texts:  51%|█████     | 5101/10000 [00:08<00:07, 657.90it/s]Tokenizing texts:  52%|█████▏    | 5169/10000 [00:08<00:07, 639.08it/s]Tokenizing texts:  52%|█████▏    | 5234/10000 [00:08<00:07, 636.29it/s]Tokenizing texts:  53%|█████▎    | 5305/10000 [00:08<00:07, 655.47it/s]Tokenizing texts:  54%|█████▍    | 5390/10000 [00:08<00:06, 706.05it/s]Tokenizing texts:  55%|█████▍    | 5462/10000 [00:08<00:06, 702.43it/s]Tokenizing texts:  55%|█████▌    | 5533/10000 [00:08<00:07, 637.80it/s]Tokenizing texts:  56%|█████▌    | 5601/10000 [00:08<00:06, 647.16it/s]Tokenizing texts:  57%|█████▋    | 5689/10000 [00:08<00:06, 710.05it/s]Tokenizing texts:  58%|█████▊    | 5762/10000 [00:09<00:06, 671.59it/s]Tokenizing texts:  58%|█████▊    | 5831/10000 [00:09<00:06, 600.08it/s]Tokenizing texts:  59%|█████▉    | 5901/10000 [00:09<00:06, 625.00it/s]Tokenizing texts:  60%|█████▉    | 5966/10000 [00:09<00:06, 628.30it/s]Tokenizing texts:  60%|██████    | 6031/10000 [00:09<00:06, 626.83it/s]Tokenizing texts:  61%|██████    | 6101/10000 [00:09<00:06, 646.61it/s]Tokenizing texts:  62%|██████▏   | 6177/10000 [00:09<00:05, 672.03it/s]Tokenizing texts:  62%|██████▏   | 6245/10000 [00:09<00:05, 653.12it/s]Tokenizing texts:  63%|██████▎   | 6311/10000 [00:09<00:05, 639.39it/s]Tokenizing texts:  64%|██████▍   | 6376/10000 [00:10<00:05, 626.91it/s]Tokenizing texts:  64%|██████▍   | 6442/10000 [00:10<00:05, 634.95it/s]Tokenizing texts:  65%|██████▌   | 6506/10000 [00:10<00:05, 606.18it/s]Tokenizing texts:  66%|██████▌   | 6588/10000 [00:10<00:05, 666.19it/s]Tokenizing texts:  67%|██████▋   | 6656/10000 [00:10<00:05, 644.82it/s]Tokenizing texts:  67%|██████▋   | 6722/10000 [00:10<00:05, 645.87it/s]Tokenizing texts:  68%|██████▊   | 6796/10000 [00:10<00:04, 671.28it/s]Tokenizing texts:  69%|██████▊   | 6869/10000 [00:10<00:04, 686.00it/s]Tokenizing texts:  69%|██████▉   | 6938/10000 [00:10<00:04, 619.87it/s]Tokenizing texts:  70%|███████   | 7002/10000 [00:11<00:04, 619.41it/s]Tokenizing texts:  71%|███████   | 7065/10000 [00:11<00:05, 577.30it/s]Tokenizing texts:  71%|███████▏  | 7128/10000 [00:11<00:04, 590.00it/s]Tokenizing texts:  72%|███████▏  | 7192/10000 [00:11<00:04, 599.40it/s]Tokenizing texts:  73%|███████▎  | 7255/10000 [00:11<00:04, 607.87it/s]Tokenizing texts:  73%|███████▎  | 7336/10000 [00:11<00:04, 664.89it/s]Tokenizing texts:  74%|███████▍  | 7404/10000 [00:11<00:03, 653.02it/s]Tokenizing texts:  75%|███████▍  | 7470/10000 [00:11<00:04, 592.75it/s]Tokenizing texts:  75%|███████▌  | 7545/10000 [00:11<00:03, 634.18it/s]Tokenizing texts:  76%|███████▌  | 7618/10000 [00:12<00:03, 655.50it/s]Tokenizing texts:  77%|███████▋  | 7695/10000 [00:12<00:03, 687.29it/s]Tokenizing texts:  78%|███████▊  | 7765/10000 [00:12<00:03, 639.34it/s]Tokenizing texts:  78%|███████▊  | 7831/10000 [00:12<00:03, 627.84it/s]Tokenizing texts:  79%|███████▉  | 7895/10000 [00:12<00:03, 621.76it/s]Tokenizing texts:  80%|███████▉  | 7975/10000 [00:12<00:03, 669.04it/s]Tokenizing texts:  80%|████████  | 8043/10000 [00:12<00:03, 643.50it/s]Tokenizing texts:  81%|████████  | 8108/10000 [00:12<00:03, 591.27it/s]Tokenizing texts:  82%|████████▏ | 8171/10000 [00:12<00:03, 598.22it/s]Tokenizing texts:  82%|████████▏ | 8232/10000 [00:13<00:03, 585.52it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:13<00:02, 614.10it/s]Tokenizing texts:  84%|████████▎ | 8364/10000 [00:13<00:02, 615.96it/s]Tokenizing texts:  84%|████████▍ | 8427/10000 [00:13<00:02, 599.44it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:13<00:02, 647.72it/s]Tokenizing texts:  86%|████████▌ | 8570/10000 [00:13<00:02, 636.51it/s]Tokenizing texts:  86%|████████▋ | 8648/10000 [00:13<00:01, 677.37it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:13<00:01, 679.15it/s]Tokenizing texts:  88%|████████▊ | 8794/10000 [00:13<00:01, 672.33it/s]Tokenizing texts:  89%|████████▊ | 8862/10000 [00:14<00:01, 610.68it/s]Tokenizing texts:  89%|████████▉ | 8945/10000 [00:14<00:01, 669.88it/s]Tokenizing texts:  90%|█████████ | 9014/10000 [00:14<00:01, 636.77it/s]Tokenizing texts:  91%|█████████ | 9080/10000 [00:14<00:01, 637.48it/s]Tokenizing texts:  92%|█████████▏| 9152/10000 [00:14<00:01, 660.35it/s]Tokenizing texts:  92%|█████████▏| 9225/10000 [00:14<00:01, 666.07it/s]Tokenizing texts:  93%|█████████▎| 9293/10000 [00:14<00:01, 651.41it/s]Tokenizing texts:  94%|█████████▎| 9359/10000 [00:14<00:01, 599.47it/s]Tokenizing texts:  94%|█████████▍| 9422/10000 [00:14<00:00, 606.88it/s]Tokenizing texts:  95%|█████████▍| 9484/10000 [00:15<00:00, 559.27it/s]Tokenizing texts:  96%|█████████▌| 9557/10000 [00:15<00:00, 604.48it/s]Tokenizing texts:  96%|█████████▌| 9619/10000 [00:15<00:00, 594.44it/s]Tokenizing texts:  97%|█████████▋| 9682/10000 [00:15<00:00, 602.96it/s]Tokenizing texts:  97%|█████████▋| 9743/10000 [00:15<00:00, 589.55it/s]Tokenizing texts:  98%|█████████▊| 9803/10000 [00:15<00:00, 589.81it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:15<00:00, 614.83it/s]Tokenizing texts:  99%|█████████▉| 9948/10000 [00:15<00:00, 656.80it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:15<00:00, 631.44it/s]
2025-12-09 12:03:03.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 12.168539047241211
2025-12-09 12:03:04.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 12.16819953918457
2025-12-09 12:03:04.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 12.037919044494629
2025-12-09 12:03:04.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 11.866774559020996
2025-12-09 12:03:05.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 11.132781028747559
2025-12-09 12:03:05.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 10.501056671142578
2025-12-09 12:03:06.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 10.909903526306152
2025-12-09 12:03:06.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 11.324875831604004
2025-12-09 12:03:06.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 10.779099464416504
2025-12-09 12:03:07.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 10.287450790405273
2025-12-09 12:03:07.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 9.537697792053223
2025-12-09 12:03:07.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 9.169065475463867
2025-12-09 12:03:08.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 8.947460174560547
2025-12-09 12:03:08.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 9.165604591369629
2025-12-09 12:03:09.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 8.727946281433105
2025-12-09 12:03:09.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 8.566184043884277
2025-12-09 12:03:09.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 8.552005767822266
2025-12-09 12:03:10.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 8.304247856140137
2025-12-09 12:03:10.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 8.595519065856934
2025-12-09 12:03:11.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 8.630112648010254
2025-12-09 12:03:11.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 8.826141357421875
2025-12-09 12:03:11.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 8.762600898742676
2025-12-09 12:03:12.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 8.71163558959961
2025-12-09 12:03:12.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 8.954106330871582
2025-12-09 12:03:12.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 9.006125450134277
2025-12-09 12:03:13.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 9.220657348632812
2025-12-09 12:03:13.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 8.668848991394043
2025-12-09 12:03:14.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 9.196259498596191
2025-12-09 12:03:14.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 9.536850929260254
2025-12-09 12:03:14.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 8.786057472229004
2025-12-09 12:03:15.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 9.319596290588379
2025-12-09 12:03:15.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 8.588491439819336
2025-12-09 12:03:16.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 8.504171371459961
2025-12-09 12:03:16.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 8.43568229675293
2025-12-09 12:03:16.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 8.167017936706543
2025-12-09 12:03:17.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 8.625432014465332
2025-12-09 12:03:17.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 8.983650207519531
2025-12-09 12:03:17.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 8.433639526367188
2025-12-09 12:03:18.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 9.547548294067383
2025-12-09 12:03:18.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 9.040298461914062
2025-12-09 12:03:19.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 8.52469253540039
2025-12-09 12:03:19.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 9.107250213623047
2025-12-09 12:03:19.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 7.9643402099609375
2025-12-09 12:03:20.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 8.623324394226074
2025-12-09 12:03:20.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 8.35185718536377
2025-12-09 12:03:21.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 10.290319442749023
2025-12-09 12:03:21.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 8.882758140563965
2025-12-09 12:03:21.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 8.739002227783203
2025-12-09 12:03:22.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 9.291352272033691
2025-12-09 12:03:22.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 9.047734260559082
2025-12-09 12:03:22.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 8.590036392211914
2025-12-09 12:03:23.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 8.729555130004883
2025-12-09 12:03:23.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 8.38708209991455
2025-12-09 12:03:24.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 9.431687355041504
2025-12-09 12:03:24.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 8.596596717834473
2025-12-09 12:03:24.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 8.54660415649414
2025-12-09 12:03:25.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 8.927042961120605
2025-12-09 12:03:25.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 8.542497634887695
2025-12-09 12:03:26.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 8.246445655822754
2025-12-09 12:03:26.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 8.93012523651123
2025-12-09 12:03:26.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 9.44402027130127
2025-12-09 12:03:27.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 9.078843116760254
2025-12-09 12:03:27.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 8.749868392944336
2025-12-09 12:03:27.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 9.407398223876953
2025-12-09 12:03:28.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 8.895334243774414
2025-12-09 12:03:28.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 8.77794361114502
2025-12-09 12:03:29.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 8.965127944946289
2025-12-09 12:03:29.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 8.996365547180176
2025-12-09 12:03:29.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 8.769930839538574
2025-12-09 12:03:30.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 9.177145004272461
2025-12-09 12:03:30.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 8.572196960449219
2025-12-09 12:03:31.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 8.257353782653809
2025-12-09 12:03:31.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 8.924062728881836
2025-12-09 12:03:31.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 8.721534729003906
2025-12-09 12:03:32.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 7.9420928955078125
2025-12-09 12:03:32.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 8.19239330291748
2025-12-09 12:03:32.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 8.432991027832031
2025-12-09 12:03:33.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 8.284586906433105
2025-12-09 12:03:33.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 8.519695281982422
2025-12-09 12:03:34.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 8.425789833068848
2025-12-09 12:03:34.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 8.494449615478516
2025-12-09 12:03:34.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 8.838717460632324
2025-12-09 12:03:35.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 8.303451538085938
2025-12-09 12:03:35.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 8.70512580871582
2025-12-09 12:03:36.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 8.345458030700684
2025-12-09 12:03:36.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 8.493965148925781
2025-12-09 12:03:36.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 8.791851043701172
2025-12-09 12:03:37.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 8.378778457641602
2025-12-09 12:03:37.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 7.780649662017822
2025-12-09 12:03:37.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 8.828789710998535
2025-12-09 12:03:38.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 8.034598350524902
2025-12-09 12:03:38.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 9.247048377990723
2025-12-09 12:03:39.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 8.231432914733887
2025-12-09 12:03:39.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 8.33799934387207
2025-12-09 12:03:39.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 9.215999603271484
2025-12-09 12:03:40.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 7.661698341369629
2025-12-09 12:03:40.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 8.48477554321289
2025-12-09 12:03:41.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 8.314648628234863
2025-12-09 12:03:41.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 8.27014446258545
2025-12-09 12:03:41.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 8.441388130187988
2025-12-09 12:03:42.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999997089396424 Training loss: 8.920764923095703
2025-12-09 12:03:42.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002999998835758683 Training loss: 8.430259704589844
2025-12-09 12:03:43.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029999973804574606 Training loss: 8.438392639160156
2025-12-09 12:03:43.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0029999953430365394 Training loss: 8.107978820800781
2025-12-09 12:03:43.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.002999992723496711 Training loss: 8.731163024902344
2025-12-09 12:03:44.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.002999989521838991 Training loss: 8.507635116577148
2025-12-09 12:03:44.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029999857380646226 Training loss: 8.735038757324219
2025-12-09 12:03:44.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.002999981372175074 Training loss: 8.25521469116211
2025-12-09 12:03:45.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0029999764241720396 Training loss: 8.368293762207031
2025-12-09 12:03:45.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0029999708940574394 Training loss: 8.17463207244873
2025-12-09 12:03:46.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0029999647818334195 Training loss: 8.11275577545166
2025-12-09 12:03:46.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.002999958087502352 Training loss: 8.225893020629883
2025-12-09 12:03:46.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029999508110668356 Training loss: 8.092479705810547
2025-12-09 12:03:47.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029999429525296934 Training loss: 8.273438453674316
2025-12-09 12:03:47.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0029999345118939752 Training loss: 7.915706157684326
2025-12-09 12:03:48.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0029999254891629563 Training loss: 8.77283000946045
2025-12-09 12:03:48.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.002999915884340139 Training loss: 8.177062034606934
2025-12-09 12:03:48.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0029999056974292504 Training loss: 8.396835327148438
2025-12-09 12:03:49.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029998949284342435 Training loss: 9.833806037902832
2025-12-09 12:03:49.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.002999883577359298 Training loss: 8.897649765014648
2025-12-09 12:03:49.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.002999871644208819 Training loss: 8.229412078857422
2025-12-09 12:03:50.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002999859128987437 Training loss: 8.146883010864258
2025-12-09 12:03:50.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00299984603170001 Training loss: 7.996079921722412
2025-12-09 12:03:51.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0029998323523516197 Training loss: 8.525043487548828
2025-12-09 12:03:51.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029998180909475754 Training loss: 8.235027313232422
2025-12-09 12:03:51.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.002999803247493411 Training loss: 7.9242634773254395
2025-12-09 12:03:52.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.002999787821994888 Training loss: 8.285581588745117
2025-12-09 12:03:52.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0029997718144579915 Training loss: 8.411406517028809
2025-12-09 12:03:53.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0029997552248889354 Training loss: 8.474655151367188
2025-12-09 12:03:53.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.002999738053294156 Training loss: 8.199769020080566
2025-12-09 12:03:53.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0029997202996803183 Training loss: 8.282227516174316
2025-12-09 12:03:54.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002999701964054312 Training loss: 8.583748817443848
2025-12-09 12:03:54.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0029996830464232523 Training loss: 7.872481822967529
2025-12-09 12:03:54.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0029996635467944813 Training loss: 8.251551628112793
2025-12-09 12:03:55.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0029996434651755662 Training loss: 8.194286346435547
2025-12-09 12:03:55.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0029996228015743004 Training loss: 7.8773908615112305
2025-12-09 12:03:56.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.002999601555998703 Training loss: 8.045247077941895
2025-12-09 12:03:56.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.002999579728457019 Training loss: 8.084756851196289
2025-12-09 12:03:56.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.002999557318957719 Training loss: 8.172513961791992
2025-12-09 12:03:57.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0029995343275095003 Training loss: 8.150802612304688
2025-12-09 12:03:57.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0029995107541212845 Training loss: 7.973457336425781
2025-12-09 12:03:58.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.002999486598802221 Training loss: 8.001011848449707
2025-12-09 12:03:58.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0029994618615616832 Training loss: 7.8259172439575195
2025-12-09 12:03:58.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.002999436542409272 Training loss: 7.846826553344727
2025-12-09 12:03:59.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0029994106413548122 Training loss: 8.879945755004883
2025-12-09 12:03:59.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0029993841584083558 Training loss: 8.075441360473633
2025-12-09 12:04:00.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.002999357093580181 Training loss: 8.641799926757812
2025-12-09 12:04:00.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0029993294468807904 Training loss: 7.669943332672119
2025-12-09 12:04:00.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0029993012183209137 Training loss: 8.171186447143555
2025-12-09 12:04:01.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0029992724079115052 Training loss: 7.914414405822754
2025-12-09 12:04:01.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.002999243015663746 Training loss: 8.261052131652832
2025-12-09 12:04:01.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0029992130415890427 Training loss: 7.72782039642334
2025-12-09 12:04:02.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.002999182485699028 Training loss: 7.434980869293213
2025-12-09 12:04:02.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0029991513480055595 Training loss: 8.562345504760742
2025-12-09 12:04:03.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002999119628520721 Training loss: 7.908749580383301
2025-12-09 12:04:03.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0029990873272568224 Training loss: 7.923461437225342
2025-12-09 12:04:03.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0029990544442264 Training loss: 7.739980220794678
2025-12-09 12:04:04.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.002999020979442214 Training loss: 8.439451217651367
2025-12-09 12:04:04.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.002998986932917252 Training loss: 7.963468551635742
2025-12-09 12:04:05.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.002998952304664726 Training loss: 7.8594889640808105
2025-12-09 12:04:05.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.002998917094698076 Training loss: 7.941227436065674
2025-12-09 12:04:05.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.002998881303030965 Training loss: 8.210864067077637
2025-12-09 12:04:06.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0029988449296772836 Training loss: 9.012285232543945
2025-12-09 12:04:06.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.002998807974651147 Training loss: 8.012402534484863
2025-12-09 12:04:06.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0029987704379668976 Training loss: 8.495115280151367
2025-12-09 12:04:07.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002998732319639102 Training loss: 8.143921852111816
2025-12-09 12:04:07.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0029986936196825537 Training loss: 7.936934471130371
2025-12-09 12:04:08.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.002998654338112271 Training loss: 8.341075897216797
2025-12-09 12:04:08.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0029986144749434987 Training loss: 7.744725227355957
2025-12-09 12:04:08.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0029985740301917063 Training loss: 7.9021830558776855
2025-12-09 12:04:09.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00299853300387259 Training loss: 7.797260284423828
2025-12-09 12:04:09.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0029984913960020712 Training loss: 8.445213317871094
2025-12-09 12:04:10.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0029984492065962976 Training loss: 8.073799133300781
2025-12-09 12:04:10.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0029984064356716415 Training loss: 7.8042311668396
2025-12-09 12:04:10.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0029983630832447015 Training loss: 7.514059066772461
2025-12-09 12:04:11.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002998319149332302 Training loss: 7.586207866668701
2025-12-09 12:04:11.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0029982746339514933 Training loss: 7.7270402908325195
2025-12-09 12:04:12.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0029982295371195496 Training loss: 7.470876693725586
2025-12-09 12:04:12.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.002998183858853974 Training loss: 7.872135162353516
2025-12-09 12:04:12.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0029981375991724917 Training loss: 7.962465763092041
2025-12-09 12:04:13.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0029980907580930563 Training loss: 8.017176628112793
2025-12-09 12:04:13.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.002998043335633845 Training loss: 8.049196243286133
2025-12-09 12:04:13.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002997995331813262 Training loss: 8.615678787231445
2025-12-09 12:04:14.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.002997946746649937 Training loss: 7.981034755706787
2025-12-09 12:04:14.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0029978975801627245 Training loss: 7.689546585083008
2025-12-09 12:04:15.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0029978478323707046 Training loss: 7.913559913635254
2025-12-09 12:04:15.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0029977975032931844 Training loss: 7.723172664642334
2025-12-09 12:04:15.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.002997746592949695 Training loss: 7.924260139465332
2025-12-09 12:04:16.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.002997695101359994 Training loss: 8.46692943572998
2025-12-09 12:04:16.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.002997643028544064 Training loss: 8.002805709838867
2025-12-09 12:04:17.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0029975903745221143 Training loss: 7.722670555114746
2025-12-09 12:04:17.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.002997537139314578 Training loss: 7.953119277954102
2025-12-09 12:04:17.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0029974833229421145 Training loss: 8.118199348449707
2025-12-09 12:04:18.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002997428925425609 Training loss: 7.675201416015625
2025-12-09 12:04:18.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0029973739467861736 Training loss: 7.726465702056885
2025-12-09 12:04:18.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.002997318387045142 Training loss: 8.373549461364746
2025-12-09 12:04:19.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0029972622462240777 Training loss: 8.099431037902832
2025-12-09 12:04:19.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0029972055243447666 Training loss: 8.293815612792969
2025-12-09 12:04:20.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.002997148221429223 Training loss: 7.552394390106201
2025-12-09 12:04:20.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.002997090337499683 Training loss: 7.725776672363281
2025-12-09 12:04:20.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0029970318725786116 Training loss: 8.005001068115234
2025-12-09 12:04:21.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0029969728266886976 Training loss: 7.414003849029541
2025-12-09 12:04:21.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0029969131998528555 Training loss: 8.69561767578125
2025-12-09 12:04:22.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0029968529920942253 Training loss: 7.789858341217041
2025-12-09 12:04:22.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0029967922034361727 Training loss: 8.108770370483398
2025-12-09 12:04:22.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.002996730833902288 Training loss: 8.030241012573242
2025-12-09 12:04:23.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.002996668883516388 Training loss: 7.631700038909912
2025-12-09 12:04:23.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002996606352302514 Training loss: 8.18083381652832
2025-12-09 12:04:24.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.002996543240284934 Training loss: 7.555999755859375
2025-12-09 12:04:24.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0029964795474881397 Training loss: 7.8999738693237305
2025-12-09 12:04:24.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.002996415273936849 Training loss: 7.998547554016113
2025-12-09 12:04:25.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.002996350419656006 Training loss: 7.705105304718018
2025-12-09 12:04:25.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0029962849846707786 Training loss: 7.6308794021606445
2025-12-09 12:04:25.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0029962189690065613 Training loss: 8.414258003234863
2025-12-09 12:04:26.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.0029961523726889736 Training loss: 7.651503086090088
2025-12-09 12:04:26.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0029960851957438594 Training loss: 7.965803146362305
2025-12-09 12:04:27.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.00299601743819729 Training loss: 8.000645637512207
2025-12-09 12:04:27.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0029959491000755597 Training loss: 7.608913898468018
2025-12-09 12:04:27.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0029958801814051897 Training loss: 10.589278221130371
2025-12-09 12:04:28.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.002995810682212926 Training loss: 7.863589286804199
2025-12-09 12:04:28.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0029957406025257396 Training loss: 7.5029802322387695
2025-12-09 12:04:29.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.002995669942370827 Training loss: 8.384268760681152
2025-12-09 12:04:29.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0029955987017756106 Training loss: 8.2410306930542
2025-12-09 12:04:29.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0029955268807677375 Training loss: 7.86726188659668
2025-12-09 12:04:30.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.002995454479375079 Training loss: 8.017538070678711
2025-12-09 12:04:30.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0029953814976257337 Training loss: 7.823336601257324
2025-12-09 12:04:30.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.002995307935548024 Training loss: 8.36798095703125
2025-12-09 12:04:31.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.002995233793170498 Training loss: 7.569406509399414
2025-12-09 12:04:31.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0029951590705219284 Training loss: 7.785573959350586
2025-12-09 12:04:32.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0029950837676313144 Training loss: 7.390821933746338
2025-12-09 12:04:32.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0029950078845278794 Training loss: 7.918975830078125
2025-12-09 12:04:32.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0029949314212410717 Training loss: 7.939461708068848
2025-12-09 12:04:33.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0029948543778005655 Training loss: 7.736032485961914
2025-12-09 12:04:33.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00299477675423626 Training loss: 8.039152145385742
2025-12-09 12:04:34.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.002994698550578279 Training loss: 7.648166656494141
2025-12-09 12:04:34.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0029946197668569725 Training loss: 8.14023494720459
2025-12-09 12:04:34.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.002994540403102914 Training loss: 7.832481384277344
2025-12-09 12:04:35.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0029944604593469034 Training loss: 7.586228370666504
2025-12-09 12:04:35.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0029943799356199658 Training loss: 7.349318981170654
2025-12-09 12:04:36.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0029942988319533507 Training loss: 8.341889381408691
2025-12-09 12:04:36.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.002994217148378532 Training loss: 8.054177284240723
2025-12-09 12:04:36.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.002994134884927211 Training loss: 7.2953596115112305
2025-12-09 12:04:37.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.002994052041631311 Training loss: 7.475031852722168
2025-12-09 12:04:37.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0029939686185229825 Training loss: 7.728867530822754
2025-12-09 12:04:37.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0029938846156346006 Training loss: 7.805543422698975
2025-12-09 12:04:38.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.002993800032998765 Training loss: 7.852500915527344
2025-12-09 12:04:38.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.002993714870648301 Training loss: 8.021766662597656
2025-12-09 12:04:39.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0029936291286162577 Training loss: 7.51059627532959
2025-12-09 12:04:39.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00299354280693591 Training loss: 8.137961387634277
2025-12-09 12:04:39.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.002993455905640758 Training loss: 8.051043510437012
2025-12-09 12:04:40.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0029933684247645267 Training loss: 7.994422912597656
2025-12-09 12:04:40.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.002993280364341165 Training loss: 7.505424976348877
2025-12-09 12:04:41.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.002993191724404848 Training loss: 7.620665073394775
2025-12-09 12:04:41.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0029931025049899744 Training loss: 7.379814147949219
2025-12-09 12:04:41.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.002993012706131169 Training loss: 7.545896053314209
2025-12-09 12:04:42.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.002992922327863281 Training loss: 8.857083320617676
2025-12-09 12:04:42.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.002992831370221385 Training loss: 8.27722454071045
2025-12-09 12:04:42.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.002992739833240779 Training loss: 7.337457656860352
2025-12-09 12:04:43.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0029926477169569866 Training loss: 7.7116007804870605
2025-12-09 12:04:43.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0029925550214057565 Training loss: 8.003059387207031
2025-12-09 12:04:44.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.002992461746623063 Training loss: 7.640996932983398
2025-12-09 12:04:44.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0029923678926451034 Training loss: 7.822957992553711
2025-12-09 12:04:44.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.0029922734595083005 Training loss: 7.693032264709473
2025-12-09 12:04:45.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.002992178447249302 Training loss: 7.226501941680908
2025-12-09 12:04:45.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0029920828559049806 Training loss: 7.243802547454834
2025-12-09 12:04:46.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0029919866855124336 Training loss: 8.280625343322754
2025-12-09 12:04:46.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0029918899361089826 Training loss: 7.97306489944458
2025-12-09 12:04:46.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0029917926077321732 Training loss: 7.751775741577148
2025-12-09 12:04:47.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.002991694700419778 Training loss: 7.622353553771973
2025-12-09 12:04:47.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.002991596214209793 Training loss: 7.156530857086182
2025-12-09 12:04:48.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0029914971491404375 Training loss: 7.426365852355957
2025-12-09 12:04:48.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0029913975052501575 Training loss: 7.789122581481934
2025-12-09 12:04:48.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.002991297282577623 Training loss: 8.08400821685791
2025-12-09 12:04:49.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0029911964811617287 Training loss: 7.8817524909973145
2025-12-09 12:04:49.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0029910951010415927 Training loss: 7.866608619689941
2025-12-09 12:04:49.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0029909931422565593 Training loss: 8.159562110900879
2025-12-09 12:04:50.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0029908906048461965 Training loss: 8.992995262145996
2025-12-09 12:04:50.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.002990787488850297 Training loss: 8.103958129882812
2025-12-09 12:04:51.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0029906837943088787 Training loss: 7.6525115966796875
2025-12-09 12:04:51.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0029905795212621824 Training loss: 7.5293684005737305
2025-12-09 12:04:51.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.002990474669750676 Training loss: 7.2724609375
2025-12-09 12:04:52.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.002990369239815048 Training loss: 7.933836460113525
2025-12-09 12:04:52.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.002990263231496216 Training loss: 7.956457614898682
2025-12-09 12:04:53.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029901566448353183 Training loss: 8.278911590576172
2025-12-09 12:04:53.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029900494798737196 Training loss: 7.508092403411865
2025-12-09 12:04:53.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002989941736653009 Training loss: 7.729472637176514
2025-12-09 12:04:54.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.002989833415214999 Training loss: 7.539694786071777
2025-12-09 12:04:54.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0029897245156017267 Training loss: 7.680337905883789
2025-12-09 12:04:55.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.002989615037855454 Training loss: 8.135112762451172
2025-12-09 12:04:55.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0029895049820186682 Training loss: 7.6096673011779785
2025-12-09 12:04:55.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0029893943481340787 Training loss: 8.056486129760742
2025-12-09 12:04:56.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0029892831362446203 Training loss: 7.761537075042725
2025-12-09 12:04:56.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002989171346393453 Training loss: 7.46617317199707
2025-12-09 12:04:56.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00298905897862396 Training loss: 8.127654075622559
2025-12-09 12:04:57.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0029889460329797484 Training loss: 7.715699195861816
2025-12-09 12:04:57.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0029888325095046506 Training loss: 7.3484272956848145
2025-12-09 12:04:58.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0029887184082427226 Training loss: 7.831009387969971
2025-12-09 12:04:58.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002988603729238246 Training loss: 7.706952095031738
2025-12-09 12:04:58.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0029884884725357237 Training loss: 7.648706436157227
2025-12-09 12:04:59.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0029883726381798865 Training loss: 7.493594169616699
2025-12-09 12:04:59.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0029882562262156854 Training loss: 7.95682954788208
2025-12-09 12:05:00.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.002988139236688299 Training loss: 7.3254618644714355
2025-12-09 12:05:00.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0029880216696431287 Training loss: 8.394657135009766
2025-12-09 12:05:00.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0029879035251257993 Training loss: 7.174067497253418
2025-12-09 12:05:01.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.002987784803182161 Training loss: 7.8452324867248535
2025-12-09 12:05:01.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0029876655038582863 Training loss: 7.305748462677002
2025-12-09 12:05:01.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0029875456272004746 Training loss: 7.810461521148682
2025-12-09 12:05:02.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0029874251732552462 Training loss: 7.566861629486084
2025-12-09 12:05:02.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.002987304142069348 Training loss: 7.763727188110352
2025-12-09 12:05:03.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.002987182533689749 Training loss: 7.7091288566589355
2025-12-09 12:05:03.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0029870603481636443 Training loss: 7.600371360778809
2025-12-09 12:05:03.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0029869375855384505 Training loss: 7.76673698425293
2025-12-09 12:05:04.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0029868142458618096 Training loss: 7.615748405456543
2025-12-09 12:05:04.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0029866903291815875 Training loss: 7.688324451446533
2025-12-09 12:05:05.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.002986565835545874 Training loss: 7.422463893890381
2025-12-09 12:05:05.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0029864407650029823 Training loss: 7.844451904296875
2025-12-09 12:05:05.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00298631511760145 Training loss: 7.806727409362793
2025-12-09 12:05:06.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0029861888933900385 Training loss: 8.299793243408203
2025-12-09 12:05:06.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.002986062092417733 Training loss: 7.545504570007324
2025-12-09 12:05:07.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0029859347147337422 Training loss: 7.819276332855225
2025-12-09 12:05:07.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.002985806760387499 Training loss: 7.5803046226501465
2025-12-09 12:05:07.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00298567822942866 Training loss: 7.617293357849121
2025-12-09 12:05:08.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0029855491219071056 Training loss: 7.3759918212890625
2025-12-09 12:05:08.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00298541943787294 Training loss: 7.738917827606201
2025-12-09 12:05:08.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.002985289177376491 Training loss: 7.319075107574463
2025-12-09 12:05:09.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00298515834046831 Training loss: 7.487687587738037
2025-12-09 12:05:09.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.002985026927199172 Training loss: 8.2134370803833
2025-12-09 12:05:10.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0029848949376200767 Training loss: 6.907441139221191
2025-12-09 12:05:10.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0029847623717822462 Training loss: 7.682377338409424
2025-12-09 12:05:10.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0029846292297371264 Training loss: 7.180931568145752
2025-12-09 12:05:11.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.002984495511536388 Training loss: 7.741201877593994
2025-12-09 12:05:11.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0029843612172319235 Training loss: 7.836179256439209
2025-12-09 12:05:12.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.002984226346875851 Training loss: 7.574856758117676
2025-12-09 12:05:12.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0029840909005205093 Training loss: 8.042719841003418
2025-12-09 12:05:12.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.002983954878218464 Training loss: 7.548962593078613
2025-12-09 12:05:13.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.002983818280022502 Training loss: 7.876096248626709
2025-12-09 12:05:13.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0029836811059856354 Training loss: 7.249215602874756
2025-12-09 12:05:13.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0029835433561610975 Training loss: 7.597513675689697
2025-12-09 12:05:14.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0029834050306023468 Training loss: 8.307367324829102
2025-12-09 12:05:14.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0029832661293630646 Training loss: 7.978440761566162
2025-12-09 12:05:15.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.002983126652497156 Training loss: 7.653257369995117
2025-12-09 12:05:15.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.002982986600058749 Training loss: 7.481471061706543
2025-12-09 12:05:15.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0029828459721021965 Training loss: 7.179604530334473
2025-12-09 12:05:16.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.002982704768682071 Training loss: 7.169223308563232
2025-12-09 12:05:16.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0029825629898531728 Training loss: 7.490571975708008
2025-12-09 12:05:17.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.002982420635670523 Training loss: 7.63946533203125
2025-12-09 12:05:17.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.002982277706189366 Training loss: 7.1183671951293945
2025-12-09 12:05:17.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00298213420146517 Training loss: 8.199593544006348
2025-12-09 12:05:18.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0029819901215536273 Training loss: 7.291031360626221
2025-12-09 12:05:18.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.002981845466510651 Training loss: 7.349200248718262
2025-12-09 12:05:19.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0029817002363923804 Training loss: 7.419382572174072
2025-12-09 12:05:19.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.002981554431255176 Training loss: 7.87661600112915
2025-12-09 12:05:19.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.002981408051155621 Training loss: 7.564476013183594
2025-12-09 12:05:20.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.002981261096150524 Training loss: 7.582244873046875
2025-12-09 12:05:20.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.002981113566296915 Training loss: 7.408059120178223
2025-12-09 12:05:20.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0029809654616520464 Training loss: 8.064220428466797
2025-12-09 12:05:21.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.0029808167822733956 Training loss: 7.836197853088379
2025-12-09 12:05:21.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0029806675282186626 Training loss: 7.505942344665527
2025-12-09 12:05:22.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.002980517699545769 Training loss: 7.756376266479492
2025-12-09 12:05:22.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0029803672963128617 Training loss: 7.333209037780762
2025-12-09 12:05:22.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0029802163185783073 Training loss: 7.273781776428223
2025-12-09 12:05:23.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0029800647664006996 Training loss: 7.389610767364502
2025-12-09 12:05:23.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.002979912639838851 Training loss: 7.32506799697876
2025-12-09 12:05:24.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0029797599389518002 Training loss: 7.353185653686523
2025-12-09 12:05:24.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0029796066637988072 Training loss: 7.822593688964844
2025-12-09 12:05:24.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.002979452814439354 Training loss: 7.376995086669922
2025-12-09 12:05:25.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0029792983909331487 Training loss: 7.789217472076416
2025-12-09 12:05:25.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0029791433933401175 Training loss: 7.300959587097168
2025-12-09 12:05:26.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0029789878217204137 Training loss: 7.294981002807617
2025-12-09 12:05:26.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0029788316761344114 Training loss: 7.471282958984375
2025-12-09 12:05:26.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0029786749566427066 Training loss: 8.041264533996582
2025-12-09 12:05:27.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00297851766330612 Training loss: 8.354113578796387
2025-12-09 12:05:27.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.002978359796185695 Training loss: 7.608892440795898
2025-12-09 12:05:27.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0029782013553426943 Training loss: 7.225861549377441
2025-12-09 12:05:28.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0029780423408386075 Training loss: 6.952423572540283
2025-12-09 12:05:28.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0029778827527351445 Training loss: 7.2568888664245605
2025-12-09 12:05:29.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0029777225910942386 Training loss: 7.5060296058654785
2025-12-09 12:05:29.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.002977561855978045 Training loss: 7.765711784362793
2025-12-09 12:05:29.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.002977400547448942 Training loss: 7.726012229919434
2025-12-09 12:05:30.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0029772386655695306 Training loss: 7.302404880523682
2025-12-09 12:05:30.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0029770762104026336 Training loss: 7.625859260559082
2025-12-09 12:05:31.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.002976913182011297 Training loss: 7.466353893280029
2025-12-09 12:05:31.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0029767495804587886 Training loss: 7.536914348602295
2025-12-09 12:05:31.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.002976585405808599 Training loss: 7.389770030975342
2025-12-09 12:05:32.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0029764206581244412 Training loss: 7.626290321350098
2025-12-09 12:05:32.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.002976255337470251 Training loss: 7.309196949005127
2025-12-09 12:05:32.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.002976089443910186 Training loss: 7.403093338012695
2025-12-09 12:05:33.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0029759229775086255 Training loss: 7.339930534362793
2025-12-09 12:05:33.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0029757559383301727 Training loss: 7.68182373046875
2025-12-09 12:05:34.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.0029755883264396517 Training loss: 7.522730350494385
2025-12-09 12:05:34.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00297542014190211 Training loss: 7.131068706512451
2025-12-09 12:05:34.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0029752513847828162 Training loss: 7.468193054199219
2025-12-09 12:05:35.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0029750820551472617 Training loss: 8.001252174377441
2025-12-09 12:05:35.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0029749121530611602 Training loss: 7.436176300048828
2025-12-09 12:05:36.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0029747416785904472 Training loss: 7.045905590057373
2025-12-09 12:05:36.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.002974570631801281 Training loss: 7.395650386810303
2025-12-09 12:05:36.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0029743990127600413 Training loss: 7.311349868774414
2025-12-09 12:05:37.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.002974226821533329 Training loss: 7.1699934005737305
2025-12-09 12:05:37.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0029740540581879703 Training loss: 7.320450305938721
2025-12-09 12:05:38.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0029738807227910093 Training loss: 7.338700771331787
2025-12-09 12:05:38.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.002973706815409715 Training loss: 7.389975070953369
2025-12-09 12:05:38.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0029735323361115775 Training loss: 6.899746894836426
2025-12-09 12:05:39.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.002973357284964309 Training loss: 7.45779275894165
2025-12-09 12:05:39.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0029731816620358425 Training loss: 7.320603370666504
2025-12-09 12:05:39.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.002973005467394334 Training loss: 7.0971245765686035
2025-12-09 12:05:40.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0029728287011081627 Training loss: 7.585000038146973
2025-12-09 12:05:40.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.002972651363245927 Training loss: 8.003585815429688
2025-12-09 12:05:41.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.002972473453876448 Training loss: 7.5009918212890625
2025-12-09 12:05:41.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0029722949730687687 Training loss: 7.148714542388916
2025-12-09 12:05:41.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0029721159208921546 Training loss: 7.2114667892456055
2025-12-09 12:05:42.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.0029719362974160927 Training loss: 6.732142925262451
2025-12-09 12:05:42.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0029717561027102907 Training loss: 7.408398151397705
2025-12-09 12:05:43.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.002971575336844679 Training loss: 8.902434349060059
2025-12-09 12:05:43.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.002971393999889409 Training loss: 7.768752574920654
2025-12-09 12:05:43.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.002971212091914854 Training loss: 7.617345809936523
2025-12-09 12:05:44.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0029710296129916093 Training loss: 7.871537685394287
2025-12-09 12:05:44.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0029708465631904913 Training loss: 7.124324798583984
2025-12-09 12:05:45.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.002970662942582538 Training loss: 7.655190467834473
2025-12-09 12:05:45.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.002970478751239009 Training loss: 7.5186638832092285
2025-12-09 12:05:45.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0029702939892313853 Training loss: 7.78718900680542
2025-12-09 12:05:46.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.002970108656631369 Training loss: 7.751289367675781
2025-12-09 12:05:46.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.002969922753510885 Training loss: 7.154994010925293
2025-12-09 12:05:46.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.002969736279942078 Training loss: 7.428118705749512
2025-12-09 12:05:47.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.002969549235997315 Training loss: 7.405599117279053
2025-12-09 12:05:47.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.002969361621749184 Training loss: 7.333964824676514
2025-12-09 12:05:48.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.002969173437270495 Training loss: 7.346081733703613
2025-12-09 12:05:48.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0029689846826342773 Training loss: 7.726729393005371
2025-12-09 12:05:48.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.002968795357913784 Training loss: 7.237308025360107
2025-12-09 12:05:49.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0029686054631824885 Training loss: 7.451004505157471
2025-12-09 12:05:49.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0029684149985140847 Training loss: 7.9895806312561035
2025-12-09 12:05:50.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0029682239639824883 Training loss: 6.756377220153809
2025-12-09 12:05:50.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.002968032359661836 Training loss: 7.640073299407959
2025-12-09 12:05:50.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.002967840185626486 Training loss: 7.768279552459717
2025-12-09 12:05:51.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0029676474419510174 Training loss: 7.023130893707275
2025-12-09 12:05:51.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0029674541287102296 Training loss: 7.31964635848999
2025-12-09 12:05:51.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.002967260245979144 Training loss: 7.1235857009887695
2025-12-09 12:05:52.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.002967065793833003 Training loss: 7.395777702331543
2025-12-09 12:05:52.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.002966870772347269 Training loss: 7.232073783874512
2025-12-09 12:05:53.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0029666751815976273 Training loss: 6.5179924964904785
2025-12-09 12:05:53.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0029664790216599813 Training loss: 7.254438877105713
2025-12-09 12:05:53.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0029662822926104578 Training loss: 7.463017463684082
2025-12-09 12:05:54.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0029660849945254038 Training loss: 7.157426357269287
2025-12-09 12:05:54.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0029658871274813856 Training loss: 7.247989654541016
2025-12-09 12:05:55.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0029656886915551926 Training loss: 7.298454284667969
2025-12-09 12:05:55.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0029654896868238335 Training loss: 7.49432897567749
2025-12-09 12:05:55.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0029652901133645384 Training loss: 7.451444149017334
2025-12-09 12:05:56.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0029650899712547574 Training loss: 7.107768535614014
2025-12-09 12:05:56.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0029648892605721624 Training loss: 7.149296760559082
2025-12-09 12:05:57.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0029646879813946445 Training loss: 7.650585651397705
2025-12-09 12:05:57.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.002964486133800317 Training loss: 7.033329963684082
2025-12-09 12:05:57.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0029642837178675122 Training loss: 7.348543167114258
2025-12-09 12:05:58.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.002964080733674784 Training loss: 7.289059162139893
2025-12-09 12:05:58.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0029638771813009076 Training loss: 7.447580814361572
2025-12-09 12:05:58.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.002963673060824877 Training loss: 7.296175956726074
2025-12-09 12:05:59.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0029634683723259066 Training loss: 7.086608409881592
2025-12-09 12:05:59.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0029632631158834333 Training loss: 7.2053446769714355
2025-12-09 12:06:00.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0029630572915771117 Training loss: 7.334184169769287
2025-12-09 12:06:00.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.00296285089948682 Training loss: 7.1627960205078125
2025-12-09 12:06:00.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0029626439396926536 Training loss: 7.1093597412109375
2025-12-09 12:06:01.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0029624364122749296 Training loss: 7.300909996032715
2025-12-09 12:06:01.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0029622283173141866 Training loss: 7.191904544830322
2025-12-09 12:06:02.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00296201965489118 Training loss: 7.846867561340332
2025-12-09 12:06:02.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00296181042508689 Training loss: 7.014070510864258
2025-12-09 12:06:02.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.0029616006279825128 Training loss: 7.225979804992676
2025-12-09 12:06:03.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.002961390263659467 Training loss: 7.304464340209961
2025-12-09 12:06:03.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0029611793321993912 Training loss: 7.617890357971191
2025-12-09 12:06:03.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.0029609678336841444 Training loss: 7.23627233505249
2025-12-09 12:06:04.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0029607557681958037 Training loss: 7.489741325378418
2025-12-09 12:06:04.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0029605431358166686 Training loss: 7.032442569732666
2025-12-09 12:06:05.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.002960329936629257 Training loss: 7.046566963195801
2025-12-09 12:06:05.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.002960116170716308 Training loss: 7.50184965133667
2025-12-09 12:06:05.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0029599018381607787 Training loss: 6.89267635345459
2025-12-09 12:06:06.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0029596869390458485 Training loss: 7.305286884307861
2025-12-09 12:06:06.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.002959471473454915 Training loss: 7.488260746002197
2025-12-09 12:06:07.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0029592554414715967 Training loss: 7.347727298736572
2025-12-09 12:06:07.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.002959038843179731 Training loss: 7.846968173980713
2025-12-09 12:06:07.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0029588216786633763 Training loss: 7.436587333679199
2025-12-09 12:06:08.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0029586039480068087 Training loss: 7.119050025939941
2025-12-09 12:06:08.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0029583856512945257 Training loss: 7.155237197875977
2025-12-09 12:06:09.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0029581667886112435 Training loss: 7.19290828704834
2025-12-09 12:06:09.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0029579473600418998 Training loss: 7.2276411056518555
2025-12-09 12:06:09.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0029577273656716495 Training loss: 7.408811092376709
2025-12-09 12:06:10.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0029575068055858675 Training loss: 8.033695220947266
2025-12-09 12:06:10.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.0029572856798701507 Training loss: 7.524200916290283
2025-12-09 12:06:10.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0029570639886103123 Training loss: 7.431853771209717
2025-12-09 12:06:11.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0029568417318923865 Training loss: 7.144318580627441
2025-12-09 12:06:11.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.002956618909802627 Training loss: 7.884860038757324
2025-12-09 12:06:12.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0029563955224275068 Training loss: 7.166338920593262
2025-12-09 12:06:12.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0029561715698537185 Training loss: 7.283447742462158
2025-12-09 12:06:12.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0029559470521681726 Training loss: 7.237269878387451
2025-12-09 12:06:13.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.002955721969458001 Training loss: 7.195850849151611
2025-12-09 12:06:13.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0029554963218105536 Training loss: 8.375782012939453
2025-12-09 12:06:14.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0029552701093133998 Training loss: 7.225215911865234
2025-12-09 12:06:14.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0029550433320543286 Training loss: 7.370856285095215
2025-12-09 12:06:14.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0029548159901213473 Training loss: 7.209486961364746
2025-12-09 12:06:15.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.0029545880836026835 Training loss: 6.958374977111816
2025-12-09 12:06:15.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0029543596125867827 Training loss: 7.23484468460083
2025-12-09 12:06:16.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00295413057716231 Training loss: 6.9928812980651855
2025-12-09 12:06:16.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.00295390097741815 Training loss: 7.421380996704102
2025-12-09 12:06:16.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0029536708134434058 Training loss: 6.920261859893799
2025-12-09 12:06:17.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.002953440085327399 Training loss: 6.782752513885498
2025-12-09 12:06:17.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0029532087931596718 Training loss: 7.15894889831543
2025-12-09 12:06:17.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0029529769370299826 Training loss: 7.720632553100586
2025-12-09 12:06:18.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0029527445170283114 Training loss: 7.119885444641113
2025-12-09 12:06:18.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.002952511533244856 Training loss: 7.73104190826416
2025-12-09 12:06:19.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0029522779857700326 Training loss: 7.2258992195129395
2025-12-09 12:06:19.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0029520438746944754 Training loss: 7.290097713470459
2025-12-09 12:06:19.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00295180920010904 Training loss: 7.07147216796875
2025-12-09 12:06:20.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.0029515739621047976 Training loss: 6.9002604484558105
2025-12-09 12:06:20.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0029513381607730402 Training loss: 7.172797203063965
2025-12-09 12:06:21.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.002951101796205278 Training loss: 7.124924182891846
2025-12-09 12:06:21.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0029508648684932392 Training loss: 7.275188446044922
2025-12-09 12:06:21.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0029506273777288703 Training loss: 7.34880256652832
2025-12-09 12:06:22.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.002950389324004337 Training loss: 6.855504035949707
2025-12-09 12:06:22.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.002950150707412024 Training loss: 7.142389297485352
2025-12-09 12:06:22.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.002949911528044533 Training loss: 7.38575553894043
2025-12-09 12:06:23.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0029496717859946848 Training loss: 7.619508266448975
2025-12-09 12:06:23.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0029494314813555194 Training loss: 6.3888163566589355
2025-12-09 12:06:24.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.002949190614220294 Training loss: 7.212879180908203
2025-12-09 12:06:24.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.002948949184682484 Training loss: 7.439016342163086
2025-12-09 12:06:24.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0029487071928357834 Training loss: 7.092256546020508
2025-12-09 12:06:25.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0029484646387741057 Training loss: 7.333472728729248
2025-12-09 12:06:25.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0029482215225915803 Training loss: 7.116616725921631
2025-12-09 12:06:26.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0029479778443825553 Training loss: 7.2615790367126465
2025-12-09 12:06:26.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.002947733604241599 Training loss: 6.992416858673096
2025-12-09 12:06:26.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.002947488802263496 Training loss: 8.073732376098633
2025-12-09 12:06:27.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0029472434385432477 Training loss: 7.128294944763184
2025-12-09 12:06:27.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0029469975131760765 Training loss: 7.354494571685791
2025-12-09 12:06:28.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0029467510262574203 Training loss: 7.371677875518799
2025-12-09 12:06:28.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0029465039778829366 Training loss: 6.922715663909912
2025-12-09 12:06:28.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0029462563681484995 Training loss: 7.180916786193848
2025-12-09 12:06:29.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.002946008197150202 Training loss: 7.357366561889648
2025-12-09 12:06:29.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0029457594649843536 Training loss: 7.437880992889404
2025-12-09 12:06:29.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0029455101717474836 Training loss: 7.210668563842773
2025-12-09 12:06:30.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0029452603175363365 Training loss: 7.654489040374756
2025-12-09 12:06:30.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0029450099024478766 Training loss: 7.166394233703613
2025-12-09 12:06:31.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.002944758926579285 Training loss: 6.847701549530029
2025-12-09 12:06:31.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.002944507390027961 Training loss: 7.345418453216553
2025-12-09 12:06:31.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0029442552928915203 Training loss: 7.088543891906738
2025-12-09 12:06:32.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.002944002635267797 Training loss: 7.093252182006836
2025-12-09 12:06:32.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.002943749417254843 Training loss: 7.03333044052124
2025-12-09 12:06:33.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0029434956389509264 Training loss: 7.18618631362915
2025-12-09 12:06:33.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0029432413004545346 Training loss: 7.330366134643555
2025-12-09 12:06:33.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.002942986401864371 Training loss: 7.33860445022583
2025-12-09 12:06:34.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.002942730943279357 Training loss: 7.507803916931152
2025-12-09 12:06:34.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0029424749247986305 Training loss: 7.973235130310059
2025-12-09 12:06:35.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.002942218346521548 Training loss: 7.070809364318848
2025-12-09 12:06:35.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.0029419612085476816 Training loss: 7.292600631713867
2025-12-09 12:06:35.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0029417035109768224 Training loss: 7.390521049499512
2025-12-09 12:06:36.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.002941445253908978 Training loss: 7.676778793334961
2025-12-09 12:06:36.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.0029411864374443717 Training loss: 7.302097320556641
2025-12-09 12:06:36.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.002940927061683446 Training loss: 7.241367340087891
2025-12-09 12:06:37.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.002940667126726859 Training loss: 7.251165390014648
2025-12-09 12:06:37.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0029404066326754875 Training loss: 7.566499710083008
2025-12-09 12:06:38.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0029401455796304234 Training loss: 7.087713241577148
2025-12-09 12:06:38.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0029398839676929756 Training loss: 7.299651622772217
2025-12-09 12:06:38.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.002939621796964672 Training loss: 7.326873302459717
2025-12-09 12:06:39.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.002939359067547255 Training loss: 6.9446516036987305
2025-12-09 12:06:39.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.002939095779542685 Training loss: 6.850310802459717
2025-12-09 12:06:40.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0029388319330531385 Training loss: 6.803328037261963
2025-12-09 12:06:40.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0029385675281810106 Training loss: 7.232563495635986
2025-12-09 12:06:40.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.00293830256502891 Training loss: 7.513540267944336
2025-12-09 12:06:41.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0029380370436996642 Training loss: 7.026556015014648
2025-12-09 12:06:41.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0029377709642963174 Training loss: 7.124156475067139
2025-12-09 12:06:41.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0029375043269221296 Training loss: 7.421077728271484
2025-12-09 12:06:42.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.002937237131680577 Training loss: 7.1109299659729
2025-12-09 12:06:42.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.002936969378675354 Training loss: 6.828606128692627
2025-12-09 12:06:43.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0029367010680103685 Training loss: 6.760356426239014
2025-12-09 12:06:43.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0029364321997897482 Training loss: 7.164602756500244
2025-12-09 12:06:43.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0029361627741178358 Training loss: 7.13765811920166
2025-12-09 12:06:44.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.002935892791099189 Training loss: 7.403748512268066
2025-12-09 12:06:44.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.002935622250838583 Training loss: 7.251437187194824
2025-12-09 12:06:45.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0029353511534410104 Training loss: 7.149361610412598
2025-12-09 12:06:45.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.002935079499011677 Training loss: 7.264241695404053
2025-12-09 12:06:45.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0029348072876560086 Training loss: 7.174276828765869
2025-12-09 12:06:46.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0029345345194796437 Training loss: 6.982966423034668
2025-12-09 12:06:46.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0029342611945884388 Training loss: 7.1624627113342285
2025-12-09 12:06:47.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0029339873130884656 Training loss: 7.153261184692383
2025-12-09 12:06:47.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0029337128750860125 Training loss: 6.860299110412598
2025-12-09 12:06:47.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0029334378806875837 Training loss: 7.186639785766602
2025-12-09 12:06:48.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.002933162329999899 Training loss: 7.285192012786865
2025-12-09 12:06:48.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.002932886223129894 Training loss: 7.806605339050293
2025-12-09 12:06:48.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0029326095601847203 Training loss: 6.94173002243042
2025-12-09 12:06:49.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0029323323412717463 Training loss: 7.13330602645874
2025-12-09 12:06:49.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0029320545664985537 Training loss: 7.086843490600586
2025-12-09 12:06:50.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0029317762359729427 Training loss: 7.030515193939209
2025-12-09 12:06:50.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.002931497349802928 Training loss: 7.6529412269592285
2025-12-09 12:06:50.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.002931217908096739 Training loss: 7.423081398010254
2025-12-09 12:06:51.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0029309379109628223 Training loss: 7.398996353149414
2025-12-09 12:06:51.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0029306573585098387 Training loss: 7.409176349639893
2025-12-09 12:06:52.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0029303762508466656 Training loss: 7.090560436248779
2025-12-09 12:06:52.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0029300945880823956 Training loss: 7.277975559234619
2025-12-09 12:06:52.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0029298123703263364 Training loss: 7.248349666595459
2025-12-09 12:06:53.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.002929529597688011 Training loss: 7.543102264404297
2025-12-09 12:06:53.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0029292462702771574 Training loss: 7.105649948120117
2025-12-09 12:06:54.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0029289623882037302 Training loss: 7.066293716430664
2025-12-09 12:06:54.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0029286779515778983 Training loss: 7.199514389038086
2025-12-09 12:06:54.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0029283929605100458 Training loss: 7.065055847167969
2025-12-09 12:06:55.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0029281074151107727 Training loss: 7.4436750411987305
2025-12-09 12:06:55.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.002927821315490893 Training loss: 7.350640773773193
2025-12-09 12:06:55.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0029275346617614363 Training loss: 7.0805816650390625
2025-12-09 12:06:56.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.002927247454033648 Training loss: 7.306181907653809
2025-12-09 12:06:56.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.002926959692418988 Training loss: 6.839278697967529
2025-12-09 12:06:57.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0029266713770291293 Training loss: 6.8165740966796875
2025-12-09 12:06:57.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0029263825079759638 Training loss: 7.676717758178711
2025-12-09 12:06:57.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0029260930853715937 Training loss: 6.956326007843018
2025-12-09 12:06:58.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0029258031093283396 Training loss: 7.638692378997803
2025-12-09 12:06:58.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0029255125799587355 Training loss: 6.9899396896362305
2025-12-09 12:06:59.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.002925221497375529 Training loss: 7.13065767288208
2025-12-09 12:06:59.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0029249298616916856 Training loss: 6.8950114250183105
2025-12-09 12:06:59.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.002924637673020382 Training loss: 7.289898872375488
2025-12-09 12:07:00.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.002924344931475011 Training loss: 6.83799409866333
2025-12-09 12:07:00.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0029240516371691803 Training loss: 7.566865921020508
2025-12-09 12:07:00.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.002923757790216711 Training loss: 7.742667198181152
2025-12-09 12:07:01.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0029234633907316405 Training loss: 7.286265850067139
2025-12-09 12:07:01.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0029231684388282184 Training loss: 7.028067588806152
2025-12-09 12:07:02.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00292287293462091 Training loss: 7.12000846862793
2025-12-09 12:07:02.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0029225768782243956 Training loss: 7.166844844818115
2025-12-09 12:07:02.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0029222802697535678 Training loss: 7.464938163757324
2025-12-09 12:07:03.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.002921983109323535 Training loss: 7.2643818855285645
2025-12-09 12:07:03.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0029216853970496196 Training loss: 7.4419097900390625
2025-12-09 12:07:04.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0029213871330473575 Training loss: 7.8969621658325195
2025-12-09 12:07:04.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.002921088317432499 Training loss: 7.174073219299316
2025-12-09 12:07:04.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0029207889503210095 Training loss: 6.9609808921813965
2025-12-09 12:07:05.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.002920489031829067 Training loss: 7.06082010269165
2025-12-09 12:07:05.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.002920188562073063 Training loss: 7.316932201385498
2025-12-09 12:07:06.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0029198875411696056 Training loss: 6.999670505523682
2025-12-09 12:07:06.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0029195859692355145 Training loss: 7.121528148651123
2025-12-09 12:07:06.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.002919283846387824 Training loss: 7.2197957038879395
2025-12-09 12:07:07.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0029189811727437813 Training loss: 7.164552688598633
2025-12-09 12:07:07.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.002918677948420849 Training loss: 7.0397515296936035
2025-12-09 12:07:07.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0029183741735367024 Training loss: 6.808945178985596
2025-12-09 12:07:08.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0029180698482092304 Training loss: 7.168923377990723
2025-12-09 12:07:08.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0029177649725565355 Training loss: 7.102568626403809
2025-12-09 12:07:09.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0029174595466969345 Training loss: 7.117680072784424
2025-12-09 12:07:09.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0029171535707489572 Training loss: 7.3757452964782715
2025-12-09 12:07:09.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0029168470448313463 Training loss: 7.367024898529053
2025-12-09 12:07:10.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.002916539969063059 Training loss: 7.94282341003418
2025-12-09 12:07:10.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0029162323435632654 Training loss: 6.733090400695801
2025-12-09 12:07:11.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.002915924168451349 Training loss: 7.2844743728637695
2025-12-09 12:07:11.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.002915615443846906 Training loss: 7.081639289855957
2025-12-09 12:07:11.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0029153061698697475 Training loss: 7.375520706176758
2025-12-09 12:07:12.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0029149963466398956 Training loss: 7.260066986083984
2025-12-09 12:07:12.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.002914685974277587 Training loss: 7.250692844390869
2025-12-09 12:07:13.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.002914375052903271 Training loss: 7.169824123382568
2025-12-09 12:07:13.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.002914063582637611 Training loss: 7.417738437652588
2025-12-09 12:07:13.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.002913751563601481 Training loss: 7.178385257720947
2025-12-09 12:07:14.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0029134389959159708 Training loss: 6.909395217895508
2025-12-09 12:07:14.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0029131258797023816 Training loss: 7.236818313598633
2025-12-09 12:07:14.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0029128122150822266 Training loss: 7.195979595184326
2025-12-09 12:07:15.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0029124980021772344 Training loss: 7.399289131164551
2025-12-09 12:07:15.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0029121832411093443 Training loss: 6.9441752433776855
2025-12-09 12:07:16.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0029118679320007087 Training loss: 7.372409343719482
2025-12-09 12:07:16.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0029115520749736935 Training loss: 7.114468574523926
2025-12-09 12:07:16.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0029112356701508756 Training loss: 7.333223819732666
2025-12-09 12:07:17.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0029109187176550463 Training loss: 7.319642066955566
2025-12-09 12:07:17.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0029106012176092085 Training loss: 7.290993690490723
2025-12-09 12:07:18.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0029102831701365785 Training loss: 7.1789631843566895
2025-12-09 12:07:18.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0029099645753605827 Training loss: 7.095932960510254
2025-12-09 12:07:18.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.002909645433404863 Training loss: 7.291586875915527
2025-12-09 12:07:19.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0029093257443932713 Training loss: 6.7524847984313965
2025-12-09 12:07:19.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0029090055084498734 Training loss: 7.645443439483643
2025-12-09 12:07:19.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0029086847256989457 Training loss: 6.90233039855957
2025-12-09 12:07:20.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0029083633962649785 Training loss: 7.306560516357422
2025-12-09 12:07:20.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0029080415202726727 Training loss: 7.282541275024414
2025-12-09 12:07:21.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0029077190978469432 Training loss: 7.126474857330322
2025-12-09 12:07:21.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0029073961291129153 Training loss: 7.162455081939697
2025-12-09 12:07:21.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0029070726141959265 Training loss: 6.929574012756348
2025-12-09 12:07:22.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.002906748553221527 Training loss: 7.673549652099609
2025-12-09 12:07:22.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0029064239463154782 Training loss: 6.829510688781738
2025-12-09 12:07:23.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.002906098793603754 Training loss: 7.009186744689941
2025-12-09 12:07:23.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.00290577309521254 Training loss: 7.394894599914551
2025-12-09 12:07:23.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.002905446851268233 Training loss: 7.065290927886963
2025-12-09 12:07:24.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.002905120061897442 Training loss: 7.316622257232666
2025-12-09 12:07:24.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.002904792727226987 Training loss: 6.57033634185791
2025-12-09 12:07:25.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.002904464847383902 Training loss: 7.300996780395508
2025-12-09 12:07:25.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.002904136422495429 Training loss: 6.92333459854126
2025-12-09 12:07:25.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0029038074526890243 Training loss: 7.121811389923096
2025-12-09 12:07:26.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.002903477938092354 Training loss: 7.183277130126953
2025-12-09 12:07:26.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.002903147878833296 Training loss: 6.963923454284668
2025-12-09 12:07:26.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.002902817275039941 Training loss: 7.299398899078369
2025-12-09 12:07:27.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0029024861268405894 Training loss: 7.058904647827148
2025-12-09 12:07:27.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0029021544343637525 Training loss: 7.815908908843994
2025-12-09 12:07:28.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0029018221977381554 Training loss: 7.220495223999023
2025-12-09 12:07:28.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0029014894170927307 Training loss: 7.150101184844971
2025-12-09 12:07:28.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0029011560925566253 Training loss: 7.024643421173096
2025-12-09 12:07:29.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.002900822224259196 Training loss: 6.723631381988525
2025-12-09 12:07:29.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0029004878123300095 Training loss: 7.204756259918213
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 3.28 GiB is free. Including non-PyTorch memory, this process has 90.93 GiB memory in use. Of the allocated memory 89.96 GiB is allocated by PyTorch, and 216.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 50/10000 [00:00<00:20, 489.53it/s]Tokenizing texts:   1%|▏         | 133/10000 [00:00<00:14, 686.27it/s]Tokenizing texts:   2%|▏         | 202/10000 [00:00<00:15, 628.82it/s]Tokenizing texts:   3%|▎         | 266/10000 [00:00<00:15, 622.39it/s]Tokenizing texts:   3%|▎         | 330/10000 [00:00<00:15, 625.92it/s]Tokenizing texts:   4%|▍         | 393/10000 [00:00<00:16, 588.13it/s]Tokenizing texts:   5%|▍         | 453/10000 [00:00<00:17, 561.23it/s]Tokenizing texts:   5%|▌         | 527/10000 [00:00<00:15, 613.03it/s]Tokenizing texts:   6%|▌         | 590/10000 [00:00<00:16, 570.46it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:01<00:16, 566.20it/s]Tokenizing texts:   7%|▋         | 709/10000 [00:01<00:16, 554.68it/s]Tokenizing texts:   8%|▊         | 770/10000 [00:01<00:16, 568.64it/s]Tokenizing texts:   8%|▊         | 828/10000 [00:01<00:16, 561.59it/s]Tokenizing texts:   9%|▉         | 888/10000 [00:01<00:15, 571.89it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:01<00:14, 642.82it/s]Tokenizing texts:  10%|█         | 1046/10000 [00:01<00:13, 668.46it/s]Tokenizing texts:  11%|█         | 1114/10000 [00:01<00:14, 607.49it/s]Tokenizing texts:  12%|█▏        | 1176/10000 [00:01<00:15, 577.56it/s]Tokenizing texts:  12%|█▏        | 1235/10000 [00:02<00:15, 576.80it/s]Tokenizing texts:  13%|█▎        | 1303/10000 [00:02<00:14, 597.00it/s]Tokenizing texts:  14%|█▎        | 1364/10000 [00:02<00:14, 579.70it/s]Tokenizing texts:  14%|█▍        | 1425/10000 [00:02<00:14, 583.16it/s]Tokenizing texts:  15%|█▌        | 1502/10000 [00:02<00:13, 633.18it/s]Tokenizing texts:  16%|█▌        | 1576/10000 [00:02<00:12, 661.26it/s]Tokenizing texts:  16%|█▋        | 1643/10000 [00:02<00:13, 610.43it/s]Tokenizing texts:  17%|█▋        | 1706/10000 [00:02<00:14, 589.19it/s]Tokenizing texts:  18%|█▊        | 1781/10000 [00:02<00:13, 630.20it/s]Tokenizing texts:  18%|█▊        | 1845/10000 [00:03<00:13, 590.29it/s]Tokenizing texts:  19%|█▉        | 1909/10000 [00:03<00:13, 603.34it/s]Tokenizing texts:  20%|█▉        | 1971/10000 [00:03<00:14, 555.97it/s]Tokenizing texts:  20%|██        | 2040/10000 [00:03<00:13, 590.45it/s]Tokenizing texts:  21%|██        | 2101/10000 [00:03<00:14, 546.18it/s]Tokenizing texts:  22%|██▏       | 2158/10000 [00:03<00:14, 551.94it/s]Tokenizing texts:  22%|██▏       | 2230/10000 [00:03<00:13, 596.35it/s]Tokenizing texts:  23%|██▎       | 2297/10000 [00:03<00:12, 616.59it/s]Tokenizing texts:  24%|██▎       | 2360/10000 [00:03<00:12, 595.14it/s]Tokenizing texts:  24%|██▍       | 2428/10000 [00:04<00:12, 618.98it/s]Tokenizing texts:  25%|██▍       | 2491/10000 [00:04<00:12, 612.78it/s]Tokenizing texts:  26%|██▌       | 2560/10000 [00:04<00:11, 632.82it/s]Tokenizing texts:  26%|██▌       | 2624/10000 [00:04<00:13, 560.45it/s]Tokenizing texts:  27%|██▋       | 2682/10000 [00:04<00:13, 559.95it/s]Tokenizing texts:  27%|██▋       | 2740/10000 [00:04<00:12, 563.33it/s]Tokenizing texts:  28%|██▊       | 2798/10000 [00:04<00:13, 522.22it/s]Tokenizing texts:  29%|██▊       | 2854/10000 [00:04<00:13, 529.20it/s]Tokenizing texts:  29%|██▉       | 2930/10000 [00:04<00:11, 591.15it/s]Tokenizing texts:  30%|██▉       | 2991/10000 [00:05<00:11, 586.16it/s]Tokenizing texts:  31%|███       | 3051/10000 [00:05<00:11, 583.59it/s]Tokenizing texts:  31%|███       | 3123/10000 [00:05<00:11, 619.63it/s]Tokenizing texts:  32%|███▏      | 3186/10000 [00:05<00:11, 610.42it/s]Tokenizing texts:  33%|███▎      | 3254/10000 [00:05<00:10, 628.16it/s]Tokenizing texts:  33%|███▎      | 3318/10000 [00:05<00:10, 630.23it/s]Tokenizing texts:  34%|███▍      | 3383/10000 [00:05<00:10, 632.82it/s]Tokenizing texts:  35%|███▍      | 3462/10000 [00:05<00:09, 678.98it/s]Tokenizing texts:  35%|███▌      | 3531/10000 [00:05<00:10, 641.72it/s]Tokenizing texts:  36%|███▌      | 3596/10000 [00:06<00:10, 639.48it/s]Tokenizing texts:  37%|███▋      | 3661/10000 [00:06<00:10, 625.20it/s]Tokenizing texts:  37%|███▋      | 3724/10000 [00:06<00:10, 626.13it/s]Tokenizing texts:  38%|███▊      | 3787/10000 [00:06<00:10, 591.27it/s]Tokenizing texts:  38%|███▊      | 3847/10000 [00:06<00:10, 583.22it/s]Tokenizing texts:  39%|███▉      | 3906/10000 [00:06<00:10, 577.94it/s]Tokenizing texts:  40%|███▉      | 3968/10000 [00:06<00:10, 585.57it/s]Tokenizing texts:  40%|████      | 4027/10000 [00:06<00:10, 579.05it/s]Tokenizing texts:  41%|████      | 4103/10000 [00:06<00:09, 629.20it/s]Tokenizing texts:  42%|████▏     | 4174/10000 [00:06<00:08, 652.28it/s]Tokenizing texts:  42%|████▏     | 4240/10000 [00:07<00:09, 625.92it/s]Tokenizing texts:  43%|████▎     | 4303/10000 [00:07<00:09, 619.65it/s]Tokenizing texts:  44%|████▍     | 4381/10000 [00:07<00:08, 663.80it/s]Tokenizing texts:  44%|████▍     | 4448/10000 [00:07<00:08, 654.30it/s]Tokenizing texts:  45%|████▌     | 4514/10000 [00:07<00:08, 637.75it/s]Tokenizing texts:  46%|████▌     | 4578/10000 [00:07<00:08, 634.59it/s]Tokenizing texts:  46%|████▋     | 4645/10000 [00:07<00:08, 642.41it/s]Tokenizing texts:  47%|████▋     | 4729/10000 [00:07<00:07, 697.31it/s]Tokenizing texts:  48%|████▊     | 4799/10000 [00:07<00:08, 631.49it/s]Tokenizing texts:  49%|████▉     | 4875/10000 [00:08<00:07, 656.40it/s]Tokenizing texts:  49%|████▉     | 4945/10000 [00:08<00:07, 666.68it/s]Tokenizing texts:  50%|█████     | 5022/10000 [00:08<00:07, 694.97it/s]Tokenizing texts:  51%|█████     | 5093/10000 [00:08<00:07, 634.00it/s]Tokenizing texts:  52%|█████▏    | 5158/10000 [00:08<00:07, 630.91it/s]Tokenizing texts:  52%|█████▏    | 5223/10000 [00:08<00:07, 610.79it/s]Tokenizing texts:  53%|█████▎    | 5288/10000 [00:08<00:07, 618.93it/s]Tokenizing texts:  54%|█████▍    | 5375/10000 [00:08<00:06, 688.50it/s]Tokenizing texts:  54%|█████▍    | 5445/10000 [00:08<00:06, 667.57it/s]Tokenizing texts:  55%|█████▌    | 5513/10000 [00:09<00:07, 605.59it/s]Tokenizing texts:  56%|█████▌    | 5580/10000 [00:09<00:07, 622.50it/s]Tokenizing texts:  57%|█████▋    | 5667/10000 [00:09<00:06, 689.17it/s]Tokenizing texts:  57%|█████▋    | 5738/10000 [00:09<00:06, 673.75it/s]Tokenizing texts:  58%|█████▊    | 5807/10000 [00:09<00:06, 604.47it/s]Tokenizing texts:  59%|█████▊    | 5870/10000 [00:09<00:06, 610.61it/s]Tokenizing texts:  59%|█████▉    | 5933/10000 [00:09<00:06, 599.47it/s]Tokenizing texts:  60%|█████▉    | 5997/10000 [00:09<00:06, 605.81it/s]Tokenizing texts:  61%|██████    | 6066/10000 [00:09<00:06, 620.99it/s]Tokenizing texts:  61%|██████▏   | 6138/10000 [00:10<00:05, 645.32it/s]Tokenizing texts:  62%|██████▏   | 6204/10000 [00:10<00:05, 646.29it/s]Tokenizing texts:  63%|██████▎   | 6269/10000 [00:10<00:05, 628.08it/s]Tokenizing texts:  63%|██████▎   | 6333/10000 [00:10<00:06, 594.84it/s]Tokenizing texts:  64%|██████▍   | 6398/10000 [00:10<00:05, 607.44it/s]Tokenizing texts:  65%|██████▍   | 6460/10000 [00:10<00:05, 610.76it/s]Tokenizing texts:  65%|██████▌   | 6522/10000 [00:10<00:05, 583.14it/s]Tokenizing texts:  66%|██████▌   | 6600/10000 [00:10<00:05, 637.97it/s]Tokenizing texts:  67%|██████▋   | 6665/10000 [00:10<00:05, 632.03it/s]Tokenizing texts:  67%|██████▋   | 6729/10000 [00:10<00:05, 631.31it/s]Tokenizing texts:  68%|██████▊   | 6802/10000 [00:11<00:04, 659.61it/s]Tokenizing texts:  69%|██████▉   | 6875/10000 [00:11<00:04, 672.57it/s]Tokenizing texts:  69%|██████▉   | 6943/10000 [00:11<00:05, 601.90it/s]Tokenizing texts:  70%|███████   | 7008/10000 [00:11<00:04, 614.10it/s]Tokenizing texts:  71%|███████   | 7071/10000 [00:11<00:05, 563.38it/s]Tokenizing texts:  71%|███████▏  | 7132/10000 [00:11<00:04, 575.35it/s]Tokenizing texts:  72%|███████▏  | 7192/10000 [00:11<00:04, 581.96it/s]Tokenizing texts:  73%|███████▎  | 7252/10000 [00:11<00:04, 584.45it/s]Tokenizing texts:  73%|███████▎  | 7336/10000 [00:11<00:04, 645.52it/s]Tokenizing texts:  74%|███████▍  | 7401/10000 [00:12<00:04, 629.48it/s]Tokenizing texts:  75%|███████▍  | 7465/10000 [00:12<00:04, 572.97it/s]Tokenizing texts:  75%|███████▌  | 7542/10000 [00:12<00:03, 624.91it/s]Tokenizing texts:  76%|███████▌  | 7616/10000 [00:12<00:03, 651.99it/s]Tokenizing texts:  77%|███████▋  | 7684/10000 [00:12<00:03, 659.73it/s]Tokenizing texts:  78%|███████▊  | 7751/10000 [00:12<00:03, 634.87it/s]Tokenizing texts:  78%|███████▊  | 7816/10000 [00:12<00:03, 599.85it/s]Tokenizing texts:  79%|███████▉  | 7881/10000 [00:12<00:03, 611.86it/s]Tokenizing texts:  80%|███████▉  | 7955/10000 [00:12<00:03, 644.98it/s]Tokenizing texts:  80%|████████  | 8024/10000 [00:13<00:03, 657.64it/s]Tokenizing texts:  81%|████████  | 8091/10000 [00:13<00:03, 567.62it/s]Tokenizing texts:  82%|████████▏ | 8151/10000 [00:13<00:03, 574.39it/s]Tokenizing texts:  82%|████████▏ | 8217/10000 [00:13<00:02, 595.44it/s]Tokenizing texts:  83%|████████▎ | 8278/10000 [00:13<00:03, 562.23it/s]Tokenizing texts:  84%|████████▎ | 8350/10000 [00:13<00:02, 594.69it/s]Tokenizing texts:  84%|████████▍ | 8411/10000 [00:13<00:02, 574.57it/s]Tokenizing texts:  85%|████████▍ | 8483/10000 [00:13<00:02, 610.18it/s]Tokenizing texts:  86%|████████▌ | 8554/10000 [00:13<00:02, 635.15it/s]Tokenizing texts:  86%|████████▌ | 8619/10000 [00:14<00:02, 633.09it/s]Tokenizing texts:  87%|████████▋ | 8688/10000 [00:14<00:02, 649.29it/s]Tokenizing texts:  88%|████████▊ | 8761/10000 [00:14<00:01, 672.61it/s]Tokenizing texts:  88%|████████▊ | 8829/10000 [00:14<00:01, 597.57it/s]Tokenizing texts:  89%|████████▉ | 8903/10000 [00:14<00:01, 635.10it/s]Tokenizing texts:  90%|████████▉ | 8969/10000 [00:14<00:01, 638.84it/s]Tokenizing texts:  90%|█████████ | 9035/10000 [00:14<00:01, 613.71it/s]Tokenizing texts:  91%|█████████ | 9099/10000 [00:14<00:01, 615.84it/s]Tokenizing texts:  92%|█████████▏| 9168/10000 [00:14<00:01, 634.25it/s]Tokenizing texts:  92%|█████████▏| 9237/10000 [00:15<00:01, 648.48it/s]Tokenizing texts:  93%|█████████▎| 9303/10000 [00:15<00:01, 632.30it/s]Tokenizing texts:  94%|█████████▎| 9367/10000 [00:15<00:01, 569.96it/s]Tokenizing texts:  94%|█████████▍| 9428/10000 [00:15<00:00, 579.67it/s]Tokenizing texts:  95%|█████████▍| 9487/10000 [00:15<00:00, 527.90it/s]Tokenizing texts:  96%|█████████▌| 9563/10000 [00:15<00:00, 584.53it/s]Tokenizing texts:  96%|█████████▌| 9624/10000 [00:15<00:00, 570.14it/s]Tokenizing texts:  97%|█████████▋| 9685/10000 [00:15<00:00, 565.37it/s]Tokenizing texts:  97%|█████████▋| 9744/10000 [00:15<00:00, 571.34it/s]Tokenizing texts:  98%|█████████▊| 9803/10000 [00:16<00:00, 575.90it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:16<00:00, 599.97it/s]Tokenizing texts:  99%|█████████▉| 9946/10000 [00:16<00:00, 642.85it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:16<00:00, 611.98it/s]
2025-12-09 12:08:18.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 12.170064926147461
2025-12-09 12:08:19.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 12.184918403625488
2025-12-09 12:08:19.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 11.754435539245605
2025-12-09 12:08:20.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 12.27744197845459
2025-12-09 12:08:20.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 11.831865310668945
2025-12-09 12:08:20.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 14.916245460510254
2025-12-09 12:08:21.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 11.486196517944336
2025-12-09 12:08:21.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 11.31525993347168
2025-12-09 12:08:21.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 11.023606300354004
2025-12-09 12:08:22.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 10.214353561401367
2025-12-09 12:08:22.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 9.86246109008789
2025-12-09 12:08:23.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 9.579612731933594
2025-12-09 12:08:23.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 10.253408432006836
2025-12-09 12:08:23.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 10.104713439941406
2025-12-09 12:08:24.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 10.649124145507812
2025-12-09 12:08:24.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 11.402729034423828
2025-12-09 12:08:25.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 10.756668090820312
2025-12-09 12:08:25.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 10.350683212280273
2025-12-09 12:08:25.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 10.91805362701416
2025-12-09 12:08:26.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 10.321366310119629
2025-12-09 12:08:26.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 10.615242958068848
2025-12-09 12:08:26.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 11.06346607208252
2025-12-09 12:08:27.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 10.7371187210083
2025-12-09 12:08:27.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 10.925910949707031
2025-12-09 12:08:28.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 10.363612174987793
2025-12-09 12:08:28.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 10.53652286529541
2025-12-09 12:08:28.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 10.149921417236328
2025-12-09 12:08:29.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 9.477069854736328
2025-12-09 12:08:29.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 9.719392776489258
2025-12-09 12:08:30.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 10.145552635192871
2025-12-09 12:08:30.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 9.310223579406738
2025-12-09 12:08:30.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 9.468799591064453
2025-12-09 12:08:31.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 9.521818161010742
2025-12-09 12:08:31.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 9.629865646362305
2025-12-09 12:08:31.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 10.103878021240234
2025-12-09 12:08:32.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 9.9768648147583
2025-12-09 12:08:32.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 9.500727653503418
2025-12-09 12:08:33.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 9.748136520385742
2025-12-09 12:08:33.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 9.906102180480957
2025-12-09 12:08:33.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 9.449374198913574
2025-12-09 12:08:34.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 9.277169227600098
2025-12-09 12:08:34.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 9.883162498474121
2025-12-09 12:08:35.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 9.790602684020996
2025-12-09 12:08:35.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 9.258489608764648
2025-12-09 12:08:35.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 9.511161804199219
2025-12-09 12:08:36.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 9.714949607849121
2025-12-09 12:08:36.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 9.396007537841797
2025-12-09 12:08:36.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 10.306133270263672
2025-12-09 12:08:37.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 8.879067420959473
2025-12-09 12:08:37.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 9.412596702575684
2025-12-09 12:08:38.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 9.650125503540039
2025-12-09 12:08:38.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 9.59280776977539
2025-12-09 12:08:38.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 9.271748542785645
2025-12-09 12:08:39.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 9.351101875305176
2025-12-09 12:08:39.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 9.179855346679688
2025-12-09 12:08:40.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 9.229619026184082
2025-12-09 12:08:40.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 9.2984037399292
2025-12-09 12:08:40.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 9.09618091583252
2025-12-09 12:08:41.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 9.728814125061035
2025-12-09 12:08:41.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 8.871776580810547
2025-12-09 12:08:41.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 8.845198631286621
2025-12-09 12:08:42.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 9.005327224731445
2025-12-09 12:08:42.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 8.70460033416748
2025-12-09 12:08:43.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 8.990307807922363
2025-12-09 12:08:43.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 8.98755931854248
2025-12-09 12:08:43.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 9.136075973510742
2025-12-09 12:08:44.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 9.705595970153809
2025-12-09 12:08:44.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 8.994864463806152
2025-12-09 12:08:45.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 9.073720932006836
2025-12-09 12:08:45.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 8.893858909606934
2025-12-09 12:08:45.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 9.131304740905762
2025-12-09 12:08:46.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 8.978266716003418
2025-12-09 12:08:46.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 9.040152549743652
2025-12-09 12:08:46.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 10.171576499938965
2025-12-09 12:08:47.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 9.474857330322266
2025-12-09 12:08:47.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 9.661016464233398
2025-12-09 12:08:48.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 8.941466331481934
2025-12-09 12:08:48.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 9.137253761291504
2025-12-09 12:08:48.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 8.939679145812988
2025-12-09 12:08:49.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 8.99409008026123
2025-12-09 12:08:49.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 8.604039192199707
2025-12-09 12:08:50.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 8.987570762634277
2025-12-09 12:08:50.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 10.443532943725586
2025-12-09 12:08:50.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 8.703121185302734
2025-12-09 12:08:51.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 8.924248695373535
2025-12-09 12:08:51.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 9.061330795288086
2025-12-09 12:08:51.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 8.660703659057617
2025-12-09 12:08:52.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 8.77477741241455
2025-12-09 12:08:52.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 8.866009712219238
2025-12-09 12:08:53.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 8.45508098602295
2025-12-09 12:08:53.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 8.733882904052734
2025-12-09 12:08:53.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 8.404272079467773
2025-12-09 12:08:54.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 8.606319427490234
2025-12-09 12:08:54.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 9.244502067565918
2025-12-09 12:08:55.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 8.738530158996582
2025-12-09 12:08:55.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 8.500126838684082
2025-12-09 12:08:55.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 9.194744110107422
2025-12-09 12:08:56.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 8.297425270080566
2025-12-09 12:08:56.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 8.366095542907715
2025-12-09 12:08:56.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 8.139660835266113
2025-12-09 12:08:57.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999999029798808 Training loss: 8.825663566589355
2025-12-09 12:08:57.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00999999611919561 Training loss: 9.192920684814453
2025-12-09 12:08:58.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009999991268191535 Training loss: 8.678776741027832
2025-12-09 12:08:58.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009999984476788465 Training loss: 8.922257423400879
2025-12-09 12:08:58.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009999975744989035 Training loss: 8.315218925476074
2025-12-09 12:08:59.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009999965072796636 Training loss: 8.5543851852417
2025-12-09 12:08:59.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009999952460215409 Training loss: 8.319211959838867
2025-12-09 12:09:00.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.009999937907250246 Training loss: 8.577340126037598
2025-12-09 12:09:00.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.009999921413906798 Training loss: 8.453024864196777
2025-12-09 12:09:00.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.009999902980191464 Training loss: 7.965429306030273
2025-12-09 12:09:01.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009999882606111399 Training loss: 8.30908489227295
2025-12-09 12:09:01.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.009999860291674507 Training loss: 8.209409713745117
2025-12-09 12:09:01.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009999836036889453 Training loss: 8.130854606628418
2025-12-09 12:09:02.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009999809841765645 Training loss: 8.371667861938477
2025-12-09 12:09:02.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00999978170631325 Training loss: 8.269380569458008
2025-12-09 12:09:03.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009999751630543188 Training loss: 8.26938247680664
2025-12-09 12:09:03.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00999971961446713 Training loss: 8.508975982666016
2025-12-09 12:09:03.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.009999685658097501 Training loss: 8.213861465454102
2025-12-09 12:09:04.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.009999649761447477 Training loss: 8.258675575256348
2025-12-09 12:09:04.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009999611924530994 Training loss: 8.440298080444336
2025-12-09 12:09:05.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.00999957214736273 Training loss: 8.043511390686035
2025-12-09 12:09:05.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.009999530429958124 Training loss: 9.581023216247559
2025-12-09 12:09:05.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.009999486772333366 Training loss: 8.29524040222168
2025-12-09 12:09:06.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0099994411745054 Training loss: 8.045818328857422
2025-12-09 12:09:06.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.009999393636491919 Training loss: 8.36774730682373
2025-12-09 12:09:07.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00999934415831137 Training loss: 8.058642387390137
2025-12-09 12:09:07.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009999292739982958 Training loss: 7.847105026245117
2025-12-09 12:09:07.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009999239381526638 Training loss: 8.452837944030762
2025-12-09 12:09:08.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009999184082963117 Training loss: 8.287938117980957
2025-12-09 12:09:08.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009999126844313852 Training loss: 7.968504428863525
2025-12-09 12:09:08.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00999906766560106 Training loss: 8.757999420166016
2025-12-09 12:09:09.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009999006546847707 Training loss: 8.773309707641602
2025-12-09 12:09:09.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.009998943488077507 Training loss: 8.178851127624512
2025-12-09 12:09:10.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009998878489314937 Training loss: 8.448615074157715
2025-12-09 12:09:10.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009998811550585221 Training loss: 8.193500518798828
2025-12-09 12:09:10.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009998742671914335 Training loss: 7.843728065490723
2025-12-09 12:09:11.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.00999867185332901 Training loss: 8.440666198730469
2025-12-09 12:09:11.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00999859909485673 Training loss: 8.545753479003906
2025-12-09 12:09:12.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00999852439652573 Training loss: 8.017623901367188
2025-12-09 12:09:12.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009998447758365002 Training loss: 8.155682563781738
2025-12-09 12:09:12.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009998369180404282 Training loss: 8.505350112915039
2025-12-09 12:09:13.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00999828866267407 Training loss: 8.17191219329834
2025-12-09 12:09:13.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.00999820620520561 Training loss: 7.76599645614624
2025-12-09 12:09:13.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009998121808030905 Training loss: 7.8285698890686035
2025-12-09 12:09:14.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.009998035471182706 Training loss: 7.961518287658691
2025-12-09 12:09:14.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00999794719469452 Training loss: 7.995956897735596
2025-12-09 12:09:15.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.009997856978600603 Training loss: 7.729369163513184
2025-12-09 12:09:15.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009997764822935967 Training loss: 8.489041328430176
2025-12-09 12:09:15.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.009997670727736378 Training loss: 7.627173900604248
2025-12-09 12:09:16.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009997574693038351 Training loss: 8.046211242675781
2025-12-09 12:09:16.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009997476718879152 Training loss: 8.012835502624512
2025-12-09 12:09:17.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009997376805296809 Training loss: 7.858917713165283
2025-12-09 12:09:17.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009997274952330094 Training loss: 7.798433303833008
2025-12-09 12:09:17.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00999717116001853 Training loss: 7.6611833572387695
2025-12-09 12:09:18.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009997065428402403 Training loss: 7.77113151550293
2025-12-09 12:09:18.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009996957757522741 Training loss: 7.629952907562256
2025-12-09 12:09:18.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009996848147421333 Training loss: 8.32978343963623
2025-12-09 12:09:19.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009996736598140715 Training loss: 7.759995937347412
2025-12-09 12:09:19.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.009996623109724174 Training loss: 7.7806925773620605
2025-12-09 12:09:20.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.009996507682215754 Training loss: 7.711127758026123
2025-12-09 12:09:20.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.009996390315660254 Training loss: 8.064342498779297
2025-12-09 12:09:20.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.009996271010103216 Training loss: 8.056184768676758
2025-12-09 12:09:21.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.009996149765590946 Training loss: 7.973913192749023
2025-12-09 12:09:21.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00999602658217049 Training loss: 7.673681735992432
2025-12-09 12:09:22.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.009995901459889657 Training loss: 7.743933200836182
2025-12-09 12:09:22.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.009995774398797007 Training loss: 7.9503936767578125
2025-12-09 12:09:22.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.009995645398941846 Training loss: 7.854694843292236
2025-12-09 12:09:23.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.009995514460374237 Training loss: 8.840680122375488
2025-12-09 12:09:23.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.009995381583144995 Training loss: 7.926991939544678
2025-12-09 12:09:24.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.009995246767305689 Training loss: 7.991504669189453
2025-12-09 12:09:24.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.009995110012908634 Training loss: 8.647549629211426
2025-12-09 12:09:24.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.009994971320006905 Training loss: 8.638116836547852
2025-12-09 12:09:25.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.009994830688654326 Training loss: 7.837826728820801
2025-12-09 12:09:25.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.009994688118905472 Training loss: 7.982965469360352
2025-12-09 12:09:25.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.009994543610815672 Training loss: 7.909854888916016
2025-12-09 12:09:26.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.009994397164441006 Training loss: 7.636953830718994
2025-12-09 12:09:26.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.009994248779838311 Training loss: 7.7177629470825195
2025-12-09 12:09:27.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.009994098457065167 Training loss: 7.593714714050293
2025-12-09 12:09:27.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.009993946196179913 Training loss: 7.90408992767334
2025-12-09 12:09:27.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.009993791997241638 Training loss: 7.447245121002197
2025-12-09 12:09:28.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.009993635860310187 Training loss: 7.987897872924805
2025-12-09 12:09:28.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00999347778544615 Training loss: 8.807696342468262
2025-12-09 12:09:29.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.009993317772710874 Training loss: 8.011459350585938
2025-12-09 12:09:29.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.009993155822166457 Training loss: 7.672153949737549
2025-12-09 12:09:29.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.009992991933875747 Training loss: 7.905570030212402
2025-12-09 12:09:30.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.009992826107902348 Training loss: 7.966920852661133
2025-12-09 12:09:30.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.009992658344310614 Training loss: 8.014066696166992
2025-12-09 12:09:30.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00999248864316565 Training loss: 7.788671016693115
2025-12-09 12:09:31.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.009992317004533314 Training loss: 8.097505569458008
2025-12-09 12:09:31.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.009992143428480213 Training loss: 7.436538219451904
2025-12-09 12:09:32.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.009991967915073714 Training loss: 9.000367164611816
2025-12-09 12:09:32.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.009991790464381926 Training loss: 7.687380790710449
2025-12-09 12:09:32.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.009991611076473714 Training loss: 7.817506313323975
2025-12-09 12:09:33.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.009991429751418698 Training loss: 7.845398426055908
2025-12-09 12:09:33.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.009991246489287245 Training loss: 7.898739814758301
2025-12-09 12:09:34.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.009991061290150474 Training loss: 8.389544486999512
2025-12-09 12:09:34.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.009990874154080258 Training loss: 8.080915451049805
2025-12-09 12:09:34.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.009990685081149222 Training loss: 7.8143534660339355
2025-12-09 12:09:35.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.009990494071430742 Training loss: 7.810513973236084
2025-12-09 12:09:35.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.009990301124998944 Training loss: 7.586644172668457
2025-12-09 12:09:36.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.009990106241928705 Training loss: 8.30894947052002
2025-12-09 12:09:36.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.009989909422295658 Training loss: 7.760539531707764
2025-12-09 12:09:36.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.009989710666176184 Training loss: 7.8273162841796875
2025-12-09 12:09:37.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.009989509973647417 Training loss: 8.076226234436035
2025-12-09 12:09:37.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.009989307344787242 Training loss: 8.00147819519043
2025-12-09 12:09:37.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.009989102779674294 Training loss: 7.648038387298584
2025-12-09 12:09:38.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00998889627838796 Training loss: 8.294280052185059
2025-12-09 12:09:38.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00998868784100838 Training loss: 8.10207462310791
2025-12-09 12:09:39.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.009988477467616446 Training loss: 7.8098063468933105
2025-12-09 12:09:39.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0099882651582938 Training loss: 7.996528148651123
2025-12-09 12:09:39.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.009988050913122831 Training loss: 7.8935041427612305
2025-12-09 12:09:40.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.009987834732186687 Training loss: 7.483582019805908
2025-12-09 12:09:40.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.009987616615569263 Training loss: 7.841493606567383
2025-12-09 12:09:41.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.009987396563355204 Training loss: 7.943686008453369
2025-12-09 12:09:41.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.009987174575629911 Training loss: 7.738529205322266
2025-12-09 12:09:41.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.009986950652479532 Training loss: 7.780220031738281
2025-12-09 12:09:42.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.009986724793990967 Training loss: 7.507023811340332
2025-12-09 12:09:42.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.009986497000251867 Training loss: 7.7358198165893555
2025-12-09 12:09:42.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.009986267271350633 Training loss: 7.841141700744629
2025-12-09 12:09:43.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.00998603560737642 Training loss: 8.848093032836914
2025-12-09 12:09:43.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.009985802008419132 Training loss: 8.524969100952148
2025-12-09 12:09:44.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.009985566474569425 Training loss: 8.15163516998291
2025-12-09 12:09:44.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.009985329005918702 Training loss: 8.094366073608398
2025-12-09 12:09:44.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.009985089602559125 Training loss: 7.855593204498291
2025-12-09 12:09:45.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.009984848264583597 Training loss: 8.451111793518066
2025-12-09 12:09:45.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00998460499208578 Training loss: 8.240860939025879
2025-12-09 12:09:46.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00998435978516008 Training loss: 7.794686794281006
2025-12-09 12:09:46.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00998411264390166 Training loss: 7.4082722663879395
2025-12-09 12:09:46.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.009983863568406429 Training loss: 7.827634811401367
2025-12-09 12:09:47.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.009983612558771048 Training loss: 8.044790267944336
2025-12-09 12:09:47.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00998335961509293 Training loss: 7.893807411193848
2025-12-09 12:09:48.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.009983104737470239 Training loss: 7.61346435546875
2025-12-09 12:09:48.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.009982847926001886 Training loss: 7.987704753875732
2025-12-09 12:09:48.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.009982589180787534 Training loss: 7.532567501068115
2025-12-09 12:09:49.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.009982328501927597 Training loss: 7.991108417510986
2025-12-09 12:09:49.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.009982065889523242 Training loss: 7.823573112487793
2025-12-09 12:09:49.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00998180134367638 Training loss: 8.040810585021973
2025-12-09 12:09:50.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.009981534864489678 Training loss: 7.717865467071533
2025-12-09 12:09:50.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.009981266452066553 Training loss: 7.781715393066406
2025-12-09 12:09:51.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.009980996106511169 Training loss: 7.907845973968506
2025-12-09 12:09:51.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.009980723827928441 Training loss: 7.4603705406188965
2025-12-09 12:09:51.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.009980449616424037 Training loss: 7.384414196014404
2025-12-09 12:09:52.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00998017347210437 Training loss: 7.438211917877197
2025-12-09 12:09:52.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.009979895395076608 Training loss: 7.722017765045166
2025-12-09 12:09:53.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.009979615385448668 Training loss: 7.570188522338867
2025-12-09 12:09:53.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.009979333443329217 Training loss: 7.740850925445557
2025-12-09 12:09:53.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00997904956882767 Training loss: 7.847959041595459
2025-12-09 12:09:54.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.009978763762054192 Training loss: 7.808260917663574
2025-12-09 12:09:54.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0099784760231197 Training loss: 7.644620418548584
2025-12-09 12:09:54.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.00997818635213586 Training loss: 7.466878414154053
2025-12-09 12:09:55.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.009977894749215089 Training loss: 7.768025875091553
2025-12-09 12:09:55.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00997760121447055 Training loss: 7.487614154815674
2025-12-09 12:09:56.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.009977305748016158 Training loss: 7.807164669036865
2025-12-09 12:09:56.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00997700834996658 Training loss: 7.479662895202637
2025-12-09 12:09:56.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.009976709020437229 Training loss: 7.512643814086914
2025-12-09 12:09:57.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.00997640775954427 Training loss: 7.740013122558594
2025-12-09 12:09:57.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.009976104567404616 Training loss: 7.899715900421143
2025-12-09 12:09:58.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.009975799444135928 Training loss: 7.656102180480957
2025-12-09 12:09:58.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.009975492389856622 Training loss: 7.662411212921143
2025-12-09 12:09:58.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.009975183404685856 Training loss: 7.383532524108887
2025-12-09 12:09:59.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.009974872488743543 Training loss: 7.691189765930176
2025-12-09 12:09:59.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.009974559642150344 Training loss: 7.698823928833008
2025-12-09 12:10:00.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.009974244865027668 Training loss: 7.542869567871094
2025-12-09 12:10:00.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.009973928157497673 Training loss: 7.766332626342773
2025-12-09 12:10:00.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.009973609519683268 Training loss: 8.39306926727295
2025-12-09 12:10:01.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.009973288951708112 Training loss: 7.504817008972168
2025-12-09 12:10:01.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.009972966453696608 Training loss: 7.6773881912231445
2025-12-09 12:10:01.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.009972642025773911 Training loss: 7.503420829772949
2025-12-09 12:10:02.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.009972315668065928 Training loss: 7.82017183303833
2025-12-09 12:10:02.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00997198738069931 Training loss: 7.844531059265137
2025-12-09 12:10:03.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.009971657163801459 Training loss: 7.538690567016602
2025-12-09 12:10:03.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.009971325017500525 Training loss: 7.709425926208496
2025-12-09 12:10:03.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00997099094192541 Training loss: 7.522182464599609
2025-12-09 12:10:04.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.009970654937205762 Training loss: 7.742945671081543
2025-12-09 12:10:04.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.009970317003471976 Training loss: 7.5407233238220215
2025-12-09 12:10:05.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.009969977140855197 Training loss: 7.855037212371826
2025-12-09 12:10:05.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.009969635349487322 Training loss: 7.270535945892334
2025-12-09 12:10:05.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.009969291629500991 Training loss: 7.783672332763672
2025-12-09 12:10:06.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.009968945981029596 Training loss: 7.636234760284424
2025-12-09 12:10:06.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.009968598404207276 Training loss: 7.436007499694824
2025-12-09 12:10:06.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.009968248899168919 Training loss: 7.629038333892822
2025-12-09 12:10:07.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.00996789746605016 Training loss: 7.426293849945068
2025-12-09 12:10:07.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.009967544104987387 Training loss: 7.807324409484863
2025-12-09 12:10:08.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.009967188816117727 Training loss: 7.685818195343018
2025-12-09 12:10:08.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.009966831599579066 Training loss: 7.583306789398193
2025-12-09 12:10:08.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00996647245551003 Training loss: 8.31084156036377
2025-12-09 12:10:09.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.009966111384049996 Training loss: 7.5246806144714355
2025-12-09 12:10:09.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.009965748385339089 Training loss: 8.415411949157715
2025-12-09 12:10:10.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.00996538345951818 Training loss: 7.569220542907715
2025-12-09 12:10:10.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.009965016606728895 Training loss: 7.672807216644287
2025-12-09 12:10:10.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.009964647827113595 Training loss: 7.511192321777344
2025-12-09 12:10:11.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.009964277120815402 Training loss: 8.138970375061035
2025-12-09 12:10:11.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.009963904487978178 Training loss: 8.002615928649902
2025-12-09 12:10:12.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.009963529928746533 Training loss: 7.594285488128662
2025-12-09 12:10:12.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.009963153443265827 Training loss: 7.645310401916504
2025-12-09 12:10:12.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.00996277503168217 Training loss: 7.52149772644043
2025-12-09 12:10:13.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00996239469414241 Training loss: 9.012992858886719
2025-12-09 12:10:13.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.009962012430794153 Training loss: 7.391480922698975
2025-12-09 12:10:13.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.009961628241785746 Training loss: 8.210860252380371
2025-12-09 12:10:14.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.009961242127266288 Training loss: 7.7436699867248535
2025-12-09 12:10:14.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.009960854087385618 Training loss: 7.419352054595947
2025-12-09 12:10:15.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.00996046412229433 Training loss: 7.608795642852783
2025-12-09 12:10:15.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.009960072232143761 Training loss: 7.620068550109863
2025-12-09 12:10:15.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.009959678417085997 Training loss: 7.683347225189209
2025-12-09 12:10:16.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.009959282677273869 Training loss: 7.517338752746582
2025-12-09 12:10:16.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.009958885012860954 Training loss: 7.703037261962891
2025-12-09 12:10:17.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.009958485424001582 Training loss: 7.520098686218262
2025-12-09 12:10:17.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.009958083910850821 Training loss: 7.625129222869873
2025-12-09 12:10:17.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.009957680473564495 Training loss: 7.634808540344238
2025-12-09 12:10:18.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.009957275112299165 Training loss: 7.308627605438232
2025-12-09 12:10:18.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.009956867827212149 Training loss: 7.314211845397949
2025-12-09 12:10:18.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.009956458618461502 Training loss: 7.978985786437988
2025-12-09 12:10:19.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.009956047486206033 Training loss: 7.59154748916626
2025-12-09 12:10:19.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.00995563443060529 Training loss: 7.486171245574951
2025-12-09 12:10:20.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00995521945181958 Training loss: 7.4280548095703125
2025-12-09 12:10:20.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00995480255000994 Training loss: 7.615382671356201
2025-12-09 12:10:20.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.009954383725338167 Training loss: 7.890956401824951
2025-12-09 12:10:21.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.009953962977966795 Training loss: 7.73366117477417
2025-12-09 12:10:21.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00995354030805911 Training loss: 7.610858917236328
2025-12-09 12:10:22.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.009953115715779141 Training loss: 7.766814708709717
2025-12-09 12:10:22.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.009952689201291663 Training loss: 7.546913146972656
2025-12-09 12:10:22.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0099522607647622 Training loss: 7.671908378601074
2025-12-09 12:10:23.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00995183040635702 Training loss: 7.5001115798950195
2025-12-09 12:10:23.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.009951398126243134 Training loss: 7.750489234924316
2025-12-09 12:10:24.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.009950963924588304 Training loss: 7.490151882171631
2025-12-09 12:10:24.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.009950527801561034 Training loss: 8.283973693847656
2025-12-09 12:10:24.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.009950089757330574 Training loss: 7.722535133361816
2025-12-09 12:10:25.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.009949649792066922 Training loss: 7.52833366394043
2025-12-09 12:10:25.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00994920790594082 Training loss: 7.726585388183594
2025-12-09 12:10:25.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.009948764099123755 Training loss: 7.429949760437012
2025-12-09 12:10:26.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.00994831837178796 Training loss: 7.554405212402344
2025-12-09 12:10:26.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.009947870724106411 Training loss: 7.362534523010254
2025-12-09 12:10:27.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.009947421156252837 Training loss: 7.373218536376953
2025-12-09 12:10:27.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.009946969668401697 Training loss: 7.729368686676025
2025-12-09 12:10:27.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.009946516260728214 Training loss: 7.620269775390625
2025-12-09 12:10:28.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.009946060933408342 Training loss: 7.277665615081787
2025-12-09 12:10:28.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.009945603686618785 Training loss: 7.541835308074951
2025-12-09 12:10:29.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.009945144520536991 Training loss: 7.708131790161133
2025-12-09 12:10:29.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.009944683435341155 Training loss: 7.325390338897705
2025-12-09 12:10:29.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.009944220431210215 Training loss: 7.681406497955322
2025-12-09 12:10:30.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.009943755508323854 Training loss: 7.711911201477051
2025-12-09 12:10:30.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.009943288666862497 Training loss: 7.072324752807617
2025-12-09 12:10:30.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.00994281990700732 Training loss: 7.478427410125732
2025-12-09 12:10:31.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.009942349228940238 Training loss: 7.608855724334717
2025-12-09 12:10:31.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00994187663284391 Training loss: 7.219157695770264
2025-12-09 12:10:32.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.009941402118901743 Training loss: 7.436713218688965
2025-12-09 12:10:32.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.009940925687297887 Training loss: 7.518409252166748
2025-12-09 12:10:32.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.009940447338217234 Training loss: 7.60910177230835
2025-12-09 12:10:33.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.009939967071845425 Training loss: 7.3450493812561035
2025-12-09 12:10:33.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.009939484888368837 Training loss: 7.2926812171936035
2025-12-09 12:10:34.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.009939000787974602 Training loss: 7.557651519775391
2025-12-09 12:10:34.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.009938514770850585 Training loss: 7.773036003112793
2025-12-09 12:10:34.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.009938026837185403 Training loss: 7.457873344421387
2025-12-09 12:10:35.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.009937536987168413 Training loss: 7.8358869552612305
2025-12-09 12:10:35.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.009937045220989716 Training loss: 7.753752708435059
2025-12-09 12:10:36.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.009936551538840153 Training loss: 7.52466344833374
2025-12-09 12:10:36.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.009936055940911319 Training loss: 7.474754333496094
2025-12-09 12:10:36.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.009935558427395541 Training loss: 7.374904632568359
2025-12-09 12:10:37.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.009935058998485898 Training loss: 7.710376262664795
2025-12-09 12:10:37.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.009934557654376204 Training loss: 7.382701873779297
2025-12-09 12:10:37.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.009934054395261025 Training loss: 7.562123775482178
2025-12-09 12:10:38.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.009933549221335665 Training loss: 7.758755207061768
2025-12-09 12:10:38.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.009933042132796171 Training loss: 7.503002643585205
2025-12-09 12:10:39.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.009932533129839334 Training loss: 7.297426700592041
2025-12-09 12:10:39.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00993202221266269 Training loss: 7.895272731781006
2025-12-09 12:10:39.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.009931509381464514 Training loss: 7.370941162109375
2025-12-09 12:10:40.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.009930994636443828 Training loss: 7.502427577972412
2025-12-09 12:10:40.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.009930477977800391 Training loss: 7.5730671882629395
2025-12-09 12:10:41.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.009929959405734712 Training loss: 7.507777690887451
2025-12-09 12:10:41.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.009929438920448038 Training loss: 7.405078411102295
2025-12-09 12:10:41.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.009928916522142357 Training loss: 7.482422828674316
2025-12-09 12:10:42.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0099283922110204 Training loss: 7.280696392059326
2025-12-09 12:10:42.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.009927865987285648 Training loss: 7.204545021057129
2025-12-09 12:10:42.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.009927337851142314 Training loss: 7.272457599639893
2025-12-09 12:10:43.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.009926807802795359 Training loss: 7.3040876388549805
2025-12-09 12:10:43.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.009926275842450481 Training loss: 7.5086565017700195
2025-12-09 12:10:44.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.009925741970314128 Training loss: 8.262140274047852
2025-12-09 12:10:44.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.009925206186593483 Training loss: 7.542784214019775
2025-12-09 12:10:44.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.009924668491496473 Training loss: 7.32572078704834
2025-12-09 12:10:45.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.009924128885231769 Training loss: 8.07414722442627
2025-12-09 12:10:45.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.009923587368008779 Training loss: 8.38573169708252
2025-12-09 12:10:46.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.009923043940037657 Training loss: 7.513071060180664
2025-12-09 12:10:46.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.009922498601529295 Training loss: 7.588703632354736
2025-12-09 12:10:46.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.00992195135269533 Training loss: 7.699126243591309
2025-12-09 12:10:47.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.009921402193748138 Training loss: 7.510539531707764
2025-12-09 12:10:47.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.009920851124900838 Training loss: 7.941081523895264
2025-12-09 12:10:48.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.009920298146367286 Training loss: 7.652683258056641
2025-12-09 12:10:48.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.009919743258362085 Training loss: 7.740662574768066
2025-12-09 12:10:48.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.009919186461100576 Training loss: 7.3388872146606445
2025-12-09 12:10:49.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.009918627754798839 Training loss: 8.08931827545166
2025-12-09 12:10:49.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0099180671396737 Training loss: 7.312778949737549
2025-12-09 12:10:49.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.009917504615942721 Training loss: 7.873477935791016
2025-12-09 12:10:50.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.009916940183824205 Training loss: 7.021512031555176
2025-12-09 12:10:50.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.009916373843537201 Training loss: 7.4081830978393555
2025-12-09 12:10:51.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.009915805595301492 Training loss: 7.2504048347473145
2025-12-09 12:10:51.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.009915235439337602 Training loss: 7.077890396118164
2025-12-09 12:10:51.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.009914663375866804 Training loss: 7.401708602905273
2025-12-09 12:10:52.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.009914089405111097 Training loss: 8.433760643005371
2025-12-09 12:10:52.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.009913513527293234 Training loss: 7.24807071685791
2025-12-09 12:10:53.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.009912935742636698 Training loss: 7.72402811050415
2025-12-09 12:10:53.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.009912356051365718 Training loss: 7.553000450134277
2025-12-09 12:10:53.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.009911774453705257 Training loss: 7.34434700012207
2025-12-09 12:10:54.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00991119094988103 Training loss: 7.497420310974121
2025-12-09 12:10:54.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.009910605540119475 Training loss: 7.398504734039307
2025-12-09 12:10:55.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.009910018224647781 Training loss: 7.406291961669922
2025-12-09 12:10:55.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.009909429003693876 Training loss: 7.377997398376465
2025-12-09 12:10:55.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.009908837877486422 Training loss: 7.792779922485352
2025-12-09 12:10:56.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.009908244846254825 Training loss: 7.293363571166992
2025-12-09 12:10:56.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.009907649910229228 Training loss: 7.202423095703125
2025-12-09 12:10:56.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.009907053069640516 Training loss: 7.250880241394043
2025-12-09 12:10:57.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.009906454324720308 Training loss: 7.551741600036621
2025-12-09 12:10:57.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.009905853675700968 Training loss: 7.603823184967041
2025-12-09 12:10:58.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.009905251122815597 Training loss: 7.812702178955078
2025-12-09 12:10:58.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00990464666629803 Training loss: 7.273106098175049
2025-12-09 12:10:58.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.009904040306382847 Training loss: 7.633992671966553
2025-12-09 12:10:59.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.009903432043305365 Training loss: 7.485961437225342
2025-12-09 12:10:59.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.009902821877301638 Training loss: 7.553870677947998
2025-12-09 12:11:00.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00990220980860846 Training loss: 7.369173526763916
2025-12-09 12:11:00.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.009901595837463363 Training loss: 7.182636260986328
2025-12-09 12:11:00.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.009900979964104618 Training loss: 7.387495040893555
2025-12-09 12:11:01.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00990036218877123 Training loss: 7.474361419677734
2025-12-09 12:11:01.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.00989974251170295 Training loss: 7.3514909744262695
2025-12-09 12:11:01.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00989912093314026 Training loss: 7.317968845367432
2025-12-09 12:11:02.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.009898497453324384 Training loss: 7.713379859924316
2025-12-09 12:11:02.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.009897872072497281 Training loss: 7.669250011444092
2025-12-09 12:11:03.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.00989724479090165 Training loss: 7.455208778381348
2025-12-09 12:11:03.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.009896615608780924 Training loss: 7.459601879119873
2025-12-09 12:11:03.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.00989598452637928 Training loss: 7.341777324676514
2025-12-09 12:11:04.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.009895351543941628 Training loss: 7.325250625610352
2025-12-09 12:11:04.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.009894716661713616 Training loss: 7.599484920501709
2025-12-09 12:11:05.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.009894079879941628 Training loss: 8.131826400756836
2025-12-09 12:11:05.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.009893441198872787 Training loss: 7.342393398284912
2025-12-09 12:11:05.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.009892800618754954 Training loss: 7.144485950469971
2025-12-09 12:11:06.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.009892158139836724 Training loss: 7.552736759185791
2025-12-09 12:11:06.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.00989151376236743 Training loss: 7.09807825088501
2025-12-09 12:11:07.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.009890867486597146 Training loss: 7.582273960113525
2025-12-09 12:11:07.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.009890219312776677 Training loss: 7.272790908813477
2025-12-09 12:11:07.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.009889569241157564 Training loss: 7.790186405181885
2025-12-09 12:11:08.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.00988891727199209 Training loss: 7.7741498947143555
2025-12-09 12:11:08.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.009888263405533271 Training loss: 7.498084545135498
2025-12-09 12:11:08.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.009887607642034859 Training loss: 7.629440784454346
2025-12-09 12:11:09.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.009886949981751346 Training loss: 7.526432514190674
2025-12-09 12:11:09.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.009886290424937952 Training loss: 7.369968414306641
2025-12-09 12:11:10.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.009885628971850642 Training loss: 7.356497287750244
2025-12-09 12:11:10.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.009884965622746112 Training loss: 7.848931789398193
2025-12-09 12:11:10.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.009884300377881794 Training loss: 7.395492076873779
2025-12-09 12:11:11.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.009883633237515857 Training loss: 7.2290520668029785
2025-12-09 12:11:11.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.009882964201907207 Training loss: 7.10294771194458
2025-12-09 12:11:12.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.00988229327131548 Training loss: 7.681521892547607
2025-12-09 12:11:12.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.009881620446001056 Training loss: 7.494019031524658
2025-12-09 12:11:12.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.00988094572622504 Training loss: 7.592530250549316
2025-12-09 12:11:13.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.00988026911224928 Training loss: 7.594344615936279
2025-12-09 12:11:13.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.00987959060433636 Training loss: 6.903561115264893
2025-12-09 12:11:13.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.009878910202749589 Training loss: 7.602128505706787
2025-12-09 12:11:14.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.009878227907753022 Training loss: 7.494284152984619
2025-12-09 12:11:14.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.009877543719611444 Training loss: 7.454542636871338
2025-12-09 12:11:15.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.009876857638590373 Training loss: 7.539631366729736
2025-12-09 12:11:15.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.009876169664956067 Training loss: 7.437558650970459
2025-12-09 12:11:15.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.009875479798975512 Training loss: 7.4343485832214355
2025-12-09 12:11:16.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.009874788040916432 Training loss: 7.03497314453125
2025-12-09 12:11:16.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.009874094391047288 Training loss: 7.338759899139404
2025-12-09 12:11:17.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.009873398849637267 Training loss: 7.393733978271484
2025-12-09 12:11:17.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.009872701416956299 Training loss: 7.449803352355957
2025-12-09 12:11:17.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.009872002093275042 Training loss: 6.609776496887207
2025-12-09 12:11:18.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.00987130087886489 Training loss: 7.118298053741455
2025-12-09 12:11:18.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.009870597773997972 Training loss: 7.310823917388916
2025-12-09 12:11:19.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.009869892778947148 Training loss: 6.787532806396484
2025-12-09 12:11:19.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.009869185893986013 Training loss: 7.455399990081787
2025-12-09 12:11:19.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.009868477119388895 Training loss: 7.685577392578125
2025-12-09 12:11:20.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.009867766455430856 Training loss: 7.957728385925293
2025-12-09 12:11:20.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.009867053902387693 Training loss: 7.4016804695129395
2025-12-09 12:11:20.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.009866339460535929 Training loss: 7.871706485748291
2025-12-09 12:11:21.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.009865623130152828 Training loss: 7.691350936889648
2025-12-09 12:11:21.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.009864904911516384 Training loss: 7.426024436950684
2025-12-09 12:11:22.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.009864184804905323 Training loss: 7.637496471405029
2025-12-09 12:11:22.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.009863462810599103 Training loss: 7.7560529708862305
2025-12-09 12:11:22.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.009862738928877922 Training loss: 7.497246265411377
2025-12-09 12:11:23.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.009862013160022696 Training loss: 7.326463222503662
2025-12-09 12:11:23.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.009861285504315085 Training loss: 7.425178050994873
2025-12-09 12:11:24.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00986055596203748 Training loss: 7.3292412757873535
2025-12-09 12:11:24.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.009859824533472998 Training loss: 7.353517532348633
2025-12-09 12:11:24.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.009859091218905498 Training loss: 7.161673069000244
2025-12-09 12:11:25.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.00985835601861956 Training loss: 7.671576023101807
2025-12-09 12:11:25.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.009857618932900504 Training loss: 7.222116947174072
2025-12-09 12:11:25.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.009856879962034375 Training loss: 7.1739888191223145
2025-12-09 12:11:26.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.009856139106307955 Training loss: 7.991626739501953
2025-12-09 12:11:26.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.009855396366008757 Training loss: 7.440537452697754
2025-12-09 12:11:27.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.009854651741425023 Training loss: 7.294981956481934
2025-12-09 12:11:27.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.009853905232845728 Training loss: 7.397503852844238
2025-12-09 12:11:27.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.009853156840560576 Training loss: 7.2816290855407715
2025-12-09 12:11:28.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.009852406564860004 Training loss: 7.8152337074279785
2025-12-09 12:11:28.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.009851654406035179 Training loss: 7.371341228485107
2025-12-09 12:11:29.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.009850900364378 Training loss: 7.889325141906738
2025-12-09 12:11:29.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.009850144440181096 Training loss: 8.205016136169434
2025-12-09 12:11:29.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.009849386633737824 Training loss: 7.2632737159729
2025-12-09 12:11:30.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.009848626945342278 Training loss: 7.766486644744873
2025-12-09 12:11:30.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.009847865375289276 Training loss: 7.1428914070129395
2025-12-09 12:11:31.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.009847101923874366 Training loss: 7.393019676208496
2025-12-09 12:11:31.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.009846336591393832 Training loss: 7.491754531860352
2025-12-09 12:11:31.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.009845569378144686 Training loss: 7.153282642364502
2025-12-09 12:11:32.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.009844800284424663 Training loss: 6.593554496765137
2025-12-09 12:11:32.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.00984402931053224 Training loss: 7.691162586212158
2025-12-09 12:11:32.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.009843256456766609 Training loss: 8.095849990844727
2025-12-09 12:11:33.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.009842481723427705 Training loss: 7.014219284057617
2025-12-09 12:11:33.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.009841705110816185 Training loss: 7.712803363800049
2025-12-09 12:11:34.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00984092661923344 Training loss: 7.516927242279053
2025-12-09 12:11:34.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.009840146248981585 Training loss: 7.529959201812744
2025-12-09 12:11:34.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.009839364000363466 Training loss: 7.772701740264893
2025-12-09 12:11:35.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00983857987368266 Training loss: 7.472396373748779
2025-12-09 12:11:35.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.009837793869243468 Training loss: 6.994795799255371
2025-12-09 12:11:36.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.009837005987350926 Training loss: 6.762487411499023
2025-12-09 12:11:36.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.009836216228310797 Training loss: 6.88243293762207
2025-12-09 12:11:36.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.009835424592429568 Training loss: 7.398321628570557
2025-12-09 12:11:37.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.009834631080014457 Training loss: 7.611600399017334
2025-12-09 12:11:37.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.009833835691373412 Training loss: 7.529797554016113
2025-12-09 12:11:37.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.00983303842681511 Training loss: 7.403950214385986
2025-12-09 12:11:38.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.009832239286648949 Training loss: 7.301347732543945
2025-12-09 12:11:38.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.009831438271185065 Training loss: 7.726388454437256
2025-12-09 12:11:39.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.009830635380734313 Training loss: 7.250888347625732
2025-12-09 12:11:39.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.009829830615608279 Training loss: 7.446057319641113
2025-12-09 12:11:39.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.009829023976119278 Training loss: 7.567038059234619
2025-12-09 12:11:40.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.009828215462580352 Training loss: 7.579563140869141
2025-12-09 12:11:40.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.009827405075305266 Training loss: 7.8742499351501465
2025-12-09 12:11:41.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.009826592814608518 Training loss: 7.51179838180542
2025-12-09 12:11:41.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.00982577868080533 Training loss: 7.528382778167725
2025-12-09 12:11:41.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.009824962674211653 Training loss: 7.257623195648193
2025-12-09 12:11:42.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.009824144795144159 Training loss: 7.529794216156006
2025-12-09 12:11:42.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.009823325043920255 Training loss: 7.353743076324463
2025-12-09 12:11:43.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.009822503420858067 Training loss: 7.2763671875
2025-12-09 12:11:43.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.009821679926276456 Training loss: 7.497102737426758
2025-12-09 12:11:43.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.009820854560494998 Training loss: 7.80940055847168
2025-12-09 12:11:44.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.009820027323834007 Training loss: 7.501173973083496
2025-12-09 12:11:44.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.009819198216614512 Training loss: 7.291538238525391
2025-12-09 12:11:44.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.009818367239158278 Training loss: 7.486466884613037
2025-12-09 12:11:45.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.009817534391787787 Training loss: 7.261708736419678
2025-12-09 12:11:45.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.009816699674826256 Training loss: 7.419973373413086
2025-12-09 12:11:46.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.009815863088597618 Training loss: 7.315542221069336
2025-12-09 12:11:46.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.009815024633426537 Training loss: 7.880640029907227
2025-12-09 12:11:46.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0098141843096384 Training loss: 7.744953155517578
2025-12-09 12:11:47.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.009813342117559323 Training loss: 7.632628440856934
2025-12-09 12:11:47.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.009812498057516142 Training loss: 8.361857414245605
2025-12-09 12:11:48.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.009811652129836422 Training loss: 7.379230976104736
2025-12-09 12:11:48.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.009810804334848449 Training loss: 7.632153511047363
2025-12-09 12:11:48.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.009809954672881238 Training loss: 7.205657482147217
2025-12-09 12:11:49.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.009809103144264524 Training loss: 7.294486999511719
2025-12-09 12:11:49.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.009808249749328769 Training loss: 7.4048075675964355
2025-12-09 12:11:50.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.009807394488405159 Training loss: 7.236947536468506
2025-12-09 12:11:50.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.009806537361825607 Training loss: 7.476173400878906
2025-12-09 12:11:50.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.009805678369922742 Training loss: 7.323919773101807
2025-12-09 12:11:51.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.009804817513029926 Training loss: 7.470529556274414
2025-12-09 12:11:51.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.009803954791481238 Training loss: 7.483381271362305
2025-12-09 12:11:51.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.009803090205611487 Training loss: 7.256255149841309
2025-12-09 12:11:52.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.009802223755756198 Training loss: 7.211571216583252
2025-12-09 12:11:52.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.009801355442251625 Training loss: 7.214414596557617
2025-12-09 12:11:53.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.009800485265434745 Training loss: 7.273148059844971
2025-12-09 12:11:53.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.009799613225643253 Training loss: 7.493269443511963
2025-12-09 12:11:53.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.009798739323215573 Training loss: 7.44381046295166
2025-12-09 12:11:54.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00979786355849085 Training loss: 6.868198394775391
2025-12-09 12:11:54.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.009796985931808949 Training loss: 6.963034629821777
2025-12-09 12:11:55.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.009796106443510462 Training loss: 7.580628871917725
2025-12-09 12:11:55.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.009795225093936702 Training loss: 8.474045753479004
2025-12-09 12:11:55.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.009794341883429699 Training loss: 7.672577381134033
2025-12-09 12:11:56.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.009793456812332214 Training loss: 7.333316802978516
2025-12-09 12:11:56.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.009792569880987725 Training loss: 8.371347427368164
2025-12-09 12:11:56.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.009791681089740432 Training loss: 7.4932475090026855
2025-12-09 12:11:57.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.009790790438935257 Training loss: 7.167281150817871
2025-12-09 12:11:57.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.009789897928917846 Training loss: 7.522182941436768
2025-12-09 12:11:58.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.009789003560034561 Training loss: 7.472830295562744
2025-12-09 12:11:58.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.009788107332632493 Training loss: 7.408020496368408
2025-12-09 12:11:58.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.009787209247059453 Training loss: 7.436878204345703
2025-12-09 12:11:59.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.009786309303663962 Training loss: 7.256771087646484
2025-12-09 12:11:59.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.009785407502795277 Training loss: 7.503124713897705
2025-12-09 12:12:00.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.009784503844803368 Training loss: 7.3758673667907715
2025-12-09 12:12:00.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.009783598330038924 Training loss: 7.35767936706543
2025-12-09 12:12:00.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.009782690958853361 Training loss: 7.662302017211914
2025-12-09 12:12:01.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.009781781731598813 Training loss: 7.986825942993164
2025-12-09 12:12:01.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.00978087064862813 Training loss: 7.295827388763428
2025-12-09 12:12:02.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.009779957710294886 Training loss: 8.332552909851074
2025-12-09 12:12:02.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.009779042916953376 Training loss: 7.397876739501953
2025-12-09 12:12:02.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.009778126268958612 Training loss: 7.115326404571533
2025-12-09 12:12:03.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.009777207766666329 Training loss: 7.123432159423828
2025-12-09 12:12:03.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.00977628741043298 Training loss: 7.230593204498291
2025-12-09 12:12:03.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.009775365200615735 Training loss: 7.017728805541992
2025-12-09 12:12:04.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.009774441137572488 Training loss: 7.292820930480957
2025-12-09 12:12:04.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.009773515221661847 Training loss: 7.211131572723389
2025-12-09 12:12:05.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.009772587453243142 Training loss: 7.450753211975098
2025-12-09 12:12:05.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.009771657832676426 Training loss: 7.433441638946533
2025-12-09 12:12:05.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.009770726360322463 Training loss: 7.300180435180664
2025-12-09 12:12:06.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.009769793036542742 Training loss: 7.278715133666992
2025-12-09 12:12:06.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.009768857861699462 Training loss: 7.516940593719482
2025-12-09 12:12:07.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.009767920836155552 Training loss: 7.418342113494873
2025-12-09 12:12:07.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.009766981960274652 Training loss: 7.058056831359863
2025-12-09 12:12:07.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.009766041234421121 Training loss: 7.587108135223389
2025-12-09 12:12:08.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.009765098658960036 Training loss: 7.254961967468262
2025-12-09 12:12:08.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.00976415423425719 Training loss: 7.871984958648682
2025-12-09 12:12:08.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0097632079606791 Training loss: 7.9509806632995605
2025-12-09 12:12:09.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.009762259838592994 Training loss: 7.290749549865723
2025-12-09 12:12:09.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.009761309868366819 Training loss: 8.080552101135254
2025-12-09 12:12:10.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.009760358050369242 Training loss: 7.329413890838623
2025-12-09 12:12:10.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.009759404384969644 Training loss: 7.351097106933594
2025-12-09 12:12:10.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.009758448872538121 Training loss: 7.902377605438232
2025-12-09 12:12:11.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.009757491513445493 Training loss: 7.717203617095947
2025-12-09 12:12:11.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.009756532308063294 Training loss: 7.429409503936768
2025-12-09 12:12:12.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.009755571256763764 Training loss: 7.2099103927612305
2025-12-09 12:12:12.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.009754608359919878 Training loss: 7.727548599243164
2025-12-09 12:12:12.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.009753643617905313 Training loss: 7.668389797210693
2025-12-09 12:12:13.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.009752677031094465 Training loss: 7.57176399230957
2025-12-09 12:12:13.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.009751708599862451 Training loss: 7.128892421722412
2025-12-09 12:12:14.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.009750738324585098 Training loss: 7.357558727264404
2025-12-09 12:12:14.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.009749766205638952 Training loss: 7.3873748779296875
2025-12-09 12:12:14.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.009748792243401274 Training loss: 8.239011764526367
2025-12-09 12:12:15.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.009747816438250036 Training loss: 7.435668468475342
2025-12-09 12:12:15.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.009746838790563934 Training loss: 7.249029636383057
2025-12-09 12:12:15.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.009745859300722371 Training loss: 7.461339473724365
2025-12-09 12:12:16.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.009744877969105468 Training loss: 7.839138507843018
2025-12-09 12:12:16.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.009743894796094062 Training loss: 7.599367618560791
2025-12-09 12:12:17.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.009742909782069702 Training loss: 7.2118611335754395
2025-12-09 12:12:17.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.009741922927414652 Training loss: 7.466182231903076
2025-12-09 12:12:17.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.009740934232511893 Training loss: 7.237792491912842
2025-12-09 12:12:18.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.009739943697745118 Training loss: 7.878453254699707
2025-12-09 12:12:18.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.009738951323498732 Training loss: 7.242652893066406
2025-12-09 12:12:19.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.009737957110157859 Training loss: 7.183384895324707
2025-12-09 12:12:19.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.009736961058108331 Training loss: 7.120657444000244
2025-12-09 12:12:19.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.009735963167736698 Training loss: 7.417238235473633
2025-12-09 12:12:20.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.009734963439430222 Training loss: 7.280272006988525
2025-12-09 12:12:20.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.009733961873576877 Training loss: 7.187076568603516
2025-12-09 12:12:20.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.009732958470565352 Training loss: 7.5882673263549805
2025-12-09 12:12:21.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.009731953230785049 Training loss: 7.417818546295166
2025-12-09 12:12:21.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.009730946154626078 Training loss: 7.130001068115234
2025-12-09 12:12:22.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.00972993724247927 Training loss: 8.098163604736328
2025-12-09 12:12:22.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.009728926494736164 Training loss: 7.723449230194092
2025-12-09 12:12:22.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.009727913911789008 Training loss: 7.166381359100342
2025-12-09 12:12:23.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.009726899494030768 Training loss: 7.298944473266602
2025-12-09 12:12:23.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.009725883241855119 Training loss: 7.358877658843994
2025-12-09 12:12:24.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.009724865155656449 Training loss: 7.444201469421387
2025-12-09 12:12:24.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.009723845235829857 Training loss: 7.356246471405029
2025-12-09 12:12:24.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.009722823482771155 Training loss: 7.395292282104492
2025-12-09 12:12:25.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.009721799896876864 Training loss: 7.930926322937012
2025-12-09 12:12:25.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.009720774478544218 Training loss: 7.37047815322876
2025-12-09 12:12:26.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.009719747228171163 Training loss: 7.3419294357299805
2025-12-09 12:12:26.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.009718718146156354 Training loss: 7.350857257843018
2025-12-09 12:12:26.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.00971768723289916 Training loss: 7.321159839630127
2025-12-09 12:12:27.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.009716654488799652 Training loss: 7.347329616546631
2025-12-09 12:12:27.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.009715619914258624 Training loss: 7.285747528076172
2025-12-09 12:12:27.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00971458350967757 Training loss: 7.209366798400879
2025-12-09 12:12:28.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.009713545275458703 Training loss: 7.065714359283447
2025-12-09 12:12:28.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.009712505212004938 Training loss: 6.884268760681152
2025-12-09 12:12:29.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.009711463319719903 Training loss: 7.965512275695801
2025-12-09 12:12:29.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.009710419599007938 Training loss: 7.210707187652588
2025-12-09 12:12:29.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.009709374050274088 Training loss: 7.7941083908081055
2025-12-09 12:12:30.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.009708326673924114 Training loss: 7.474368095397949
2025-12-09 12:12:30.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.009707277470364482 Training loss: 7.417293071746826
2025-12-09 12:12:31.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.009706226440002363 Training loss: 7.027703285217285
2025-12-09 12:12:31.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.009705173583245644 Training loss: 7.43784761428833
2025-12-09 12:12:31.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.009704118900502918 Training loss: 7.358889579772949
2025-12-09 12:12:32.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.009703062392183489 Training loss: 7.510584354400635
2025-12-09 12:12:32.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.009702004058697363 Training loss: 7.240025997161865
2025-12-09 12:12:33.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.00970094390045526 Training loss: 7.694633960723877
2025-12-09 12:12:33.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00969988191786861 Training loss: 7.279272556304932
2025-12-09 12:12:33.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.009698818111349544 Training loss: 7.334325790405273
2025-12-09 12:12:34.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.009697752481310905 Training loss: 7.427333354949951
2025-12-09 12:12:34.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.009696685028166244 Training loss: 7.587650775909424
2025-12-09 12:12:34.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.00969561575232982 Training loss: 7.06468391418457
2025-12-09 12:12:35.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.009694544654216595 Training loss: 7.3552165031433105
2025-12-09 12:12:35.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.009693471734242244 Training loss: 6.934377670288086
2025-12-09 12:12:36.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.009692396992823146 Training loss: 7.115750312805176
2025-12-09 12:12:36.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.009691320430376385 Training loss: 7.861351013183594
2025-12-09 12:12:36.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.009690242047319756 Training loss: 7.456040859222412
2025-12-09 12:12:37.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.009689161844071757 Training loss: 7.280932426452637
2025-12-09 12:12:37.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.009688079821051594 Training loss: 7.219519138336182
2025-12-09 12:12:38.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.009686995978679181 Training loss: 7.389139175415039
2025-12-09 12:12:38.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.009685910317375132 Training loss: 7.50624942779541
2025-12-09 12:12:38.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.009684822837560777 Training loss: 7.645272254943848
2025-12-09 12:12:39.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.00968373353965814 Training loss: 7.443727970123291
2025-12-09 12:12:39.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.009682642424089958 Training loss: 7.522425174713135
2025-12-09 12:12:39.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.009681549491279673 Training loss: 7.613651752471924
2025-12-09 12:12:40.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.00968045474165143 Training loss: 7.452301025390625
2025-12-09 12:12:40.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.00967935817563008 Training loss: 7.9926958084106445
2025-12-09 12:12:41.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00967825979364118 Training loss: 7.3258819580078125
2025-12-09 12:12:41.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.009677159596110986 Training loss: 7.542942523956299
2025-12-09 12:12:41.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.009676057583466471 Training loss: 7.506320476531982
2025-12-09 12:12:42.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.009674953756135297 Training loss: 7.27277135848999
2025-12-09 12:12:42.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.009673848114545842 Training loss: 7.9960551261901855
2025-12-09 12:12:43.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.009672740659127184 Training loss: 7.1671223640441895
2025-12-09 12:12:43.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.009671631390309103 Training loss: 7.480555534362793
2025-12-09 12:12:43.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.009670520308522083 Training loss: 7.3441267013549805
2025-12-09 12:12:44.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.009669407414197318 Training loss: 7.146144866943359
2025-12-09 12:12:44.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.009668292707766698 Training loss: 7.326939105987549
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 3.29 GiB is free. Including non-PyTorch memory, this process has 90.93 GiB memory in use. Of the allocated memory 89.96 GiB is allocated by PyTorch, and 216.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   1%|          | 52/10000 [00:00<00:19, 517.06it/s]Tokenizing texts:   1%|▏         | 136/10000 [00:00<00:15, 630.30it/s]Tokenizing texts:   2%|▏         | 204/10000 [00:00<00:15, 649.21it/s]Tokenizing texts:   3%|▎         | 269/10000 [00:00<00:15, 646.01it/s]Tokenizing texts:   3%|▎         | 334/10000 [00:00<00:15, 619.68it/s]Tokenizing texts:   4%|▍         | 396/10000 [00:00<00:15, 607.71it/s]Tokenizing texts:   5%|▍         | 457/10000 [00:00<00:16, 585.81it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 580.29it/s]Tokenizing texts:   6%|▌         | 597/10000 [00:00<00:15, 603.35it/s]Tokenizing texts:   7%|▋         | 658/10000 [00:01<00:15, 596.42it/s]Tokenizing texts:   7%|▋         | 718/10000 [00:01<00:15, 581.63it/s]Tokenizing texts:   8%|▊         | 777/10000 [00:01<00:15, 583.28it/s]Tokenizing texts:   8%|▊         | 836/10000 [00:01<00:15, 579.63it/s]Tokenizing texts:   9%|▉         | 899/10000 [00:01<00:15, 592.41it/s]Tokenizing texts:  10%|▉         | 981/10000 [00:01<00:13, 657.83it/s]Tokenizing texts:  11%|█         | 1051/10000 [00:01<00:14, 638.82it/s]Tokenizing texts:  11%|█         | 1116/10000 [00:01<00:13, 636.57it/s]Tokenizing texts:  12%|█▏        | 1180/10000 [00:01<00:14, 602.82it/s]Tokenizing texts:  12%|█▏        | 1241/10000 [00:02<00:14, 595.14it/s]Tokenizing texts:  13%|█▎        | 1308/10000 [00:02<00:14, 615.65it/s]Tokenizing texts:  14%|█▎        | 1370/10000 [00:02<00:14, 587.99it/s]Tokenizing texts:  14%|█▍        | 1440/10000 [00:02<00:13, 618.43it/s]Tokenizing texts:  15%|█▌        | 1518/10000 [00:02<00:12, 654.61it/s]Tokenizing texts:  16%|█▌        | 1594/10000 [00:02<00:12, 684.51it/s]Tokenizing texts:  17%|█▋        | 1663/10000 [00:02<00:13, 616.53it/s]Tokenizing texts:  17%|█▋        | 1727/10000 [00:02<00:13, 616.84it/s]Tokenizing texts:  18%|█▊        | 1793/10000 [00:02<00:13, 628.75it/s]Tokenizing texts:  19%|█▊        | 1857/10000 [00:03<00:13, 610.80it/s]Tokenizing texts:  19%|█▉        | 1919/10000 [00:03<00:13, 610.00it/s]Tokenizing texts:  20%|█▉        | 1981/10000 [00:03<00:13, 578.97it/s]Tokenizing texts:  21%|██        | 2052/10000 [00:03<00:12, 612.28it/s]Tokenizing texts:  21%|██        | 2114/10000 [00:03<00:14, 560.74it/s]Tokenizing texts:  22%|██▏       | 2172/10000 [00:03<00:13, 561.54it/s]Tokenizing texts:  22%|██▏       | 2249/10000 [00:03<00:12, 617.82it/s]Tokenizing texts:  23%|██▎       | 2312/10000 [00:03<00:12, 600.38it/s]Tokenizing texts:  24%|██▍       | 2381/10000 [00:03<00:12, 625.20it/s]Tokenizing texts:  24%|██▍       | 2445/10000 [00:04<00:12, 628.82it/s]Tokenizing texts:  25%|██▌       | 2509/10000 [00:04<00:11, 625.34it/s]Tokenizing texts:  26%|██▌       | 2572/10000 [00:04<00:12, 612.75it/s]Tokenizing texts:  26%|██▋       | 2634/10000 [00:04<00:12, 574.93it/s]Tokenizing texts:  27%|██▋       | 2694/10000 [00:04<00:13, 554.56it/s]Tokenizing texts:  28%|██▊       | 2768/10000 [00:04<00:11, 603.53it/s]Tokenizing texts:  28%|██▊       | 2830/10000 [00:04<00:12, 555.70it/s]Tokenizing texts:  29%|██▉       | 2889/10000 [00:04<00:12, 560.49it/s]Tokenizing texts:  30%|██▉       | 2952/10000 [00:04<00:12, 578.09it/s]Tokenizing texts:  30%|███       | 3024/10000 [00:05<00:11, 600.14it/s]Tokenizing texts:  31%|███       | 3086/10000 [00:05<00:11, 598.67it/s]Tokenizing texts:  32%|███▏      | 3153/10000 [00:05<00:11, 610.17it/s]Tokenizing texts:  32%|███▏      | 3227/10000 [00:05<00:10, 645.87it/s]Tokenizing texts:  33%|███▎      | 3294/10000 [00:05<00:10, 645.14it/s]Tokenizing texts:  34%|███▎      | 3360/10000 [00:05<00:10, 649.01it/s]Tokenizing texts:  34%|███▍      | 3426/10000 [00:05<00:10, 646.40it/s]Tokenizing texts:  35%|███▍      | 3497/10000 [00:05<00:10, 649.68it/s]Tokenizing texts:  36%|███▌      | 3565/10000 [00:05<00:09, 657.86it/s]Tokenizing texts:  36%|███▋      | 3631/10000 [00:05<00:09, 655.76it/s]Tokenizing texts:  37%|███▋      | 3697/10000 [00:06<00:10, 628.87it/s]Tokenizing texts:  38%|███▊      | 3761/10000 [00:06<00:10, 617.84it/s]Tokenizing texts:  38%|███▊      | 3823/10000 [00:06<00:10, 585.20it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:06<00:09, 616.52it/s]Tokenizing texts:  40%|███▉      | 3958/10000 [00:06<00:10, 602.58it/s]Tokenizing texts:  40%|████      | 4019/10000 [00:06<00:10, 588.70it/s]Tokenizing texts:  41%|████      | 4096/10000 [00:06<00:09, 635.19it/s]Tokenizing texts:  42%|████▏     | 4169/10000 [00:06<00:08, 660.18it/s]Tokenizing texts:  42%|████▏     | 4236/10000 [00:06<00:08, 660.38it/s]Tokenizing texts:  43%|████▎     | 4303/10000 [00:07<00:09, 629.08it/s]Tokenizing texts:  44%|████▍     | 4382/10000 [00:07<00:08, 672.56it/s]Tokenizing texts:  44%|████▍     | 4450/10000 [00:07<00:08, 666.90it/s]Tokenizing texts:  45%|████▌     | 4518/10000 [00:07<00:08, 639.16it/s]Tokenizing texts:  46%|████▌     | 4585/10000 [00:07<00:08, 641.68it/s]Tokenizing texts:  47%|████▋     | 4660/10000 [00:07<00:07, 668.83it/s]Tokenizing texts:  47%|████▋     | 4739/10000 [00:07<00:07, 701.38it/s]Tokenizing texts:  48%|████▊     | 4810/10000 [00:07<00:07, 660.74it/s]Tokenizing texts:  49%|████▉     | 4881/10000 [00:07<00:07, 674.55it/s]Tokenizing texts:  50%|████▉     | 4951/10000 [00:07<00:07, 681.79it/s]Tokenizing texts:  50%|█████     | 5029/10000 [00:08<00:07, 700.60it/s]Tokenizing texts:  51%|█████     | 5100/10000 [00:08<00:07, 650.29it/s]Tokenizing texts:  52%|█████▏    | 5166/10000 [00:08<00:07, 631.77it/s]Tokenizing texts:  52%|█████▏    | 5230/10000 [00:08<00:07, 632.72it/s]Tokenizing texts:  53%|█████▎    | 5299/10000 [00:08<00:07, 647.98it/s]Tokenizing texts:  54%|█████▍    | 5383/10000 [00:08<00:06, 701.42it/s]Tokenizing texts:  55%|█████▍    | 5454/10000 [00:08<00:06, 686.50it/s]Tokenizing texts:  55%|█████▌    | 5524/10000 [00:08<00:07, 625.98it/s]Tokenizing texts:  56%|█████▌    | 5592/10000 [00:08<00:06, 640.32it/s]Tokenizing texts:  57%|█████▋    | 5677/10000 [00:09<00:06, 698.47it/s]Tokenizing texts:  57%|█████▋    | 5748/10000 [00:09<00:06, 667.21it/s]Tokenizing texts:  58%|█████▊    | 5816/10000 [00:09<00:06, 607.35it/s]Tokenizing texts:  59%|█████▉    | 5885/10000 [00:09<00:06, 628.54it/s]Tokenizing texts:  60%|█████▉    | 5950/10000 [00:09<00:06, 614.47it/s]Tokenizing texts:  60%|██████    | 6013/10000 [00:09<00:06, 602.12it/s]Tokenizing texts:  61%|██████    | 6090/10000 [00:09<00:06, 643.23it/s]Tokenizing texts:  62%|██████▏   | 6160/10000 [00:09<00:05, 658.46it/s]Tokenizing texts:  62%|██████▏   | 6227/10000 [00:09<00:05, 640.75it/s]Tokenizing texts:  63%|██████▎   | 6292/10000 [00:10<00:05, 627.74it/s]Tokenizing texts:  64%|██████▎   | 6356/10000 [00:10<00:05, 613.25it/s]Tokenizing texts:  64%|██████▍   | 6423/10000 [00:10<00:05, 626.58it/s]Tokenizing texts:  65%|██████▍   | 6486/10000 [00:10<00:05, 622.94it/s]Tokenizing texts:  65%|██████▌   | 6549/10000 [00:10<00:05, 621.64it/s]Tokenizing texts:  66%|██████▌   | 6622/10000 [00:10<00:05, 652.50it/s]Tokenizing texts:  67%|██████▋   | 6688/10000 [00:10<00:05, 648.81it/s]Tokenizing texts:  68%|██████▊   | 6753/10000 [00:10<00:05, 629.78it/s]Tokenizing texts:  68%|██████▊   | 6836/10000 [00:10<00:04, 687.50it/s]Tokenizing texts:  69%|██████▉   | 6906/10000 [00:11<00:05, 613.60it/s]Tokenizing texts:  70%|██████▉   | 6971/10000 [00:11<00:04, 621.38it/s]Tokenizing texts:  70%|███████   | 7035/10000 [00:11<00:04, 626.42it/s]Tokenizing texts:  71%|███████   | 7099/10000 [00:11<00:05, 570.80it/s]Tokenizing texts:  72%|███████▏  | 7164/10000 [00:11<00:04, 585.15it/s]Tokenizing texts:  72%|███████▏  | 7229/10000 [00:11<00:04, 597.12it/s]Tokenizing texts:  73%|███████▎  | 7311/10000 [00:11<00:04, 659.43it/s]Tokenizing texts:  74%|███████▍  | 7378/10000 [00:11<00:04, 646.05it/s]Tokenizing texts:  74%|███████▍  | 7444/10000 [00:11<00:04, 571.68it/s]Tokenizing texts:  75%|███████▌  | 7525/10000 [00:12<00:03, 633.21it/s]Tokenizing texts:  76%|███████▌  | 7597/10000 [00:12<00:03, 656.87it/s]Tokenizing texts:  77%|███████▋  | 7665/10000 [00:12<00:03, 660.05it/s]Tokenizing texts:  77%|███████▋  | 7733/10000 [00:12<00:03, 640.28it/s]Tokenizing texts:  78%|███████▊  | 7798/10000 [00:12<00:03, 616.46it/s]Tokenizing texts:  79%|███████▊  | 7868/10000 [00:12<00:03, 633.06it/s]Tokenizing texts:  79%|███████▉  | 7941/10000 [00:12<00:03, 660.01it/s]Tokenizing texts:  80%|████████  | 8018/10000 [00:12<00:02, 682.81it/s]Tokenizing texts:  81%|████████  | 8087/10000 [00:12<00:03, 577.02it/s]Tokenizing texts:  81%|████████▏ | 8148/10000 [00:13<00:03, 584.14it/s]Tokenizing texts:  82%|████████▏ | 8215/10000 [00:13<00:02, 606.88it/s]Tokenizing texts:  83%|████████▎ | 8278/10000 [00:13<00:02, 574.79it/s]Tokenizing texts:  84%|████████▎ | 8350/10000 [00:13<00:02, 608.23it/s]Tokenizing texts:  84%|████████▍ | 8413/10000 [00:13<00:02, 587.89it/s]Tokenizing texts:  85%|████████▍ | 8485/10000 [00:13<00:02, 622.38it/s]Tokenizing texts:  86%|████████▌ | 8559/10000 [00:13<00:02, 649.47it/s]Tokenizing texts:  86%|████████▋ | 8626/10000 [00:13<00:02, 654.84it/s]Tokenizing texts:  87%|████████▋ | 8698/10000 [00:13<00:01, 671.97it/s]Tokenizing texts:  88%|████████▊ | 8774/10000 [00:14<00:01, 692.77it/s]Tokenizing texts:  88%|████████▊ | 8844/10000 [00:14<00:01, 598.18it/s]Tokenizing texts:  89%|████████▉ | 8924/10000 [00:14<00:01, 649.12it/s]Tokenizing texts:  90%|████████▉ | 8996/10000 [00:14<00:01, 666.15it/s]Tokenizing texts:  91%|█████████ | 9065/10000 [00:14<00:01, 621.50it/s]Tokenizing texts:  91%|█████████▏| 9138/10000 [00:14<00:01, 650.01it/s]Tokenizing texts:  92%|█████████▏| 9214/10000 [00:14<00:01, 676.42it/s]Tokenizing texts:  93%|█████████▎| 9283/10000 [00:14<00:01, 628.71it/s]Tokenizing texts:  93%|█████████▎| 9348/10000 [00:14<00:01, 630.65it/s]Tokenizing texts:  94%|█████████▍| 9413/10000 [00:15<00:00, 601.91it/s]Tokenizing texts:  95%|█████████▍| 9475/10000 [00:15<00:00, 600.70it/s]Tokenizing texts:  95%|█████████▌| 9536/10000 [00:15<00:00, 562.42it/s]Tokenizing texts:  96%|█████████▌| 9597/10000 [00:15<00:00, 569.36it/s]Tokenizing texts:  97%|█████████▋| 9659/10000 [00:15<00:00, 580.18it/s]Tokenizing texts:  97%|█████████▋| 9718/10000 [00:15<00:00, 577.19it/s]Tokenizing texts:  98%|█████████▊| 9777/10000 [00:15<00:00, 550.72it/s]Tokenizing texts:  99%|█████████▊| 9855/10000 [00:15<00:00, 614.01it/s]Tokenizing texts:  99%|█████████▉| 9926/10000 [00:15<00:00, 639.04it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:15<00:00, 625.09it/s]
2025-12-09 12:13:34.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 12.201460838317871
2025-12-09 12:13:34.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 12.198165893554688
2025-12-09 12:13:35.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 12.148820877075195
2025-12-09 12:13:35.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 12.130343437194824
2025-12-09 12:13:36.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 12.205236434936523
2025-12-09 12:13:36.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 12.147665977478027
2025-12-09 12:13:37.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 12.14795207977295
2025-12-09 12:13:37.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 12.15354061126709
2025-12-09 12:13:37.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 12.1448392868042
2025-12-09 12:13:38.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 12.139297485351562
2025-12-09 12:13:38.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 12.1558256149292
2025-12-09 12:13:39.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 12.144689559936523
2025-12-09 12:13:39.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 12.119601249694824
2025-12-09 12:13:40.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 12.113824844360352
2025-12-09 12:13:40.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 12.133066177368164
2025-12-09 12:13:41.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 12.171608924865723
2025-12-09 12:13:41.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 12.105823516845703
2025-12-09 12:13:42.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 12.133622169494629
2025-12-09 12:13:42.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 12.083449363708496
2025-12-09 12:13:43.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 12.036575317382812
2025-12-09 12:13:43.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 12.099519729614258
2025-12-09 12:13:44.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 12.097342491149902
2025-12-09 12:13:44.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 12.041513442993164
2025-12-09 12:13:45.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 12.081128120422363
2025-12-09 12:13:45.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 12.13351058959961
2025-12-09 12:13:46.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 12.010446548461914
2025-12-09 12:13:46.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 12.089242935180664
2025-12-09 12:13:47.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 11.981820106506348
2025-12-09 12:13:47.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 12.021293640136719
2025-12-09 12:13:48.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 12.038382530212402
2025-12-09 12:13:48.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 11.917837142944336
2025-12-09 12:13:49.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 12.017256736755371
2025-12-09 12:13:49.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 11.959402084350586
2025-12-09 12:13:50.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 12.026069641113281
2025-12-09 12:13:50.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 11.93464183807373
2025-12-09 12:13:51.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 11.86706256866455
2025-12-09 12:13:51.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 11.875059127807617
2025-12-09 12:13:52.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 11.861076354980469
2025-12-09 12:13:52.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 11.828588485717773
2025-12-09 12:13:53.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 11.875213623046875
2025-12-09 12:13:53.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 11.80467414855957
2025-12-09 12:13:54.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 11.856610298156738
2025-12-09 12:13:54.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 11.866037368774414
2025-12-09 12:13:55.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 11.745052337646484
2025-12-09 12:13:55.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 11.705772399902344
2025-12-09 12:13:56.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 11.649694442749023
2025-12-09 12:13:56.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 11.51529598236084
2025-12-09 12:13:57.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 11.591415405273438
2025-12-09 12:13:57.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 11.419876098632812
2025-12-09 12:13:58.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 11.57651424407959
2025-12-09 12:13:58.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 11.51744556427002
2025-12-09 12:13:59.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 11.562071800231934
2025-12-09 12:13:59.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 11.516592025756836
2025-12-09 12:14:00.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 11.346236228942871
2025-12-09 12:14:00.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 11.169846534729004
2025-12-09 12:14:01.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 11.13955307006836
2025-12-09 12:14:01.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 11.158439636230469
2025-12-09 12:14:02.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 11.04804801940918
2025-12-09 12:14:02.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 11.116479873657227
2025-12-09 12:14:03.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 10.704198837280273
2025-12-09 12:14:03.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 10.806921005249023
2025-12-09 12:14:04.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 10.731898307800293
2025-12-09 12:14:04.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 10.63736343383789
2025-12-09 12:14:05.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 10.36595344543457
2025-12-09 12:14:05.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 10.558974266052246
2025-12-09 12:14:06.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 10.674817085266113
2025-12-09 12:14:06.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 10.520334243774414
2025-12-09 12:14:07.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 9.962148666381836
2025-12-09 12:14:07.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 9.902466773986816
2025-12-09 12:14:08.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 9.816038131713867
2025-12-09 12:14:08.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 10.370506286621094
2025-12-09 12:14:09.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 9.88889217376709
2025-12-09 12:14:09.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 9.456573486328125
2025-12-09 12:14:10.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 9.507525444030762
2025-12-09 12:14:10.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 9.481301307678223
2025-12-09 12:14:11.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 9.606810569763184
2025-12-09 12:14:11.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 9.253793716430664
2025-12-09 12:14:12.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 9.286761283874512
2025-12-09 12:14:12.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 9.168549537658691
2025-12-09 12:14:13.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 9.341309547424316
2025-12-09 12:14:13.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 9.042522430419922
2025-12-09 12:14:14.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 8.909781455993652
2025-12-09 12:14:14.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 9.552318572998047
2025-12-09 12:14:15.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 8.46994686126709
2025-12-09 12:14:15.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 8.780956268310547
2025-12-09 12:14:16.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 8.721671104431152
2025-12-09 12:14:16.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 8.949548721313477
2025-12-09 12:14:17.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 8.747241973876953
2025-12-09 12:14:17.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 8.662983894348145
2025-12-09 12:14:18.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 8.558809280395508
2025-12-09 12:14:18.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 8.513489723205566
2025-12-09 12:14:19.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 8.818072319030762
2025-12-09 12:14:19.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 8.633659362792969
2025-12-09 12:14:20.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 8.545601844787598
2025-12-09 12:14:20.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 8.82810115814209
2025-12-09 12:14:21.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 8.468365669250488
2025-12-09 12:14:21.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 8.40158462524414
2025-12-09 12:14:22.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 8.300299644470215
2025-12-09 12:14:22.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 8.331571578979492
2025-12-09 12:14:23.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 8.064627647399902
2025-12-09 12:14:23.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.999999029798809e-05 Training loss: 8.010393142700195
2025-12-09 12:14:24.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 9.999996119195611e-05 Training loss: 7.958607196807861
2025-12-09 12:14:24.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 9.999991268191536e-05 Training loss: 8.007575035095215
2025-12-09 12:14:25.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 9.999984476788465e-05 Training loss: 8.197789192199707
2025-12-09 12:14:25.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 9.999975744989037e-05 Training loss: 7.820580005645752
2025-12-09 12:14:26.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 9.999965072796636e-05 Training loss: 7.927764892578125
2025-12-09 12:14:26.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 9.999952460215408e-05 Training loss: 7.815101623535156
2025-12-09 12:14:27.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.999937907250246e-05 Training loss: 8.320295333862305
2025-12-09 12:14:27.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 9.999921413906798e-05 Training loss: 7.654260635375977
2025-12-09 12:14:28.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 9.999902980191464e-05 Training loss: 7.923309803009033
2025-12-09 12:14:28.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 9.999882606111399e-05 Training loss: 7.884395599365234
2025-12-09 12:14:29.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 9.999860291674508e-05 Training loss: 7.997189998626709
2025-12-09 12:14:29.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 9.999836036889453e-05 Training loss: 8.025753021240234
2025-12-09 12:14:30.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 9.999809841765644e-05 Training loss: 7.697027683258057
2025-12-09 12:14:30.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 9.999781706313251e-05 Training loss: 7.967260837554932
2025-12-09 12:14:31.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 9.999751630543188e-05 Training loss: 8.057418823242188
2025-12-09 12:14:31.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 9.99971961446713e-05 Training loss: 8.087964057922363
2025-12-09 12:14:32.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 9.999685658097502e-05 Training loss: 8.03530502319336
2025-12-09 12:14:32.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 9.999649761447478e-05 Training loss: 7.931018352508545
2025-12-09 12:14:33.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 9.999611924530994e-05 Training loss: 7.93073034286499
2025-12-09 12:14:33.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 9.999572147362731e-05 Training loss: 7.892167568206787
2025-12-09 12:14:34.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 9.999530429958124e-05 Training loss: 8.222652435302734
2025-12-09 12:14:34.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 9.999486772333366e-05 Training loss: 8.195026397705078
2025-12-09 12:14:35.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 9.999441174505399e-05 Training loss: 8.005648612976074
2025-12-09 12:14:35.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 9.999393636491918e-05 Training loss: 7.802584171295166
2025-12-09 12:14:36.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 9.99934415831137e-05 Training loss: 7.846959590911865
2025-12-09 12:14:36.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 9.99929273998296e-05 Training loss: 7.72840690612793
2025-12-09 12:14:37.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 9.99923938152664e-05 Training loss: 8.02266788482666
2025-12-09 12:14:37.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 9.999184082963118e-05 Training loss: 8.07529354095459
2025-12-09 12:14:38.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 9.999126844313853e-05 Training loss: 7.938948631286621
2025-12-09 12:14:38.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 9.999067665601061e-05 Training loss: 7.783435821533203
2025-12-09 12:14:39.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 9.999006546847707e-05 Training loss: 7.889222145080566
2025-12-09 12:14:39.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 9.998943488077508e-05 Training loss: 7.8291778564453125
2025-12-09 12:14:40.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 9.998878489314938e-05 Training loss: 7.886870861053467
2025-12-09 12:14:40.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 9.99881155058522e-05 Training loss: 7.7832183837890625
2025-12-09 12:14:41.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 9.998742671914335e-05 Training loss: 8.177409172058105
2025-12-09 12:14:41.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 9.99867185332901e-05 Training loss: 7.955257892608643
2025-12-09 12:14:42.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 9.998599094856732e-05 Training loss: 7.900577545166016
2025-12-09 12:14:42.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 9.99852439652573e-05 Training loss: 8.43873405456543
2025-12-09 12:14:43.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 9.998447758365002e-05 Training loss: 7.633056163787842
2025-12-09 12:14:43.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 9.998369180404283e-05 Training loss: 7.281134128570557
2025-12-09 12:14:44.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 9.99828866267407e-05 Training loss: 8.001338005065918
2025-12-09 12:14:44.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 9.998206205205611e-05 Training loss: 8.01901912689209
2025-12-09 12:14:45.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 9.998121808030906e-05 Training loss: 8.415409088134766
2025-12-09 12:14:45.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 9.998035471182708e-05 Training loss: 7.539086818695068
2025-12-09 12:14:46.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 9.997947194694519e-05 Training loss: 7.834521293640137
2025-12-09 12:14:46.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 9.997856978600604e-05 Training loss: 7.5832133293151855
2025-12-09 12:14:47.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 9.997764822935967e-05 Training loss: 7.601181507110596
2025-12-09 12:14:47.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 9.997670727736378e-05 Training loss: 7.815179824829102
2025-12-09 12:14:48.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 9.99757469303835e-05 Training loss: 8.041348457336426
2025-12-09 12:14:48.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 9.997476718879153e-05 Training loss: 7.950033187866211
2025-12-09 12:14:49.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 9.99737680529681e-05 Training loss: 7.452704906463623
2025-12-09 12:14:49.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 9.997274952330094e-05 Training loss: 7.941738128662109
2025-12-09 12:14:50.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 9.997171160018531e-05 Training loss: 7.548715114593506
2025-12-09 12:14:50.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 9.997065428402403e-05 Training loss: 8.410324096679688
2025-12-09 12:14:51.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 9.996957757522742e-05 Training loss: 7.584394931793213
2025-12-09 12:14:51.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 9.996848147421334e-05 Training loss: 7.176816940307617
2025-12-09 12:14:52.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 9.996736598140714e-05 Training loss: 7.720573902130127
2025-12-09 12:14:52.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 9.996623109724174e-05 Training loss: 7.408543109893799
2025-12-09 12:14:53.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 9.996507682215754e-05 Training loss: 8.244821548461914
2025-12-09 12:14:53.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 9.996390315660253e-05 Training loss: 7.485951900482178
2025-12-09 12:14:54.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 9.996271010103216e-05 Training loss: 7.724372863769531
2025-12-09 12:14:54.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 9.996149765590946e-05 Training loss: 7.589991569519043
2025-12-09 12:14:55.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 9.99602658217049e-05 Training loss: 7.772660255432129
2025-12-09 12:14:55.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 9.995901459889658e-05 Training loss: 7.59696626663208
2025-12-09 12:14:56.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 9.995774398797007e-05 Training loss: 7.5380659103393555
2025-12-09 12:14:56.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 9.995645398941846e-05 Training loss: 7.441972732543945
2025-12-09 12:14:57.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 9.995514460374238e-05 Training loss: 7.782124996185303
2025-12-09 12:14:57.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 9.995381583144996e-05 Training loss: 7.607110977172852
2025-12-09 12:14:58.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 9.995246767305688e-05 Training loss: 7.973625659942627
2025-12-09 12:14:58.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 9.995110012908634e-05 Training loss: 7.308711528778076
2025-12-09 12:14:59.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 9.994971320006905e-05 Training loss: 7.476075649261475
2025-12-09 12:14:59.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 9.994830688654326e-05 Training loss: 7.528695583343506
2025-12-09 12:15:00.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 9.994688118905472e-05 Training loss: 7.1392998695373535
2025-12-09 12:15:00.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 9.994543610815671e-05 Training loss: 7.614917278289795
2025-12-09 12:15:01.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 9.994397164441007e-05 Training loss: 7.987492561340332
2025-12-09 12:15:01.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 9.994248779838311e-05 Training loss: 7.843216896057129
2025-12-09 12:15:02.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 9.994098457065166e-05 Training loss: 8.18347454071045
2025-12-09 12:15:02.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 9.993946196179913e-05 Training loss: 7.974472522735596
2025-12-09 12:15:03.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 9.993791997241639e-05 Training loss: 7.816973686218262
2025-12-09 12:15:03.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 9.993635860310187e-05 Training loss: 7.709012508392334
2025-12-09 12:15:04.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 9.99347778544615e-05 Training loss: 7.515712261199951
2025-12-09 12:15:04.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 9.993317772710874e-05 Training loss: 7.630693435668945
2025-12-09 12:15:05.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 9.993155822166457e-05 Training loss: 7.647212028503418
2025-12-09 12:15:05.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 9.992991933875748e-05 Training loss: 7.770523548126221
2025-12-09 12:15:06.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 9.99282610790235e-05 Training loss: 7.598071098327637
2025-12-09 12:15:06.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 9.992658344310614e-05 Training loss: 7.446518898010254
2025-12-09 12:15:07.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 9.992488643165651e-05 Training loss: 7.788105487823486
2025-12-09 12:15:07.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 9.992317004533313e-05 Training loss: 7.231708526611328
2025-12-09 12:15:08.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 9.992143428480214e-05 Training loss: 7.4229607582092285
2025-12-09 12:15:08.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 9.991967915073714e-05 Training loss: 7.648131370544434
2025-12-09 12:15:09.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 9.991790464381926e-05 Training loss: 7.309208393096924
2025-12-09 12:15:09.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 9.991611076473714e-05 Training loss: 7.604617595672607
2025-12-09 12:15:10.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 9.991429751418697e-05 Training loss: 7.410830974578857
2025-12-09 12:15:10.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 9.991246489287245e-05 Training loss: 7.276089668273926
2025-12-09 12:15:11.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 9.991061290150475e-05 Training loss: 7.119787693023682
2025-12-09 12:15:11.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 9.990874154080259e-05 Training loss: 7.473388671875
2025-12-09 12:15:12.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 9.990685081149222e-05 Training loss: 7.425786018371582
2025-12-09 12:15:12.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 9.990494071430742e-05 Training loss: 7.874197006225586
2025-12-09 12:15:13.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 9.990301124998945e-05 Training loss: 7.675078868865967
2025-12-09 12:15:13.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 9.990106241928706e-05 Training loss: 7.292667865753174
2025-12-09 12:15:14.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 9.989909422295659e-05 Training loss: 7.555862903594971
2025-12-09 12:15:14.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 9.989710666176186e-05 Training loss: 7.4891228675842285
2025-12-09 12:15:15.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 9.989509973647417e-05 Training loss: 7.14741325378418
2025-12-09 12:15:15.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 9.989307344787242e-05 Training loss: 7.3188652992248535
2025-12-09 12:15:16.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 9.989102779674293e-05 Training loss: 7.2083916664123535
2025-12-09 12:15:16.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 9.98889627838796e-05 Training loss: 7.315304756164551
2025-12-09 12:15:17.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 9.98868784100838e-05 Training loss: 7.329986572265625
2025-12-09 12:15:17.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 9.988477467616447e-05 Training loss: 7.46726131439209
2025-12-09 12:15:18.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 9.988265158293799e-05 Training loss: 7.705692291259766
2025-12-09 12:15:18.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 9.98805091312283e-05 Training loss: 7.646030426025391
2025-12-09 12:15:19.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 9.987834732186687e-05 Training loss: 7.006268501281738
2025-12-09 12:15:19.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 9.987616615569263e-05 Training loss: 7.44155216217041
2025-12-09 12:15:20.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 9.987396563355205e-05 Training loss: 7.203571796417236
2025-12-09 12:15:20.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 9.987174575629911e-05 Training loss: 7.1688947677612305
2025-12-09 12:15:21.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 9.986950652479532e-05 Training loss: 7.508630752563477
2025-12-09 12:15:21.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 9.986724793990966e-05 Training loss: 7.764993667602539
2025-12-09 12:15:22.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 9.986497000251866e-05 Training loss: 7.182361602783203
2025-12-09 12:15:22.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 9.986267271350633e-05 Training loss: 7.634476661682129
2025-12-09 12:15:23.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 9.98603560737642e-05 Training loss: 7.403061389923096
2025-12-09 12:15:23.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 9.985802008419131e-05 Training loss: 8.420363426208496
2025-12-09 12:15:24.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 9.985566474569424e-05 Training loss: 7.331049919128418
2025-12-09 12:15:24.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 9.985329005918702e-05 Training loss: 7.38128137588501
2025-12-09 12:15:25.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 9.985089602559125e-05 Training loss: 6.942327499389648
2025-12-09 12:15:25.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 9.984848264583597e-05 Training loss: 7.653675079345703
2025-12-09 12:15:26.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 9.98460499208578e-05 Training loss: 7.116410732269287
2025-12-09 12:15:26.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 9.98435978516008e-05 Training loss: 7.886065483093262
2025-12-09 12:15:27.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 9.98411264390166e-05 Training loss: 7.4708051681518555
2025-12-09 12:15:27.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 9.983863568406428e-05 Training loss: 7.416419982910156
2025-12-09 12:15:28.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 9.983612558771049e-05 Training loss: 7.105575084686279
2025-12-09 12:15:28.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 9.983359615092931e-05 Training loss: 7.48132848739624
2025-12-09 12:15:29.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 9.983104737470239e-05 Training loss: 7.697038650512695
2025-12-09 12:15:29.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 9.982847926001886e-05 Training loss: 7.230849266052246
2025-12-09 12:15:30.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 9.982589180787534e-05 Training loss: 7.549211502075195
2025-12-09 12:15:30.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 9.982328501927599e-05 Training loss: 8.271438598632812
2025-12-09 12:15:31.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 9.982065889523242e-05 Training loss: 6.983007907867432
2025-12-09 12:15:31.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 9.98180134367638e-05 Training loss: 7.109930038452148
2025-12-09 12:15:32.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 9.981534864489679e-05 Training loss: 7.964678764343262
2025-12-09 12:15:32.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 9.981266452066553e-05 Training loss: 7.827180862426758
2025-12-09 12:15:33.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 9.98099610651117e-05 Training loss: 7.91472053527832
2025-12-09 12:15:33.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 9.980723827928441e-05 Training loss: 7.126764297485352
2025-12-09 12:15:34.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 9.980449616424037e-05 Training loss: 7.601544380187988
2025-12-09 12:15:34.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 9.98017347210437e-05 Training loss: 6.980252742767334
2025-12-09 12:15:35.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 9.979895395076609e-05 Training loss: 7.883555889129639
2025-12-09 12:15:35.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 9.979615385448669e-05 Training loss: 7.490159511566162
2025-12-09 12:15:36.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 9.979333443329217e-05 Training loss: 7.287847995758057
2025-12-09 12:15:36.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 9.97904956882767e-05 Training loss: 7.42538595199585
2025-12-09 12:15:37.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 9.978763762054194e-05 Training loss: 7.087042808532715
2025-12-09 12:15:37.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 9.978476023119701e-05 Training loss: 7.159426212310791
2025-12-09 12:15:38.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 9.978186352135861e-05 Training loss: 6.763211250305176
2025-12-09 12:15:38.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 9.977894749215089e-05 Training loss: 7.780646800994873
2025-12-09 12:15:39.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 9.97760121447055e-05 Training loss: 7.106659412384033
2025-12-09 12:15:39.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 9.977305748016159e-05 Training loss: 7.082874774932861
2025-12-09 12:15:40.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 9.977008349966582e-05 Training loss: 7.426329612731934
2025-12-09 12:15:40.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 9.976709020437229e-05 Training loss: 7.241764068603516
2025-12-09 12:15:41.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 9.97640775954427e-05 Training loss: 7.395303249359131
2025-12-09 12:15:41.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 9.976104567404617e-05 Training loss: 6.991986274719238
2025-12-09 12:15:42.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 9.97579944413593e-05 Training loss: 7.837040901184082
2025-12-09 12:15:42.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 9.975492389856622e-05 Training loss: 7.158387660980225
2025-12-09 12:15:43.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 9.975183404685856e-05 Training loss: 7.238653659820557
2025-12-09 12:15:43.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 9.974872488743543e-05 Training loss: 6.943161487579346
2025-12-09 12:15:44.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 9.974559642150345e-05 Training loss: 7.462873935699463
2025-12-09 12:15:44.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 9.974244865027669e-05 Training loss: 7.811570644378662
2025-12-09 12:15:45.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 9.973928157497674e-05 Training loss: 7.234695911407471
2025-12-09 12:15:45.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 9.973609519683268e-05 Training loss: 6.998234748840332
2025-12-09 12:15:46.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 9.973288951708111e-05 Training loss: 7.206283092498779
2025-12-09 12:15:46.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 9.972966453696608e-05 Training loss: 7.179570198059082
2025-12-09 12:15:47.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 9.972642025773912e-05 Training loss: 7.368072032928467
2025-12-09 12:15:47.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 9.972315668065929e-05 Training loss: 7.110312461853027
2025-12-09 12:15:48.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 9.97198738069931e-05 Training loss: 7.489466667175293
2025-12-09 12:15:48.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 9.971657163801458e-05 Training loss: 7.018795967102051
2025-12-09 12:15:49.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 9.971325017500526e-05 Training loss: 7.566162586212158
2025-12-09 12:15:49.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 9.970990941925411e-05 Training loss: 8.517322540283203
2025-12-09 12:15:50.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 9.970654937205762e-05 Training loss: 8.184199333190918
2025-12-09 12:15:50.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 9.970317003471976e-05 Training loss: 7.605200290679932
2025-12-09 12:15:51.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 9.969977140855198e-05 Training loss: 7.5016045570373535
2025-12-09 12:15:51.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 9.969635349487321e-05 Training loss: 7.546253204345703
2025-12-09 12:15:52.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 9.969291629500991e-05 Training loss: 7.013237953186035
2025-12-09 12:15:52.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 9.968945981029596e-05 Training loss: 7.6832475662231445
2025-12-09 12:15:53.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 9.968598404207275e-05 Training loss: 7.134426593780518
2025-12-09 12:15:53.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 9.96824889916892e-05 Training loss: 7.283824920654297
2025-12-09 12:15:54.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 9.96789746605016e-05 Training loss: 7.824086666107178
2025-12-09 12:15:54.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 9.967544104987387e-05 Training loss: 7.303545951843262
2025-12-09 12:15:55.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 9.967188816117727e-05 Training loss: 7.016197681427002
2025-12-09 12:15:55.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 9.966831599579066e-05 Training loss: 7.604879856109619
2025-12-09 12:15:56.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 9.96647245551003e-05 Training loss: 7.314484119415283
2025-12-09 12:15:56.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 9.966111384049997e-05 Training loss: 7.809372901916504
2025-12-09 12:15:57.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 9.965748385339089e-05 Training loss: 7.022244453430176
2025-12-09 12:15:57.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 9.96538345951818e-05 Training loss: 7.1209845542907715
2025-12-09 12:15:58.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 9.965016606728894e-05 Training loss: 7.191397190093994
2025-12-09 12:15:58.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 9.964647827113595e-05 Training loss: 7.286648750305176
2025-12-09 12:15:59.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 9.964277120815401e-05 Training loss: 7.240849018096924
2025-12-09 12:15:59.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 9.963904487978177e-05 Training loss: 7.25460147857666
2025-12-09 12:16:00.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 9.963529928746534e-05 Training loss: 7.022228240966797
2025-12-09 12:16:00.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 9.963153443265828e-05 Training loss: 7.3609137535095215
2025-12-09 12:16:01.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 9.96277503168217e-05 Training loss: 7.30460786819458
2025-12-09 12:16:01.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 9.96239469414241e-05 Training loss: 6.758696556091309
2025-12-09 12:16:02.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 9.962012430794153e-05 Training loss: 8.226387023925781
2025-12-09 12:16:02.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 9.961628241785747e-05 Training loss: 7.243266582489014
2025-12-09 12:16:03.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 9.961242127266288e-05 Training loss: 6.924282073974609
2025-12-09 12:16:03.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 9.960854087385619e-05 Training loss: 7.130082607269287
2025-12-09 12:16:04.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 9.96046412229433e-05 Training loss: 7.284675121307373
2025-12-09 12:16:04.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 9.960072232143762e-05 Training loss: 7.282827377319336
2025-12-09 12:16:05.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 9.959678417085997e-05 Training loss: 6.762303829193115
2025-12-09 12:16:05.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 9.95928267727387e-05 Training loss: 7.430505275726318
2025-12-09 12:16:06.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 9.958885012860954e-05 Training loss: 7.027013301849365
2025-12-09 12:16:06.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 9.958485424001583e-05 Training loss: 7.307384490966797
2025-12-09 12:16:07.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 9.958083910850821e-05 Training loss: 6.853944301605225
2025-12-09 12:16:07.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 9.957680473564495e-05 Training loss: 7.052374839782715
2025-12-09 12:16:08.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 9.957275112299165e-05 Training loss: 7.018294811248779
2025-12-09 12:16:08.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 9.956867827212148e-05 Training loss: 7.644937038421631
2025-12-09 12:16:09.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 9.956458618461502e-05 Training loss: 7.135405540466309
2025-12-09 12:16:09.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 9.956047486206032e-05 Training loss: 7.026998043060303
2025-12-09 12:16:10.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 9.955634430605291e-05 Training loss: 7.554964065551758
2025-12-09 12:16:10.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 9.955219451819579e-05 Training loss: 6.918619155883789
2025-12-09 12:16:11.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 9.954802550009942e-05 Training loss: 7.025590419769287
2025-12-09 12:16:11.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 9.954383725338167e-05 Training loss: 6.821561336517334
2025-12-09 12:16:12.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 9.953962977966795e-05 Training loss: 6.973448753356934
2025-12-09 12:16:12.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 9.953540308059111e-05 Training loss: 7.462558269500732
2025-12-09 12:16:13.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 9.953115715779141e-05 Training loss: 6.693186283111572
2025-12-09 12:16:13.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 9.952689201291664e-05 Training loss: 6.989119052886963
2025-12-09 12:16:14.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 9.9522607647622e-05 Training loss: 7.8633294105529785
2025-12-09 12:16:14.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 9.951830406357019e-05 Training loss: 6.896899700164795
2025-12-09 12:16:15.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 9.951398126243134e-05 Training loss: 7.305821418762207
2025-12-09 12:16:15.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 9.950963924588303e-05 Training loss: 7.7399749755859375
2025-12-09 12:16:16.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 9.950527801561033e-05 Training loss: 7.74749231338501
2025-12-09 12:16:16.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 9.950089757330574e-05 Training loss: 7.357757568359375
2025-12-09 12:16:17.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 9.949649792066922e-05 Training loss: 6.831017017364502
2025-12-09 12:16:17.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 9.94920790594082e-05 Training loss: 7.239655017852783
2025-12-09 12:16:18.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 9.948764099123755e-05 Training loss: 7.0019145011901855
2025-12-09 12:16:18.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 9.94831837178796e-05 Training loss: 7.097341537475586
2025-12-09 12:16:19.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.947870724106412e-05 Training loss: 6.854701042175293
2025-12-09 12:16:19.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.947421156252836e-05 Training loss: 7.253712177276611
2025-12-09 12:16:20.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.946969668401697e-05 Training loss: 6.946214199066162
2025-12-09 12:16:20.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 9.946516260728214e-05 Training loss: 7.178765773773193
2025-12-09 12:16:21.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 9.946060933408341e-05 Training loss: 7.252036094665527
2025-12-09 12:16:21.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 9.945603686618785e-05 Training loss: 7.172789096832275
2025-12-09 12:16:22.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 9.945144520536992e-05 Training loss: 7.217391490936279
2025-12-09 12:16:22.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 9.944683435341155e-05 Training loss: 7.07758903503418
2025-12-09 12:16:23.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 9.944220431210216e-05 Training loss: 7.182199478149414
2025-12-09 12:16:23.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 9.943755508323855e-05 Training loss: 6.9811110496521
2025-12-09 12:16:24.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 9.943288666862498e-05 Training loss: 6.823312282562256
2025-12-09 12:16:24.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 9.942819907007321e-05 Training loss: 7.1427130699157715
2025-12-09 12:16:25.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 9.942349228940237e-05 Training loss: 7.144467830657959
2025-12-09 12:16:25.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 9.941876632843909e-05 Training loss: 7.233497619628906
2025-12-09 12:16:26.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 9.941402118901744e-05 Training loss: 6.887997150421143
2025-12-09 12:16:26.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 9.940925687297886e-05 Training loss: 7.7915425300598145
2025-12-09 12:16:27.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 9.940447338217234e-05 Training loss: 7.002638339996338
2025-12-09 12:16:27.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 9.939967071845423e-05 Training loss: 7.197865962982178
2025-12-09 12:16:28.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 9.939484888368838e-05 Training loss: 7.091440677642822
2025-12-09 12:16:28.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 9.939000787974602e-05 Training loss: 7.3059611320495605
2025-12-09 12:16:29.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 9.938514770850587e-05 Training loss: 6.761619567871094
2025-12-09 12:16:29.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 9.938026837185404e-05 Training loss: 7.4472455978393555
2025-12-09 12:16:30.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 9.937536987168413e-05 Training loss: 7.197834491729736
2025-12-09 12:16:30.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 9.937045220989715e-05 Training loss: 6.974542140960693
2025-12-09 12:16:31.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 9.936551538840155e-05 Training loss: 6.856101036071777
2025-12-09 12:16:31.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.93605594091132e-05 Training loss: 6.937452793121338
2025-12-09 12:16:32.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.935558427395542e-05 Training loss: 7.082546234130859
2025-12-09 12:16:32.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 9.935058998485897e-05 Training loss: 7.020440578460693
2025-12-09 12:16:33.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 9.934557654376205e-05 Training loss: 6.955607891082764
2025-12-09 12:16:33.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 9.934054395261026e-05 Training loss: 7.309118747711182
2025-12-09 12:16:34.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 9.933549221335664e-05 Training loss: 6.9603729248046875
2025-12-09 12:16:34.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 9.933042132796171e-05 Training loss: 6.848506927490234
2025-12-09 12:16:35.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 9.932533129839334e-05 Training loss: 6.985394477844238
2025-12-09 12:16:35.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 9.932022212662691e-05 Training loss: 7.105172157287598
2025-12-09 12:16:36.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 9.931509381464515e-05 Training loss: 7.142360210418701
2025-12-09 12:16:36.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 9.930994636443829e-05 Training loss: 6.798501491546631
2025-12-09 12:16:37.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 9.930477977800392e-05 Training loss: 7.017794609069824
2025-12-09 12:16:37.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 9.929959405734712e-05 Training loss: 6.960572719573975
2025-12-09 12:16:38.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 9.929438920448037e-05 Training loss: 6.394477367401123
2025-12-09 12:16:38.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 9.928916522142357e-05 Training loss: 6.960611343383789
2025-12-09 12:16:39.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 9.928392211020401e-05 Training loss: 7.207242965698242
2025-12-09 12:16:39.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.927865987285649e-05 Training loss: 6.971323013305664
2025-12-09 12:16:40.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 9.927337851142314e-05 Training loss: 6.843529224395752
2025-12-09 12:16:40.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 9.926807802795359e-05 Training loss: 7.014187812805176
2025-12-09 12:16:41.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 9.926275842450483e-05 Training loss: 7.254826068878174
2025-12-09 12:16:41.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 9.925741970314129e-05 Training loss: 7.079670429229736
2025-12-09 12:16:42.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 9.925206186593484e-05 Training loss: 6.928071022033691
2025-12-09 12:16:42.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 9.924668491496474e-05 Training loss: 6.267629623413086
2025-12-09 12:16:43.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 9.92412888523177e-05 Training loss: 6.572628498077393
2025-12-09 12:16:43.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 9.923587368008778e-05 Training loss: 7.02730131149292
2025-12-09 12:16:44.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 9.923043940037657e-05 Training loss: 6.384024620056152
2025-12-09 12:16:44.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 9.922498601529296e-05 Training loss: 6.691583156585693
2025-12-09 12:16:45.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 9.92195135269533e-05 Training loss: 7.019784450531006
2025-12-09 12:16:45.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 9.921402193748139e-05 Training loss: 6.854006767272949
2025-12-09 12:16:46.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 9.920851124900837e-05 Training loss: 7.668516159057617
2025-12-09 12:16:46.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 9.920298146367286e-05 Training loss: 6.859020709991455
2025-12-09 12:16:47.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 9.919743258362085e-05 Training loss: 6.933253288269043
2025-12-09 12:16:47.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 9.919186461100576e-05 Training loss: 6.83120059967041
2025-12-09 12:16:48.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 9.91862775479884e-05 Training loss: 6.897619724273682
2025-12-09 12:16:48.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 9.9180671396737e-05 Training loss: 6.981490135192871
2025-12-09 12:16:49.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 9.91750461594272e-05 Training loss: 6.9054646492004395
2025-12-09 12:16:49.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 9.916940183824206e-05 Training loss: 7.106533050537109
2025-12-09 12:16:50.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 9.916373843537201e-05 Training loss: 6.640992164611816
2025-12-09 12:16:50.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 9.915805595301491e-05 Training loss: 7.000094413757324
2025-12-09 12:16:51.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 9.915235439337603e-05 Training loss: 6.648430824279785
2025-12-09 12:16:51.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 9.914663375866804e-05 Training loss: 6.801923751831055
2025-12-09 12:16:52.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 9.914089405111098e-05 Training loss: 6.923483848571777
2025-12-09 12:16:52.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 9.913513527293235e-05 Training loss: 7.589090347290039
2025-12-09 12:16:53.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 9.912935742636698e-05 Training loss: 6.927874565124512
2025-12-09 12:16:53.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 9.912356051365718e-05 Training loss: 7.542843341827393
2025-12-09 12:16:54.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 9.911774453705258e-05 Training loss: 6.963726997375488
2025-12-09 12:16:54.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 9.91119094988103e-05 Training loss: 7.073121547698975
2025-12-09 12:16:55.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 9.910605540119475e-05 Training loss: 7.059233665466309
2025-12-09 12:16:55.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 9.91001822464778e-05 Training loss: 6.630779266357422
2025-12-09 12:16:56.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 9.909429003693876e-05 Training loss: 7.194968223571777
2025-12-09 12:16:56.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 9.908837877486423e-05 Training loss: 7.6617631912231445
2025-12-09 12:16:57.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 9.908244846254826e-05 Training loss: 6.990063190460205
2025-12-09 12:16:57.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 9.907649910229229e-05 Training loss: 6.744412899017334
2025-12-09 12:16:58.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 9.907053069640517e-05 Training loss: 7.140771865844727
2025-12-09 12:16:58.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 9.90645432472031e-05 Training loss: 7.141327857971191
2025-12-09 12:16:59.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 9.905853675700969e-05 Training loss: 7.198388576507568
2025-12-09 12:16:59.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 9.905251122815596e-05 Training loss: 6.673981189727783
2025-12-09 12:17:00.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 9.90464666629803e-05 Training loss: 6.679752349853516
2025-12-09 12:17:00.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 9.904040306382846e-05 Training loss: 6.8273210525512695
2025-12-09 12:17:01.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 9.903432043305365e-05 Training loss: 8.022760391235352
2025-12-09 12:17:01.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 9.902821877301637e-05 Training loss: 6.689763069152832
2025-12-09 12:17:02.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 9.90220980860846e-05 Training loss: 7.286319732666016
2025-12-09 12:17:02.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 9.901595837463363e-05 Training loss: 6.743813991546631
2025-12-09 12:17:03.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 9.900979964104617e-05 Training loss: 7.114931583404541
2025-12-09 12:17:03.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 9.900362188771231e-05 Training loss: 6.850356101989746
2025-12-09 12:17:04.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 9.899742511702951e-05 Training loss: 7.10296630859375
2025-12-09 12:17:04.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 9.89912093314026e-05 Training loss: 6.68695592880249
2025-12-09 12:17:05.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 9.898497453324384e-05 Training loss: 7.138900279998779
2025-12-09 12:17:05.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 9.897872072497281e-05 Training loss: 6.9122443199157715
2025-12-09 12:17:06.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 9.897244790901649e-05 Training loss: 6.822948932647705
2025-12-09 12:17:06.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 9.896615608780925e-05 Training loss: 6.970348358154297
2025-12-09 12:17:07.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 9.895984526379281e-05 Training loss: 7.132620334625244
2025-12-09 12:17:07.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 9.895351543941629e-05 Training loss: 6.81585168838501
2025-12-09 12:17:08.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 9.894716661713617e-05 Training loss: 6.97437047958374
2025-12-09 12:17:08.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 9.894079879941627e-05 Training loss: 7.044064521789551
2025-12-09 12:17:09.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 9.893441198872787e-05 Training loss: 6.905606746673584
2025-12-09 12:17:09.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 9.892800618754954e-05 Training loss: 7.011264801025391
2025-12-09 12:17:10.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 9.892158139836725e-05 Training loss: 7.133823871612549
2025-12-09 12:17:10.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 9.891513762367431e-05 Training loss: 7.826428413391113
2025-12-09 12:17:11.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 9.890867486597146e-05 Training loss: 6.887823581695557
2025-12-09 12:17:11.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 9.890219312776676e-05 Training loss: 6.57088565826416
2025-12-09 12:17:12.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 9.889569241157563e-05 Training loss: 7.023700714111328
2025-12-09 12:17:12.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 9.888917271992091e-05 Training loss: 7.203060150146484
2025-12-09 12:17:13.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 9.888263405533271e-05 Training loss: 6.67720365524292
2025-12-09 12:17:13.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 9.88760764203486e-05 Training loss: 6.971421718597412
2025-12-09 12:17:14.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 9.886949981751346e-05 Training loss: 6.338496685028076
2025-12-09 12:17:14.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 9.886290424937952e-05 Training loss: 6.704822540283203
2025-12-09 12:17:15.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 9.885628971850642e-05 Training loss: 6.611301422119141
2025-12-09 12:17:15.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 9.884965622746111e-05 Training loss: 7.146856784820557
2025-12-09 12:17:16.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 9.884300377881795e-05 Training loss: 7.070377349853516
2025-12-09 12:17:16.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 9.883633237515858e-05 Training loss: 7.096350193023682
2025-12-09 12:17:17.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 9.882964201907207e-05 Training loss: 6.910870552062988
2025-12-09 12:17:17.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 9.882293271315481e-05 Training loss: 6.676077365875244
2025-12-09 12:17:18.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 9.881620446001056e-05 Training loss: 7.206750869750977
2025-12-09 12:17:18.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 9.88094572622504e-05 Training loss: 7.442686557769775
2025-12-09 12:17:19.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 9.88026911224928e-05 Training loss: 6.703261852264404
2025-12-09 12:17:19.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 9.879590604336359e-05 Training loss: 6.702146053314209
2025-12-09 12:17:20.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 9.87891020274959e-05 Training loss: 6.989600658416748
2025-12-09 12:17:20.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 9.878227907753021e-05 Training loss: 6.8826680183410645
2025-12-09 12:17:21.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 9.877543719611444e-05 Training loss: 6.688409328460693
2025-12-09 12:17:21.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 9.876857638590373e-05 Training loss: 6.907704830169678
2025-12-09 12:17:22.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 9.876169664956067e-05 Training loss: 7.007709980010986
2025-12-09 12:17:22.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 9.875479798975512e-05 Training loss: 6.817147731781006
2025-12-09 12:17:23.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 9.874788040916432e-05 Training loss: 7.094610691070557
2025-12-09 12:17:23.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 9.874094391047289e-05 Training loss: 7.269367218017578
2025-12-09 12:17:24.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 9.873398849637268e-05 Training loss: 7.207357883453369
2025-12-09 12:17:24.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 9.872701416956299e-05 Training loss: 7.072795391082764
2025-12-09 12:17:25.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 9.872002093275042e-05 Training loss: 6.999342918395996
2025-12-09 12:17:25.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 9.871300878864891e-05 Training loss: 6.811472415924072
2025-12-09 12:17:26.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 9.870597773997972e-05 Training loss: 6.665997505187988
2025-12-09 12:17:26.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 9.869892778947148e-05 Training loss: 7.048861503601074
2025-12-09 12:17:27.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 9.869185893986012e-05 Training loss: 6.987733364105225
2025-12-09 12:17:27.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 9.868477119388896e-05 Training loss: 7.290078163146973
2025-12-09 12:17:28.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 9.867766455430857e-05 Training loss: 7.089778423309326
2025-12-09 12:17:28.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 9.867053902387693e-05 Training loss: 6.987163543701172
2025-12-09 12:17:29.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 9.86633946053593e-05 Training loss: 6.7960638999938965
2025-12-09 12:17:29.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 9.865623130152828e-05 Training loss: 6.929266929626465
2025-12-09 12:17:30.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 9.864904911516384e-05 Training loss: 7.037351131439209
2025-12-09 12:17:30.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 9.864184804905323e-05 Training loss: 6.976030349731445
2025-12-09 12:17:31.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 9.863462810599105e-05 Training loss: 7.885922431945801
2025-12-09 12:17:31.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 9.862738928877922e-05 Training loss: 7.070119380950928
2025-12-09 12:17:32.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 9.862013160022696e-05 Training loss: 7.120083808898926
2025-12-09 12:17:32.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 9.861285504315085e-05 Training loss: 6.833922863006592
2025-12-09 12:17:33.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 9.860555962037479e-05 Training loss: 7.344913482666016
2025-12-09 12:17:33.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 9.859824533472998e-05 Training loss: 6.788093566894531
2025-12-09 12:17:34.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 9.859091218905498e-05 Training loss: 6.9280781745910645
2025-12-09 12:17:34.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 9.85835601861956e-05 Training loss: 7.077279090881348
2025-12-09 12:17:35.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 9.857618932900503e-05 Training loss: 6.826587200164795
2025-12-09 12:17:35.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 9.856879962034374e-05 Training loss: 6.777883052825928
2025-12-09 12:17:36.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 9.856139106307955e-05 Training loss: 6.991292953491211
2025-12-09 12:17:36.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 9.855396366008758e-05 Training loss: 6.939033031463623
2025-12-09 12:17:37.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 9.854651741425023e-05 Training loss: 6.76240873336792
2025-12-09 12:17:37.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 9.853905232845728e-05 Training loss: 7.345029354095459
2025-12-09 12:17:38.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 9.853156840560575e-05 Training loss: 7.2232770919799805
2025-12-09 12:17:38.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 9.852406564860003e-05 Training loss: 7.413918972015381
2025-12-09 12:17:39.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 9.851654406035179e-05 Training loss: 6.967384338378906
2025-12-09 12:17:39.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 9.850900364378e-05 Training loss: 6.686665058135986
2025-12-09 12:17:40.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 9.850144440181096e-05 Training loss: 6.650005340576172
2025-12-09 12:17:40.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 9.849386633737825e-05 Training loss: 6.869967460632324
2025-12-09 12:17:41.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 9.848626945342278e-05 Training loss: 7.262928009033203
2025-12-09 12:17:41.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 9.847865375289275e-05 Training loss: 7.116573810577393
2025-12-09 12:17:42.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 9.847101923874367e-05 Training loss: 6.784945487976074
2025-12-09 12:17:42.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 9.846336591393833e-05 Training loss: 6.593211650848389
2025-12-09 12:17:43.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 9.845569378144686e-05 Training loss: 6.688342571258545
2025-12-09 12:17:43.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 9.844800284424664e-05 Training loss: 7.1148200035095215
2025-12-09 12:17:44.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 9.844029310532239e-05 Training loss: 7.076229572296143
2025-12-09 12:17:44.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 9.843256456766609e-05 Training loss: 7.130420207977295
2025-12-09 12:17:45.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 9.842481723427705e-05 Training loss: 6.65859842300415
2025-12-09 12:17:45.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 9.841705110816187e-05 Training loss: 6.803127288818359
2025-12-09 12:17:46.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 9.840926619233441e-05 Training loss: 6.775847911834717
2025-12-09 12:17:46.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 9.840146248981585e-05 Training loss: 6.911198616027832
2025-12-09 12:17:47.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 9.839364000363467e-05 Training loss: 7.251853942871094
2025-12-09 12:17:47.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 9.83857987368266e-05 Training loss: 6.777528762817383
2025-12-09 12:17:48.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 9.837793869243468e-05 Training loss: 7.120370864868164
2025-12-09 12:17:48.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 9.837005987350926e-05 Training loss: 6.818387985229492
2025-12-09 12:17:49.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 9.836216228310798e-05 Training loss: 6.927734375
2025-12-09 12:17:49.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 9.835424592429567e-05 Training loss: 6.497568130493164
2025-12-09 12:17:50.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 9.834631080014457e-05 Training loss: 7.123384952545166
2025-12-09 12:17:50.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 9.833835691373413e-05 Training loss: 7.887584686279297
2025-12-09 12:17:51.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 9.83303842681511e-05 Training loss: 6.761197566986084
2025-12-09 12:17:51.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 9.83223928664895e-05 Training loss: 6.374819278717041
2025-12-09 12:17:52.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 9.831438271185065e-05 Training loss: 6.970478057861328
2025-12-09 12:17:52.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 9.830635380734313e-05 Training loss: 6.7639851570129395
2025-12-09 12:17:53.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 9.82983061560828e-05 Training loss: 7.038540363311768
2025-12-09 12:17:53.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 9.829023976119279e-05 Training loss: 6.868737697601318
2025-12-09 12:17:54.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 9.828215462580353e-05 Training loss: 7.226618766784668
2025-12-09 12:17:54.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 9.827405075305267e-05 Training loss: 6.315864086151123
2025-12-09 12:17:55.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 9.826592814608518e-05 Training loss: 7.047847747802734
2025-12-09 12:17:55.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 9.825778680805331e-05 Training loss: 7.679070949554443
2025-12-09 12:17:56.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 9.824962674211653e-05 Training loss: 6.759533882141113
2025-12-09 12:17:56.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 9.824144795144159e-05 Training loss: 6.819834232330322
2025-12-09 12:17:57.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 9.823325043920254e-05 Training loss: 7.1888747215271
2025-12-09 12:17:57.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 9.822503420858069e-05 Training loss: 6.720859050750732
2025-12-09 12:17:58.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 9.821679926276456e-05 Training loss: 5.888225555419922
2025-12-09 12:17:58.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 9.820854560494999e-05 Training loss: 6.682766437530518
2025-12-09 12:17:59.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 9.820027323834006e-05 Training loss: 6.578769683837891
2025-12-09 12:17:59.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 9.819198216614512e-05 Training loss: 7.096408367156982
2025-12-09 12:18:00.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 9.818367239158278e-05 Training loss: 7.261899948120117
2025-12-09 12:18:00.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 9.817534391787789e-05 Training loss: 6.789036750793457
2025-12-09 12:18:01.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 9.816699674826255e-05 Training loss: 7.219666957855225
2025-12-09 12:18:01.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 9.815863088597618e-05 Training loss: 6.606621742248535
2025-12-09 12:18:02.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 9.815024633426538e-05 Training loss: 6.484560966491699
2025-12-09 12:18:02.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 9.814184309638402e-05 Training loss: 6.766353607177734
2025-12-09 12:18:03.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 9.813342117559323e-05 Training loss: 6.7219014167785645
2025-12-09 12:18:03.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 9.812498057516143e-05 Training loss: 6.833155632019043
2025-12-09 12:18:04.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 9.811652129836421e-05 Training loss: 6.9275922775268555
2025-12-09 12:18:04.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 9.810804334848449e-05 Training loss: 6.972350597381592
2025-12-09 12:18:05.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 9.809954672881238e-05 Training loss: 6.8293046951293945
2025-12-09 12:18:05.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 9.809103144264525e-05 Training loss: 7.207862377166748
2025-12-09 12:18:06.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 9.808249749328768e-05 Training loss: 7.583083629608154
2025-12-09 12:18:06.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 9.80739448840516e-05 Training loss: 6.534850120544434
2025-12-09 12:18:07.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 9.806537361825606e-05 Training loss: 6.758131504058838
2025-12-09 12:18:07.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 9.805678369922742e-05 Training loss: 6.65854549407959
2025-12-09 12:18:08.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 9.804817513029927e-05 Training loss: 6.709959983825684
2025-12-09 12:18:08.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 9.803954791481239e-05 Training loss: 7.081752300262451
2025-12-09 12:18:09.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 9.803090205611487e-05 Training loss: 7.017223834991455
2025-12-09 12:18:09.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 9.802223755756198e-05 Training loss: 6.629779815673828
2025-12-09 12:18:10.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 9.801355442251625e-05 Training loss: 6.967712879180908
2025-12-09 12:18:10.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 9.800485265434744e-05 Training loss: 7.09370756149292
2025-12-09 12:18:11.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 9.799613225643253e-05 Training loss: 6.977283477783203
2025-12-09 12:18:11.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 9.798739323215574e-05 Training loss: 7.8907294273376465
2025-12-09 12:18:12.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 9.797863558490849e-05 Training loss: 6.74324893951416
2025-12-09 12:18:12.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 9.79698593180895e-05 Training loss: 6.773463726043701
2025-12-09 12:18:13.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 9.796106443510462e-05 Training loss: 6.914499282836914
2025-12-09 12:18:13.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 9.795225093936702e-05 Training loss: 6.509944438934326
2025-12-09 12:18:14.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 9.794341883429699e-05 Training loss: 6.962883949279785
2025-12-09 12:18:14.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 9.793456812332215e-05 Training loss: 6.560309410095215
2025-12-09 12:18:15.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 9.792569880987726e-05 Training loss: 6.60616397857666
2025-12-09 12:18:15.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 9.791681089740432e-05 Training loss: 6.733482360839844
2025-12-09 12:18:16.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 9.790790438935256e-05 Training loss: 6.809152126312256
2025-12-09 12:18:16.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 9.789897928917847e-05 Training loss: 6.969155311584473
2025-12-09 12:18:17.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 9.789003560034561e-05 Training loss: 6.7805070877075195
2025-12-09 12:18:17.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 9.788107332632495e-05 Training loss: 7.05375862121582
2025-12-09 12:18:18.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 9.787209247059452e-05 Training loss: 7.034958839416504
2025-12-09 12:18:18.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 9.786309303663963e-05 Training loss: 6.722305774688721
2025-12-09 12:18:19.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 9.785407502795278e-05 Training loss: 7.021128177642822
2025-12-09 12:18:19.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 9.784503844803368e-05 Training loss: 6.631049633026123
2025-12-09 12:18:20.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 9.783598330038925e-05 Training loss: 6.558966159820557
2025-12-09 12:18:20.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 9.782690958853362e-05 Training loss: 7.0172600746154785
2025-12-09 12:18:21.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 9.781781731598812e-05 Training loss: 6.7465105056762695
2025-12-09 12:18:21.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 9.780870648628128e-05 Training loss: 6.845406532287598
2025-12-09 12:18:22.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 9.779957710294886e-05 Training loss: 6.585273742675781
2025-12-09 12:18:22.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 9.779042916953376e-05 Training loss: 6.624042510986328
2025-12-09 12:18:23.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 9.778126268958613e-05 Training loss: 7.107329368591309
2025-12-09 12:18:23.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 9.77720776666633e-05 Training loss: 6.843500137329102
2025-12-09 12:18:24.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 9.77628741043298e-05 Training loss: 7.076526165008545
2025-12-09 12:18:24.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 9.775365200615735e-05 Training loss: 7.856600284576416
2025-12-09 12:18:25.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 9.774441137572487e-05 Training loss: 6.534757614135742
2025-12-09 12:18:25.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 9.773515221661846e-05 Training loss: 6.443720817565918
2025-12-09 12:18:26.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 9.772587453243143e-05 Training loss: 7.1438493728637695
2025-12-09 12:18:26.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 9.771657832676427e-05 Training loss: 7.258294582366943
2025-12-09 12:18:27.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 9.770726360322463e-05 Training loss: 6.614674091339111
2025-12-09 12:18:27.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 9.769793036542741e-05 Training loss: 6.944258213043213
2025-12-09 12:18:28.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 9.768857861699463e-05 Training loss: 7.1230387687683105
2025-12-09 12:18:28.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 9.767920836155553e-05 Training loss: 6.974764347076416
2025-12-09 12:18:29.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 9.766981960274653e-05 Training loss: 6.517910957336426
2025-12-09 12:18:29.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 9.766041234421122e-05 Training loss: 7.131550312042236
2025-12-09 12:18:30.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 9.765098658960036e-05 Training loss: 6.874779224395752
2025-12-09 12:18:30.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 9.764154234257192e-05 Training loss: 6.5649309158325195
2025-12-09 12:18:31.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 9.763207960679101e-05 Training loss: 6.814658164978027
2025-12-09 12:18:31.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 9.762259838592994e-05 Training loss: 6.9639201164245605
2025-12-09 12:18:32.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 9.761309868366819e-05 Training loss: 6.847074031829834
2025-12-09 12:18:32.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 9.760358050369243e-05 Training loss: 6.764219284057617
2025-12-09 12:18:33.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 9.759404384969643e-05 Training loss: 6.7635178565979
2025-12-09 12:18:33.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 9.758448872538122e-05 Training loss: 6.6548237800598145
2025-12-09 12:18:34.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 9.757491513445493e-05 Training loss: 6.6668853759765625
2025-12-09 12:18:34.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 9.756532308063293e-05 Training loss: 6.586925029754639
2025-12-09 12:18:35.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 9.755571256763765e-05 Training loss: 6.630019664764404
2025-12-09 12:18:35.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 9.754608359919879e-05 Training loss: 6.773043632507324
2025-12-09 12:18:36.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 9.753643617905313e-05 Training loss: 6.987175464630127
2025-12-09 12:18:36.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 9.752677031094466e-05 Training loss: 6.683854103088379
2025-12-09 12:18:37.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 9.751708599862452e-05 Training loss: 6.910032749176025
2025-12-09 12:18:37.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 9.750738324585098e-05 Training loss: 7.4921183586120605
2025-12-09 12:18:38.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 9.749766205638952e-05 Training loss: 7.036288261413574
2025-12-09 12:18:38.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 9.748792243401273e-05 Training loss: 6.8856940269470215
2025-12-09 12:18:39.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 9.747816438250037e-05 Training loss: 6.98500394821167
2025-12-09 12:18:39.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 9.746838790563934e-05 Training loss: 7.417036056518555
2025-12-09 12:18:40.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 9.74585930072237e-05 Training loss: 6.825568675994873
2025-12-09 12:18:40.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 9.744877969105469e-05 Training loss: 7.112146854400635
2025-12-09 12:18:41.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 9.743894796094062e-05 Training loss: 6.7445268630981445
2025-12-09 12:18:41.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 9.742909782069701e-05 Training loss: 5.664857387542725
2025-12-09 12:18:42.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 9.741922927414651e-05 Training loss: 7.0383100509643555
2025-12-09 12:18:42.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 9.740934232511894e-05 Training loss: 6.997041702270508
2025-12-09 12:18:43.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 9.739943697745118e-05 Training loss: 6.863752365112305
2025-12-09 12:18:43.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 9.738951323498732e-05 Training loss: 6.773538112640381
2025-12-09 12:18:44.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 9.737957110157858e-05 Training loss: 7.103038787841797
2025-12-09 12:18:44.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 9.736961058108332e-05 Training loss: 6.645627498626709
2025-12-09 12:18:45.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 9.735963167736698e-05 Training loss: 7.199855804443359
2025-12-09 12:18:45.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 9.734963439430222e-05 Training loss: 7.4180989265441895
2025-12-09 12:18:46.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 9.733961873576878e-05 Training loss: 6.6369099617004395
2025-12-09 12:18:46.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 9.732958470565353e-05 Training loss: 5.934669494628906
2025-12-09 12:18:47.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 9.731953230785049e-05 Training loss: 6.681446075439453
2025-12-09 12:18:47.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 9.730946154626079e-05 Training loss: 6.809177398681641
2025-12-09 12:18:48.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 9.729937242479271e-05 Training loss: 6.466629505157471
2025-12-09 12:18:48.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 9.728926494736164e-05 Training loss: 6.649724960327148
2025-12-09 12:18:49.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 9.727913911789009e-05 Training loss: 6.602793216705322
2025-12-09 12:18:49.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 9.726899494030768e-05 Training loss: 6.516312122344971
2025-12-09 12:18:50.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 9.725883241855119e-05 Training loss: 6.77212381362915
2025-12-09 12:18:50.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 9.724865155656448e-05 Training loss: 6.67781400680542
2025-12-09 12:18:51.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 9.723845235829857e-05 Training loss: 6.60806131362915
2025-12-09 12:18:51.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 9.722823482771155e-05 Training loss: 6.498438358306885
2025-12-09 12:18:52.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 9.721799896876864e-05 Training loss: 6.708985805511475
2025-12-09 12:18:52.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 9.720774478544219e-05 Training loss: 6.767940521240234
2025-12-09 12:18:53.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 9.719747228171163e-05 Training loss: 6.515172958374023
2025-12-09 12:18:53.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 9.718718146156355e-05 Training loss: 6.70039176940918
2025-12-09 12:18:54.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 9.717687232899159e-05 Training loss: 7.291956424713135
2025-12-09 12:18:54.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 9.716654488799652e-05 Training loss: 6.736533164978027
2025-12-09 12:18:55.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 9.715619914258624e-05 Training loss: 6.991942882537842
2025-12-09 12:18:55.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 9.71458350967757e-05 Training loss: 6.724308013916016
2025-12-09 12:18:56.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 9.713545275458703e-05 Training loss: 6.736911296844482
2025-12-09 12:18:56.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 9.712505212004938e-05 Training loss: 6.713204860687256
2025-12-09 12:18:57.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 9.711463319719904e-05 Training loss: 6.3801960945129395
2025-12-09 12:18:57.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 9.710419599007939e-05 Training loss: 6.907357692718506
2025-12-09 12:18:58.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 9.70937405027409e-05 Training loss: 6.490434169769287
2025-12-09 12:18:58.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 9.708326673924115e-05 Training loss: 6.911360263824463
2025-12-09 12:18:59.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 9.707277470364482e-05 Training loss: 6.789649963378906
2025-12-09 12:18:59.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 9.706226440002363e-05 Training loss: 7.338633060455322
2025-12-09 12:19:00.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 9.705173583245645e-05 Training loss: 7.8845624923706055
2025-12-09 12:19:00.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 9.704118900502919e-05 Training loss: 6.933419227600098
2025-12-09 12:19:01.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 9.703062392183489e-05 Training loss: 6.741028308868408
2025-12-09 12:19:01.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 9.702004058697363e-05 Training loss: 6.409341812133789
2025-12-09 12:19:02.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 9.700943900455262e-05 Training loss: 6.963708877563477
2025-12-09 12:19:02.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 9.69988191786861e-05 Training loss: 6.992076873779297
2025-12-09 12:19:03.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 9.698818111349543e-05 Training loss: 6.975104331970215
2025-12-09 12:19:03.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 9.697752481310904e-05 Training loss: 7.095663070678711
2025-12-09 12:19:04.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 9.696685028166244e-05 Training loss: 6.868760585784912
2025-12-09 12:19:04.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 9.69561575232982e-05 Training loss: 6.934168338775635
2025-12-09 12:19:05.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 9.694544654216596e-05 Training loss: 6.933757305145264
2025-12-09 12:19:05.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 9.693471734242243e-05 Training loss: 6.40122127532959
2025-12-09 12:19:06.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 9.692396992823145e-05 Training loss: 6.782092571258545
2025-12-09 12:19:06.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 9.691320430376385e-05 Training loss: 7.765603065490723
2025-12-09 12:19:07.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 9.690242047319755e-05 Training loss: 6.828197479248047
2025-12-09 12:19:07.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 9.689161844071757e-05 Training loss: 6.285799503326416
2025-12-09 12:19:08.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 9.688079821051595e-05 Training loss: 6.898669242858887
2025-12-09 12:19:08.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 9.68699597867918e-05 Training loss: 6.846005916595459
2025-12-09 12:19:09.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 9.685910317375134e-05 Training loss: 6.684167861938477
2025-12-09 12:19:09.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 9.684822837560776e-05 Training loss: 6.3616790771484375
2025-12-09 12:19:10.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 9.683733539658139e-05 Training loss: 7.244637966156006
2025-12-09 12:19:10.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 9.682642424089958e-05 Training loss: 6.4165449142456055
2025-12-09 12:19:11.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 9.681549491279674e-05 Training loss: 6.817345142364502
2025-12-09 12:19:11.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 9.68045474165143e-05 Training loss: 6.505551338195801
2025-12-09 12:19:12.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 9.679358175630081e-05 Training loss: 6.459743976593018
2025-12-09 12:19:12.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 9.67825979364118e-05 Training loss: 6.952930450439453
2025-12-09 12:19:13.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 9.677159596110987e-05 Training loss: 6.716345310211182
2025-12-09 12:19:13.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 9.676057583466472e-05 Training loss: 6.879832744598389
2025-12-09 12:19:14.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 9.674953756135298e-05 Training loss: 6.665195465087891
2025-12-09 12:19:14.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 9.673848114545843e-05 Training loss: 6.415004253387451
2025-12-09 12:19:15.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 9.672740659127184e-05 Training loss: 6.692310810089111
2025-12-09 12:19:15.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 9.671631390309102e-05 Training loss: 6.939666271209717
2025-12-09 12:19:16.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 9.670520308522084e-05 Training loss: 6.704211235046387
2025-12-09 12:19:16.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 9.66940741419732e-05 Training loss: 6.74124813079834
2025-12-09 12:19:17.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 9.668292707766699e-05 Training loss: 6.298605442047119
2025-12-09 12:19:17.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 9.667176189662818e-05 Training loss: 6.580564022064209
2025-12-09 12:19:18.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 9.666057860318979e-05 Training loss: 6.8371663093566895
2025-12-09 12:19:18.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 9.66493772016918e-05 Training loss: 6.847355365753174
2025-12-09 12:19:19.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 9.663815769648129e-05 Training loss: 6.2148823738098145
2025-12-09 12:19:19.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 9.66269200919123e-05 Training loss: 6.283356189727783
2025-12-09 12:19:20.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 9.661566439234593e-05 Training loss: 6.715173244476318
2025-12-09 12:19:20.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 9.660439060215031e-05 Training loss: 6.8908514976501465
2025-12-09 12:19:21.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 9.659309872570058e-05 Training loss: 6.744766712188721
2025-12-09 12:19:21.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 9.658178876737886e-05 Training loss: 5.823761463165283
2025-12-09 12:19:22.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 9.657046073157436e-05 Training loss: 6.515230655670166
2025-12-09 12:19:22.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 9.655911462268327e-05 Training loss: 6.687621116638184
2025-12-09 12:19:23.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 9.65477504451088e-05 Training loss: 7.555334091186523
2025-12-09 12:19:23.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 9.653636820326113e-05 Training loss: 6.749786376953125
2025-12-09 12:19:24.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 9.652496790155751e-05 Training loss: 6.429986476898193
2025-12-09 12:19:24.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 9.651354954442218e-05 Training loss: 6.9612274169921875
2025-12-09 12:19:25.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 9.650211313628637e-05 Training loss: 6.69189453125
2025-12-09 12:19:25.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 9.649065868158832e-05 Training loss: 6.454546928405762
2025-12-09 12:19:26.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 9.64791861847733e-05 Training loss: 6.341172218322754
2025-12-09 12:19:26.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 9.646769565029354e-05 Training loss: 6.666464328765869
2025-12-09 12:19:27.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 9.64561870826083e-05 Training loss: 6.115477561950684
2025-12-09 12:19:27.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 9.644466048618385e-05 Training loss: 6.510224342346191
2025-12-09 12:19:28.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 9.643311586549342e-05 Training loss: 7.1062397956848145
2025-12-09 12:19:28.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 9.642155322501725e-05 Training loss: 6.871111869812012
2025-12-09 12:19:29.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 9.640997256924257e-05 Training loss: 6.659710884094238
2025-12-09 12:19:29.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 9.639837390266361e-05 Training loss: 6.645952224731445
2025-12-09 12:19:30.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 9.638675722978161e-05 Training loss: 7.334997177124023
2025-12-09 12:19:30.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 9.637512255510475e-05 Training loss: 6.842645168304443
2025-12-09 12:19:31.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 9.63634698831482e-05 Training loss: 6.4908952713012695
2025-12-09 12:19:31.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 9.635179921843418e-05 Training loss: 6.5238189697265625
2025-12-09 12:19:32.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 9.634011056549182e-05 Training loss: 6.366976737976074
2025-12-09 12:19:32.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 9.632840392885727e-05 Training loss: 6.804482936859131
2025-12-09 12:19:33.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 9.631667931307364e-05 Training loss: 6.846884250640869
2025-12-09 12:19:33.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 9.630493672269102e-05 Training loss: 6.7610321044921875
2025-12-09 12:19:34.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 9.629317616226649e-05 Training loss: 6.822249889373779
2025-12-09 12:19:34.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 9.628139763636408e-05 Training loss: 6.7115325927734375
2025-12-09 12:19:35.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 9.626960114955483e-05 Training loss: 6.740397930145264
2025-12-09 12:19:35.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 9.62577867064167e-05 Training loss: 6.4956512451171875
2025-12-09 12:19:36.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 9.624595431153467e-05 Training loss: 6.6793084144592285
2025-12-09 12:19:36.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 9.623410396950064e-05 Training loss: 7.453585624694824
2025-12-09 12:19:37.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 9.62222356849135e-05 Training loss: 6.977187156677246
2025-12-09 12:19:37.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 9.621034946237911e-05 Training loss: 6.743869781494141
2025-12-09 12:19:38.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 9.619844530651027e-05 Training loss: 6.827457904815674
2025-12-09 12:19:38.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 9.618652322192675e-05 Training loss: 6.459737300872803
2025-12-09 12:19:39.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 9.61745832132553e-05 Training loss: 6.208218574523926
2025-12-09 12:19:39.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 9.616262528512957e-05 Training loss: 6.6135687828063965
2025-12-09 12:19:40.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 9.615064944219022e-05 Training loss: 6.315272331237793
2025-12-09 12:19:40.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 9.613865568908485e-05 Training loss: 6.715641975402832
2025-12-09 12:19:41.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 9.612664403046797e-05 Training loss: 6.316501617431641
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 2.87 GiB is free. Including non-PyTorch memory, this process has 91.28 GiB memory in use. Of the allocated memory 90.25 GiB is allocated by PyTorch, and 278.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 50/10000 [00:00<00:19, 499.37it/s]Tokenizing texts:   1%|▏         | 135/10000 [00:00<00:14, 701.55it/s]Tokenizing texts:   2%|▏         | 206/10000 [00:00<00:15, 624.53it/s]Tokenizing texts:   3%|▎         | 274/10000 [00:00<00:15, 643.95it/s]Tokenizing texts:   3%|▎         | 340/10000 [00:00<00:15, 614.44it/s]Tokenizing texts:   4%|▍         | 403/10000 [00:00<00:15, 606.44it/s]Tokenizing texts:   5%|▍         | 464/10000 [00:00<00:16, 576.76it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 576.77it/s]Tokenizing texts:   6%|▌         | 597/10000 [00:00<00:15, 601.71it/s]Tokenizing texts:   7%|▋         | 658/10000 [00:01<00:15, 594.30it/s]Tokenizing texts:   7%|▋         | 718/10000 [00:01<00:16, 579.28it/s]Tokenizing texts:   8%|▊         | 777/10000 [00:01<00:15, 579.92it/s]Tokenizing texts:   8%|▊         | 836/10000 [00:01<00:15, 575.93it/s]Tokenizing texts:   9%|▉         | 899/10000 [00:01<00:15, 588.88it/s]Tokenizing texts:  10%|▉         | 980/10000 [00:01<00:13, 653.24it/s]Tokenizing texts:  11%|█         | 1051/10000 [00:01<00:14, 635.73it/s]Tokenizing texts:  11%|█         | 1115/10000 [00:01<00:14, 632.11it/s]Tokenizing texts:  12%|█▏        | 1179/10000 [00:01<00:14, 597.86it/s]Tokenizing texts:  12%|█▏        | 1240/10000 [00:02<00:14, 594.78it/s]Tokenizing texts:  13%|█▎        | 1305/10000 [00:02<00:14, 608.99it/s]Tokenizing texts:  14%|█▎        | 1367/10000 [00:02<00:14, 597.61it/s]Tokenizing texts:  14%|█▍        | 1427/10000 [00:02<00:14, 596.70it/s]Tokenizing texts:  15%|█▌        | 1505/10000 [00:02<00:13, 648.14it/s]Tokenizing texts:  16%|█▌        | 1582/10000 [00:02<00:12, 683.10it/s]Tokenizing texts:  17%|█▋        | 1651/10000 [00:02<00:13, 618.85it/s]Tokenizing texts:  17%|█▋        | 1715/10000 [00:02<00:13, 608.87it/s]Tokenizing texts:  18%|█▊        | 1785/10000 [00:02<00:12, 633.55it/s]Tokenizing texts:  18%|█▊        | 1850/10000 [00:03<00:13, 606.13it/s]Tokenizing texts:  19%|█▉        | 1914/10000 [00:03<00:13, 614.61it/s]Tokenizing texts:  20%|█▉        | 1977/10000 [00:03<00:14, 571.45it/s]Tokenizing texts:  20%|██        | 2050/10000 [00:03<00:12, 612.80it/s]Tokenizing texts:  21%|██        | 2113/10000 [00:03<00:14, 556.71it/s]Tokenizing texts:  22%|██▏       | 2171/10000 [00:03<00:14, 558.64it/s]Tokenizing texts:  22%|██▏       | 2247/10000 [00:03<00:12, 612.73it/s]Tokenizing texts:  23%|██▎       | 2311/10000 [00:03<00:12, 600.00it/s]Tokenizing texts:  24%|██▍       | 2380/10000 [00:03<00:12, 623.77it/s]Tokenizing texts:  24%|██▍       | 2444/10000 [00:04<00:12, 626.37it/s]Tokenizing texts:  25%|██▌       | 2508/10000 [00:04<00:12, 620.24it/s]Tokenizing texts:  26%|██▌       | 2571/10000 [00:04<00:12, 606.11it/s]Tokenizing texts:  26%|██▋       | 2632/10000 [00:04<00:12, 569.47it/s]Tokenizing texts:  27%|██▋       | 2693/10000 [00:04<00:12, 577.23it/s]Tokenizing texts:  28%|██▊       | 2757/10000 [00:04<00:12, 592.16it/s]Tokenizing texts:  28%|██▊       | 2817/10000 [00:04<00:13, 542.92it/s]Tokenizing texts:  29%|██▊       | 2873/10000 [00:04<00:13, 544.69it/s]Tokenizing texts:  29%|██▉       | 2946/10000 [00:04<00:11, 588.87it/s]Tokenizing texts:  30%|███       | 3014/10000 [00:05<00:11, 614.08it/s]Tokenizing texts:  31%|███       | 3077/10000 [00:05<00:11, 603.68it/s]Tokenizing texts:  31%|███▏      | 3141/10000 [00:05<00:11, 612.16it/s]Tokenizing texts:  32%|███▏      | 3205/10000 [00:05<00:11, 608.91it/s]Tokenizing texts:  33%|███▎      | 3278/10000 [00:05<00:10, 642.93it/s]Tokenizing texts:  33%|███▎      | 3343/10000 [00:05<00:10, 639.78it/s]Tokenizing texts:  34%|███▍      | 3408/10000 [00:05<00:10, 635.42it/s]Tokenizing texts:  35%|███▍      | 3490/10000 [00:05<00:09, 679.38it/s]Tokenizing texts:  36%|███▌      | 3559/10000 [00:05<00:10, 642.54it/s]Tokenizing texts:  36%|███▌      | 3624/10000 [00:05<00:09, 642.47it/s]Tokenizing texts:  37%|███▋      | 3689/10000 [00:06<00:09, 634.45it/s]Tokenizing texts:  38%|███▊      | 3756/10000 [00:06<00:10, 609.52it/s]Tokenizing texts:  38%|███▊      | 3818/10000 [00:06<00:10, 590.20it/s]Tokenizing texts:  39%|███▉      | 3887/10000 [00:06<00:09, 612.61it/s]Tokenizing texts:  39%|███▉      | 3949/10000 [00:06<00:10, 591.71it/s]Tokenizing texts:  40%|████      | 4009/10000 [00:06<00:10, 587.84it/s]Tokenizing texts:  41%|████      | 4085/10000 [00:06<00:09, 636.48it/s]Tokenizing texts:  42%|████▏     | 4154/10000 [00:06<00:09, 648.37it/s]Tokenizing texts:  42%|████▏     | 4224/10000 [00:06<00:08, 654.85it/s]Tokenizing texts:  43%|████▎     | 4290/10000 [00:07<00:08, 634.61it/s]Tokenizing texts:  44%|████▎     | 4366/10000 [00:07<00:08, 667.13it/s]Tokenizing texts:  44%|████▍     | 4437/10000 [00:07<00:08, 677.10it/s]Tokenizing texts:  45%|████▌     | 4505/10000 [00:07<00:08, 637.46it/s]Tokenizing texts:  46%|████▌     | 4575/10000 [00:07<00:08, 654.05it/s]Tokenizing texts:  46%|████▋     | 4641/10000 [00:07<00:08, 645.65it/s]Tokenizing texts:  47%|████▋     | 4728/10000 [00:07<00:07, 709.22it/s]Tokenizing texts:  48%|████▊     | 4800/10000 [00:07<00:08, 643.01it/s]Tokenizing texts:  49%|████▉     | 4875/10000 [00:07<00:07, 665.11it/s]Tokenizing texts:  49%|████▉     | 4946/10000 [00:08<00:07, 673.54it/s]Tokenizing texts:  50%|█████     | 5025/10000 [00:08<00:07, 704.10it/s]Tokenizing texts:  51%|█████     | 5097/10000 [00:08<00:07, 645.01it/s]Tokenizing texts:  52%|█████▏    | 5163/10000 [00:08<00:07, 640.64it/s]Tokenizing texts:  52%|█████▏    | 5229/10000 [00:08<00:07, 627.97it/s]Tokenizing texts:  53%|█████▎    | 5295/10000 [00:08<00:07, 635.63it/s]Tokenizing texts:  54%|█████▍    | 5380/10000 [00:08<00:06, 696.10it/s]Tokenizing texts:  55%|█████▍    | 5451/10000 [00:08<00:06, 680.06it/s]Tokenizing texts:  55%|█████▌    | 5520/10000 [00:08<00:07, 620.09it/s]Tokenizing texts:  56%|█████▌    | 5591/10000 [00:09<00:06, 638.17it/s]Tokenizing texts:  57%|█████▋    | 5675/10000 [00:09<00:06, 693.94it/s]Tokenizing texts:  57%|█████▋    | 5746/10000 [00:09<00:06, 666.55it/s]Tokenizing texts:  58%|█████▊    | 5814/10000 [00:09<00:06, 605.46it/s]Tokenizing texts:  59%|█████▉    | 5884/10000 [00:09<00:06, 630.23it/s]Tokenizing texts:  59%|█████▉    | 5949/10000 [00:09<00:06, 612.63it/s]Tokenizing texts:  60%|██████    | 6012/10000 [00:09<00:06, 600.84it/s]Tokenizing texts:  61%|██████    | 6089/10000 [00:09<00:06, 646.87it/s]Tokenizing texts:  62%|██████▏   | 6158/10000 [00:09<00:05, 657.06it/s]Tokenizing texts:  62%|██████▏   | 6225/10000 [00:09<00:05, 642.25it/s]Tokenizing texts:  63%|██████▎   | 6290/10000 [00:10<00:05, 641.07it/s]Tokenizing texts:  64%|██████▎   | 6355/10000 [00:10<00:06, 606.78it/s]Tokenizing texts:  64%|██████▍   | 6423/10000 [00:10<00:05, 621.36it/s]Tokenizing texts:  65%|██████▍   | 6486/10000 [00:10<00:05, 619.67it/s]Tokenizing texts:  65%|██████▌   | 6549/10000 [00:10<00:05, 619.04it/s]Tokenizing texts:  66%|██████▌   | 6622/10000 [00:10<00:05, 649.85it/s]Tokenizing texts:  67%|██████▋   | 6688/10000 [00:10<00:05, 646.17it/s]Tokenizing texts:  68%|██████▊   | 6753/10000 [00:10<00:05, 628.03it/s]Tokenizing texts:  68%|██████▊   | 6836/10000 [00:10<00:04, 685.87it/s]Tokenizing texts:  69%|██████▉   | 6905/10000 [00:11<00:05, 611.28it/s]Tokenizing texts:  70%|██████▉   | 6971/10000 [00:11<00:04, 620.13it/s]Tokenizing texts:  70%|███████   | 7035/10000 [00:11<00:04, 625.15it/s]Tokenizing texts:  71%|███████   | 7099/10000 [00:11<00:05, 570.49it/s]Tokenizing texts:  72%|███████▏  | 7164/10000 [00:11<00:04, 584.87it/s]Tokenizing texts:  72%|███████▏  | 7228/10000 [00:11<00:04, 600.01it/s]Tokenizing texts:  73%|███████▎  | 7308/10000 [00:11<00:04, 654.10it/s]Tokenizing texts:  74%|███████▍  | 7375/10000 [00:11<00:04, 648.31it/s]Tokenizing texts:  74%|███████▍  | 7441/10000 [00:11<00:04, 608.35it/s]Tokenizing texts:  75%|███████▌  | 7504/10000 [00:12<00:04, 612.03it/s]Tokenizing texts:  76%|███████▌  | 7571/10000 [00:12<00:03, 624.77it/s]Tokenizing texts:  76%|███████▋  | 7643/10000 [00:12<00:03, 651.83it/s]Tokenizing texts:  77%|███████▋  | 7711/10000 [00:12<00:03, 659.93it/s]Tokenizing texts:  78%|███████▊  | 7778/10000 [00:12<00:03, 632.52it/s]Tokenizing texts:  78%|███████▊  | 7842/10000 [00:12<00:03, 619.35it/s]Tokenizing texts:  79%|███████▉  | 7905/10000 [00:12<00:03, 622.04it/s]Tokenizing texts:  80%|███████▉  | 7985/10000 [00:12<00:02, 671.76it/s]Tokenizing texts:  81%|████████  | 8053/10000 [00:12<00:03, 623.02it/s]Tokenizing texts:  81%|████████  | 8117/10000 [00:13<00:03, 575.57it/s]Tokenizing texts:  82%|████████▏ | 8185/10000 [00:13<00:03, 602.13it/s]Tokenizing texts:  82%|████████▏ | 8247/10000 [00:13<00:03, 567.53it/s]Tokenizing texts:  83%|████████▎ | 8318/10000 [00:13<00:02, 596.77it/s]Tokenizing texts:  84%|████████▍ | 8385/10000 [00:13<00:02, 615.64it/s]Tokenizing texts:  84%|████████▍ | 8448/10000 [00:13<00:02, 616.81it/s]Tokenizing texts:  85%|████████▌ | 8515/10000 [00:13<00:02, 628.75it/s]Tokenizing texts:  86%|████████▌ | 8579/10000 [00:13<00:02, 629.33it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:13<00:02, 664.87it/s]Tokenizing texts:  87%|████████▋ | 8726/10000 [00:13<00:01, 678.04it/s]Tokenizing texts:  88%|████████▊ | 8795/10000 [00:14<00:01, 667.75it/s]Tokenizing texts:  89%|████████▊ | 8862/10000 [00:14<00:01, 603.69it/s]Tokenizing texts:  89%|████████▉ | 8945/10000 [00:14<00:01, 662.73it/s]Tokenizing texts:  90%|█████████ | 9013/10000 [00:14<00:01, 627.09it/s]Tokenizing texts:  91%|█████████ | 9080/10000 [00:14<00:01, 629.52it/s]Tokenizing texts:  92%|█████████▏| 9151/10000 [00:14<00:01, 650.64it/s]Tokenizing texts:  92%|█████████▏| 9225/10000 [00:14<00:01, 657.90it/s]Tokenizing texts:  93%|█████████▎| 9292/10000 [00:14<00:01, 642.10it/s]Tokenizing texts:  94%|█████████▎| 9357/10000 [00:14<00:01, 622.74it/s]Tokenizing texts:  94%|█████████▍| 9420/10000 [00:15<00:00, 599.61it/s]Tokenizing texts:  95%|█████████▍| 9481/10000 [00:15<00:00, 550.87it/s]Tokenizing texts:  96%|█████████▌| 9555/10000 [00:15<00:00, 600.00it/s]Tokenizing texts:  96%|█████████▌| 9617/10000 [00:15<00:00, 579.16it/s]Tokenizing texts:  97%|█████████▋| 9679/10000 [00:15<00:00, 588.13it/s]Tokenizing texts:  97%|█████████▋| 9739/10000 [00:15<00:00, 581.09it/s]Tokenizing texts:  98%|█████████▊| 9798/10000 [00:15<00:00, 577.57it/s]Tokenizing texts:  99%|█████████▊| 9868/10000 [00:15<00:00, 611.87it/s]Tokenizing texts:  99%|█████████▉| 9941/10000 [00:15<00:00, 640.14it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:16<00:00, 623.08it/s]
2025-12-09 12:20:30.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 12.188782691955566
2025-12-09 12:20:30.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 12.174444198608398
2025-12-09 12:20:31.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 12.144030570983887
2025-12-09 12:20:31.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 12.17935848236084
2025-12-09 12:20:32.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 12.161232948303223
2025-12-09 12:20:32.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 12.14616870880127
2025-12-09 12:20:33.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 12.160811424255371
2025-12-09 12:20:33.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 12.133784294128418
2025-12-09 12:20:34.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 12.11612319946289
2025-12-09 12:20:34.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 12.146688461303711
2025-12-09 12:20:35.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 12.12402629852295
2025-12-09 12:20:35.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 12.071239471435547
2025-12-09 12:20:36.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 12.09512710571289
2025-12-09 12:20:36.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 12.062712669372559
2025-12-09 12:20:37.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 12.04948616027832
2025-12-09 12:20:37.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 12.121193885803223
2025-12-09 12:20:38.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 11.954026222229004
2025-12-09 12:20:38.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 11.993003845214844
2025-12-09 12:20:39.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 11.976103782653809
2025-12-09 12:20:39.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 11.967857360839844
2025-12-09 12:20:40.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 11.919740676879883
2025-12-09 12:20:40.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 11.95875358581543
2025-12-09 12:20:41.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 11.947139739990234
2025-12-09 12:20:41.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 11.659972190856934
2025-12-09 12:20:42.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 11.6832857131958
2025-12-09 12:20:42.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 11.737570762634277
2025-12-09 12:20:43.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 11.498892784118652
2025-12-09 12:20:43.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 11.83128547668457
2025-12-09 12:20:44.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 11.568790435791016
2025-12-09 12:20:44.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 11.460875511169434
2025-12-09 12:20:45.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 11.426647186279297
2025-12-09 12:20:45.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 11.222281455993652
2025-12-09 12:20:46.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 11.130450248718262
2025-12-09 12:20:46.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 10.968555450439453
2025-12-09 12:20:47.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 10.90891170501709
2025-12-09 12:20:47.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 11.058290481567383
2025-12-09 12:20:48.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 10.615336418151855
2025-12-09 12:20:48.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 10.950026512145996
2025-12-09 12:20:49.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 10.875797271728516
2025-12-09 12:20:49.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 10.365668296813965
2025-12-09 12:20:50.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 10.011601448059082
2025-12-09 12:20:50.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 10.17039680480957
2025-12-09 12:20:51.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 9.812211990356445
2025-12-09 12:20:51.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 9.948208808898926
2025-12-09 12:20:52.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 9.826486587524414
2025-12-09 12:20:52.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 9.541305541992188
2025-12-09 12:20:53.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 9.334948539733887
2025-12-09 12:20:53.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 9.57007122039795
2025-12-09 12:20:54.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 9.086962699890137
2025-12-09 12:20:54.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 9.032302856445312
2025-12-09 12:20:55.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 9.31021499633789
2025-12-09 12:20:55.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 8.96768856048584
2025-12-09 12:20:56.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 8.994295120239258
2025-12-09 12:20:56.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 8.65422534942627
2025-12-09 12:20:57.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 8.462798118591309
2025-12-09 12:20:57.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 8.510784149169922
2025-12-09 12:20:58.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 8.38158893585205
2025-12-09 12:20:58.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 8.249397277832031
2025-12-09 12:20:59.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 8.169649124145508
2025-12-09 12:20:59.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 8.474555015563965
2025-12-09 12:21:00.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 8.34271240234375
2025-12-09 12:21:00.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 8.339387893676758
2025-12-09 12:21:01.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 8.313979148864746
2025-12-09 12:21:01.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 8.255382537841797
2025-12-09 12:21:02.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 8.290842056274414
2025-12-09 12:21:02.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 7.870612144470215
2025-12-09 12:21:03.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 7.746570587158203
2025-12-09 12:21:03.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 8.215826988220215
2025-12-09 12:21:04.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 7.874950408935547
2025-12-09 12:21:04.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 8.550620079040527
2025-12-09 12:21:05.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 7.81047248840332
2025-12-09 12:21:05.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 8.405535697937012
2025-12-09 12:21:06.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 8.072908401489258
2025-12-09 12:21:06.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 8.14838695526123
2025-12-09 12:21:07.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 8.215425491333008
2025-12-09 12:21:07.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 7.812533378601074
2025-12-09 12:21:08.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 8.137750625610352
2025-12-09 12:21:08.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 8.43032169342041
2025-12-09 12:21:09.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 8.259703636169434
2025-12-09 12:21:09.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 7.899823188781738
2025-12-09 12:21:10.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 8.175036430358887
2025-12-09 12:21:10.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 7.636178493499756
2025-12-09 12:21:11.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 7.744719982147217
2025-12-09 12:21:11.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 8.34550666809082
2025-12-09 12:21:12.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 7.865810394287109
2025-12-09 12:21:12.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 8.995126724243164
2025-12-09 12:21:13.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 7.747104167938232
2025-12-09 12:21:13.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 7.7078375816345215
2025-12-09 12:21:14.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 7.974325656890869
2025-12-09 12:21:14.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 8.22165298461914
2025-12-09 12:21:15.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 7.810426712036133
2025-12-09 12:21:15.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 7.643614768981934
2025-12-09 12:21:16.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 7.649393081665039
2025-12-09 12:21:16.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 7.665740966796875
2025-12-09 12:21:17.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 7.602553367614746
2025-12-09 12:21:17.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 7.798756122589111
2025-12-09 12:21:18.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 7.855906963348389
2025-12-09 12:21:18.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 7.481542587280273
2025-12-09 12:21:19.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 7.696374416351318
2025-12-09 12:21:19.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 7.769283771514893
2025-12-09 12:21:20.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999997089396425 Training loss: 7.492011547088623
2025-12-09 12:21:20.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029999988357586825 Training loss: 7.733391284942627
2025-12-09 12:21:21.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000299999738045746 Training loss: 7.775341987609863
2025-12-09 12:21:21.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0002999995343036539 Training loss: 7.672926425933838
2025-12-09 12:21:22.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00029999927234967104 Training loss: 8.083788871765137
2025-12-09 12:21:22.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.00029999895218389905 Training loss: 7.608232021331787
2025-12-09 12:21:23.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0002999985738064622 Training loss: 7.624826431274414
2025-12-09 12:21:23.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00029999813721750737 Training loss: 8.131349563598633
2025-12-09 12:21:24.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.00029999764241720394 Training loss: 7.6111249923706055
2025-12-09 12:21:24.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0002999970894057439 Training loss: 8.011396408081055
2025-12-09 12:21:25.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029999647818334195 Training loss: 7.6698317527771
2025-12-09 12:21:25.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0002999958087502352 Training loss: 7.763233184814453
2025-12-09 12:21:26.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.00029999508110668355 Training loss: 7.711025238037109
2025-12-09 12:21:26.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0002999942952529693 Training loss: 7.667514324188232
2025-12-09 12:21:27.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00029999345118939745 Training loss: 7.746524810791016
2025-12-09 12:21:27.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0002999925489162956 Training loss: 7.365662097930908
2025-12-09 12:21:28.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029999158843401386 Training loss: 7.650025844573975
2025-12-09 12:21:28.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.000299990569742925 Training loss: 7.20205020904541
2025-12-09 12:21:29.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002999894928434243 Training loss: 7.5733747482299805
2025-12-09 12:21:29.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.00029998835773592975 Training loss: 8.310541152954102
2025-12-09 12:21:30.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.00029998716442088184 Training loss: 7.255277633666992
2025-12-09 12:21:30.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0002999859128987437 Training loss: 7.652935028076172
2025-12-09 12:21:31.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00029998460317000097 Training loss: 7.266719341278076
2025-12-09 12:21:31.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00029998323523516195 Training loss: 7.651053428649902
2025-12-09 12:21:32.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002999818090947575 Training loss: 7.519716739654541
2025-12-09 12:21:32.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00029998032474934106 Training loss: 7.485326290130615
2025-12-09 12:21:33.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0002999787821994887 Training loss: 7.5593719482421875
2025-12-09 12:21:33.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00029997718144579913 Training loss: 7.322648525238037
2025-12-09 12:21:34.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0002999755224888935 Training loss: 7.601352214813232
2025-12-09 12:21:34.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029997380532941555 Training loss: 7.204314231872559
2025-12-09 12:21:35.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00029997202996803177 Training loss: 7.237954616546631
2025-12-09 12:21:35.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0002999701964054312 Training loss: 7.756885051727295
2025-12-09 12:21:36.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0002999683046423252 Training loss: 7.41697883605957
2025-12-09 12:21:36.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0002999663546794481 Training loss: 7.290273189544678
2025-12-09 12:21:37.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.00029996434651755657 Training loss: 8.753403663635254
2025-12-09 12:21:37.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.00029996228015743 Training loss: 7.487626075744629
2025-12-09 12:21:38.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002999601555998703 Training loss: 7.215329647064209
2025-12-09 12:21:38.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0002999579728457019 Training loss: 7.4393768310546875
2025-12-09 12:21:39.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0002999557318957719 Training loss: 7.354517459869385
2025-12-09 12:21:39.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.00029995343275095003 Training loss: 7.699073314666748
2025-12-09 12:21:40.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.00029995107541212843 Training loss: 7.394265651702881
2025-12-09 12:21:40.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00029994865988022205 Training loss: 7.407994747161865
2025-12-09 12:21:41.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0002999461861561683 Training loss: 7.671948432922363
2025-12-09 12:21:41.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0002999436542409271 Training loss: 7.211996078491211
2025-12-09 12:21:42.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0002999410641354812 Training loss: 7.839519023895264
2025-12-09 12:21:42.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00029993841584083553 Training loss: 7.147340774536133
2025-12-09 12:21:43.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00029993570935801805 Training loss: 7.351314067840576
2025-12-09 12:21:43.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.000299932944688079 Training loss: 7.175620079040527
2025-12-09 12:21:44.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.00029993012183209135 Training loss: 7.541374206542969
2025-12-09 12:21:44.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002999272407911505 Training loss: 8.197352409362793
2025-12-09 12:21:45.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.00029992430156637454 Training loss: 7.574273586273193
2025-12-09 12:21:45.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00029992130415890426 Training loss: 7.266273021697998
2025-12-09 12:21:46.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.00029991824856990276 Training loss: 7.713816165924072
2025-12-09 12:21:46.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0002999151348005559 Training loss: 7.088119029998779
2025-12-09 12:21:47.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0002999119628520721 Training loss: 7.051084518432617
2025-12-09 12:21:47.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.00029990873272568226 Training loss: 7.461830139160156
2025-12-09 12:21:48.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.00029990544442263996 Training loss: 7.369263172149658
2025-12-09 12:21:48.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002999020979442214 Training loss: 7.286739826202393
2025-12-09 12:21:49.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0002998986932917252 Training loss: 7.565497398376465
2025-12-09 12:21:49.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.00029989523046647257 Training loss: 6.915327548980713
2025-12-09 12:21:50.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00029989170946980755 Training loss: 7.389556884765625
2025-12-09 12:21:50.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00029988813030309644 Training loss: 7.65376091003418
2025-12-09 12:21:51.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0002998844929677283 Training loss: 7.5735554695129395
2025-12-09 12:21:51.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00029988079746511465 Training loss: 8.860498428344727
2025-12-09 12:21:52.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.00029987704379668973 Training loss: 7.323353290557861
2025-12-09 12:21:52.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0002998732319639102 Training loss: 7.4631195068359375
2025-12-09 12:21:53.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.00029986936196825536 Training loss: 7.850420951843262
2025-12-09 12:21:53.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0002998654338112271 Training loss: 7.050968170166016
2025-12-09 12:21:54.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00029986144749434985 Training loss: 7.617872714996338
2025-12-09 12:21:54.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0002998574030191706 Training loss: 7.600150108337402
2025-12-09 12:21:55.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.000299853300387259 Training loss: 7.859177589416504
2025-12-09 12:21:55.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00029984913960020714 Training loss: 7.662201404571533
2025-12-09 12:21:56.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.00029984492065962976 Training loss: 7.375117301940918
2025-12-09 12:21:56.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00029984064356716414 Training loss: 8.295684814453125
2025-12-09 12:21:57.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0002998363083244701 Training loss: 7.287317276000977
2025-12-09 12:21:57.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00029983191493323017 Training loss: 7.476119518280029
2025-12-09 12:21:58.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0002998274633951493 Training loss: 7.312543869018555
2025-12-09 12:21:58.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00029982295371195494 Training loss: 7.5114827156066895
2025-12-09 12:21:59.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00029981838588539735 Training loss: 7.493231296539307
2025-12-09 12:21:59.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.00029981375991724915 Training loss: 7.183638572692871
2025-12-09 12:22:00.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0002998090758093056 Training loss: 7.199141025543213
2025-12-09 12:22:00.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00029980433356338447 Training loss: 7.159573554992676
2025-12-09 12:22:01.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0002997995331813262 Training loss: 7.318190097808838
2025-12-09 12:22:01.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.00029979467466499367 Training loss: 7.307717323303223
2025-12-09 12:22:02.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00029978975801627243 Training loss: 7.024955749511719
2025-12-09 12:22:02.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0002997847832370704 Training loss: 7.159353733062744
2025-12-09 12:22:03.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0002997797503293184 Training loss: 6.921091079711914
2025-12-09 12:22:03.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00029977465929496947 Training loss: 8.05911636352539
2025-12-09 12:22:04.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0002997695101359994 Training loss: 7.249794006347656
2025-12-09 12:22:04.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0002997643028544064 Training loss: 6.971057891845703
2025-12-09 12:22:05.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0002997590374522114 Training loss: 7.224427223205566
2025-12-09 12:22:05.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0002997537139314577 Training loss: 7.470342636108398
2025-12-09 12:22:06.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0002997483322942114 Training loss: 7.102756023406982
2025-12-09 12:22:06.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0002997428925425609 Training loss: 7.320789337158203
2025-12-09 12:22:07.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0002997373946786173 Training loss: 7.782594680786133
2025-12-09 12:22:07.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.00029973183870451417 Training loss: 8.1922025680542
2025-12-09 12:22:08.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0002997262246224077 Training loss: 6.775104522705078
2025-12-09 12:22:08.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00029972055243447665 Training loss: 7.5176215171813965
2025-12-09 12:22:09.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.00029971482214292223 Training loss: 7.297370433807373
2025-12-09 12:22:09.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.00029970903374996826 Training loss: 6.87294340133667
2025-12-09 12:22:10.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0002997031872578611 Training loss: 7.258229732513428
2025-12-09 12:22:10.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00029969728266886973 Training loss: 7.026768684387207
2025-12-09 12:22:11.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00029969131998528554 Training loss: 7.2957611083984375
2025-12-09 12:22:11.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0002996852992094225 Training loss: 7.046902656555176
2025-12-09 12:22:12.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.00029967922034361723 Training loss: 7.565504550933838
2025-12-09 12:22:12.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0002996730833902287 Training loss: 7.195613384246826
2025-12-09 12:22:13.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00029966688835163875 Training loss: 7.540211200714111
2025-12-09 12:22:13.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00029966063523025136 Training loss: 7.19476842880249
2025-12-09 12:22:14.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00029965432402849333 Training loss: 6.811445713043213
2025-12-09 12:22:14.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0002996479547488139 Training loss: 7.265832424163818
2025-12-09 12:22:15.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0002996415273936849 Training loss: 7.1617865562438965
2025-12-09 12:22:15.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00029963504196560056 Training loss: 7.714066505432129
2025-12-09 12:22:16.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.00029962849846707786 Training loss: 7.266205787658691
2025-12-09 12:22:16.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0002996218969006561 Training loss: 6.905048847198486
2025-12-09 12:22:17.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00029961523726889733 Training loss: 7.434126853942871
2025-12-09 12:22:17.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00029960851957438594 Training loss: 7.147252559661865
2025-12-09 12:22:18.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.000299601743819729 Training loss: 7.281554698944092
2025-12-09 12:22:18.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00029959491000755594 Training loss: 7.077820301055908
2025-12-09 12:22:19.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00029958801814051897 Training loss: 7.159626483917236
2025-12-09 12:22:19.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0002995810682212926 Training loss: 7.2383713722229
2025-12-09 12:22:20.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0002995740602525739 Training loss: 7.258257865905762
2025-12-09 12:22:20.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0002995669942370827 Training loss: 7.370626449584961
2025-12-09 12:22:21.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.000299559870177561 Training loss: 7.2535481452941895
2025-12-09 12:22:21.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0002995526880767737 Training loss: 7.034478664398193
2025-12-09 12:22:22.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.00029954544793750785 Training loss: 7.21697473526001
2025-12-09 12:22:22.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00029953814976257335 Training loss: 8.11747932434082
2025-12-09 12:22:23.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0002995307935548024 Training loss: 7.008217811584473
2025-12-09 12:22:23.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0002995233793170498 Training loss: 6.864396095275879
2025-12-09 12:22:24.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.00029951590705219283 Training loss: 7.181149482727051
2025-12-09 12:22:24.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0002995083767631314 Training loss: 7.07420015335083
2025-12-09 12:22:25.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0002995007884527879 Training loss: 7.5258402824401855
2025-12-09 12:22:25.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.00029949314212410715 Training loss: 7.24080753326416
2025-12-09 12:22:26.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.00029948543778005656 Training loss: 7.264304161071777
2025-12-09 12:22:26.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00029947767542362597 Training loss: 7.225785732269287
2025-12-09 12:22:27.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0002994698550578279 Training loss: 7.036707878112793
2025-12-09 12:22:27.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0002994619766856972 Training loss: 6.826416492462158
2025-12-09 12:22:28.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00029945404031029134 Training loss: 7.312766075134277
2025-12-09 12:22:28.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.00029944604593469033 Training loss: 7.128983974456787
2025-12-09 12:22:29.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00029943799356199656 Training loss: 7.279306411743164
2025-12-09 12:22:29.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.00029942988319533504 Training loss: 6.77372932434082
2025-12-09 12:22:30.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0002994217148378532 Training loss: 6.941922664642334
2025-12-09 12:22:30.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00029941348849272105 Training loss: 7.204277515411377
2025-12-09 12:22:31.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0002994052041631311 Training loss: 6.624100685119629
2025-12-09 12:22:31.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0002993968618522982 Training loss: 7.0743231773376465
2025-12-09 12:22:32.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0002993884615634601 Training loss: 7.385269641876221
2025-12-09 12:22:32.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.00029938000329987645 Training loss: 7.124320983886719
2025-12-09 12:22:33.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00029937148706483003 Training loss: 7.324812412261963
2025-12-09 12:22:33.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.00029936291286162577 Training loss: 6.756494045257568
2025-12-09 12:22:34.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.000299354280693591 Training loss: 7.181506156921387
2025-12-09 12:22:34.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0002993455905640758 Training loss: 7.4439897537231445
2025-12-09 12:22:35.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0002993368424764526 Training loss: 6.924532413482666
2025-12-09 12:22:35.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0002993280364341165 Training loss: 7.098599910736084
2025-12-09 12:22:36.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00029931917244048473 Training loss: 7.1860151290893555
2025-12-09 12:22:36.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0002993102504989974 Training loss: 7.206268310546875
2025-12-09 12:22:37.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00029930127061311685 Training loss: 6.898766994476318
2025-12-09 12:22:37.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0002992922327863281 Training loss: 7.2686052322387695
2025-12-09 12:22:38.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00029928313702213844 Training loss: 6.843591690063477
2025-12-09 12:22:38.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.00029927398332407784 Training loss: 6.905237197875977
2025-12-09 12:22:39.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00029926477169569865 Training loss: 7.951791286468506
2025-12-09 12:22:39.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00029925550214057565 Training loss: 7.091726303100586
2025-12-09 12:22:40.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.00029924617466230624 Training loss: 7.359088897705078
2025-12-09 12:22:40.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.00029923678926451034 Training loss: 7.206450462341309
2025-12-09 12:22:41.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00029922734595083005 Training loss: 6.928484916687012
2025-12-09 12:22:41.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0002992178447249302 Training loss: 6.9301300048828125
2025-12-09 12:22:42.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.00029920828559049805 Training loss: 6.241344928741455
2025-12-09 12:22:42.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0002991986685512433 Training loss: 6.614350318908691
2025-12-09 12:22:43.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0002991889936108982 Training loss: 7.093343257904053
2025-12-09 12:22:43.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0002991792607732173 Training loss: 6.948230266571045
2025-12-09 12:22:44.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0002991694700419778 Training loss: 7.789380073547363
2025-12-09 12:22:44.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00029915962142097925 Training loss: 7.268045425415039
2025-12-09 12:22:45.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00029914971491404373 Training loss: 6.7175517082214355
2025-12-09 12:22:45.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00029913975052501575 Training loss: 6.853254795074463
2025-12-09 12:22:46.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0002991297282577623 Training loss: 7.085665225982666
2025-12-09 12:22:46.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00029911964811617285 Training loss: 7.502862930297852
2025-12-09 12:22:47.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00029910951010415926 Training loss: 6.977637767791748
2025-12-09 12:22:47.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0002990993142256559 Training loss: 7.126972198486328
2025-12-09 12:22:48.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0002990890604846196 Training loss: 6.5964250564575195
2025-12-09 12:22:48.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00029907874888502966 Training loss: 6.741867542266846
2025-12-09 12:22:49.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.00029906837943088785 Training loss: 7.25412130355835
2025-12-09 12:22:49.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.00029905795212621823 Training loss: 7.055274963378906
2025-12-09 12:22:50.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.00029904746697506754 Training loss: 7.168618202209473
2025-12-09 12:22:50.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0002990369239815048 Training loss: 6.616494655609131
2025-12-09 12:22:51.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.00029902632314962157 Training loss: 7.215983867645264
2025-12-09 12:22:51.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002990156644835318 Training loss: 7.140087127685547
2025-12-09 12:22:52.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.00029900494798737194 Training loss: 7.021543502807617
2025-12-09 12:22:52.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00029899417366530085 Training loss: 6.700840950012207
2025-12-09 12:22:53.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.00029898334152149984 Training loss: 7.0353217124938965
2025-12-09 12:22:53.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0002989724515601726 Training loss: 7.369460105895996
2025-12-09 12:22:54.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0002989615037855454 Training loss: 7.0872015953063965
2025-12-09 12:22:54.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0002989504982018668 Training loss: 6.8650970458984375
2025-12-09 12:22:55.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.00029893943481340785 Training loss: 7.907649517059326
2025-12-09 12:22:55.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.000298928313624462 Training loss: 7.0981364250183105
2025-12-09 12:22:56.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0002989171346393453 Training loss: 6.429078578948975
2025-12-09 12:22:56.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00029890589786239595 Training loss: 7.264065265655518
2025-12-09 12:22:57.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0002988946032979748 Training loss: 6.864343643188477
2025-12-09 12:22:57.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.000298883250950465 Training loss: 6.750553607940674
2025-12-09 12:22:58.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0002988718408242722 Training loss: 6.996353626251221
2025-12-09 12:22:58.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.00029886037292382455 Training loss: 6.645390510559082
2025-12-09 12:22:59.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00029884884725357236 Training loss: 6.593333721160889
2025-12-09 12:22:59.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0002988372638179886 Training loss: 6.545547962188721
2025-12-09 12:23:00.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002988256226215685 Training loss: 7.263132095336914
2025-12-09 12:23:00.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0002988139236688299 Training loss: 6.799861431121826
2025-12-09 12:23:01.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.00029880216696431285 Training loss: 7.2041826248168945
2025-12-09 12:23:01.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0002987903525125799 Training loss: 6.927257061004639
2025-12-09 12:23:02.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.000298778480318216 Training loss: 6.6665544509887695
2025-12-09 12:23:02.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0002987665503858286 Training loss: 7.207803726196289
2025-12-09 12:23:03.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0002987545627200474 Training loss: 7.5673370361328125
2025-12-09 12:23:03.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0002987425173255246 Training loss: 6.686209678649902
2025-12-09 12:23:04.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0002987304142069348 Training loss: 7.081945419311523
2025-12-09 12:23:04.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0002987182533689749 Training loss: 7.512413024902344
2025-12-09 12:23:05.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0002987060348163644 Training loss: 6.709562301635742
2025-12-09 12:23:05.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.000298693758553845 Training loss: 6.627894401550293
2025-12-09 12:23:06.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.00029868142458618096 Training loss: 7.06186056137085
2025-12-09 12:23:06.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0002986690329181587 Training loss: 6.961027145385742
2025-12-09 12:23:07.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00029865658355458736 Training loss: 7.1782732009887695
2025-12-09 12:23:07.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0002986440765002982 Training loss: 7.742118835449219
2025-12-09 12:23:08.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.000298631511760145 Training loss: 7.1069135665893555
2025-12-09 12:23:08.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0002986188893390038 Training loss: 6.814944744110107
2025-12-09 12:23:09.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0002986062092417733 Training loss: 7.217160224914551
2025-12-09 12:23:09.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.00029859347147337417 Training loss: 6.985872268676758
2025-12-09 12:23:10.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0002985806760387499 Training loss: 7.592942714691162
2025-12-09 12:23:10.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00029856782294286594 Training loss: 6.4228315353393555
2025-12-09 12:23:11.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00029855491219071053 Training loss: 6.933017730712891
2025-12-09 12:23:11.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.000298541943787294 Training loss: 6.963496208190918
2025-12-09 12:23:12.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00029852891773764906 Training loss: 6.65458869934082
2025-12-09 12:23:12.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00029851583404683096 Training loss: 6.928836345672607
2025-12-09 12:23:13.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0002985026927199172 Training loss: 6.718790531158447
2025-12-09 12:23:13.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.00029848949376200766 Training loss: 6.995616436004639
2025-12-09 12:23:14.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0002984762371782246 Training loss: 7.295806884765625
2025-12-09 12:23:14.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00029846292297371264 Training loss: 7.112307548522949
2025-12-09 12:23:15.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0002984495511536388 Training loss: 6.818396091461182
2025-12-09 12:23:15.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0002984361217231923 Training loss: 7.1696271896362305
2025-12-09 12:23:16.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.00029842263468758505 Training loss: 6.7975640296936035
2025-12-09 12:23:16.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0002984090900520509 Training loss: 6.865509510040283
2025-12-09 12:23:17.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00029839548782184636 Training loss: 7.030539035797119
2025-12-09 12:23:17.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.00029838182800225017 Training loss: 7.406294345855713
2025-12-09 12:23:18.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00029836811059856354 Training loss: 6.830258846282959
2025-12-09 12:23:18.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.00029835433561610974 Training loss: 6.634660720825195
2025-12-09 12:23:19.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0002983405030602346 Training loss: 6.863336086273193
2025-12-09 12:23:19.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.00029832661293630644 Training loss: 7.474697589874268
2025-12-09 12:23:20.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.00029831266524971557 Training loss: 7.009188175201416
2025-12-09 12:23:20.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0002982986600058749 Training loss: 7.418682098388672
2025-12-09 12:23:21.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0002982845972102196 Training loss: 7.135552883148193
2025-12-09 12:23:21.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0002982704768682071 Training loss: 6.952149868011475
2025-12-09 12:23:22.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00029825629898531724 Training loss: 6.7174530029296875
2025-12-09 12:23:22.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0002982420635670523 Training loss: 6.866261005401611
2025-12-09 12:23:23.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.00029822777061893653 Training loss: 7.085995197296143
2025-12-09 12:23:23.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00029821342014651694 Training loss: 6.891943454742432
2025-12-09 12:23:24.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0002981990121553627 Training loss: 7.256250381469727
2025-12-09 12:23:24.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0002981845466510651 Training loss: 6.67479133605957
2025-12-09 12:23:25.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00029817002363923803 Training loss: 6.789850234985352
2025-12-09 12:23:25.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00029815544312551754 Training loss: 6.900368690490723
2025-12-09 12:23:26.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00029814080511556207 Training loss: 7.262360095977783
2025-12-09 12:23:26.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00029812610961505234 Training loss: 6.747485160827637
2025-12-09 12:23:27.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.00029811135662969143 Training loss: 6.788418769836426
2025-12-09 12:23:27.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.00029809654616520456 Training loss: 6.807806491851807
2025-12-09 12:23:28.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00029808167822733953 Training loss: 7.129422187805176
2025-12-09 12:23:28.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0002980667528218662 Training loss: 7.063125133514404
2025-12-09 12:23:29.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002980517699545769 Training loss: 6.538910865783691
2025-12-09 12:23:29.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0002980367296312861 Training loss: 6.8812055587768555
2025-12-09 12:23:30.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00029802163185783074 Training loss: 6.820864677429199
2025-12-09 12:23:30.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.00029800647664006993 Training loss: 6.6293768882751465
2025-12-09 12:23:31.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0002979912639838851 Training loss: 6.854920864105225
2025-12-09 12:23:31.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.00029797599389518 Training loss: 7.584568023681641
2025-12-09 12:23:32.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0002979606663798807 Training loss: 6.790043830871582
2025-12-09 12:23:32.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0002979452814439354 Training loss: 7.191828727722168
2025-12-09 12:23:33.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.00029792983909331485 Training loss: 7.133690357208252
2025-12-09 12:23:33.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0002979143393340117 Training loss: 6.781800270080566
2025-12-09 12:23:34.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.00029789878217204133 Training loss: 6.495738506317139
2025-12-09 12:23:34.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00029788316761344106 Training loss: 6.8415141105651855
2025-12-09 12:23:35.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00029786749566427064 Training loss: 7.789327621459961
2025-12-09 12:23:35.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.000297851766330612 Training loss: 7.039257526397705
2025-12-09 12:23:36.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.00029783597961856946 Training loss: 7.360894203186035
2025-12-09 12:23:36.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00029782013553426937 Training loss: 6.700859546661377
2025-12-09 12:23:37.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.00029780423408386073 Training loss: 6.66072416305542
2025-12-09 12:23:37.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.00029778827527351443 Training loss: 6.6598358154296875
2025-12-09 12:23:38.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0002977722591094238 Training loss: 6.167579650878906
2025-12-09 12:23:38.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.00029775618559780447 Training loss: 6.901973724365234
2025-12-09 12:23:39.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.00029774005474489417 Training loss: 6.7240424156188965
2025-12-09 12:23:39.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.00029772386655695305 Training loss: 7.03380823135376
2025-12-09 12:23:40.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0002977076210402633 Training loss: 6.9681878089904785
2025-12-09 12:23:40.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.00029769131820112966 Training loss: 6.784424781799316
2025-12-09 12:23:41.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.00029767495804587885 Training loss: 6.762856960296631
2025-12-09 12:23:41.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.0002976585405808599 Training loss: 6.71380615234375
2025-12-09 12:23:42.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0002976420658124441 Training loss: 7.010233402252197
2025-12-09 12:23:42.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0002976255337470251 Training loss: 6.460738658905029
2025-12-09 12:23:43.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.00029760894439101855 Training loss: 7.141519546508789
2025-12-09 12:23:43.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0002975922977508625 Training loss: 6.870368957519531
2025-12-09 12:23:44.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0002975755938330172 Training loss: 6.822725772857666
2025-12-09 12:23:44.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.00029755883264396513 Training loss: 7.0195841789245605
2025-12-09 12:23:45.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00029754201419021094 Training loss: 6.845125675201416
2025-12-09 12:23:45.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0002975251384782816 Training loss: 6.65463399887085
2025-12-09 12:23:46.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.00029750820551472615 Training loss: 7.059190273284912
2025-12-09 12:23:46.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.00029749121530611597 Training loss: 6.83862829208374
2025-12-09 12:23:47.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0002974741678590447 Training loss: 7.066915035247803
2025-12-09 12:23:47.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.00029745706318012806 Training loss: 7.21701717376709
2025-12-09 12:23:48.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.00029743990127600406 Training loss: 6.728062629699707
2025-12-09 12:23:48.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0002974226821533329 Training loss: 7.026810169219971
2025-12-09 12:23:49.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.000297405405818797 Training loss: 6.632516860961914
2025-12-09 12:23:49.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0002973880722791009 Training loss: 7.051370143890381
2025-12-09 12:23:50.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0002973706815409715 Training loss: 7.005805492401123
2025-12-09 12:23:50.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0002973532336111577 Training loss: 7.089400291442871
2025-12-09 12:23:51.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00029733572849643085 Training loss: 6.631850242614746
2025-12-09 12:23:51.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.00029731816620358424 Training loss: 7.2124199867248535
2025-12-09 12:23:52.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0002973005467394334 Training loss: 6.417664527893066
2025-12-09 12:23:52.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.00029728287011081625 Training loss: 6.7356414794921875
2025-12-09 12:23:53.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0002972651363245927 Training loss: 6.609213352203369
2025-12-09 12:23:53.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00029724734538764475 Training loss: 6.817221164703369
2025-12-09 12:23:54.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0002972294973068768 Training loss: 6.905410289764404
2025-12-09 12:23:54.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.00029721159208921546 Training loss: 6.21352481842041
2025-12-09 12:23:55.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.00029719362974160924 Training loss: 6.953610897064209
2025-12-09 12:23:55.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.000297175610271029 Training loss: 6.389537811279297
2025-12-09 12:23:56.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.00029715753368446786 Training loss: 6.897193431854248
2025-12-09 12:23:56.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00029713939998894087 Training loss: 6.907784938812256
2025-12-09 12:23:57.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0002971212091914854 Training loss: 6.6767449378967285
2025-12-09 12:23:57.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0002971029612991609 Training loss: 6.638968467712402
2025-12-09 12:23:58.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0002970846563190491 Training loss: 6.488215923309326
2025-12-09 12:23:58.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00029706629425825374 Training loss: 6.777031898498535
2025-12-09 12:23:59.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.00029704787512390085 Training loss: 7.06546688079834
2025-12-09 12:23:59.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0002970293989231385 Training loss: 6.899211406707764
2025-12-09 12:24:00.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.0002970108656631369 Training loss: 6.63032341003418
2025-12-09 12:24:00.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0002969922753510885 Training loss: 6.630556583404541
2025-12-09 12:24:01.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00029697362799420776 Training loss: 6.988895416259766
2025-12-09 12:24:01.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0002969549235997315 Training loss: 6.776453018188477
2025-12-09 12:24:02.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0002969361621749184 Training loss: 6.453487873077393
2025-12-09 12:24:02.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.00029691734372704943 Training loss: 7.183653831481934
2025-12-09 12:24:03.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0002968984682634277 Training loss: 6.816318511962891
2025-12-09 12:24:03.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0002968795357913784 Training loss: 7.13561487197876
2025-12-09 12:24:04.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0002968605463182488 Training loss: 7.108542442321777
2025-12-09 12:24:04.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0002968414998514085 Training loss: 6.877376079559326
2025-12-09 12:24:05.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0002968223963982488 Training loss: 6.808412551879883
2025-12-09 12:24:05.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.00029680323596618355 Training loss: 6.65167760848999
2025-12-09 12:24:06.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00029678401856264857 Training loss: 6.361397743225098
2025-12-09 12:24:06.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0002967647441951017 Training loss: 6.639608860015869
2025-12-09 12:24:07.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0002967454128710229 Training loss: 6.117376804351807
2025-12-09 12:24:07.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.00029672602459791434 Training loss: 6.692080020904541
2025-12-09 12:24:08.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0002967065793833002 Training loss: 7.228443145751953
2025-12-09 12:24:08.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0002966870772347269 Training loss: 6.625426292419434
2025-12-09 12:24:09.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0002966675181597627 Training loss: 7.333661079406738
2025-12-09 12:24:09.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0002966479021659981 Training loss: 6.6580657958984375
2025-12-09 12:24:10.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.00029662822926104576 Training loss: 6.96909761428833
2025-12-09 12:24:10.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0002966084994525403 Training loss: 7.2629008293151855
2025-12-09 12:24:11.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.00029658871274813853 Training loss: 7.144023895263672
2025-12-09 12:24:11.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.00029656886915551924 Training loss: 6.836947441101074
2025-12-09 12:24:12.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0002965489686823833 Training loss: 7.232394695281982
2025-12-09 12:24:12.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.00029652901133645377 Training loss: 6.87493371963501
2025-12-09 12:24:13.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0002965089971254757 Training loss: 6.596698760986328
2025-12-09 12:24:13.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0002964889260572162 Training loss: 6.71883487701416
2025-12-09 12:24:14.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0002964687981394644 Training loss: 6.509883403778076
2025-12-09 12:24:14.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.00029644861338003165 Training loss: 6.36197566986084
2025-12-09 12:24:15.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0002964283717867512 Training loss: 5.831789970397949
2025-12-09 12:24:15.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0002964080733674784 Training loss: 7.04181432723999
2025-12-09 12:24:16.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0002963877181300907 Training loss: 6.470491886138916
2025-12-09 12:24:16.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00029636730608248766 Training loss: 6.561017990112305
2025-12-09 12:24:17.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0002963468372325906 Training loss: 6.655717849731445
2025-12-09 12:24:17.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.00029632631158834326 Training loss: 6.528258800506592
2025-12-09 12:24:18.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.00029630572915771117 Training loss: 6.090678691864014
2025-12-09 12:24:18.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.000296285089948682 Training loss: 6.5820841789245605
2025-12-09 12:24:19.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.00029626439396926533 Training loss: 6.715540409088135
2025-12-09 12:24:19.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.00029624364122749294 Training loss: 6.7396039962768555
2025-12-09 12:24:20.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0002962228317314186 Training loss: 6.757885456085205
2025-12-09 12:24:20.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00029620196548911797 Training loss: 7.060742378234863
2025-12-09 12:24:21.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.0002961810425086889 Training loss: 6.839778900146484
2025-12-09 12:24:21.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00029616006279825126 Training loss: 7.053164482116699
2025-12-09 12:24:22.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0002961390263659467 Training loss: 6.141213893890381
2025-12-09 12:24:22.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0002961179332199391 Training loss: 6.857758522033691
2025-12-09 12:24:23.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00029609678336841444 Training loss: 6.785346508026123
2025-12-09 12:24:23.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.00029607557681958035 Training loss: 7.040229320526123
2025-12-09 12:24:24.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.00029605431358166684 Training loss: 6.987583160400391
2025-12-09 12:24:24.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.00029603299366292565 Training loss: 6.310708045959473
2025-12-09 12:24:25.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00029601161707163077 Training loss: 7.016108989715576
2025-12-09 12:24:25.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.00029599018381607785 Training loss: 7.318703651428223
2025-12-09 12:24:26.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0002959686939045848 Training loss: 6.860852241516113
2025-12-09 12:24:26.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.00029594714734549146 Training loss: 6.423022747039795
2025-12-09 12:24:27.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0002959255441471597 Training loss: 6.406862258911133
2025-12-09 12:24:27.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0002959038843179731 Training loss: 6.9554290771484375
2025-12-09 12:24:28.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0002958821678663376 Training loss: 6.726370811462402
2025-12-09 12:24:28.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.00029586039480068087 Training loss: 6.685351371765137
2025-12-09 12:24:29.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0002958385651294525 Training loss: 6.534305572509766
2025-12-09 12:24:29.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00029581667886112434 Training loss: 6.513578414916992
2025-12-09 12:24:30.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.00029579473600418993 Training loss: 6.586243629455566
2025-12-09 12:24:30.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0002957727365671649 Training loss: 6.845244884490967
2025-12-09 12:24:31.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0002957506805585867 Training loss: 7.507894992828369
2025-12-09 12:24:31.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00029572856798701504 Training loss: 6.705533981323242
2025-12-09 12:24:32.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0002957063988610312 Training loss: 6.670415878295898
2025-12-09 12:24:32.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0002956841731892386 Training loss: 6.5213799476623535
2025-12-09 12:24:33.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0002956618909802627 Training loss: 6.649342060089111
2025-12-09 12:24:33.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.00029563955224275065 Training loss: 6.650908946990967
2025-12-09 12:24:34.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.00029561715698537183 Training loss: 6.844213962554932
2025-12-09 12:24:34.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0002955947052168172 Training loss: 6.415319442749023
2025-12-09 12:24:35.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0002955721969458001 Training loss: 6.537859916687012
2025-12-09 12:24:35.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0002955496321810553 Training loss: 6.816422939300537
2025-12-09 12:24:36.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.00029552701093133994 Training loss: 6.961129188537598
2025-12-09 12:24:36.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.00029550433320543284 Training loss: 6.728102684020996
2025-12-09 12:24:37.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0002954815990121347 Training loss: 6.845798492431641
2025-12-09 12:24:37.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00029545880836026833 Training loss: 6.7213454246521
2025-12-09 12:24:38.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0002954359612586782 Training loss: 6.419521331787109
2025-12-09 12:24:38.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00029541305771623095 Training loss: 6.695436477661133
2025-12-09 12:24:39.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.00029539009774181494 Training loss: 7.039922714233398
2025-12-09 12:24:39.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.00029536708134434054 Training loss: 6.537868976593018
2025-12-09 12:24:40.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.00029534400853273985 Training loss: 6.787029266357422
2025-12-09 12:24:40.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0002953208793159671 Training loss: 6.543159484863281
2025-12-09 12:24:41.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.00029529769370299823 Training loss: 6.6168060302734375
2025-12-09 12:24:41.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0002952744517028311 Training loss: 6.370161533355713
2025-12-09 12:24:42.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00029525115332448555 Training loss: 6.903572082519531
2025-12-09 12:24:42.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0002952277985770032 Training loss: 6.799563407897949
2025-12-09 12:24:43.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0002952043874694475 Training loss: 6.674545764923096
2025-12-09 12:24:43.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00029518092001090397 Training loss: 6.950685501098633
2025-12-09 12:24:44.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00029515739621047973 Training loss: 6.430619239807129
2025-12-09 12:24:44.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.000295133816077304 Training loss: 6.366275310516357
2025-12-09 12:24:45.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.0002951101796205278 Training loss: 6.733737945556641
2025-12-09 12:24:45.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0002950864868493239 Training loss: 6.341740608215332
2025-12-09 12:24:46.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.00029506273777288696 Training loss: 6.586526870727539
2025-12-09 12:24:46.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0002950389324004337 Training loss: 6.681351184844971
2025-12-09 12:24:47.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.00029501507074120237 Training loss: 6.443542957305908
2025-12-09 12:24:47.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.00029499115280445326 Training loss: 6.483232498168945
2025-12-09 12:24:48.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0002949671785994685 Training loss: 6.318404197692871
2025-12-09 12:24:48.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.00029494314813555193 Training loss: 6.239515781402588
2025-12-09 12:24:49.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.00029491906142202934 Training loss: 7.149618625640869
2025-12-09 12:24:49.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.00029489491846824837 Training loss: 6.821633338928223
2025-12-09 12:24:50.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0002948707192835783 Training loss: 6.043242454528809
2025-12-09 12:24:50.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0002948464638774105 Training loss: 6.716642379760742
2025-12-09 12:24:51.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.00029482215225915795 Training loss: 6.580132007598877
2025-12-09 12:24:51.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0002947977844382555 Training loss: 6.566055774688721
2025-12-09 12:24:52.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0002947733604241599 Training loss: 6.493320465087891
2025-12-09 12:24:52.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.00029474888022634955 Training loss: 6.4293084144592285
2025-12-09 12:24:53.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.00029472434385432474 Training loss: 6.667917251586914
2025-12-09 12:24:53.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0002946997513176076 Training loss: 6.34693717956543
2025-12-09 12:24:54.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.000294675102625742 Training loss: 6.6396565437316895
2025-12-09 12:24:54.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.00029465039778829366 Training loss: 6.590345859527588
2025-12-09 12:24:55.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0002946256368148499 Training loss: 6.6911749839782715
2025-12-09 12:24:55.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.00029460081971502015 Training loss: 6.511083602905273
2025-12-09 12:24:56.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.00029457594649843534 Training loss: 6.385643005371094
2025-12-09 12:24:56.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0002945510171747483 Training loss: 6.670419692993164
2025-12-09 12:24:57.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0002945260317536336 Training loss: 6.896018028259277
2025-12-09 12:24:57.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0002945009902447876 Training loss: 6.89819860458374
2025-12-09 12:24:58.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.00029447589265792847 Training loss: 6.503737926483154
2025-12-09 12:24:58.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.00029445073900279605 Training loss: 6.731751441955566
2025-12-09 12:24:59.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.000294425529289152 Training loss: 7.011893272399902
2025-12-09 12:24:59.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.00029440026352677966 Training loss: 8.231888771057129
2025-12-09 12:25:00.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.00029437494172548424 Training loss: 6.310595989227295
2025-12-09 12:25:00.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.00029434956389509263 Training loss: 6.788820743560791
2025-12-09 12:25:01.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0002943241300454534 Training loss: 7.905280113220215
2025-12-09 12:25:01.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.0002942986401864371 Training loss: 5.833343505859375
2025-12-09 12:25:02.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0002942730943279357 Training loss: 6.407827854156494
2025-12-09 12:25:02.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.000294247492479863 Training loss: 6.3989410400390625
2025-12-09 12:25:03.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.00029422183465215474 Training loss: 6.42039680480957
2025-12-09 12:25:03.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00029419612085476813 Training loss: 6.897237777709961
2025-12-09 12:25:04.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0002941703510976822 Training loss: 6.557707786560059
2025-12-09 12:25:04.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.00029414452539089776 Training loss: 6.663332462310791
2025-12-09 12:25:05.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00029411864374443716 Training loss: 6.184370517730713
2025-12-09 12:25:05.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0002940927061683446 Training loss: 6.626238822937012
2025-12-09 12:25:06.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.0002940667126726859 Training loss: 6.378687858581543
2025-12-09 12:25:06.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0002940406632675487 Training loss: 6.402095794677734
2025-12-09 12:25:07.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0002940145579630423 Training loss: 6.530222415924072
2025-12-09 12:25:07.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.00029398839676929756 Training loss: 6.235146522521973
2025-12-09 12:25:08.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.00029396217969646717 Training loss: 6.569255828857422
2025-12-09 12:25:08.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00029393590675472545 Training loss: 6.547133445739746
2025-12-09 12:25:09.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00029390957795426845 Training loss: 6.361693382263184
2025-12-09 12:25:09.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0002938831933053138 Training loss: 6.823136329650879
2025-12-09 12:25:10.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.000293856752818101 Training loss: 5.988265514373779
2025-12-09 12:25:10.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.00029383025650289095 Training loss: 6.697545528411865
2025-12-09 12:25:11.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0002938037043699664 Training loss: 6.612017631530762
2025-12-09 12:25:11.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0002937770964296317 Training loss: 6.7618632316589355
2025-12-09 12:25:12.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0002937504326922129 Training loss: 6.907101154327393
2025-12-09 12:25:12.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.00029372371316805767 Training loss: 6.647493839263916
2025-12-09 12:25:13.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.00029369693786753534 Training loss: 6.64600944519043
2025-12-09 12:25:13.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0002936701068010368 Training loss: 6.787437438964844
2025-12-09 12:25:14.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0002936432199789748 Training loss: 6.894039630889893
2025-12-09 12:25:14.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.00029361627741178356 Training loss: 7.05488920211792
2025-12-09 12:25:15.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.00029358927910991885 Training loss: 6.720664978027344
2025-12-09 12:25:15.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.00029356222508385827 Training loss: 6.231743812561035
2025-12-09 12:25:16.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.000293535115344101 Training loss: 6.491024017333984
2025-12-09 12:25:16.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0002935079499011677 Training loss: 6.2545695304870605
2025-12-09 12:25:17.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0002934807287656008 Training loss: 6.341223239898682
2025-12-09 12:25:17.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.00029345345194796435 Training loss: 6.541720390319824
2025-12-09 12:25:18.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0002934261194588438 Training loss: 6.632734775543213
2025-12-09 12:25:18.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.00029339873130884654 Training loss: 6.498826026916504
2025-12-09 12:25:19.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.00029337128750860124 Training loss: 6.60556697845459
2025-12-09 12:25:19.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.00029334378806875836 Training loss: 6.455318927764893
2025-12-09 12:25:20.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.00029331623299998986 Training loss: 6.389649868011475
2025-12-09 12:25:20.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0002932886223129894 Training loss: 7.18873405456543
2025-12-09 12:25:21.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.000293260956018472 Training loss: 6.155402183532715
2025-12-09 12:25:21.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0002932332341271746 Training loss: 6.994700908660889
2025-12-09 12:25:22.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00029320545664985535 Training loss: 7.468626499176025
2025-12-09 12:25:22.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.00029317762359729423 Training loss: 6.5482587814331055
2025-12-09 12:25:23.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.00029314973498029275 Training loss: 6.535046100616455
2025-12-09 12:25:23.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0002931217908096739 Training loss: 6.835795879364014
2025-12-09 12:25:24.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0002930937910962822 Training loss: 6.115656852722168
2025-12-09 12:25:24.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.00029306573585098384 Training loss: 6.839248180389404
2025-12-09 12:25:25.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.00029303762508466654 Training loss: 6.748213291168213
2025-12-09 12:25:25.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00029300945880823956 Training loss: 6.764068126678467
2025-12-09 12:25:26.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0002929812370326336 Training loss: 7.071047782897949
2025-12-09 12:25:26.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.000292952959768801 Training loss: 6.72433614730835
2025-12-09 12:25:27.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0002929246270277157 Training loss: 7.206926345825195
2025-12-09 12:25:27.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.000292896238820373 Training loss: 6.24918270111084
2025-12-09 12:25:28.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0002928677951577898 Training loss: 6.561212062835693
2025-12-09 12:25:28.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.00029283929605100455 Training loss: 6.501022815704346
2025-12-09 12:25:29.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0002928107415110772 Training loss: 6.467002868652344
2025-12-09 12:25:29.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0002927821315490893 Training loss: 6.727989673614502
2025-12-09 12:25:30.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0002927534661761436 Training loss: 6.8573808670043945
2025-12-09 12:25:30.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.00029272474540336475 Training loss: 6.451075553894043
2025-12-09 12:25:31.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.00029269596924189875 Training loss: 6.342863082885742
2025-12-09 12:25:31.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0002926671377029129 Training loss: 6.810486316680908
2025-12-09 12:25:32.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.00029263825079759635 Training loss: 6.62162971496582
2025-12-09 12:25:32.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.00029260930853715935 Training loss: 6.710310935974121
2025-12-09 12:25:33.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0002925803109328339 Training loss: 6.541512966156006
2025-12-09 12:25:33.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0002925512579958735 Training loss: 6.502023220062256
2025-12-09 12:25:34.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0002925221497375529 Training loss: 6.2329888343811035
2025-12-09 12:25:34.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.00029249298616916856 Training loss: 6.546230792999268
2025-12-09 12:25:35.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.00029246376730203817 Training loss: 6.330800533294678
2025-12-09 12:25:35.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0002924344931475011 Training loss: 6.363105773925781
2025-12-09 12:25:36.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.000292405163716918 Training loss: 6.468930721282959
2025-12-09 12:25:36.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0002923757790216711 Training loss: 6.245622158050537
2025-12-09 12:25:37.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.000292346339073164 Training loss: 6.45366907119751
2025-12-09 12:25:37.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.00029231684388282184 Training loss: 6.577494144439697
2025-12-09 12:25:38.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.000292287293462091 Training loss: 6.737673759460449
2025-12-09 12:25:38.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0002922576878224395 Training loss: 6.617131233215332
2025-12-09 12:25:39.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.00029222802697535674 Training loss: 6.8037943840026855
2025-12-09 12:25:39.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0002921983109323535 Training loss: 6.614975452423096
2025-12-09 12:25:40.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0002921685397049619 Training loss: 6.324967384338379
2025-12-09 12:25:40.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0002921387133047357 Training loss: 6.637189865112305
2025-12-09 12:25:41.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0002921088317432499 Training loss: 6.927946090698242
2025-12-09 12:25:41.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0002920788950321009 Training loss: 6.80064058303833
2025-12-09 12:25:42.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.00029204890318290666 Training loss: 6.436922073364258
2025-12-09 12:25:42.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0002920188562073063 Training loss: 6.91193962097168
2025-12-09 12:25:43.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0002919887541169605 Training loss: 6.4444355964660645
2025-12-09 12:25:43.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0002919585969235514 Training loss: 6.593950271606445
2025-12-09 12:25:44.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.00029192838463878236 Training loss: 6.732170104980469
2025-12-09 12:25:44.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0002918981172743781 Training loss: 6.283050060272217
2025-12-09 12:25:45.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.00029186779484208485 Training loss: 6.319036483764648
2025-12-09 12:25:45.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0002918374173536702 Training loss: 6.547009468078613
2025-12-09 12:25:46.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.000291806984820923 Training loss: 6.509109973907471
2025-12-09 12:25:46.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.00029177649725565353 Training loss: 6.5076422691345215
2025-12-09 12:25:47.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.00029174595466969344 Training loss: 6.920194625854492
2025-12-09 12:25:47.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.00029171535707489565 Training loss: 6.805829048156738
2025-12-09 12:25:48.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0002916847044831346 Training loss: 6.374271392822266
2025-12-09 12:25:48.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0002916539969063059 Training loss: 6.813254356384277
2025-12-09 12:25:49.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0002916232343563265 Training loss: 6.339118003845215
2025-12-09 12:25:49.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0002915924168451349 Training loss: 6.510984897613525
2025-12-09 12:25:50.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0002915615443846906 Training loss: 6.774764537811279
2025-12-09 12:25:50.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0002915306169869747 Training loss: 6.626612186431885
2025-12-09 12:25:51.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0002914996346639895 Training loss: 7.692838668823242
2025-12-09 12:25:51.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.00029146859742775865 Training loss: 6.820408821105957
2025-12-09 12:25:52.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00029143750529032707 Training loss: 6.449343681335449
2025-12-09 12:25:52.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0002914063582637611 Training loss: 5.967576503753662
2025-12-09 12:25:53.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0002913751563601481 Training loss: 6.447951316833496
2025-12-09 12:25:53.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0002913438995915971 Training loss: 6.678717136383057
2025-12-09 12:25:54.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0002913125879702381 Training loss: 6.413311958312988
2025-12-09 12:25:54.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.00029128122150822263 Training loss: 6.368016719818115
2025-12-09 12:25:55.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0002912498002177234 Training loss: 6.571152210235596
2025-12-09 12:25:55.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0002912183241109344 Training loss: 6.6538848876953125
2025-12-09 12:25:56.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.00029118679320007087 Training loss: 6.784142017364502
2025-12-09 12:25:56.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0002911552074973693 Training loss: 6.10978889465332
2025-12-09 12:25:57.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0002911235670150875 Training loss: 6.524747371673584
2025-12-09 12:25:57.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0002910918717655046 Training loss: 6.493642330169678
2025-12-09 12:25:58.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00029106012176092084 Training loss: 6.702610492706299
2025-12-09 12:25:58.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0002910283170136578 Training loss: 6.77406120300293
2025-12-09 12:25:59.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00029099645753605827 Training loss: 6.782020092010498
2025-12-09 12:25:59.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.00029096454334048627 Training loss: 6.53703498840332
2025-12-09 12:26:00.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0002909325744393271 Training loss: 6.477913856506348
2025-12-09 12:26:00.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0002909005508449873 Training loss: 6.651147842407227
2025-12-09 12:26:01.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0002908684725698946 Training loss: 6.3358001708984375
2025-12-09 12:26:01.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0002908363396264978 Training loss: 6.68843936920166
2025-12-09 12:26:02.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.00029080415202726727 Training loss: 6.662461280822754
2025-12-09 12:26:02.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0002907719097846943 Training loss: 6.528373718261719
2025-12-09 12:26:03.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0002907396129112915 Training loss: 6.370907783508301
2025-12-09 12:26:03.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.00029070726141959265 Training loss: 6.46120023727417
2025-12-09 12:26:04.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.00029067485532215267 Training loss: 6.371310710906982
2025-12-09 12:26:04.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0002906423946315478 Training loss: 6.080550670623779
2025-12-09 12:26:05.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.00029060987936037536 Training loss: 6.388772964477539
2025-12-09 12:26:05.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.00029057730952125393 Training loss: 6.59747838973999
2025-12-09 12:26:06.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0002905446851268233 Training loss: 6.778031826019287
2025-12-09 12:26:06.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0002905120061897441 Training loss: 6.8816938400268555
2025-12-09 12:26:07.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0002904792727226987 Training loss: 6.707498550415039
2025-12-09 12:26:07.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.00029044648473839014 Training loss: 6.824956893920898
2025-12-09 12:26:08.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0002904136422495429 Training loss: 6.630346298217773
2025-12-09 12:26:08.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0002903807452689024 Training loss: 6.573080539703369
2025-12-09 12:26:09.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00029034779380923535 Training loss: 6.338686943054199
2025-12-09 12:26:09.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.00029031478788332955 Training loss: 6.774095058441162
2025-12-09 12:26:10.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0002902817275039941 Training loss: 6.5289130210876465
2025-12-09 12:26:10.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.00029024861268405887 Training loss: 6.474934101104736
2025-12-09 12:26:11.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.00029021544343637526 Training loss: 6.605628967285156
2025-12-09 12:26:11.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.00029018221977381546 Training loss: 6.41925573348999
2025-12-09 12:26:12.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.00029014894170927306 Training loss: 6.515495777130127
2025-12-09 12:26:12.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0002901156092556625 Training loss: 6.286942958831787
2025-12-09 12:26:13.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0002900822224259195 Training loss: 6.887917995452881
2025-12-09 12:26:13.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0002900487812330009 Training loss: 6.3709845542907715
2025-12-09 12:26:14.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.00029001528568988453 Training loss: 6.309436321258545
2025-12-09 12:26:14.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.00028998173580956934 Training loss: 6.363369941711426
2025-12-09 12:26:15.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00028994813160507536 Training loss: 6.637786388397217
2025-12-09 12:26:15.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0002899144730894438 Training loss: 6.551698207855225
2025-12-09 12:26:16.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00028988076027573685 Training loss: 6.680747985839844
2025-12-09 12:26:16.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.00028984699317703775 Training loss: 6.420063018798828
2025-12-09 12:26:17.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0002898131718064509 Training loss: 6.549717903137207
2025-12-09 12:26:17.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.00028977929617710166 Training loss: 6.442731857299805
2025-12-09 12:26:18.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.00028974536630213657 Training loss: 6.690125942230225
2025-12-09 12:26:18.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.00028971138219472303 Training loss: 6.7402663230896
2025-12-09 12:26:19.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.00028967734386804977 Training loss: 6.358127593994141
2025-12-09 12:26:19.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.0002896432513353264 Training loss: 6.568753719329834
2025-12-09 12:26:20.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.00028960910460978337 Training loss: 6.474996566772461
2025-12-09 12:26:20.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0002895749037046725 Training loss: 6.354576110839844
2025-12-09 12:26:21.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0002895406486332665 Training loss: 6.4392409324646
2025-12-09 12:26:21.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.00028950633940885907 Training loss: 6.435054779052734
2025-12-09 12:26:22.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.00028947197604476494 Training loss: 6.454556465148926
2025-12-09 12:26:22.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.00028943755855431985 Training loss: 5.793407440185547
2025-12-09 12:26:23.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0002894030869508806 Training loss: 6.646829605102539
2025-12-09 12:26:23.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.0002893685612478249 Training loss: 6.805997848510742
2025-12-09 12:26:24.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.00028933398145855154 Training loss: 6.92694091796875
2025-12-09 12:26:24.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0002892993475964802 Training loss: 6.282048225402832
2025-12-09 12:26:25.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0002892646596750517 Training loss: 6.617573261260986
2025-12-09 12:26:25.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0002892299177077277 Training loss: 6.241048812866211
2025-12-09 12:26:26.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0002891951217079908 Training loss: 6.356611251831055
2025-12-09 12:26:26.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0002891602716893448 Training loss: 6.55258846282959
2025-12-09 12:26:27.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0002891253676653142 Training loss: 6.70468807220459
2025-12-09 12:26:27.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.00028909040964944456 Training loss: 6.568999767303467
2025-12-09 12:26:28.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.0002890553976553025 Training loss: 6.431135654449463
2025-12-09 12:26:28.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.00028902033169647543 Training loss: 6.680624961853027
2025-12-09 12:26:29.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.00028898521178657174 Training loss: 6.883256435394287
2025-12-09 12:26:29.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0002889500379392209 Training loss: 6.671437740325928
2025-12-09 12:26:30.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.000288914810168073 Training loss: 6.029820442199707
2025-12-09 12:26:30.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.00028887952848679943 Training loss: 6.350650310516357
2025-12-09 12:26:31.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.0002888441929090922 Training loss: 6.396564483642578
2025-12-09 12:26:31.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.00028880880344866447 Training loss: 6.374082565307617
2025-12-09 12:26:32.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.00028877336011925005 Training loss: 5.998293399810791
2025-12-09 12:26:32.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.000288737862934604 Training loss: 6.176441192626953
2025-12-09 12:26:33.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.00028870231190850185 Training loss: 6.5088653564453125
2025-12-09 12:26:33.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.00028866670705474047 Training loss: 6.549511432647705
2025-12-09 12:26:34.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.00028863104838713726 Training loss: 6.48929500579834
2025-12-09 12:26:34.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.00028859533591953074 Training loss: 6.718307018280029
2025-12-09 12:26:35.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.00028855956966578023 Training loss: 6.359255790710449
2025-12-09 12:26:35.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.00028852374963976585 Training loss: 6.761305809020996
2025-12-09 12:26:36.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.0002884878758553887 Training loss: 6.366689682006836
2025-12-09 12:26:36.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.00028845194832657065 Training loss: 6.513833045959473
2025-12-09 12:26:37.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.0002884159670672545 Training loss: 6.843564510345459
2025-12-09 12:26:37.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.00028837993209140385 Training loss: 6.956070899963379
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 2.87 GiB is free. Including non-PyTorch memory, this process has 91.28 GiB memory in use. Of the allocated memory 90.25 GiB is allocated by PyTorch, and 278.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 50/10000 [00:00<00:20, 495.75it/s]Tokenizing texts:   1%|▏         | 134/10000 [00:00<00:14, 695.68it/s]Tokenizing texts:   2%|▏         | 204/10000 [00:00<00:15, 630.42it/s]Tokenizing texts:   3%|▎         | 268/10000 [00:00<00:15, 633.12it/s]Tokenizing texts:   3%|▎         | 332/10000 [00:00<00:15, 611.03it/s]Tokenizing texts:   4%|▍         | 394/10000 [00:00<00:15, 604.15it/s]Tokenizing texts:   5%|▍         | 455/10000 [00:00<00:16, 577.48it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 577.60it/s]Tokenizing texts:   6%|▌         | 597/10000 [00:00<00:15, 601.24it/s]Tokenizing texts:   7%|▋         | 658/10000 [00:01<00:15, 593.74it/s]Tokenizing texts:   7%|▋         | 718/10000 [00:01<00:16, 577.77it/s]Tokenizing texts:   8%|▊         | 776/10000 [00:01<00:15, 576.71it/s]Tokenizing texts:   8%|▊         | 834/10000 [00:01<00:16, 571.50it/s]Tokenizing texts:   9%|▉         | 898/10000 [00:01<00:15, 589.93it/s]Tokenizing texts:  10%|▉         | 978/10000 [00:01<00:13, 649.23it/s]Tokenizing texts:  11%|█         | 1051/10000 [00:01<00:14, 635.52it/s]Tokenizing texts:  11%|█         | 1115/10000 [00:01<00:14, 631.43it/s]Tokenizing texts:  12%|█▏        | 1179/10000 [00:01<00:14, 595.92it/s]Tokenizing texts:  12%|█▏        | 1240/10000 [00:02<00:14, 593.00it/s]Tokenizing texts:  13%|█▎        | 1305/10000 [00:02<00:14, 606.81it/s]Tokenizing texts:  14%|█▎        | 1366/10000 [00:02<00:14, 594.79it/s]Tokenizing texts:  14%|█▍        | 1426/10000 [00:02<00:14, 594.53it/s]Tokenizing texts:  15%|█▌        | 1504/10000 [00:02<00:13, 647.36it/s]Tokenizing texts:  16%|█▌        | 1580/10000 [00:02<00:12, 679.04it/s]Tokenizing texts:  16%|█▋        | 1649/10000 [00:02<00:13, 612.92it/s]Tokenizing texts:  17%|█▋        | 1712/10000 [00:02<00:13, 604.48it/s]Tokenizing texts:  18%|█▊        | 1785/10000 [00:02<00:12, 631.96it/s]Tokenizing texts:  18%|█▊        | 1849/10000 [00:03<00:13, 604.44it/s]Tokenizing texts:  19%|█▉        | 1913/10000 [00:03<00:13, 610.11it/s]Tokenizing texts:  20%|█▉        | 1975/10000 [00:03<00:14, 565.57it/s]Tokenizing texts:  20%|██        | 2046/10000 [00:03<00:13, 604.60it/s]Tokenizing texts:  21%|██        | 2108/10000 [00:03<00:14, 553.79it/s]Tokenizing texts:  22%|██▏       | 2166/10000 [00:03<00:14, 559.16it/s]Tokenizing texts:  22%|██▏       | 2242/10000 [00:03<00:12, 610.57it/s]Tokenizing texts:  23%|██▎       | 2309/10000 [00:03<00:12, 626.93it/s]Tokenizing texts:  24%|██▎       | 2373/10000 [00:03<00:12, 610.00it/s]Tokenizing texts:  24%|██▍       | 2435/10000 [00:04<00:12, 612.46it/s]Tokenizing texts:  25%|██▍       | 2498/10000 [00:04<00:12, 615.16it/s]Tokenizing texts:  26%|██▌       | 2561/10000 [00:04<00:12, 605.08it/s]Tokenizing texts:  26%|██▌       | 2622/10000 [00:04<00:12, 572.07it/s]Tokenizing texts:  27%|██▋       | 2680/10000 [00:04<00:12, 571.22it/s]Tokenizing texts:  27%|██▋       | 2738/10000 [00:04<00:12, 570.15it/s]Tokenizing texts:  28%|██▊       | 2796/10000 [00:04<00:13, 528.31it/s]Tokenizing texts:  29%|██▊       | 2854/10000 [00:04<00:13, 537.32it/s]Tokenizing texts:  29%|██▉       | 2930/10000 [00:04<00:11, 599.10it/s]Tokenizing texts:  30%|██▉       | 2991/10000 [00:04<00:11, 592.77it/s]Tokenizing texts:  31%|███       | 3051/10000 [00:05<00:11, 589.37it/s]Tokenizing texts:  31%|███       | 3123/10000 [00:05<00:11, 625.10it/s]Tokenizing texts:  32%|███▏      | 3186/10000 [00:05<00:11, 614.62it/s]Tokenizing texts:  33%|███▎      | 3254/10000 [00:05<00:10, 633.18it/s]Tokenizing texts:  33%|███▎      | 3319/10000 [00:05<00:10, 632.58it/s]Tokenizing texts:  34%|███▍      | 3385/10000 [00:05<00:10, 638.67it/s]Tokenizing texts:  35%|███▍      | 3467/10000 [00:05<00:09, 691.62it/s]Tokenizing texts:  35%|███▌      | 3537/10000 [00:05<00:10, 642.34it/s]Tokenizing texts:  36%|███▌      | 3603/10000 [00:05<00:10, 634.48it/s]Tokenizing texts:  37%|███▋      | 3669/10000 [00:06<00:09, 635.98it/s]Tokenizing texts:  37%|███▋      | 3733/10000 [00:06<00:09, 627.06it/s]Tokenizing texts:  38%|███▊      | 3796/10000 [00:06<00:10, 570.34it/s]Tokenizing texts:  39%|███▊      | 3864/10000 [00:06<00:10, 597.71it/s]Tokenizing texts:  39%|███▉      | 3925/10000 [00:06<00:10, 590.70it/s]Tokenizing texts:  40%|███▉      | 3988/10000 [00:06<00:10, 597.44it/s]Tokenizing texts:  40%|████      | 4049/10000 [00:06<00:09, 598.49it/s]Tokenizing texts:  41%|████▏     | 4129/10000 [00:06<00:08, 656.15it/s]Tokenizing texts:  42%|████▏     | 4199/10000 [00:06<00:08, 665.56it/s]Tokenizing texts:  43%|████▎     | 4266/10000 [00:07<00:09, 615.97it/s]Tokenizing texts:  43%|████▎     | 4345/10000 [00:07<00:08, 663.69it/s]Tokenizing texts:  44%|████▍     | 4416/10000 [00:07<00:08, 676.41it/s]Tokenizing texts:  45%|████▍     | 4485/10000 [00:07<00:08, 649.45it/s]Tokenizing texts:  46%|████▌     | 4551/10000 [00:07<00:08, 642.17it/s]Tokenizing texts:  46%|████▌     | 4616/10000 [00:07<00:08, 609.58it/s]Tokenizing texts:  47%|████▋     | 4708/10000 [00:07<00:07, 694.01it/s]Tokenizing texts:  48%|████▊     | 4779/10000 [00:07<00:08, 649.11it/s]Tokenizing texts:  49%|████▊     | 4851/10000 [00:07<00:07, 668.12it/s]Tokenizing texts:  49%|████▉     | 4919/10000 [00:08<00:07, 667.82it/s]Tokenizing texts:  50%|████▉     | 4993/10000 [00:08<00:07, 682.99it/s]Tokenizing texts:  51%|█████     | 5062/10000 [00:08<00:07, 661.52it/s]Tokenizing texts:  51%|█████▏    | 5129/10000 [00:08<00:07, 636.25it/s]Tokenizing texts:  52%|█████▏    | 5194/10000 [00:08<00:07, 632.13it/s]Tokenizing texts:  53%|█████▎    | 5262/10000 [00:08<00:07, 641.24it/s]Tokenizing texts:  53%|█████▎    | 5336/10000 [00:08<00:07, 660.94it/s]Tokenizing texts:  54%|█████▍    | 5415/10000 [00:08<00:06, 697.16it/s]Tokenizing texts:  55%|█████▍    | 5485/10000 [00:08<00:06, 659.70it/s]Tokenizing texts:  56%|█████▌    | 5552/10000 [00:08<00:07, 613.14it/s]Tokenizing texts:  56%|█████▋    | 5628/10000 [00:09<00:06, 650.21it/s]Tokenizing texts:  57%|█████▋    | 5713/10000 [00:09<00:06, 703.80it/s]Tokenizing texts:  58%|█████▊    | 5785/10000 [00:09<00:06, 603.99it/s]Tokenizing texts:  58%|█████▊    | 5849/10000 [00:09<00:06, 605.23it/s]Tokenizing texts:  59%|█████▉    | 5912/10000 [00:09<00:06, 611.41it/s]Tokenizing texts:  60%|█████▉    | 5976/10000 [00:09<00:06, 618.62it/s]Tokenizing texts:  60%|██████    | 6040/10000 [00:09<00:06, 622.32it/s]Tokenizing texts:  61%|██████    | 6105/10000 [00:09<00:06, 630.00it/s]Tokenizing texts:  62%|██████▏   | 6178/10000 [00:09<00:05, 654.08it/s]Tokenizing texts:  62%|██████▏   | 6244/10000 [00:10<00:05, 636.00it/s]Tokenizing texts:  63%|██████▎   | 6309/10000 [00:10<00:05, 621.02it/s]Tokenizing texts:  64%|██████▎   | 6372/10000 [00:10<00:05, 612.54it/s]Tokenizing texts:  64%|██████▍   | 6436/10000 [00:10<00:05, 618.83it/s]Tokenizing texts:  65%|██████▍   | 6499/10000 [00:10<00:05, 615.65it/s]Tokenizing texts:  66%|██████▌   | 6564/10000 [00:10<00:05, 624.48it/s]Tokenizing texts:  66%|██████▋   | 6630/10000 [00:10<00:05, 629.63it/s]Tokenizing texts:  67%|██████▋   | 6694/10000 [00:10<00:05, 607.70it/s]Tokenizing texts:  68%|██████▊   | 6767/10000 [00:10<00:05, 641.57it/s]Tokenizing texts:  68%|██████▊   | 6849/10000 [00:11<00:04, 692.59it/s]Tokenizing texts:  69%|██████▉   | 6919/10000 [00:11<00:05, 607.74it/s]Tokenizing texts:  70%|██████▉   | 6985/10000 [00:11<00:04, 614.84it/s]Tokenizing texts:  70%|███████   | 7048/10000 [00:11<00:04, 600.13it/s]Tokenizing texts:  71%|███████   | 7110/10000 [00:11<00:05, 569.30it/s]Tokenizing texts:  72%|███████▏  | 7170/10000 [00:11<00:04, 577.13it/s]Tokenizing texts:  72%|███████▏  | 7238/10000 [00:11<00:04, 605.40it/s]Tokenizing texts:  73%|███████▎  | 7320/10000 [00:11<00:04, 665.20it/s]Tokenizing texts:  74%|███████▍  | 7388/10000 [00:11<00:04, 637.84it/s]Tokenizing texts:  75%|███████▍  | 7453/10000 [00:12<00:04, 571.01it/s]Tokenizing texts:  75%|███████▌  | 7530/10000 [00:12<00:03, 621.40it/s]Tokenizing texts:  76%|███████▌  | 7607/10000 [00:12<00:03, 661.38it/s]Tokenizing texts:  77%|███████▋  | 7675/10000 [00:12<00:03, 659.60it/s]Tokenizing texts:  77%|███████▋  | 7743/10000 [00:12<00:03, 638.98it/s]Tokenizing texts:  78%|███████▊  | 7808/10000 [00:12<00:03, 604.03it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:12<00:03, 620.03it/s]Tokenizing texts:  80%|███████▉  | 7951/10000 [00:12<00:03, 656.59it/s]Tokenizing texts:  80%|████████  | 8021/10000 [00:12<00:02, 664.93it/s]Tokenizing texts:  81%|████████  | 8089/10000 [00:13<00:03, 571.49it/s]Tokenizing texts:  82%|████████▏ | 8150/10000 [00:13<00:03, 580.26it/s]Tokenizing texts:  82%|████████▏ | 8216/10000 [00:13<00:02, 601.71it/s]Tokenizing texts:  83%|████████▎ | 8278/10000 [00:13<00:03, 567.75it/s]Tokenizing texts:  84%|████████▎ | 8350/10000 [00:13<00:02, 601.91it/s]Tokenizing texts:  84%|████████▍ | 8412/10000 [00:13<00:02, 581.25it/s]Tokenizing texts:  85%|████████▍ | 8484/10000 [00:13<00:02, 617.61it/s]Tokenizing texts:  86%|████████▌ | 8556/10000 [00:13<00:02, 644.92it/s]Tokenizing texts:  86%|████████▌ | 8622/10000 [00:13<00:02, 645.61it/s]Tokenizing texts:  87%|████████▋ | 8693/10000 [00:14<00:01, 663.98it/s]Tokenizing texts:  88%|████████▊ | 8767/10000 [00:14<00:01, 685.98it/s]Tokenizing texts:  88%|████████▊ | 8836/10000 [00:14<00:01, 604.80it/s]Tokenizing texts:  89%|████████▉ | 8914/10000 [00:14<00:01, 640.65it/s]Tokenizing texts:  90%|████████▉ | 8983/10000 [00:14<00:01, 650.30it/s]Tokenizing texts:  90%|█████████ | 9050/10000 [00:14<00:01, 622.77it/s]Tokenizing texts:  91%|█████████ | 9117/10000 [00:14<00:01, 634.83it/s]Tokenizing texts:  92%|█████████▏| 9191/10000 [00:14<00:01, 659.94it/s]Tokenizing texts:  93%|█████████▎| 9258/10000 [00:14<00:01, 651.98it/s]Tokenizing texts:  93%|█████████▎| 9324/10000 [00:15<00:01, 607.43it/s]Tokenizing texts:  94%|█████████▍| 9386/10000 [00:15<00:01, 593.89it/s]Tokenizing texts:  94%|█████████▍| 9446/10000 [00:15<00:00, 585.61it/s]Tokenizing texts:  95%|█████████▌| 9505/10000 [00:15<00:00, 541.40it/s]Tokenizing texts:  96%|█████████▌| 9580/10000 [00:15<00:00, 594.18it/s]Tokenizing texts:  96%|█████████▋| 9641/10000 [00:15<00:00, 580.27it/s]Tokenizing texts:  97%|█████████▋| 9700/10000 [00:15<00:00, 579.41it/s]Tokenizing texts:  98%|█████████▊| 9759/10000 [00:15<00:00, 538.15it/s]Tokenizing texts:  98%|█████████▊| 9835/10000 [00:15<00:00, 596.04it/s]Tokenizing texts:  99%|█████████▉| 9911/10000 [00:16<00:00, 633.33it/s]Tokenizing texts: 100%|█████████▉| 9992/10000 [00:16<00:00, 682.64it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:16<00:00, 619.18it/s]
2025-12-09 12:27:27.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 12.14067554473877
2025-12-09 12:27:27.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 12.140398025512695
2025-12-09 12:27:28.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 12.179524421691895
2025-12-09 12:27:28.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 12.186334609985352
2025-12-09 12:27:29.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 12.164158821105957
2025-12-09 12:27:29.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 12.16448974609375
2025-12-09 12:27:30.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 12.129446983337402
2025-12-09 12:27:30.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 12.108721733093262
2025-12-09 12:27:31.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 12.036223411560059
2025-12-09 12:27:31.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 11.99364185333252
2025-12-09 12:27:32.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 11.964790344238281
2025-12-09 12:27:32.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 11.938892364501953
2025-12-09 12:27:33.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 11.889070510864258
2025-12-09 12:27:33.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 11.96691608428955
2025-12-09 12:27:34.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 11.839859008789062
2025-12-09 12:27:34.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 11.725015640258789
2025-12-09 12:27:35.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 11.580595016479492
2025-12-09 12:27:35.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 11.650520324707031
2025-12-09 12:27:36.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 11.22351360321045
2025-12-09 12:27:36.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 11.375282287597656
2025-12-09 12:27:37.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 11.092700958251953
2025-12-09 12:27:37.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 10.894475936889648
2025-12-09 12:27:38.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 10.88779354095459
2025-12-09 12:27:38.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 10.611621856689453
2025-12-09 12:27:39.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 9.87359619140625
2025-12-09 12:27:39.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 9.784221649169922
2025-12-09 12:27:40.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 9.478909492492676
2025-12-09 12:27:40.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 9.556376457214355
2025-12-09 12:27:41.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 8.861369132995605
2025-12-09 12:27:41.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 8.728338241577148
2025-12-09 12:27:42.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 8.938600540161133
2025-12-09 12:27:42.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 9.0049409866333
2025-12-09 12:27:43.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 8.90845012664795
2025-12-09 12:27:43.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 8.475627899169922
2025-12-09 12:27:44.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 8.335697174072266
2025-12-09 12:27:44.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 8.26091194152832
2025-12-09 12:27:45.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 8.734188079833984
2025-12-09 12:27:45.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 8.239296913146973
2025-12-09 12:27:46.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 8.413012504577637
2025-12-09 12:27:46.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 8.197525024414062
2025-12-09 12:27:47.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 8.31199836730957
2025-12-09 12:27:47.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 8.672637939453125
2025-12-09 12:27:48.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 8.744454383850098
2025-12-09 12:27:48.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 8.268231391906738
2025-12-09 12:27:49.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 9.258037567138672
2025-12-09 12:27:49.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 8.422391891479492
2025-12-09 12:27:50.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 8.08371639251709
2025-12-09 12:27:50.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 8.391776084899902
2025-12-09 12:27:51.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 7.667593955993652
2025-12-09 12:27:51.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 7.876835346221924
2025-12-09 12:27:52.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 8.558792114257812
2025-12-09 12:27:52.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 7.716012954711914
2025-12-09 12:27:53.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 8.347959518432617
2025-12-09 12:27:53.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 8.283788681030273
2025-12-09 12:27:54.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 7.913489818572998
2025-12-09 12:27:54.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 7.991145133972168
2025-12-09 12:27:55.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 7.96306037902832
2025-12-09 12:27:55.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 8.168055534362793
2025-12-09 12:27:56.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 7.991270065307617
2025-12-09 12:27:56.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 8.036922454833984
2025-12-09 12:27:57.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 8.723335266113281
2025-12-09 12:27:57.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 8.037513732910156
2025-12-09 12:27:58.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 7.8837504386901855
2025-12-09 12:27:58.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 8.269265174865723
2025-12-09 12:27:59.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 7.938786029815674
2025-12-09 12:27:59.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 7.953577995300293
2025-12-09 12:28:00.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 8.003498077392578
2025-12-09 12:28:00.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 7.708179473876953
2025-12-09 12:28:01.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 8.026862144470215
2025-12-09 12:28:01.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 7.7747015953063965
2025-12-09 12:28:02.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 7.923528671264648
2025-12-09 12:28:02.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 7.9734392166137695
2025-12-09 12:28:03.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 7.937849521636963
2025-12-09 12:28:03.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 7.195821762084961
2025-12-09 12:28:04.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 8.39987564086914
2025-12-09 12:28:04.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 7.9482340812683105
2025-12-09 12:28:05.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 7.250980377197266
2025-12-09 12:28:05.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 7.631125450134277
2025-12-09 12:28:06.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 7.711906433105469
2025-12-09 12:28:06.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 7.716377258300781
2025-12-09 12:28:07.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 7.645511627197266
2025-12-09 12:28:07.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 7.552575588226318
2025-12-09 12:28:08.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 7.827210426330566
2025-12-09 12:28:08.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 7.556694507598877
2025-12-09 12:28:09.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 8.242199897766113
2025-12-09 12:28:09.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 7.036378383636475
2025-12-09 12:28:10.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 7.590716361999512
2025-12-09 12:28:10.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 7.666630268096924
2025-12-09 12:28:11.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 7.629794597625732
2025-12-09 12:28:11.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 8.830140113830566
2025-12-09 12:28:12.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 7.6169281005859375
2025-12-09 12:28:12.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 7.754877090454102
2025-12-09 12:28:13.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 7.624566078186035
2025-12-09 12:28:13.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 7.62432336807251
2025-12-09 12:28:14.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 7.7114458084106445
2025-12-09 12:28:14.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 7.64631462097168
2025-12-09 12:28:15.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 7.917754650115967
2025-12-09 12:28:15.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 7.559521675109863
2025-12-09 12:28:16.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 8.065061569213867
2025-12-09 12:28:16.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 7.517282485961914
2025-12-09 12:28:17.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009999999029798809 Training loss: 7.556232452392578
2025-12-09 12:28:17.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.000999999611919561 Training loss: 7.627020835876465
2025-12-09 12:28:18.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0009999991268191536 Training loss: 7.47043514251709
2025-12-09 12:28:18.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0009999984476788465 Training loss: 7.548088073730469
2025-12-09 12:28:19.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0009999975744989036 Training loss: 7.390030860900879
2025-12-09 12:28:19.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0009999965072796635 Training loss: 7.478005886077881
2025-12-09 12:28:20.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0009999952460215409 Training loss: 7.597801685333252
2025-12-09 12:28:20.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0009999937907250245 Training loss: 7.254940986633301
2025-12-09 12:28:21.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0009999921413906799 Training loss: 7.825357913970947
2025-12-09 12:28:21.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0009999902980191463 Training loss: 7.732493877410889
2025-12-09 12:28:22.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00099998826061114 Training loss: 7.617632865905762
2025-12-09 12:28:22.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0009999860291674508 Training loss: 7.125296592712402
2025-12-09 12:28:23.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0009999836036889453 Training loss: 7.4407734870910645
2025-12-09 12:28:23.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0009999809841765644 Training loss: 7.3246331214904785
2025-12-09 12:28:24.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.000999978170631325 Training loss: 7.489873886108398
2025-12-09 12:28:24.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0009999751630543187 Training loss: 7.715115070343018
2025-12-09 12:28:25.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.000999971961446713 Training loss: 7.6754045486450195
2025-12-09 12:28:25.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0009999685658097501 Training loss: 7.446727752685547
2025-12-09 12:28:26.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0009999649761447478 Training loss: 7.7982563972473145
2025-12-09 12:28:26.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0009999611924530994 Training loss: 7.32445764541626
2025-12-09 12:28:27.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.000999957214736273 Training loss: 7.668529510498047
2025-12-09 12:28:27.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0009999530429958125 Training loss: 7.672604560852051
2025-12-09 12:28:28.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0009999486772333365 Training loss: 7.06951379776001
2025-12-09 12:28:28.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00099994411745054 Training loss: 7.445232391357422
2025-12-09 12:28:29.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0009999393636491917 Training loss: 7.208754062652588
2025-12-09 12:28:29.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.000999934415831137 Training loss: 7.640268325805664
2025-12-09 12:28:30.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.000999929273998296 Training loss: 7.474569320678711
2025-12-09 12:28:30.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0009999239381526638 Training loss: 7.736284255981445
2025-12-09 12:28:31.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0009999184082963117 Training loss: 7.777828216552734
2025-12-09 12:28:31.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0009999126844313852 Training loss: 8.011225700378418
2025-12-09 12:28:32.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.000999906766560106 Training loss: 7.2317728996276855
2025-12-09 12:28:32.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0009999006546847706 Training loss: 7.477654933929443
2025-12-09 12:28:33.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0009998943488077508 Training loss: 7.822485446929932
2025-12-09 12:28:33.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0009998878489314938 Training loss: 7.409250736236572
2025-12-09 12:28:34.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.000999881155058522 Training loss: 7.449699401855469
2025-12-09 12:28:34.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0009998742671914335 Training loss: 7.671299934387207
2025-12-09 12:28:35.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.000999867185332901 Training loss: 7.381735324859619
2025-12-09 12:28:35.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.000999859909485673 Training loss: 7.457037448883057
2025-12-09 12:28:36.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.000999852439652573 Training loss: 7.881954669952393
2025-12-09 12:28:36.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0009998447758365 Training loss: 7.649499416351318
2025-12-09 12:28:37.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0009998369180404282 Training loss: 7.307946681976318
2025-12-09 12:28:37.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.000999828866267407 Training loss: 8.810543060302734
2025-12-09 12:28:38.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0009998206205205612 Training loss: 7.557161331176758
2025-12-09 12:28:38.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0009998121808030905 Training loss: 7.596920490264893
2025-12-09 12:28:39.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0009998035471182707 Training loss: 7.636497974395752
2025-12-09 12:28:39.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.000999794719469452 Training loss: 7.388187885284424
2025-12-09 12:28:40.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0009997856978600603 Training loss: 7.472849369049072
2025-12-09 12:28:40.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0009997764822935967 Training loss: 7.220998287200928
2025-12-09 12:28:41.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.000999767072773638 Training loss: 7.582981586456299
2025-12-09 12:28:41.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0009997574693038351 Training loss: 7.398190975189209
2025-12-09 12:28:42.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0009997476718879154 Training loss: 7.187769889831543
2025-12-09 12:28:42.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.000999737680529681 Training loss: 6.903786659240723
2025-12-09 12:28:43.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0009997274952330093 Training loss: 7.190800189971924
2025-12-09 12:28:43.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.000999717116001853 Training loss: 7.221450328826904
2025-12-09 12:28:44.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0009997065428402404 Training loss: 7.50406551361084
2025-12-09 12:28:44.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0009996957757522741 Training loss: 7.9144768714904785
2025-12-09 12:28:45.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0009996848147421334 Training loss: 7.160081386566162
2025-12-09 12:28:45.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0009996736598140714 Training loss: 7.258602619171143
2025-12-09 12:28:46.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0009996623109724174 Training loss: 7.341098785400391
2025-12-09 12:28:46.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0009996507682215755 Training loss: 7.218838214874268
2025-12-09 12:28:47.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0009996390315660253 Training loss: 8.070505142211914
2025-12-09 12:28:47.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0009996271010103215 Training loss: 7.538268566131592
2025-12-09 12:28:48.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0009996149765590945 Training loss: 6.951854705810547
2025-12-09 12:28:48.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.000999602658217049 Training loss: 7.139333724975586
2025-12-09 12:28:49.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0009995901459889658 Training loss: 7.173926830291748
2025-12-09 12:28:49.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0009995774398797008 Training loss: 7.308816432952881
2025-12-09 12:28:50.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0009995645398941846 Training loss: 7.088650226593018
2025-12-09 12:28:50.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0009995514460374238 Training loss: 6.956919193267822
2025-12-09 12:28:51.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0009995381583144996 Training loss: 7.161694526672363
2025-12-09 12:28:51.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0009995246767305688 Training loss: 7.443966388702393
2025-12-09 12:28:52.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0009995110012908633 Training loss: 7.373257637023926
2025-12-09 12:28:52.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0009994971320006906 Training loss: 7.4545979499816895
2025-12-09 12:28:53.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0009994830688654327 Training loss: 7.4229936599731445
2025-12-09 12:28:53.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.000999468811890547 Training loss: 7.741217136383057
2025-12-09 12:28:54.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.000999454361081567 Training loss: 7.132897853851318
2025-12-09 12:28:54.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.0009994397164441006 Training loss: 7.506896495819092
2025-12-09 12:28:55.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.000999424877983831 Training loss: 7.236813545227051
2025-12-09 12:28:55.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0009994098457065167 Training loss: 7.155708312988281
2025-12-09 12:28:56.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0009993946196179913 Training loss: 6.988373279571533
2025-12-09 12:28:56.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.000999379199724164 Training loss: 7.648619651794434
2025-12-09 12:28:57.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0009993635860310187 Training loss: 7.628645896911621
2025-12-09 12:28:57.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.000999347778544615 Training loss: 7.352725505828857
2025-12-09 12:28:58.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0009993317772710873 Training loss: 7.090492248535156
2025-12-09 12:28:58.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0009993155822166457 Training loss: 7.391252517700195
2025-12-09 12:28:59.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0009992991933875748 Training loss: 7.47422981262207
2025-12-09 12:28:59.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0009992826107902348 Training loss: 7.166608810424805
2025-12-09 12:29:00.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0009992658344310614 Training loss: 7.0017547607421875
2025-12-09 12:29:00.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.000999248864316565 Training loss: 7.116812229156494
2025-12-09 12:29:01.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0009992317004533314 Training loss: 7.016713619232178
2025-12-09 12:29:01.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0009992143428480215 Training loss: 7.351211071014404
2025-12-09 12:29:02.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0009991967915073715 Training loss: 6.971747398376465
2025-12-09 12:29:02.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0009991790464381925 Training loss: 7.1093339920043945
2025-12-09 12:29:03.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0009991611076473714 Training loss: 7.533173084259033
2025-12-09 12:29:03.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0009991429751418698 Training loss: 7.19352388381958
2025-12-09 12:29:04.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0009991246489287244 Training loss: 7.666057109832764
2025-12-09 12:29:04.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0009991061290150474 Training loss: 7.242539882659912
2025-12-09 12:29:05.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0009990874154080258 Training loss: 7.051942348480225
2025-12-09 12:29:05.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0009990685081149222 Training loss: 7.169742107391357
2025-12-09 12:29:06.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0009990494071430741 Training loss: 7.26534366607666
2025-12-09 12:29:06.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0009990301124998943 Training loss: 7.354729175567627
2025-12-09 12:29:07.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0009990106241928704 Training loss: 7.124908924102783
2025-12-09 12:29:07.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0009989909422295658 Training loss: 7.807430744171143
2025-12-09 12:29:08.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0009989710666176185 Training loss: 7.2137322425842285
2025-12-09 12:29:08.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0009989509973647418 Training loss: 7.184169292449951
2025-12-09 12:29:09.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0009989307344787242 Training loss: 7.09329080581665
2025-12-09 12:29:09.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0009989102779674292 Training loss: 7.2537713050842285
2025-12-09 12:29:10.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.000998889627838796 Training loss: 6.796212196350098
2025-12-09 12:29:10.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.000998868784100838 Training loss: 7.0967183113098145
2025-12-09 12:29:11.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0009988477467616447 Training loss: 7.452305316925049
2025-12-09 12:29:11.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0009988265158293798 Training loss: 6.860918045043945
2025-12-09 12:29:12.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.000998805091312283 Training loss: 7.225393772125244
2025-12-09 12:29:12.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0009987834732186687 Training loss: 6.8050150871276855
2025-12-09 12:29:13.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0009987616615569263 Training loss: 6.908858776092529
2025-12-09 12:29:13.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0009987396563355204 Training loss: 6.952359676361084
2025-12-09 12:29:14.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.000998717457562991 Training loss: 7.243325710296631
2025-12-09 12:29:14.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0009986950652479533 Training loss: 8.20921516418457
2025-12-09 12:29:15.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0009986724793990967 Training loss: 7.09166955947876
2025-12-09 12:29:15.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0009986497000251866 Training loss: 6.658665180206299
2025-12-09 12:29:16.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0009986267271350634 Training loss: 8.044320106506348
2025-12-09 12:29:16.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0009986035607376421 Training loss: 6.916461944580078
2025-12-09 12:29:17.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0009985802008419132 Training loss: 7.458913803100586
2025-12-09 12:29:17.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0009985566474569425 Training loss: 7.221164703369141
2025-12-09 12:29:18.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0009985329005918703 Training loss: 7.558581352233887
2025-12-09 12:29:18.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0009985089602559125 Training loss: 7.230287075042725
2025-12-09 12:29:19.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0009984848264583597 Training loss: 6.883142948150635
2025-12-09 12:29:19.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.000998460499208578 Training loss: 7.100121021270752
2025-12-09 12:29:20.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.000998435978516008 Training loss: 7.375136852264404
2025-12-09 12:29:20.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0009984112643901658 Training loss: 7.131154537200928
2025-12-09 12:29:21.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0009983863568406427 Training loss: 7.258862495422363
2025-12-09 12:29:21.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0009983612558771048 Training loss: 6.787600994110107
2025-12-09 12:29:22.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0009983359615092931 Training loss: 7.364080429077148
2025-12-09 12:29:22.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.000998310473747024 Training loss: 7.181532382965088
2025-12-09 12:29:23.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0009982847926001885 Training loss: 6.876790523529053
2025-12-09 12:29:23.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0009982589180787533 Training loss: 6.371721267700195
2025-12-09 12:29:24.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0009982328501927599 Training loss: 6.665850639343262
2025-12-09 12:29:24.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0009982065889523242 Training loss: 7.822927951812744
2025-12-09 12:29:25.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.000998180134367638 Training loss: 7.986496925354004
2025-12-09 12:29:25.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0009981534864489678 Training loss: 7.642220497131348
2025-12-09 12:29:26.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0009981266452066553 Training loss: 7.443521976470947
2025-12-09 12:29:26.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0009980996106511168 Training loss: 7.317160129547119
2025-12-09 12:29:27.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.000998072382792844 Training loss: 6.842181205749512
2025-12-09 12:29:27.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0009980449616424037 Training loss: 7.031946659088135
2025-12-09 12:29:28.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.000998017347210437 Training loss: 6.721724510192871
2025-12-09 12:29:28.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.000997989539507661 Training loss: 7.118850231170654
2025-12-09 12:29:29.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.000997961538544867 Training loss: 7.265468597412109
2025-12-09 12:29:29.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0009979333443329217 Training loss: 7.4004597663879395
2025-12-09 12:29:30.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.000997904956882767 Training loss: 6.900006294250488
2025-12-09 12:29:30.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0009978763762054192 Training loss: 6.888700485229492
2025-12-09 12:29:31.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00099784760231197 Training loss: 7.149425983428955
2025-12-09 12:29:31.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.000997818635213586 Training loss: 7.484669208526611
2025-12-09 12:29:32.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0009977894749215088 Training loss: 6.761784076690674
2025-12-09 12:29:32.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.000997760121447055 Training loss: 7.096036911010742
2025-12-09 12:29:33.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0009977305748016159 Training loss: 6.4451751708984375
2025-12-09 12:29:33.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.000997700834996658 Training loss: 7.209145545959473
2025-12-09 12:29:34.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.000997670902043723 Training loss: 6.725238800048828
2025-12-09 12:29:34.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.000997640775954427 Training loss: 7.003317356109619
2025-12-09 12:29:35.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.0009976104567404615 Training loss: 7.110528469085693
2025-12-09 12:29:35.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0009975799444135929 Training loss: 6.952028274536133
2025-12-09 12:29:36.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0009975492389856621 Training loss: 8.581509590148926
2025-12-09 12:29:36.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0009975183404685856 Training loss: 7.595184803009033
2025-12-09 12:29:37.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0009974872488743543 Training loss: 7.550781726837158
2025-12-09 12:29:37.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0009974559642150344 Training loss: 7.024559020996094
2025-12-09 12:29:38.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.000997424486502767 Training loss: 6.900722026824951
2025-12-09 12:29:38.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0009973928157497674 Training loss: 7.593240261077881
2025-12-09 12:29:39.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.000997360951968327 Training loss: 7.061790466308594
2025-12-09 12:29:39.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0009973288951708112 Training loss: 7.362603664398193
2025-12-09 12:29:40.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0009972966453696609 Training loss: 7.2053375244140625
2025-12-09 12:29:40.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0009972642025773912 Training loss: 7.190685749053955
2025-12-09 12:29:41.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0009972315668065929 Training loss: 7.135724067687988
2025-12-09 12:29:41.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.000997198738069931 Training loss: 7.344037055969238
2025-12-09 12:29:42.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.000997165716380146 Training loss: 7.036994457244873
2025-12-09 12:29:42.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0009971325017500525 Training loss: 7.046334266662598
2025-12-09 12:29:43.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0009970990941925411 Training loss: 8.143793106079102
2025-12-09 12:29:43.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0009970654937205762 Training loss: 6.636556148529053
2025-12-09 12:29:44.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0009970317003471976 Training loss: 6.847761631011963
2025-12-09 12:29:44.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0009969977140855198 Training loss: 7.113913536071777
2025-12-09 12:29:45.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009969635349487322 Training loss: 7.539679527282715
2025-12-09 12:29:45.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.000996929162950099 Training loss: 6.949917316436768
2025-12-09 12:29:46.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0009968945981029596 Training loss: 7.418377876281738
2025-12-09 12:29:46.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009968598404207275 Training loss: 6.70267915725708
2025-12-09 12:29:47.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0009968248899168918 Training loss: 6.950661659240723
2025-12-09 12:29:47.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.000996789746605016 Training loss: 7.199869155883789
2025-12-09 12:29:48.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009967544104987386 Training loss: 7.161870956420898
2025-12-09 12:29:48.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0009967188816117727 Training loss: 6.851691246032715
2025-12-09 12:29:49.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0009966831599579067 Training loss: 7.337496280670166
2025-12-09 12:29:49.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.000996647245551003 Training loss: 7.317773818969727
2025-12-09 12:29:50.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0009966111384049996 Training loss: 7.407641410827637
2025-12-09 12:29:50.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0009965748385339088 Training loss: 6.748014450073242
2025-12-09 12:29:51.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0009965383459518181 Training loss: 6.7864274978637695
2025-12-09 12:29:51.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0009965016606728893 Training loss: 7.135992527008057
2025-12-09 12:29:52.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0009964647827113596 Training loss: 6.996058940887451
2025-12-09 12:29:52.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0009964277120815403 Training loss: 7.195801734924316
2025-12-09 12:29:53.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0009963904487978177 Training loss: 7.046675682067871
2025-12-09 12:29:53.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0009963529928746534 Training loss: 6.599987983703613
2025-12-09 12:29:54.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0009963153443265829 Training loss: 6.898212909698486
2025-12-09 12:29:54.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0009962775031682168 Training loss: 6.933264255523682
2025-12-09 12:29:55.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0009962394694142409 Training loss: 6.707132339477539
2025-12-09 12:29:55.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0009962012430794153 Training loss: 6.569894790649414
2025-12-09 12:29:56.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0009961628241785747 Training loss: 7.11209774017334
2025-12-09 12:29:56.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0009961242127266288 Training loss: 6.946425437927246
2025-12-09 12:29:57.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0009960854087385617 Training loss: 6.95219087600708
2025-12-09 12:29:57.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.000996046412229433 Training loss: 6.510288238525391
2025-12-09 12:29:58.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0009960072232143762 Training loss: 6.664017200469971
2025-12-09 12:29:58.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0009959678417085997 Training loss: 6.757989883422852
2025-12-09 12:29:59.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0009959282677273868 Training loss: 6.657694339752197
2025-12-09 12:29:59.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0009958885012860954 Training loss: 6.906163692474365
2025-12-09 12:30:00.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0009958485424001581 Training loss: 6.528417110443115
2025-12-09 12:30:00.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0009958083910850822 Training loss: 6.8421430587768555
2025-12-09 12:30:01.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0009957680473564494 Training loss: 7.475564956665039
2025-12-09 12:30:01.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0009957275112299165 Training loss: 6.768740653991699
2025-12-09 12:30:02.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0009956867827212148 Training loss: 7.257426738739014
2025-12-09 12:30:02.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.00099564586184615 Training loss: 6.966517925262451
2025-12-09 12:30:03.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0009956047486206032 Training loss: 6.9245686531066895
2025-12-09 12:30:03.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0009955634430605291 Training loss: 6.722026348114014
2025-12-09 12:30:04.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.000995521945181958 Training loss: 7.196253776550293
2025-12-09 12:30:04.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0009954802550009943 Training loss: 7.021443843841553
2025-12-09 12:30:05.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.0009954383725338167 Training loss: 7.095102310180664
2025-12-09 12:30:05.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0009953962977966794 Training loss: 7.0319294929504395
2025-12-09 12:30:06.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.000995354030805911 Training loss: 7.358645915985107
2025-12-09 12:30:06.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0009953115715779141 Training loss: 6.851968288421631
2025-12-09 12:30:07.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0009952689201291663 Training loss: 7.061455249786377
2025-12-09 12:30:07.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00099522607647622 Training loss: 7.186051845550537
2025-12-09 12:30:08.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0009951830406357018 Training loss: 7.356024265289307
2025-12-09 12:30:08.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0009951398126243135 Training loss: 6.768503665924072
2025-12-09 12:30:09.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0009950963924588304 Training loss: 7.364488124847412
2025-12-09 12:30:09.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0009950527801561033 Training loss: 7.093801021575928
2025-12-09 12:30:10.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0009950089757330574 Training loss: 6.662754535675049
2025-12-09 12:30:10.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0009949649792066922 Training loss: 7.1676435470581055
2025-12-09 12:30:11.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.000994920790594082 Training loss: 6.5767340660095215
2025-12-09 12:30:11.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0009948764099123755 Training loss: 7.515737533569336
2025-12-09 12:30:12.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.000994831837178796 Training loss: 6.625072479248047
2025-12-09 12:30:12.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.000994787072410641 Training loss: 7.102452278137207
2025-12-09 12:30:13.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0009947421156252835 Training loss: 7.151952266693115
2025-12-09 12:30:13.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0009946969668401698 Training loss: 6.603006839752197
2025-12-09 12:30:14.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.0009946516260728212 Training loss: 6.99099588394165
2025-12-09 12:30:14.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.000994606093340834 Training loss: 6.746578216552734
2025-12-09 12:30:15.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0009945603686618784 Training loss: 6.7214813232421875
2025-12-09 12:30:15.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.000994514452053699 Training loss: 6.987531661987305
2025-12-09 12:30:16.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0009944683435341155 Training loss: 6.492800235748291
2025-12-09 12:30:16.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0009944220431210215 Training loss: 8.171964645385742
2025-12-09 12:30:17.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0009943755508323854 Training loss: 7.931329727172852
2025-12-09 12:30:17.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0009943288666862497 Training loss: 7.213238716125488
2025-12-09 12:30:18.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.000994281990700732 Training loss: 7.033683776855469
2025-12-09 12:30:18.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0009942349228940237 Training loss: 6.804889678955078
2025-12-09 12:30:19.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0009941876632843908 Training loss: 7.1171135902404785
2025-12-09 12:30:19.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0009941402118901744 Training loss: 6.868425369262695
2025-12-09 12:30:20.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0009940925687297885 Training loss: 6.998752593994141
2025-12-09 12:30:20.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0009940447338217234 Training loss: 6.921266078948975
2025-12-09 12:30:21.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0009939967071845423 Training loss: 7.28949499130249
2025-12-09 12:30:21.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0009939484888368837 Training loss: 7.020820617675781
2025-12-09 12:30:22.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0009939000787974601 Training loss: 6.460931777954102
2025-12-09 12:30:22.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.0009938514770850585 Training loss: 7.26609992980957
2025-12-09 12:30:23.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.0009938026837185403 Training loss: 7.109954833984375
2025-12-09 12:30:23.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0009937536987168413 Training loss: 6.8490400314331055
2025-12-09 12:30:24.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0009937045220989715 Training loss: 6.728225231170654
2025-12-09 12:30:24.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0009936551538840153 Training loss: 6.666924953460693
2025-12-09 12:30:25.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.000993605594091132 Training loss: 6.670321464538574
2025-12-09 12:30:25.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.000993555842739554 Training loss: 7.083446502685547
2025-12-09 12:30:26.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0009935058998485897 Training loss: 7.458151340484619
2025-12-09 12:30:26.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0009934557654376205 Training loss: 6.6581315994262695
2025-12-09 12:30:27.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0009934054395261025 Training loss: 7.463688373565674
2025-12-09 12:30:27.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0009933549221335664 Training loss: 7.2968645095825195
2025-12-09 12:30:28.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.000993304213279617 Training loss: 6.579867839813232
2025-12-09 12:30:28.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0009932533129839334 Training loss: 7.345582008361816
2025-12-09 12:30:29.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.000993202221266269 Training loss: 6.873445510864258
2025-12-09 12:30:29.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0009931509381464515 Training loss: 6.443754196166992
2025-12-09 12:30:30.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0009930994636443828 Training loss: 7.0998969078063965
2025-12-09 12:30:30.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0009930477977800392 Training loss: 6.905633449554443
2025-12-09 12:30:31.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.000992995940573471 Training loss: 7.1159467697143555
2025-12-09 12:30:31.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0009929438920448037 Training loss: 6.9282989501953125
2025-12-09 12:30:32.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0009928916522142356 Training loss: 6.792328834533691
2025-12-09 12:30:32.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00099283922110204 Training loss: 6.844868183135986
2025-12-09 12:30:33.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.0009927865987285648 Training loss: 6.812244892120361
2025-12-09 12:30:33.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0009927337851142314 Training loss: 6.951690673828125
2025-12-09 12:30:34.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.000992680780279536 Training loss: 7.115371227264404
2025-12-09 12:30:34.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0009926275842450482 Training loss: 7.466757297515869
2025-12-09 12:30:35.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0009925741970314129 Training loss: 6.703596591949463
2025-12-09 12:30:35.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.0009925206186593484 Training loss: 6.918215751647949
2025-12-09 12:30:36.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.0009924668491496473 Training loss: 7.294947147369385
2025-12-09 12:30:36.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.000992412888523177 Training loss: 6.72380256652832
2025-12-09 12:30:37.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.000992358736800878 Training loss: 6.79392147064209
2025-12-09 12:30:37.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0009923043940037657 Training loss: 7.181946277618408
2025-12-09 12:30:38.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0009922498601529295 Training loss: 6.146812438964844
2025-12-09 12:30:38.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.000992195135269533 Training loss: 6.9127326011657715
2025-12-09 12:30:39.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0009921402193748137 Training loss: 6.8237223625183105
2025-12-09 12:30:39.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0009920851124900836 Training loss: 7.090335845947266
2025-12-09 12:30:40.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.0009920298146367287 Training loss: 6.620946884155273
2025-12-09 12:30:40.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0009919743258362086 Training loss: 6.428158283233643
2025-12-09 12:30:41.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0009919186461100577 Training loss: 7.09984016418457
2025-12-09 12:30:41.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.000991862775479884 Training loss: 6.806264400482178
2025-12-09 12:30:42.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00099180671396737 Training loss: 6.943147659301758
2025-12-09 12:30:42.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0009917504615942721 Training loss: 8.241985321044922
2025-12-09 12:30:43.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0009916940183824206 Training loss: 6.674646377563477
2025-12-09 12:30:43.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0009916373843537201 Training loss: 7.899674892425537
2025-12-09 12:30:44.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0009915805595301491 Training loss: 6.464366912841797
2025-12-09 12:30:44.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.0009915235439337602 Training loss: 7.0322184562683105
2025-12-09 12:30:45.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0009914663375866803 Training loss: 6.550357818603516
2025-12-09 12:30:45.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0009914089405111098 Training loss: 6.336618900299072
2025-12-09 12:30:46.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0009913513527293235 Training loss: 6.9987640380859375
2025-12-09 12:30:46.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0009912935742636697 Training loss: 6.7622270584106445
2025-12-09 12:30:47.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0009912356051365717 Training loss: 6.852166652679443
2025-12-09 12:30:47.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0009911774453705258 Training loss: 7.029488563537598
2025-12-09 12:30:48.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.0009911190949881028 Training loss: 7.144678592681885
2025-12-09 12:30:48.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0009910605540119474 Training loss: 6.633153915405273
2025-12-09 12:30:49.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0009910018224647782 Training loss: 6.917763710021973
2025-12-09 12:30:49.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0009909429003693876 Training loss: 7.356454372406006
2025-12-09 12:30:50.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0009908837877486423 Training loss: 8.360695838928223
2025-12-09 12:30:50.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.0009908244846254825 Training loss: 6.387821674346924
2025-12-09 12:30:51.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.000990764991022923 Training loss: 6.939871311187744
2025-12-09 12:30:51.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0009907053069640515 Training loss: 7.0105414390563965
2025-12-09 12:30:52.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.000990645432472031 Training loss: 6.732839584350586
2025-12-09 12:30:52.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.000990585367570097 Training loss: 6.822341442108154
2025-12-09 12:30:53.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.0009905251122815596 Training loss: 7.011477470397949
2025-12-09 12:30:53.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.000990464666629803 Training loss: 6.8578200340271
2025-12-09 12:30:54.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0009904040306382847 Training loss: 6.6406450271606445
2025-12-09 12:30:54.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0009903432043305365 Training loss: 6.762495517730713
2025-12-09 12:30:55.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0009902821877301638 Training loss: 7.012618064880371
2025-12-09 12:30:55.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.000990220980860846 Training loss: 6.517641067504883
2025-12-09 12:30:56.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.0009901595837463362 Training loss: 6.710022449493408
2025-12-09 12:30:56.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0009900979964104616 Training loss: 6.661576271057129
2025-12-09 12:30:57.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.000990036218877123 Training loss: 6.884778022766113
2025-12-09 12:30:57.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.000989974251170295 Training loss: 6.483508110046387
2025-12-09 12:30:58.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.000989912093314026 Training loss: 7.29220724105835
2025-12-09 12:30:58.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0009898497453324385 Training loss: 6.7238311767578125
2025-12-09 12:30:59.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.000989787207249728 Training loss: 6.513658046722412
2025-12-09 12:30:59.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0009897244790901649 Training loss: 6.852881908416748
2025-12-09 12:31:00.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0009896615608780924 Training loss: 6.776527404785156
2025-12-09 12:31:00.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.000989598452637928 Training loss: 6.615519046783447
2025-12-09 12:31:01.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0009895351543941628 Training loss: 6.670053958892822
2025-12-09 12:31:01.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0009894716661713616 Training loss: 6.5788726806640625
2025-12-09 12:31:02.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0009894079879941627 Training loss: 6.878908634185791
2025-12-09 12:31:02.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0009893441198872788 Training loss: 6.865251064300537
2025-12-09 12:31:03.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.0009892800618754953 Training loss: 6.506026268005371
2025-12-09 12:31:03.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0009892158139836725 Training loss: 6.454689025878906
2025-12-09 12:31:04.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0009891513762367431 Training loss: 6.411715507507324
2025-12-09 12:31:04.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.0009890867486597146 Training loss: 6.383606910705566
2025-12-09 12:31:05.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0009890219312776677 Training loss: 6.438450813293457
2025-12-09 12:31:05.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0009889569241157564 Training loss: 6.779844760894775
2025-12-09 12:31:06.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.000988891727199209 Training loss: 7.086236953735352
2025-12-09 12:31:06.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.000988826340553327 Training loss: 6.719024181365967
2025-12-09 12:31:07.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0009887607642034859 Training loss: 6.450837135314941
2025-12-09 12:31:07.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0009886949981751346 Training loss: 6.8310465812683105
2025-12-09 12:31:08.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0009886290424937951 Training loss: 6.580506324768066
2025-12-09 12:31:08.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0009885628971850642 Training loss: 6.900557994842529
2025-12-09 12:31:09.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0009884965622746112 Training loss: 6.869917869567871
2025-12-09 12:31:09.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0009884300377881795 Training loss: 6.910440444946289
2025-12-09 12:31:10.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0009883633237515858 Training loss: 6.859005928039551
2025-12-09 12:31:10.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0009882964201907208 Training loss: 6.657725811004639
2025-12-09 12:31:11.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0009882293271315482 Training loss: 7.0522871017456055
2025-12-09 12:31:11.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0009881620446001056 Training loss: 6.787111759185791
2025-12-09 12:31:12.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.000988094572622504 Training loss: 7.129720687866211
2025-12-09 12:31:12.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0009880269112249281 Training loss: 6.94036340713501
2025-12-09 12:31:13.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.000987959060433636 Training loss: 6.622816562652588
2025-12-09 12:31:13.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.0009878910202749589 Training loss: 6.617386817932129
2025-12-09 12:31:14.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0009878227907753022 Training loss: 6.657501220703125
2025-12-09 12:31:14.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0009877543719611444 Training loss: 7.433607578277588
2025-12-09 12:31:15.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0009876857638590373 Training loss: 6.426724433898926
2025-12-09 12:31:15.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.0009876169664956068 Training loss: 6.529592990875244
2025-12-09 12:31:16.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0009875479798975512 Training loss: 6.785369873046875
2025-12-09 12:31:16.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0009874788040916433 Training loss: 6.69195032119751
2025-12-09 12:31:17.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0009874094391047288 Training loss: 7.250894546508789
2025-12-09 12:31:17.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.0009873398849637267 Training loss: 6.7745041847229
2025-12-09 12:31:18.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00098727014169563 Training loss: 6.897568702697754
2025-12-09 12:31:18.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.0009872002093275043 Training loss: 7.555509090423584
2025-12-09 12:31:19.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.000987130087886489 Training loss: 6.662852764129639
2025-12-09 12:31:19.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.000987059777399797 Training loss: 7.235528945922852
2025-12-09 12:31:20.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.0009869892778947148 Training loss: 6.8148512840271
2025-12-09 12:31:20.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0009869185893986011 Training loss: 6.518265247344971
2025-12-09 12:31:21.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0009868477119388895 Training loss: 6.527503490447998
2025-12-09 12:31:21.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.0009867766455430858 Training loss: 6.899430274963379
2025-12-09 12:31:22.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.0009867053902387693 Training loss: 6.828178882598877
2025-12-09 12:31:22.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0009866339460535929 Training loss: 6.5082197189331055
2025-12-09 12:31:23.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0009865623130152828 Training loss: 6.818679332733154
2025-12-09 12:31:23.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.0009864904911516383 Training loss: 6.541617393493652
2025-12-09 12:31:24.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0009864184804905323 Training loss: 6.619039535522461
2025-12-09 12:31:24.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0009863462810599105 Training loss: 6.725561141967773
2025-12-09 12:31:25.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.000986273892887792 Training loss: 6.634086608886719
2025-12-09 12:31:25.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0009862013160022696 Training loss: 6.959362983703613
2025-12-09 12:31:26.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0009861285504315085 Training loss: 6.886609077453613
2025-12-09 12:31:26.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0009860555962037478 Training loss: 6.3783111572265625
2025-12-09 12:31:27.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0009859824533472999 Training loss: 6.1480793952941895
2025-12-09 12:31:27.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0009859091218905498 Training loss: 6.619452953338623
2025-12-09 12:31:28.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.000985835601861956 Training loss: 6.318126201629639
2025-12-09 12:31:28.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.0009857618932900504 Training loss: 6.3621416091918945
2025-12-09 12:31:29.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0009856879962034375 Training loss: 7.175297737121582
2025-12-09 12:31:29.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0009856139106307956 Training loss: 7.646679878234863
2025-12-09 12:31:30.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0009855396366008756 Training loss: 6.427591323852539
2025-12-09 12:31:30.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0009854651741425023 Training loss: 6.685802459716797
2025-12-09 12:31:31.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0009853905232845728 Training loss: 6.242642402648926
2025-12-09 12:31:31.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0009853156840560575 Training loss: 6.7842254638671875
2025-12-09 12:31:32.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0009852406564860004 Training loss: 6.877872467041016
2025-12-09 12:31:32.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.000985165440603518 Training loss: 6.446649074554443
2025-12-09 12:31:33.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0009850900364378 Training loss: 7.012146472930908
2025-12-09 12:31:33.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0009850144440181096 Training loss: 5.903919219970703
2025-12-09 12:31:34.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0009849386633737824 Training loss: 6.5071306228637695
2025-12-09 12:31:34.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.0009848626945342278 Training loss: 7.304714202880859
2025-12-09 12:31:35.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0009847865375289275 Training loss: 6.520315647125244
2025-12-09 12:31:35.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0009847101923874367 Training loss: 6.617374897003174
2025-12-09 12:31:36.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.0009846336591393832 Training loss: 6.562270641326904
2025-12-09 12:31:36.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0009845569378144686 Training loss: 6.700011730194092
2025-12-09 12:31:37.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.0009844800284424663 Training loss: 6.702442169189453
2025-12-09 12:31:37.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0009844029310532238 Training loss: 6.247476577758789
2025-12-09 12:31:38.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0009843256456766609 Training loss: 6.610847473144531
2025-12-09 12:31:38.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0009842481723427705 Training loss: 6.698853015899658
2025-12-09 12:31:39.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.0009841705110816186 Training loss: 7.043126583099365
2025-12-09 12:31:39.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.000984092661923344 Training loss: 6.7924580574035645
2025-12-09 12:31:40.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0009840146248981585 Training loss: 6.713125228881836
2025-12-09 12:31:40.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.0009839364000363466 Training loss: 6.348961353302002
2025-12-09 12:31:41.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.000983857987368266 Training loss: 7.202809810638428
2025-12-09 12:31:41.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0009837793869243467 Training loss: 6.733853340148926
2025-12-09 12:31:42.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.0009837005987350927 Training loss: 6.411972522735596
2025-12-09 12:31:42.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0009836216228310797 Training loss: 6.760650634765625
2025-12-09 12:31:43.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0009835424592429566 Training loss: 6.8694963455200195
2025-12-09 12:31:43.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0009834631080014456 Training loss: 6.763239860534668
2025-12-09 12:31:44.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0009833835691373412 Training loss: 7.1541008949279785
2025-12-09 12:31:44.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.000983303842681511 Training loss: 7.059584140777588
2025-12-09 12:31:45.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.000983223928664895 Training loss: 6.552546501159668
2025-12-09 12:31:45.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0009831438271185064 Training loss: 6.684370517730713
2025-12-09 12:31:46.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.0009830635380734312 Training loss: 6.393507957458496
2025-12-09 12:31:46.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.000982983061560828 Training loss: 6.772611618041992
2025-12-09 12:31:47.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0009829023976119279 Training loss: 6.681931972503662
2025-12-09 12:31:47.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0009828215462580352 Training loss: 6.901120662689209
2025-12-09 12:31:48.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0009827405075305267 Training loss: 6.742822647094727
2025-12-09 12:31:48.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.000982659281460852 Training loss: 7.6254658699035645
2025-12-09 12:31:49.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0009825778680805331 Training loss: 6.778660297393799
2025-12-09 12:31:49.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.0009824962674211653 Training loss: 6.682653427124023
2025-12-09 12:31:50.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0009824144795144158 Training loss: 6.646317005157471
2025-12-09 12:31:50.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0009823325043920256 Training loss: 6.78264045715332
2025-12-09 12:31:51.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0009822503420858068 Training loss: 6.891898155212402
2025-12-09 12:31:51.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0009821679926276456 Training loss: 6.595344066619873
2025-12-09 12:31:52.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0009820854560494998 Training loss: 6.249652862548828
2025-12-09 12:31:52.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0009820027323834007 Training loss: 6.834436893463135
2025-12-09 12:31:53.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0009819198216614513 Training loss: 6.81496524810791
2025-12-09 12:31:53.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0009818367239158277 Training loss: 6.67080020904541
2025-12-09 12:31:54.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0009817534391787788 Training loss: 7.0691609382629395
2025-12-09 12:31:54.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0009816699674826256 Training loss: 7.2230448722839355
2025-12-09 12:31:55.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.0009815863088597618 Training loss: 6.396691799163818
2025-12-09 12:31:55.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.0009815024633426537 Training loss: 6.780519962310791
2025-12-09 12:31:56.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.00098141843096384 Training loss: 6.601596832275391
2025-12-09 12:31:56.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.0009813342117559324 Training loss: 6.258370399475098
2025-12-09 12:31:57.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.0009812498057516143 Training loss: 6.391929626464844
2025-12-09 12:31:57.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0009811652129836422 Training loss: 6.561426639556885
2025-12-09 12:31:58.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0009810804334848449 Training loss: 6.521949768066406
2025-12-09 12:31:58.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.0009809954672881237 Training loss: 6.461749076843262
2025-12-09 12:31:59.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0009809103144264523 Training loss: 6.490589618682861
2025-12-09 12:31:59.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0009808249749328768 Training loss: 6.90691614151001
2025-12-09 12:32:00.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.000980739448840516 Training loss: 7.002237319946289
2025-12-09 12:32:00.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.0009806537361825606 Training loss: 6.601584434509277
2025-12-09 12:32:01.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0009805678369922742 Training loss: 6.291700839996338
2025-12-09 12:32:01.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.0009804817513029928 Training loss: 6.802402019500732
2025-12-09 12:32:02.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.000980395479148124 Training loss: 6.512640953063965
2025-12-09 12:32:02.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0009803090205611487 Training loss: 7.329019546508789
2025-12-09 12:32:03.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.0009802223755756199 Training loss: 6.422628879547119
2025-12-09 12:32:03.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0009801355442251626 Training loss: 6.484764575958252
2025-12-09 12:32:04.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0009800485265434745 Training loss: 6.396625995635986
2025-12-09 12:32:04.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0009799613225643252 Training loss: 6.379465103149414
2025-12-09 12:32:05.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.0009798739323215572 Training loss: 6.998785018920898
2025-12-09 12:32:05.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.0009797863558490849 Training loss: 6.919552803039551
2025-12-09 12:32:06.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.000979698593180895 Training loss: 6.305868148803711
2025-12-09 12:32:06.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0009796106443510462 Training loss: 6.626132965087891
2025-12-09 12:32:07.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.00097952250939367 Training loss: 6.829841613769531
2025-12-09 12:32:07.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.0009794341883429699 Training loss: 6.513866901397705
2025-12-09 12:32:08.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0009793456812332215 Training loss: 5.972682952880859
2025-12-09 12:32:08.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0009792569880987725 Training loss: 6.951874732971191
2025-12-09 12:32:09.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0009791681089740432 Training loss: 6.672443866729736
2025-12-09 12:32:09.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.0009790790438935256 Training loss: 6.920781135559082
2025-12-09 12:32:10.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.0009789897928917846 Training loss: 6.922534942626953
2025-12-09 12:32:10.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.000978900356003456 Training loss: 6.48922061920166
2025-12-09 12:32:11.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0009788107332632495 Training loss: 6.795124053955078
2025-12-09 12:32:11.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0009787209247059453 Training loss: 7.414031505584717
2025-12-09 12:32:12.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.0009786309303663962 Training loss: 6.80191707611084
2025-12-09 12:32:12.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0009785407502795277 Training loss: 6.270362377166748
2025-12-09 12:32:13.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0009784503844803367 Training loss: 6.536116600036621
2025-12-09 12:32:13.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0009783598330038925 Training loss: 6.2461700439453125
2025-12-09 12:32:14.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0009782690958853363 Training loss: 6.4849534034729
2025-12-09 12:32:14.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0009781781731598813 Training loss: 5.841225624084473
2025-12-09 12:32:15.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.000978087064862813 Training loss: 6.802253246307373
2025-12-09 12:32:15.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0009779957710294885 Training loss: 6.436590671539307
2025-12-09 12:32:16.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0009779042916953375 Training loss: 6.9401373863220215
2025-12-09 12:32:16.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0009778126268958612 Training loss: 7.1441545486450195
2025-12-09 12:32:17.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.000977720776666633 Training loss: 6.667670249938965
2025-12-09 12:32:17.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.000977628741043298 Training loss: 6.671813011169434
2025-12-09 12:32:18.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0009775365200615734 Training loss: 7.083937168121338
2025-12-09 12:32:18.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0009774441137572487 Training loss: 6.609910488128662
2025-12-09 12:32:19.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0009773515221661846 Training loss: 6.270910263061523
2025-12-09 12:32:19.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0009772587453243141 Training loss: 6.765732288360596
2025-12-09 12:32:20.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.0009771657832676427 Training loss: 6.320289611816406
2025-12-09 12:32:20.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0009770726360322465 Training loss: 6.6426849365234375
2025-12-09 12:32:21.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.000976979303654274 Training loss: 6.279932022094727
2025-12-09 12:32:21.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0009768857861699462 Training loss: 6.3799309730529785
2025-12-09 12:32:22.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0009767920836155552 Training loss: 6.487252235412598
2025-12-09 12:32:22.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0009766981960274653 Training loss: 6.641326427459717
2025-12-09 12:32:23.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.000976604123442112 Training loss: 6.644103527069092
2025-12-09 12:32:23.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.0009765098658960035 Training loss: 6.668962478637695
2025-12-09 12:32:24.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0009764154234257191 Training loss: 6.6957621574401855
2025-12-09 12:32:24.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.00097632079606791 Training loss: 6.748109817504883
2025-12-09 12:32:25.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0009762259838592994 Training loss: 6.707984924316406
2025-12-09 12:32:25.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0009761309868366819 Training loss: 6.5715789794921875
2025-12-09 12:32:26.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0009760358050369243 Training loss: 6.949098110198975
2025-12-09 12:32:26.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0009759404384969643 Training loss: 6.562690734863281
2025-12-09 12:32:27.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0009758448872538121 Training loss: 6.450626850128174
2025-12-09 12:32:27.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0009757491513445493 Training loss: 6.783605098724365
2025-12-09 12:32:28.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0009756532308063293 Training loss: 6.56552267074585
2025-12-09 12:32:28.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0009755571256763765 Training loss: 6.247155666351318
2025-12-09 12:32:29.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0009754608359919879 Training loss: 6.4066619873046875
2025-12-09 12:32:29.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0009753643617905312 Training loss: 6.568959712982178
2025-12-09 12:32:30.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0009752677031094466 Training loss: 7.543308258056641
2025-12-09 12:32:30.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0009751708599862451 Training loss: 6.656515598297119
2025-12-09 12:32:31.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0009750738324585098 Training loss: 6.708942890167236
2025-12-09 12:32:31.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0009749766205638952 Training loss: 6.679547309875488
2025-12-09 12:32:32.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.0009748792243401273 Training loss: 6.670316696166992
2025-12-09 12:32:32.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0009747816438250037 Training loss: 6.604778289794922
2025-12-09 12:32:33.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0009746838790563935 Training loss: 6.434509754180908
2025-12-09 12:32:33.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.000974585930072237 Training loss: 6.598677635192871
2025-12-09 12:32:34.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0009744877969105468 Training loss: 7.2063751220703125
2025-12-09 12:32:34.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0009743894796094062 Training loss: 6.384339809417725
2025-12-09 12:32:35.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.0009742909782069701 Training loss: 6.421414375305176
2025-12-09 12:32:35.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0009741922927414651 Training loss: 6.912100315093994
2025-12-09 12:32:36.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0009740934232511893 Training loss: 6.47437047958374
2025-12-09 12:32:36.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0009739943697745117 Training loss: 6.406041622161865
2025-12-09 12:32:37.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0009738951323498732 Training loss: 6.827827453613281
2025-12-09 12:32:37.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0009737957110157858 Training loss: 6.679788589477539
2025-12-09 12:32:38.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0009736961058108331 Training loss: 6.343111515045166
2025-12-09 12:32:38.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0009735963167736698 Training loss: 6.290972709655762
2025-12-09 12:32:39.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.0009734963439430222 Training loss: 6.360826015472412
2025-12-09 12:32:39.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0009733961873576877 Training loss: 6.589438438415527
2025-12-09 12:32:40.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0009732958470565352 Training loss: 6.272322177886963
2025-12-09 12:32:40.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0009731953230785049 Training loss: 6.1308441162109375
2025-12-09 12:32:41.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0009730946154626079 Training loss: 7.114288806915283
2025-12-09 12:32:41.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.000972993724247927 Training loss: 6.529733657836914
2025-12-09 12:32:42.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0009728926494736163 Training loss: 6.6697282791137695
2025-12-09 12:32:42.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0009727913911789008 Training loss: 6.8188300132751465
2025-12-09 12:32:43.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0009726899494030768 Training loss: 6.813645839691162
2025-12-09 12:32:43.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0009725883241855118 Training loss: 6.390222549438477
2025-12-09 12:32:44.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0009724865155656448 Training loss: 6.571231842041016
2025-12-09 12:32:44.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0009723845235829856 Training loss: 5.862629413604736
2025-12-09 12:32:45.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0009722823482771155 Training loss: 6.917950630187988
2025-12-09 12:32:45.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0009721799896876864 Training loss: 6.549983501434326
2025-12-09 12:32:46.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0009720774478544219 Training loss: 6.39562463760376
2025-12-09 12:32:46.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0009719747228171163 Training loss: 6.268426895141602
2025-12-09 12:32:47.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0009718718146156355 Training loss: 5.957973003387451
2025-12-09 12:32:47.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0009717687232899158 Training loss: 7.3899245262146
2025-12-09 12:32:48.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0009716654488799652 Training loss: 6.790140151977539
2025-12-09 12:32:48.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0009715619914258623 Training loss: 6.582339286804199
2025-12-09 12:32:49.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.000971458350967757 Training loss: 7.538423538208008
2025-12-09 12:32:49.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0009713545275458703 Training loss: 6.64178466796875
2025-12-09 12:32:50.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0009712505212004937 Training loss: 6.320254802703857
2025-12-09 12:32:50.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0009711463319719904 Training loss: 6.982608795166016
2025-12-09 12:32:51.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0009710419599007938 Training loss: 6.774720191955566
2025-12-09 12:32:51.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0009709374050274089 Training loss: 6.806718349456787
2025-12-09 12:32:52.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0009708326673924114 Training loss: 6.104226112365723
2025-12-09 12:32:52.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0009707277470364482 Training loss: 6.7243781089782715
2025-12-09 12:32:53.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0009706226440002363 Training loss: 6.724992752075195
2025-12-09 12:32:53.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0009705173583245644 Training loss: 6.659509658813477
2025-12-09 12:32:54.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0009704118900502918 Training loss: 6.923118591308594
2025-12-09 12:32:54.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0009703062392183488 Training loss: 6.468753337860107
2025-12-09 12:32:55.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0009702004058697362 Training loss: 6.794678688049316
2025-12-09 12:32:55.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0009700943900455262 Training loss: 6.585771083831787
2025-12-09 12:32:56.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0009699881917868609 Training loss: 6.67202091217041
2025-12-09 12:32:56.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.0009698818111349544 Training loss: 6.584964752197266
2025-12-09 12:32:57.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0009697752481310904 Training loss: 7.179949760437012
2025-12-09 12:32:57.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0009696685028166244 Training loss: 6.524428367614746
2025-12-09 12:32:58.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.000969561575232982 Training loss: 6.56055212020874
2025-12-09 12:32:58.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0009694544654216595 Training loss: 6.7458014488220215
2025-12-09 12:32:59.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0009693471734242243 Training loss: 6.450080394744873
2025-12-09 12:32:59.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0009692396992823144 Training loss: 7.1581573486328125
2025-12-09 12:33:00.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0009691320430376385 Training loss: 6.170236587524414
2025-12-09 12:33:00.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0009690242047319755 Training loss: 6.537804126739502
2025-12-09 12:33:01.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.0009689161844071756 Training loss: 6.5557966232299805
2025-12-09 12:33:01.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0009688079821051594 Training loss: 5.948268890380859
2025-12-09 12:33:02.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.0009686995978679181 Training loss: 6.152361869812012
2025-12-09 12:33:02.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0009685910317375133 Training loss: 6.3395094871521
2025-12-09 12:33:03.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0009684822837560776 Training loss: 6.56405782699585
2025-12-09 12:33:03.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0009683733539658139 Training loss: 6.665807723999023
2025-12-09 12:33:04.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0009682642424089958 Training loss: 6.481404781341553
2025-12-09 12:33:04.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.0009681549491279673 Training loss: 6.463074207305908
2025-12-09 12:33:05.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.000968045474165143 Training loss: 6.672821044921875
2025-12-09 12:33:05.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0009679358175630081 Training loss: 6.6477484703063965
2025-12-09 12:33:06.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.000967825979364118 Training loss: 6.511245250701904
2025-12-09 12:33:06.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.0009677159596110987 Training loss: 5.8166823387146
2025-12-09 12:33:07.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.000967605758346647 Training loss: 6.261077880859375
2025-12-09 12:33:07.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0009674953756135297 Training loss: 6.3534417152404785
2025-12-09 12:33:08.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0009673848114545843 Training loss: 6.174889087677002
2025-12-09 12:33:08.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0009672740659127184 Training loss: 6.224475383758545
2025-12-09 12:33:09.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0009671631390309102 Training loss: 6.202189922332764
2025-12-09 12:33:09.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0009670520308522084 Training loss: 6.292776107788086
2025-12-09 12:33:10.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0009669407414197318 Training loss: 5.976868152618408
2025-12-09 12:33:10.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0009668292707766699 Training loss: 6.399212837219238
2025-12-09 12:33:11.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0009667176189662818 Training loss: 6.287137508392334
2025-12-09 12:33:11.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.0009666057860318978 Training loss: 6.465997695922852
2025-12-09 12:33:12.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.000966493772016918 Training loss: 6.1680521965026855
2025-12-09 12:33:12.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0009663815769648127 Training loss: 6.62347412109375
2025-12-09 12:33:13.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.000966269200919123 Training loss: 6.694565296173096
2025-12-09 12:33:13.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0009661566439234593 Training loss: 6.616764068603516
2025-12-09 12:33:14.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.000966043906021503 Training loss: 7.155979633331299
2025-12-09 12:33:14.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.0009659309872570057 Training loss: 6.775797367095947
2025-12-09 12:33:15.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0009658178876737886 Training loss: 6.279080867767334
2025-12-09 12:33:15.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0009657046073157437 Training loss: 6.221019268035889
2025-12-09 12:33:16.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.0009655911462268327 Training loss: 6.7108330726623535
2025-12-09 12:33:16.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.000965477504451088 Training loss: 6.705137729644775
2025-12-09 12:33:17.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.0009653636820326113 Training loss: 6.622175693511963
2025-12-09 12:33:17.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0009652496790155752 Training loss: 6.136782646179199
2025-12-09 12:33:18.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0009651354954442218 Training loss: 6.403554916381836
2025-12-09 12:33:18.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.0009650211313628637 Training loss: 6.774044513702393
2025-12-09 12:33:19.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0009649065868158832 Training loss: 5.8226518630981445
2025-12-09 12:33:19.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.0009647918618477329 Training loss: 6.73031759262085
2025-12-09 12:33:20.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0009646769565029354 Training loss: 6.359675407409668
2025-12-09 12:33:20.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.0009645618708260831 Training loss: 6.9368743896484375
2025-12-09 12:33:21.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0009644466048618386 Training loss: 6.607172966003418
2025-12-09 12:33:21.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0009643311586549342 Training loss: 7.820219993591309
2025-12-09 12:33:22.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0009642155322501725 Training loss: 6.275521755218506
2025-12-09 12:33:22.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0009640997256924257 Training loss: 6.9309210777282715
2025-12-09 12:33:23.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0009639837390266362 Training loss: 6.2943243980407715
2025-12-09 12:33:23.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0009638675722978161 Training loss: 6.67971134185791
2025-12-09 12:33:24.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0009637512255510475 Training loss: 6.384684085845947
2025-12-09 12:33:24.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.000963634698831482 Training loss: 6.483372211456299
2025-12-09 12:33:25.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.0009635179921843417 Training loss: 6.542228698730469
2025-12-09 12:33:25.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.0009634011056549182 Training loss: 6.231137752532959
2025-12-09 12:33:26.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.0009632840392885726 Training loss: 6.443962097167969
2025-12-09 12:33:26.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0009631667931307364 Training loss: 6.1614603996276855
2025-12-09 12:33:27.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.0009630493672269101 Training loss: 6.3907856941223145
2025-12-09 12:33:27.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.0009629317616226649 Training loss: 6.649807453155518
2025-12-09 12:33:28.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.0009628139763636407 Training loss: 6.5288848876953125
2025-12-09 12:33:28.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.0009626960114955483 Training loss: 6.4839277267456055
2025-12-09 12:33:29.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0009625778670641669 Training loss: 6.528022766113281
2025-12-09 12:33:29.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.0009624595431153467 Training loss: 6.700442790985107
2025-12-09 12:33:30.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.0009623410396950063 Training loss: 6.333070755004883
2025-12-09 12:33:30.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.000962222356849135 Training loss: 6.369980812072754
2025-12-09 12:33:31.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.000962103494623791 Training loss: 6.414927005767822
2025-12-09 12:33:31.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.0009619844530651026 Training loss: 6.136300563812256
2025-12-09 12:33:32.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.0009618652322192675 Training loss: 6.222356796264648
2025-12-09 12:33:32.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.0009617458321325529 Training loss: 6.462061405181885
2025-12-09 12:33:33.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.0009616262528512957 Training loss: 6.186143398284912
2025-12-09 12:33:33.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.0009615064944219022 Training loss: 6.421570777893066
2025-12-09 12:33:34.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.0009613865568908484 Training loss: 6.653454780578613
2025-12-09 12:33:34.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.0009612664403046797 Training loss: 6.4148268699646
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 2.86 GiB is free. Including non-PyTorch memory, this process has 91.28 GiB memory in use. Of the allocated memory 90.25 GiB is allocated by PyTorch, and 278.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 50/10000 [00:00<00:20, 493.17it/s]Tokenizing texts:   1%|▏         | 134/10000 [00:00<00:14, 693.53it/s]Tokenizing texts:   2%|▏         | 204/10000 [00:00<00:15, 627.07it/s]Tokenizing texts:   3%|▎         | 268/10000 [00:00<00:15, 630.14it/s]Tokenizing texts:   3%|▎         | 332/10000 [00:00<00:15, 608.44it/s]Tokenizing texts:   4%|▍         | 394/10000 [00:00<00:15, 602.42it/s]Tokenizing texts:   5%|▍         | 455/10000 [00:00<00:16, 576.44it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 575.38it/s]Tokenizing texts:   6%|▌         | 597/10000 [00:00<00:15, 599.36it/s]Tokenizing texts:   7%|▋         | 658/10000 [00:01<00:15, 591.86it/s]Tokenizing texts:   7%|▋         | 718/10000 [00:01<00:16, 576.45it/s]Tokenizing texts:   8%|▊         | 776/10000 [00:01<00:16, 575.26it/s]Tokenizing texts:   8%|▊         | 834/10000 [00:01<00:16, 569.26it/s]Tokenizing texts:   9%|▉         | 898/10000 [00:01<00:15, 587.39it/s]Tokenizing texts:  10%|▉         | 978/10000 [00:01<00:13, 646.74it/s]Tokenizing texts:  11%|█         | 1051/10000 [00:01<00:14, 633.82it/s]Tokenizing texts:  11%|█         | 1115/10000 [00:01<00:14, 630.38it/s]Tokenizing texts:  12%|█▏        | 1179/10000 [00:01<00:14, 595.52it/s]Tokenizing texts:  12%|█▏        | 1239/10000 [00:02<00:14, 594.15it/s]Tokenizing texts:  13%|█▎        | 1305/10000 [00:02<00:14, 606.28it/s]Tokenizing texts:  14%|█▎        | 1366/10000 [00:02<00:14, 594.59it/s]Tokenizing texts:  14%|█▍        | 1426/10000 [00:02<00:14, 594.82it/s]Tokenizing texts:  15%|█▌        | 1504/10000 [00:02<00:13, 647.56it/s]Tokenizing texts:  16%|█▌        | 1580/10000 [00:02<00:12, 680.08it/s]Tokenizing texts:  16%|█▋        | 1649/10000 [00:02<00:13, 613.79it/s]Tokenizing texts:  17%|█▋        | 1712/10000 [00:02<00:13, 604.27it/s]Tokenizing texts:  18%|█▊        | 1785/10000 [00:02<00:12, 632.21it/s]Tokenizing texts:  18%|█▊        | 1850/10000 [00:03<00:13, 603.83it/s]Tokenizing texts:  19%|█▉        | 1913/10000 [00:03<00:13, 610.67it/s]Tokenizing texts:  20%|█▉        | 1975/10000 [00:03<00:14, 566.59it/s]Tokenizing texts:  20%|██        | 2048/10000 [00:03<00:13, 610.52it/s]Tokenizing texts:  21%|██        | 2111/10000 [00:03<00:14, 553.14it/s]Tokenizing texts:  22%|██▏       | 2168/10000 [00:03<00:14, 553.80it/s]Tokenizing texts:  22%|██▏       | 2245/10000 [00:03<00:12, 611.30it/s]Tokenizing texts:  23%|██▎       | 2311/10000 [00:03<00:12, 601.46it/s]Tokenizing texts:  24%|██▍       | 2379/10000 [00:03<00:12, 623.24it/s]Tokenizing texts:  24%|██▍       | 2443/10000 [00:04<00:12, 625.42it/s]Tokenizing texts:  25%|██▌       | 2507/10000 [00:04<00:12, 619.48it/s]Tokenizing texts:  26%|██▌       | 2570/10000 [00:04<00:12, 604.65it/s]Tokenizing texts:  26%|██▋       | 2631/10000 [00:04<00:12, 570.80it/s]Tokenizing texts:  27%|██▋       | 2691/10000 [00:04<00:12, 577.65it/s]Tokenizing texts:  28%|██▊       | 2755/10000 [00:04<00:12, 594.04it/s]Tokenizing texts:  28%|██▊       | 2815/10000 [00:04<00:13, 538.40it/s]Tokenizing texts:  29%|██▊       | 2871/10000 [00:04<00:13, 542.85it/s]Tokenizing texts:  29%|██▉       | 2946/10000 [00:04<00:12, 587.74it/s]Tokenizing texts:  30%|███       | 3014/10000 [00:05<00:11, 613.12it/s]Tokenizing texts:  31%|███       | 3076/10000 [00:05<00:11, 601.69it/s]Tokenizing texts:  31%|███▏      | 3139/10000 [00:05<00:11, 608.78it/s]Tokenizing texts:  32%|███▏      | 3203/10000 [00:05<00:11, 614.67it/s]Tokenizing texts:  33%|███▎      | 3273/10000 [00:05<00:10, 637.03it/s]Tokenizing texts:  33%|███▎      | 3337/10000 [00:05<00:10, 631.88it/s]Tokenizing texts:  34%|███▍      | 3403/10000 [00:05<00:10, 638.00it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:05<00:09, 689.47it/s]Tokenizing texts:  36%|███▌      | 3556/10000 [00:05<00:10, 633.30it/s]Tokenizing texts:  36%|███▌      | 3621/10000 [00:05<00:10, 636.23it/s]Tokenizing texts:  37%|███▋      | 3686/10000 [00:06<00:09, 632.56it/s]Tokenizing texts:  38%|███▊      | 3754/10000 [00:06<00:09, 645.29it/s]Tokenizing texts:  38%|███▊      | 3819/10000 [00:06<00:10, 579.87it/s]Tokenizing texts:  39%|███▉      | 3887/10000 [00:06<00:10, 602.92it/s]Tokenizing texts:  39%|███▉      | 3949/10000 [00:06<00:10, 583.90it/s]Tokenizing texts:  40%|████      | 4009/10000 [00:06<00:10, 581.79it/s]Tokenizing texts:  41%|████      | 4085/10000 [00:06<00:09, 631.39it/s]Tokenizing texts:  42%|████▏     | 4154/10000 [00:06<00:09, 644.56it/s]Tokenizing texts:  42%|████▏     | 4224/10000 [00:06<00:08, 651.79it/s]Tokenizing texts:  43%|████▎     | 4290/10000 [00:07<00:09, 632.95it/s]Tokenizing texts:  44%|████▎     | 4366/10000 [00:07<00:08, 666.25it/s]Tokenizing texts:  44%|████▍     | 4437/10000 [00:07<00:08, 675.93it/s]Tokenizing texts:  45%|████▌     | 4505/10000 [00:07<00:08, 636.13it/s]Tokenizing texts:  46%|████▌     | 4575/10000 [00:07<00:08, 652.99it/s]Tokenizing texts:  46%|████▋     | 4641/10000 [00:07<00:08, 644.90it/s]Tokenizing texts:  47%|████▋     | 4728/10000 [00:07<00:07, 708.16it/s]Tokenizing texts:  48%|████▊     | 4800/10000 [00:07<00:08, 642.04it/s]Tokenizing texts:  49%|████▉     | 4875/10000 [00:07<00:07, 664.48it/s]Tokenizing texts:  49%|████▉     | 4946/10000 [00:08<00:07, 672.99it/s]Tokenizing texts:  50%|█████     | 5024/10000 [00:08<00:07, 703.17it/s]Tokenizing texts:  51%|█████     | 5096/10000 [00:08<00:07, 647.28it/s]Tokenizing texts:  52%|█████▏    | 5163/10000 [00:08<00:07, 639.76it/s]Tokenizing texts:  52%|█████▏    | 5228/10000 [00:08<00:07, 627.08it/s]Tokenizing texts:  53%|█████▎    | 5293/10000 [00:08<00:07, 632.77it/s]Tokenizing texts:  54%|█████▍    | 5379/10000 [00:08<00:06, 695.88it/s]Tokenizing texts:  55%|█████▍    | 5450/10000 [00:08<00:06, 684.63it/s]Tokenizing texts:  55%|█████▌    | 5519/10000 [00:08<00:07, 616.35it/s]Tokenizing texts:  56%|█████▌    | 5589/10000 [00:09<00:06, 637.92it/s]Tokenizing texts:  57%|█████▋    | 5673/10000 [00:09<00:06, 688.33it/s]Tokenizing texts:  57%|█████▋    | 5744/10000 [00:09<00:06, 676.88it/s]Tokenizing texts:  58%|█████▊    | 5813/10000 [00:09<00:06, 601.12it/s]Tokenizing texts:  59%|█████▉    | 5880/10000 [00:09<00:06, 619.09it/s]Tokenizing texts:  59%|█████▉    | 5944/10000 [00:09<00:06, 607.82it/s]Tokenizing texts:  60%|██████    | 6010/10000 [00:09<00:06, 597.70it/s]Tokenizing texts:  61%|██████    | 6087/10000 [00:09<00:06, 644.08it/s]Tokenizing texts:  62%|██████▏   | 6157/10000 [00:09<00:05, 654.45it/s]Tokenizing texts:  62%|██████▏   | 6224/10000 [00:10<00:05, 639.03it/s]Tokenizing texts:  63%|██████▎   | 6289/10000 [00:10<00:05, 639.14it/s]Tokenizing texts:  64%|██████▎   | 6354/10000 [00:10<00:06, 602.13it/s]Tokenizing texts:  64%|██████▍   | 6423/10000 [00:10<00:05, 619.50it/s]Tokenizing texts:  65%|██████▍   | 6486/10000 [00:10<00:05, 617.47it/s]Tokenizing texts:  65%|██████▌   | 6549/10000 [00:10<00:05, 616.77it/s]Tokenizing texts:  66%|██████▌   | 6622/10000 [00:10<00:05, 646.49it/s]Tokenizing texts:  67%|██████▋   | 6687/10000 [00:10<00:05, 641.41it/s]Tokenizing texts:  68%|██████▊   | 6752/10000 [00:10<00:05, 624.62it/s]Tokenizing texts:  68%|██████▊   | 6835/10000 [00:10<00:04, 682.43it/s]Tokenizing texts:  69%|██████▉   | 6904/10000 [00:11<00:05, 614.61it/s]Tokenizing texts:  70%|██████▉   | 6967/10000 [00:11<00:04, 612.88it/s]Tokenizing texts:  70%|███████   | 7030/10000 [00:11<00:04, 617.32it/s]Tokenizing texts:  71%|███████   | 7093/10000 [00:11<00:05, 570.22it/s]Tokenizing texts:  72%|███████▏  | 7156/10000 [00:11<00:04, 583.16it/s]Tokenizing texts:  72%|███████▏  | 7222/10000 [00:11<00:04, 603.46it/s]Tokenizing texts:  73%|███████▎  | 7299/10000 [00:11<00:04, 650.53it/s]Tokenizing texts:  74%|███████▎  | 7365/10000 [00:11<00:04, 639.32it/s]Tokenizing texts:  74%|███████▍  | 7430/10000 [00:11<00:04, 612.23it/s]Tokenizing texts:  75%|███████▍  | 7492/10000 [00:12<00:04, 598.45it/s]Tokenizing texts:  76%|███████▌  | 7561/10000 [00:12<00:03, 618.22it/s]Tokenizing texts:  76%|███████▋  | 7636/10000 [00:12<00:03, 653.23it/s]Tokenizing texts:  77%|███████▋  | 7710/10000 [00:12<00:03, 656.96it/s]Tokenizing texts:  78%|███████▊  | 7776/10000 [00:12<00:03, 634.07it/s]Tokenizing texts:  78%|███████▊  | 7840/10000 [00:12<00:03, 621.98it/s]Tokenizing texts:  79%|███████▉  | 7903/10000 [00:12<00:03, 620.42it/s]Tokenizing texts:  80%|███████▉  | 7980/10000 [00:12<00:03, 662.65it/s]Tokenizing texts:  80%|████████  | 8047/10000 [00:12<00:03, 628.87it/s]Tokenizing texts:  81%|████████  | 8111/10000 [00:13<00:03, 568.67it/s]Tokenizing texts:  82%|████████▏ | 8178/10000 [00:13<00:03, 595.43it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:13<00:03, 566.08it/s]Tokenizing texts:  83%|████████▎ | 8311/10000 [00:13<00:02, 606.68it/s]Tokenizing texts:  84%|████████▎ | 8374/10000 [00:13<00:02, 610.82it/s]Tokenizing texts:  84%|████████▍ | 8436/10000 [00:13<00:02, 595.97it/s]Tokenizing texts:  85%|████████▌ | 8511/10000 [00:13<00:02, 633.33it/s]Tokenizing texts:  86%|████████▌ | 8575/10000 [00:13<00:02, 629.34it/s]Tokenizing texts:  87%|████████▋ | 8651/10000 [00:13<00:02, 665.82it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:14<00:01, 671.60it/s]Tokenizing texts:  88%|████████▊ | 8793/10000 [00:14<00:01, 667.10it/s]Tokenizing texts:  89%|████████▊ | 8860/10000 [00:14<00:01, 599.21it/s]Tokenizing texts:  89%|████████▉ | 8943/10000 [00:14<00:01, 660.84it/s]Tokenizing texts:  90%|█████████ | 9011/10000 [00:14<00:01, 621.38it/s]Tokenizing texts:  91%|█████████ | 9080/10000 [00:14<00:01, 627.75it/s]Tokenizing texts:  92%|█████████▏| 9151/10000 [00:14<00:01, 648.36it/s]Tokenizing texts:  92%|█████████▏| 9225/10000 [00:14<00:01, 655.27it/s]Tokenizing texts:  93%|█████████▎| 9292/10000 [00:14<00:01, 639.30it/s]Tokenizing texts:  94%|█████████▎| 9357/10000 [00:15<00:01, 619.49it/s]Tokenizing texts:  94%|█████████▍| 9420/10000 [00:15<00:00, 596.32it/s]Tokenizing texts:  95%|█████████▍| 9481/10000 [00:15<00:00, 546.89it/s]Tokenizing texts:  96%|█████████▌| 9555/10000 [00:15<00:00, 596.04it/s]Tokenizing texts:  96%|█████████▌| 9616/10000 [00:15<00:00, 577.11it/s]Tokenizing texts:  97%|█████████▋| 9676/10000 [00:15<00:00, 581.90it/s]Tokenizing texts:  97%|█████████▋| 9735/10000 [00:15<00:00, 571.20it/s]Tokenizing texts:  98%|█████████▊| 9793/10000 [00:15<00:00, 570.87it/s]Tokenizing texts:  99%|█████████▊| 9865/10000 [00:15<00:00, 613.31it/s]Tokenizing texts:  99%|█████████▉| 9938/10000 [00:16<00:00, 644.85it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:16<00:00, 620.74it/s]
2025-12-09 12:34:23.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 12.156133651733398
2025-12-09 12:34:24.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 12.227204322814941
2025-12-09 12:34:24.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 12.133179664611816
2025-12-09 12:34:25.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 12.122483253479004
2025-12-09 12:34:25.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 12.11731243133545
2025-12-09 12:34:26.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 12.008040428161621
2025-12-09 12:34:26.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 12.050204277038574
2025-12-09 12:34:27.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 12.009781837463379
2025-12-09 12:34:27.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 11.89918041229248
2025-12-09 12:34:28.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 11.73050308227539
2025-12-09 12:34:28.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 11.668466567993164
2025-12-09 12:34:29.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 11.518242835998535
2025-12-09 12:34:29.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 11.369324684143066
2025-12-09 12:34:30.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 10.718974113464355
2025-12-09 12:34:30.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 10.417250633239746
2025-12-09 12:34:31.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 10.021615028381348
2025-12-09 12:34:31.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 9.61592960357666
2025-12-09 12:34:32.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 9.317426681518555
2025-12-09 12:34:32.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 8.808305740356445
2025-12-09 12:34:33.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 8.84554672241211
2025-12-09 12:34:33.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 8.455891609191895
2025-12-09 12:34:34.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 8.669471740722656
2025-12-09 12:34:34.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 8.71632194519043
2025-12-09 12:34:35.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 8.83128833770752
2025-12-09 12:34:35.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 8.436091423034668
2025-12-09 12:34:36.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 8.191953659057617
2025-12-09 12:34:36.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 8.55809497833252
2025-12-09 12:34:37.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 8.344534873962402
2025-12-09 12:34:37.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 8.43994426727295
2025-12-09 12:34:38.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 9.091907501220703
2025-12-09 12:34:38.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 8.133210182189941
2025-12-09 12:34:39.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 8.253171920776367
2025-12-09 12:34:39.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 8.268258094787598
2025-12-09 12:34:40.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 8.02462387084961
2025-12-09 12:34:40.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 7.971012115478516
2025-12-09 12:34:41.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 8.363921165466309
2025-12-09 12:34:41.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 8.211222648620605
2025-12-09 12:34:42.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 8.07450008392334
2025-12-09 12:34:42.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 8.625041007995605
2025-12-09 12:34:43.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 8.130413055419922
2025-12-09 12:34:43.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 8.197694778442383
2025-12-09 12:34:44.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 8.182013511657715
2025-12-09 12:34:44.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 8.662130355834961
2025-12-09 12:34:45.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 8.115169525146484
2025-12-09 12:34:45.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 8.122876167297363
2025-12-09 12:34:46.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 8.006805419921875
2025-12-09 12:34:46.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 7.996810436248779
2025-12-09 12:34:47.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 8.020381927490234
2025-12-09 12:34:47.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 7.998865604400635
2025-12-09 12:34:48.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 7.803746700286865
2025-12-09 12:34:48.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 7.758841037750244
2025-12-09 12:34:49.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 7.977068901062012
2025-12-09 12:34:49.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 8.326897621154785
2025-12-09 12:34:50.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 7.920353412628174
2025-12-09 12:34:50.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 8.112835884094238
2025-12-09 12:34:51.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 8.540976524353027
2025-12-09 12:34:51.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 7.919071674346924
2025-12-09 12:34:52.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 7.512101173400879
2025-12-09 12:34:52.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 7.844440460205078
2025-12-09 12:34:53.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 7.761053085327148
2025-12-09 12:34:53.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 8.642348289489746
2025-12-09 12:34:54.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 8.501410484313965
2025-12-09 12:34:54.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 8.361193656921387
2025-12-09 12:34:55.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 7.857713222503662
2025-12-09 12:34:55.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 8.279860496520996
2025-12-09 12:34:56.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 7.557778358459473
2025-12-09 12:34:56.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 8.043546676635742
2025-12-09 12:34:57.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 7.857275485992432
2025-12-09 12:34:57.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 7.732184886932373
2025-12-09 12:34:58.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 8.431806564331055
2025-12-09 12:34:58.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 7.701671123504639
2025-12-09 12:34:59.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 7.869660377502441
2025-12-09 12:34:59.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 8.073434829711914
2025-12-09 12:35:00.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 8.200453758239746
2025-12-09 12:35:00.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 7.993516445159912
2025-12-09 12:35:01.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 8.318893432617188
2025-12-09 12:35:01.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 8.110804557800293
2025-12-09 12:35:02.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 8.016843795776367
2025-12-09 12:35:02.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 7.985482692718506
2025-12-09 12:35:03.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 8.05436897277832
2025-12-09 12:35:03.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 7.671065807342529
2025-12-09 12:35:04.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 8.386488914489746
2025-12-09 12:35:04.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 7.747321128845215
2025-12-09 12:35:05.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 7.8886494636535645
2025-12-09 12:35:05.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 8.36972427368164
2025-12-09 12:35:06.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 7.9163923263549805
2025-12-09 12:35:06.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 7.992992877960205
2025-12-09 12:35:07.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 7.8532328605651855
2025-12-09 12:35:07.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 8.045341491699219
2025-12-09 12:35:08.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 7.704198837280273
2025-12-09 12:35:08.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 7.600955963134766
2025-12-09 12:35:09.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 8.670251846313477
2025-12-09 12:35:09.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 8.082326889038086
2025-12-09 12:35:10.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 7.9853739738464355
2025-12-09 12:35:10.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 7.847150802612305
2025-12-09 12:35:11.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 7.828757286071777
2025-12-09 12:35:11.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 8.023197174072266
2025-12-09 12:35:12.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 7.326354026794434
2025-12-09 12:35:12.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 7.949204444885254
2025-12-09 12:35:13.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 7.518075466156006
2025-12-09 12:35:13.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999997089396424 Training loss: 7.563284873962402
2025-12-09 12:35:14.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002999998835758683 Training loss: 7.9802632331848145
2025-12-09 12:35:14.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029999973804574606 Training loss: 8.041038513183594
2025-12-09 12:35:15.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0029999953430365394 Training loss: 7.432064056396484
2025-12-09 12:35:15.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.002999992723496711 Training loss: 7.673598766326904
2025-12-09 12:35:16.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.002999989521838991 Training loss: 7.846024990081787
2025-12-09 12:35:16.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029999857380646226 Training loss: 7.948885440826416
2025-12-09 12:35:17.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.002999981372175074 Training loss: 8.158452033996582
2025-12-09 12:35:17.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0029999764241720396 Training loss: 7.42216157913208
2025-12-09 12:35:18.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0029999708940574394 Training loss: 7.573969841003418
2025-12-09 12:35:18.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0029999647818334195 Training loss: 7.47520112991333
2025-12-09 12:35:19.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.002999958087502352 Training loss: 7.448039531707764
2025-12-09 12:35:19.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029999508110668356 Training loss: 7.380612373352051
2025-12-09 12:35:20.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029999429525296934 Training loss: 7.768858432769775
2025-12-09 12:35:20.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0029999345118939752 Training loss: 7.663100242614746
2025-12-09 12:35:21.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0029999254891629563 Training loss: 8.071245193481445
2025-12-09 12:35:21.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.002999915884340139 Training loss: 7.968743801116943
2025-12-09 12:35:22.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0029999056974292504 Training loss: 8.346000671386719
2025-12-09 12:35:22.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029998949284342435 Training loss: 7.839629173278809
2025-12-09 12:35:23.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.002999883577359298 Training loss: 8.003459930419922
2025-12-09 12:35:23.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.002999871644208819 Training loss: 7.675125598907471
2025-12-09 12:35:24.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002999859128987437 Training loss: 7.52783727645874
2025-12-09 12:35:24.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00299984603170001 Training loss: 7.4579267501831055
2025-12-09 12:35:25.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0029998323523516197 Training loss: 7.409154415130615
2025-12-09 12:35:25.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029998180909475754 Training loss: 7.323126316070557
2025-12-09 12:35:26.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.002999803247493411 Training loss: 7.956907749176025
2025-12-09 12:35:26.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.002999787821994888 Training loss: 7.826263904571533
2025-12-09 12:35:27.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0029997718144579915 Training loss: 7.4815473556518555
2025-12-09 12:35:27.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0029997552248889354 Training loss: 7.747379302978516
2025-12-09 12:35:28.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.002999738053294156 Training loss: 8.228270530700684
2025-12-09 12:35:28.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0029997202996803183 Training loss: 7.436271667480469
2025-12-09 12:35:29.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002999701964054312 Training loss: 7.457459449768066
2025-12-09 12:35:29.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0029996830464232523 Training loss: 8.674250602722168
2025-12-09 12:35:30.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0029996635467944813 Training loss: 7.701612949371338
2025-12-09 12:35:30.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0029996434651755662 Training loss: 7.844590663909912
2025-12-09 12:35:31.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0029996228015743004 Training loss: 7.291225433349609
2025-12-09 12:35:31.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.002999601555998703 Training loss: 7.258176326751709
2025-12-09 12:35:32.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.002999579728457019 Training loss: 7.732066631317139
2025-12-09 12:35:32.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.002999557318957719 Training loss: 7.813348770141602
2025-12-09 12:35:33.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0029995343275095003 Training loss: 7.281991004943848
2025-12-09 12:35:33.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0029995107541212845 Training loss: 7.106618881225586
2025-12-09 12:35:34.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.002999486598802221 Training loss: 7.4172282218933105
2025-12-09 12:35:34.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0029994618615616832 Training loss: 7.6154584884643555
2025-12-09 12:35:35.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.002999436542409272 Training loss: 7.580666542053223
2025-12-09 12:35:35.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0029994106413548122 Training loss: 7.960174083709717
2025-12-09 12:35:36.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0029993841584083558 Training loss: 7.428039073944092
2025-12-09 12:35:36.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.002999357093580181 Training loss: 7.2521820068359375
2025-12-09 12:35:37.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0029993294468807904 Training loss: 7.210601329803467
2025-12-09 12:35:37.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0029993012183209137 Training loss: 7.335565090179443
2025-12-09 12:35:38.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0029992724079115052 Training loss: 7.670980453491211
2025-12-09 12:35:38.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.002999243015663746 Training loss: 7.460160732269287
2025-12-09 12:35:39.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0029992130415890427 Training loss: 6.940673351287842
2025-12-09 12:35:39.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.002999182485699028 Training loss: 8.019207000732422
2025-12-09 12:35:40.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0029991513480055595 Training loss: 7.497653484344482
2025-12-09 12:35:40.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002999119628520721 Training loss: 6.969456672668457
2025-12-09 12:35:41.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0029990873272568224 Training loss: 7.237069606781006
2025-12-09 12:35:41.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0029990544442264 Training loss: 7.301672458648682
2025-12-09 12:35:42.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.002999020979442214 Training loss: 7.628939628601074
2025-12-09 12:35:42.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.002998986932917252 Training loss: 7.463438987731934
2025-12-09 12:35:43.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.002998952304664726 Training loss: 7.6451334953308105
2025-12-09 12:35:43.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.002998917094698076 Training loss: 7.865772247314453
2025-12-09 12:35:44.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.002998881303030965 Training loss: 7.857181072235107
2025-12-09 12:35:44.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0029988449296772836 Training loss: 7.232583045959473
2025-12-09 12:35:45.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.002998807974651147 Training loss: 7.468986511230469
2025-12-09 12:35:45.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0029987704379668976 Training loss: 7.8957648277282715
2025-12-09 12:35:46.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002998732319639102 Training loss: 7.342176914215088
2025-12-09 12:35:46.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0029986936196825537 Training loss: 7.711145401000977
2025-12-09 12:35:47.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.002998654338112271 Training loss: 7.612067222595215
2025-12-09 12:35:47.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0029986144749434987 Training loss: 7.091088771820068
2025-12-09 12:35:48.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0029985740301917063 Training loss: 7.435986042022705
2025-12-09 12:35:48.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00299853300387259 Training loss: 7.521569728851318
2025-12-09 12:35:49.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0029984913960020712 Training loss: 8.230520248413086
2025-12-09 12:35:49.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0029984492065962976 Training loss: 7.605867385864258
2025-12-09 12:35:50.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0029984064356716415 Training loss: 7.751553535461426
2025-12-09 12:35:50.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0029983630832447015 Training loss: 7.733783721923828
2025-12-09 12:35:51.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002998319149332302 Training loss: 7.258194923400879
2025-12-09 12:35:51.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0029982746339514933 Training loss: 7.711668968200684
2025-12-09 12:35:52.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0029982295371195496 Training loss: 7.547277450561523
2025-12-09 12:35:52.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.002998183858853974 Training loss: 7.47218132019043
2025-12-09 12:35:53.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0029981375991724917 Training loss: 7.5282182693481445
2025-12-09 12:35:53.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0029980907580930563 Training loss: 7.407065391540527
2025-12-09 12:35:54.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.002998043335633845 Training loss: 7.253798961639404
2025-12-09 12:35:54.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002997995331813262 Training loss: 7.652093410491943
2025-12-09 12:35:55.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.002997946746649937 Training loss: 7.0485076904296875
2025-12-09 12:35:55.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0029978975801627245 Training loss: 7.444953441619873
2025-12-09 12:35:56.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0029978478323707046 Training loss: 7.5709309577941895
2025-12-09 12:35:56.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0029977975032931844 Training loss: 7.394115447998047
2025-12-09 12:35:57.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.002997746592949695 Training loss: 7.6234259605407715
2025-12-09 12:35:57.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.002997695101359994 Training loss: 7.978821277618408
2025-12-09 12:35:58.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.002997643028544064 Training loss: 7.769648551940918
2025-12-09 12:35:58.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0029975903745221143 Training loss: 7.611551761627197
2025-12-09 12:35:59.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.002997537139314578 Training loss: 6.955480575561523
2025-12-09 12:35:59.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0029974833229421145 Training loss: 7.9598259925842285
2025-12-09 12:36:00.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002997428925425609 Training loss: 7.281016826629639
2025-12-09 12:36:00.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0029973739467861736 Training loss: 8.397321701049805
2025-12-09 12:36:01.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.002997318387045142 Training loss: 7.77268648147583
2025-12-09 12:36:01.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0029972622462240777 Training loss: 7.8747029304504395
2025-12-09 12:36:02.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0029972055243447666 Training loss: 8.013182640075684
2025-12-09 12:36:02.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.002997148221429223 Training loss: 7.085651874542236
2025-12-09 12:36:03.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.002997090337499683 Training loss: 7.461136341094971
2025-12-09 12:36:03.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0029970318725786116 Training loss: 7.576071739196777
2025-12-09 12:36:04.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0029969728266886976 Training loss: 8.232529640197754
2025-12-09 12:36:04.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0029969131998528555 Training loss: 7.890774726867676
2025-12-09 12:36:05.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0029968529920942253 Training loss: 7.675462245941162
2025-12-09 12:36:05.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0029967922034361727 Training loss: 6.810453414916992
2025-12-09 12:36:06.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.002996730833902288 Training loss: 7.427056312561035
2025-12-09 12:36:06.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.002996668883516388 Training loss: 7.174643039703369
2025-12-09 12:36:07.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002996606352302514 Training loss: 7.246232032775879
2025-12-09 12:36:07.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.002996543240284934 Training loss: 7.397884845733643
2025-12-09 12:36:08.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0029964795474881397 Training loss: 7.336134910583496
2025-12-09 12:36:08.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.002996415273936849 Training loss: 7.172075271606445
2025-12-09 12:36:09.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.002996350419656006 Training loss: 7.206644058227539
2025-12-09 12:36:09.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0029962849846707786 Training loss: 7.363614559173584
2025-12-09 12:36:10.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0029962189690065613 Training loss: 8.136932373046875
2025-12-09 12:36:10.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.0029961523726889736 Training loss: 7.192973613739014
2025-12-09 12:36:11.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0029960851957438594 Training loss: 7.290044784545898
2025-12-09 12:36:11.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.00299601743819729 Training loss: 7.245914459228516
2025-12-09 12:36:12.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0029959491000755597 Training loss: 6.804122447967529
2025-12-09 12:36:12.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0029958801814051897 Training loss: 7.928483486175537
2025-12-09 12:36:13.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.002995810682212926 Training loss: 6.980862140655518
2025-12-09 12:36:13.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0029957406025257396 Training loss: 7.799705982208252
2025-12-09 12:36:14.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.002995669942370827 Training loss: 7.27365255355835
2025-12-09 12:36:14.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0029955987017756106 Training loss: 6.974376201629639
2025-12-09 12:36:15.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0029955268807677375 Training loss: 6.976175308227539
2025-12-09 12:36:15.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.002995454479375079 Training loss: 7.552326202392578
2025-12-09 12:36:16.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0029953814976257337 Training loss: 7.305262088775635
2025-12-09 12:36:16.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.002995307935548024 Training loss: 8.285120964050293
2025-12-09 12:36:17.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.002995233793170498 Training loss: 7.247173309326172
2025-12-09 12:36:17.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0029951590705219284 Training loss: 8.075826644897461
2025-12-09 12:36:18.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0029950837676313144 Training loss: 7.327878952026367
2025-12-09 12:36:18.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0029950078845278794 Training loss: 8.078453063964844
2025-12-09 12:36:19.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0029949314212410717 Training loss: 6.960271835327148
2025-12-09 12:36:19.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0029948543778005655 Training loss: 7.218355178833008
2025-12-09 12:36:20.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00299477675423626 Training loss: 7.751616477966309
2025-12-09 12:36:20.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.002994698550578279 Training loss: 7.668755054473877
2025-12-09 12:36:21.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0029946197668569725 Training loss: 7.356481552124023
2025-12-09 12:36:21.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.002994540403102914 Training loss: 8.381908416748047
2025-12-09 12:36:22.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0029944604593469034 Training loss: 7.334590435028076
2025-12-09 12:36:22.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0029943799356199658 Training loss: 7.2896833419799805
2025-12-09 12:36:23.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0029942988319533507 Training loss: 6.822766304016113
2025-12-09 12:36:23.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.002994217148378532 Training loss: 6.966688632965088
2025-12-09 12:36:24.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.002994134884927211 Training loss: 7.383446216583252
2025-12-09 12:36:24.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.002994052041631311 Training loss: 7.677572727203369
2025-12-09 12:36:25.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0029939686185229825 Training loss: 7.32232666015625
2025-12-09 12:36:25.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0029938846156346006 Training loss: 7.720403671264648
2025-12-09 12:36:26.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.002993800032998765 Training loss: 7.266273498535156
2025-12-09 12:36:26.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.002993714870648301 Training loss: 7.292321681976318
2025-12-09 12:36:27.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0029936291286162577 Training loss: 7.183983325958252
2025-12-09 12:36:27.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00299354280693591 Training loss: 8.337530136108398
2025-12-09 12:36:28.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.002993455905640758 Training loss: 6.967970371246338
2025-12-09 12:36:28.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0029933684247645267 Training loss: 7.072299957275391
2025-12-09 12:36:29.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.002993280364341165 Training loss: 6.988829135894775
2025-12-09 12:36:29.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.002993191724404848 Training loss: 7.221435070037842
2025-12-09 12:36:30.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0029931025049899744 Training loss: 7.181130409240723
2025-12-09 12:36:30.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.002993012706131169 Training loss: 7.546276092529297
2025-12-09 12:36:31.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.002992922327863281 Training loss: 7.299635410308838
2025-12-09 12:36:31.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.002992831370221385 Training loss: 7.055481910705566
2025-12-09 12:36:32.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.002992739833240779 Training loss: 6.953155040740967
2025-12-09 12:36:32.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0029926477169569866 Training loss: 6.811448574066162
2025-12-09 12:36:33.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0029925550214057565 Training loss: 6.928567886352539
2025-12-09 12:36:33.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.002992461746623063 Training loss: 7.635134220123291
2025-12-09 12:36:34.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0029923678926451034 Training loss: 7.177014350891113
2025-12-09 12:36:34.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.0029922734595083005 Training loss: 7.184389114379883
2025-12-09 12:36:35.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.002992178447249302 Training loss: 7.3639912605285645
2025-12-09 12:36:35.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0029920828559049806 Training loss: 7.029836177825928
2025-12-09 12:36:36.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0029919866855124336 Training loss: 7.199979305267334
2025-12-09 12:36:36.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0029918899361089826 Training loss: 7.808544158935547
2025-12-09 12:36:37.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0029917926077321732 Training loss: 6.742968559265137
2025-12-09 12:36:37.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.002991694700419778 Training loss: 7.350744247436523
2025-12-09 12:36:38.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.002991596214209793 Training loss: 6.90786075592041
2025-12-09 12:36:38.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0029914971491404375 Training loss: 7.155825614929199
2025-12-09 12:36:39.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0029913975052501575 Training loss: 7.449651718139648
2025-12-09 12:36:39.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.002991297282577623 Training loss: 7.417242527008057
2025-12-09 12:36:40.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0029911964811617287 Training loss: 7.256302833557129
2025-12-09 12:36:40.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0029910951010415927 Training loss: 7.131447792053223
2025-12-09 12:36:41.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0029909931422565593 Training loss: 7.201226711273193
2025-12-09 12:36:41.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0029908906048461965 Training loss: 6.861081600189209
2025-12-09 12:36:42.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.002990787488850297 Training loss: 7.743654251098633
2025-12-09 12:36:42.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0029906837943088787 Training loss: 7.705829620361328
2025-12-09 12:36:43.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0029905795212621824 Training loss: 7.069291591644287
2025-12-09 12:36:43.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.002990474669750676 Training loss: 7.282191276550293
2025-12-09 12:36:44.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.002990369239815048 Training loss: 6.797283172607422
2025-12-09 12:36:44.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.002990263231496216 Training loss: 7.23510217666626
2025-12-09 12:36:45.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029901566448353183 Training loss: 7.447329521179199
2025-12-09 12:36:45.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029900494798737196 Training loss: 7.90653657913208
2025-12-09 12:36:46.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002989941736653009 Training loss: 7.078420639038086
2025-12-09 12:36:46.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.002989833415214999 Training loss: 6.6799540519714355
2025-12-09 12:36:47.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0029897245156017267 Training loss: 7.002367973327637
2025-12-09 12:36:47.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.002989615037855454 Training loss: 7.284544944763184
2025-12-09 12:36:48.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0029895049820186682 Training loss: 7.192999839782715
2025-12-09 12:36:48.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0029893943481340787 Training loss: 7.028484344482422
2025-12-09 12:36:49.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0029892831362446203 Training loss: 6.94254732131958
2025-12-09 12:36:49.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002989171346393453 Training loss: 7.01826286315918
2025-12-09 12:36:50.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00298905897862396 Training loss: 7.147156238555908
2025-12-09 12:36:50.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0029889460329797484 Training loss: 7.430874824523926
2025-12-09 12:36:51.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0029888325095046506 Training loss: 7.057380676269531
2025-12-09 12:36:51.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0029887184082427226 Training loss: 6.98240852355957
2025-12-09 12:36:52.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002988603729238246 Training loss: 7.252293109893799
2025-12-09 12:36:52.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0029884884725357237 Training loss: 7.571056842803955
2025-12-09 12:36:53.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0029883726381798865 Training loss: 6.947271347045898
2025-12-09 12:36:53.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0029882562262156854 Training loss: 7.080585956573486
2025-12-09 12:36:54.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.002988139236688299 Training loss: 7.116509437561035
2025-12-09 12:36:54.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0029880216696431287 Training loss: 7.093214988708496
2025-12-09 12:36:55.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0029879035251257993 Training loss: 7.151116847991943
2025-12-09 12:36:55.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.002987784803182161 Training loss: 7.063230514526367
2025-12-09 12:36:56.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0029876655038582863 Training loss: 7.160028457641602
2025-12-09 12:36:56.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0029875456272004746 Training loss: 7.30150032043457
2025-12-09 12:36:57.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0029874251732552462 Training loss: 7.775661468505859
2025-12-09 12:36:57.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.002987304142069348 Training loss: 6.943418025970459
2025-12-09 12:36:58.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.002987182533689749 Training loss: 7.195211887359619
2025-12-09 12:36:58.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0029870603481636443 Training loss: 6.998470306396484
2025-12-09 12:36:59.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0029869375855384505 Training loss: 6.981747150421143
2025-12-09 12:36:59.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0029868142458618096 Training loss: 7.1367950439453125
2025-12-09 12:37:00.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0029866903291815875 Training loss: 6.916213512420654
2025-12-09 12:37:00.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.002986565835545874 Training loss: 6.988048076629639
2025-12-09 12:37:01.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0029864407650029823 Training loss: 6.323390007019043
2025-12-09 12:37:01.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00298631511760145 Training loss: 6.805907249450684
2025-12-09 12:37:02.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0029861888933900385 Training loss: 7.108670234680176
2025-12-09 12:37:02.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.002986062092417733 Training loss: 6.9833173751831055
2025-12-09 12:37:03.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0029859347147337422 Training loss: 6.81965970993042
2025-12-09 12:37:03.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.002985806760387499 Training loss: 6.9066643714904785
2025-12-09 12:37:04.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00298567822942866 Training loss: 6.796511173248291
2025-12-09 12:37:04.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0029855491219071056 Training loss: 6.976408958435059
2025-12-09 12:37:05.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00298541943787294 Training loss: 6.420199394226074
2025-12-09 12:37:05.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.002985289177376491 Training loss: 7.918447017669678
2025-12-09 12:37:06.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00298515834046831 Training loss: 7.101349830627441
2025-12-09 12:37:06.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.002985026927199172 Training loss: 7.186498641967773
2025-12-09 12:37:07.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0029848949376200767 Training loss: 7.123830318450928
2025-12-09 12:37:07.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0029847623717822462 Training loss: 7.005001544952393
2025-12-09 12:37:08.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0029846292297371264 Training loss: 6.638638496398926
2025-12-09 12:37:08.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.002984495511536388 Training loss: 6.769707679748535
2025-12-09 12:37:09.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0029843612172319235 Training loss: 6.929978370666504
2025-12-09 12:37:09.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.002984226346875851 Training loss: 7.360756874084473
2025-12-09 12:37:10.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0029840909005205093 Training loss: 7.3398051261901855
2025-12-09 12:37:10.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.002983954878218464 Training loss: 7.21536922454834
2025-12-09 12:37:11.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.002983818280022502 Training loss: 6.614010334014893
2025-12-09 12:37:11.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0029836811059856354 Training loss: 7.402873992919922
2025-12-09 12:37:12.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0029835433561610975 Training loss: 6.690492153167725
2025-12-09 12:37:12.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0029834050306023468 Training loss: 7.767541885375977
2025-12-09 12:37:13.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0029832661293630646 Training loss: 6.904040336608887
2025-12-09 12:37:13.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.002983126652497156 Training loss: 6.9857001304626465
2025-12-09 12:37:14.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.002982986600058749 Training loss: 7.071020603179932
2025-12-09 12:37:14.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0029828459721021965 Training loss: 6.769723415374756
2025-12-09 12:37:15.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.002982704768682071 Training loss: 6.535689830780029
2025-12-09 12:37:15.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0029825629898531728 Training loss: 7.510522365570068
2025-12-09 12:37:16.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.002982420635670523 Training loss: 7.032610893249512
2025-12-09 12:37:16.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.002982277706189366 Training loss: 6.984041213989258
2025-12-09 12:37:17.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00298213420146517 Training loss: 7.0957441329956055
2025-12-09 12:37:17.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0029819901215536273 Training loss: 6.36883544921875
2025-12-09 12:37:18.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.002981845466510651 Training loss: 7.046177864074707
2025-12-09 12:37:18.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0029817002363923804 Training loss: 6.86103630065918
2025-12-09 12:37:19.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.002981554431255176 Training loss: 6.921699047088623
2025-12-09 12:37:19.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.002981408051155621 Training loss: 6.6835832595825195
2025-12-09 12:37:20.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.002981261096150524 Training loss: 6.43231725692749
2025-12-09 12:37:20.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.002981113566296915 Training loss: 7.461329936981201
2025-12-09 12:37:21.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0029809654616520464 Training loss: 6.918179035186768
2025-12-09 12:37:21.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.0029808167822733956 Training loss: 6.90882682800293
2025-12-09 12:37:22.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0029806675282186626 Training loss: 7.204232215881348
2025-12-09 12:37:22.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.002980517699545769 Training loss: 7.2528157234191895
2025-12-09 12:37:23.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0029803672963128617 Training loss: 6.852258682250977
2025-12-09 12:37:23.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0029802163185783073 Training loss: 7.1095499992370605
2025-12-09 12:37:24.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0029800647664006996 Training loss: 7.465506076812744
2025-12-09 12:37:24.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.002979912639838851 Training loss: 6.4128007888793945
2025-12-09 12:37:25.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0029797599389518002 Training loss: 6.882584571838379
2025-12-09 12:37:25.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0029796066637988072 Training loss: 6.519912242889404
2025-12-09 12:37:26.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.002979452814439354 Training loss: 7.327162265777588
2025-12-09 12:37:26.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0029792983909331487 Training loss: 6.829402446746826
2025-12-09 12:37:27.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0029791433933401175 Training loss: 6.839085102081299
2025-12-09 12:37:27.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0029789878217204137 Training loss: 7.273695945739746
2025-12-09 12:37:28.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0029788316761344114 Training loss: 7.142333984375
2025-12-09 12:37:28.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0029786749566427066 Training loss: 6.433525085449219
2025-12-09 12:37:29.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00297851766330612 Training loss: 6.939318656921387
2025-12-09 12:37:29.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.002978359796185695 Training loss: 7.122011184692383
2025-12-09 12:37:30.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0029782013553426943 Training loss: 7.008528232574463
2025-12-09 12:37:30.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0029780423408386075 Training loss: 6.703464984893799
2025-12-09 12:37:31.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0029778827527351445 Training loss: 6.903643608093262
2025-12-09 12:37:31.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0029777225910942386 Training loss: 6.734084129333496
2025-12-09 12:37:32.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.002977561855978045 Training loss: 6.804619312286377
2025-12-09 12:37:32.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.002977400547448942 Training loss: 6.803740501403809
2025-12-09 12:37:33.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0029772386655695306 Training loss: 6.807707786560059
2025-12-09 12:37:33.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0029770762104026336 Training loss: 6.837757110595703
2025-12-09 12:37:34.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.002976913182011297 Training loss: 6.632287502288818
2025-12-09 12:37:34.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0029767495804587886 Training loss: 6.893893241882324
2025-12-09 12:37:35.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.002976585405808599 Training loss: 6.5727386474609375
2025-12-09 12:37:35.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0029764206581244412 Training loss: 7.26678991317749
2025-12-09 12:37:36.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.002976255337470251 Training loss: 6.958654880523682
2025-12-09 12:37:36.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.002976089443910186 Training loss: 7.9936065673828125
2025-12-09 12:37:37.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0029759229775086255 Training loss: 7.02994441986084
2025-12-09 12:37:37.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0029757559383301727 Training loss: 6.691374778747559
2025-12-09 12:37:38.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.0029755883264396517 Training loss: 6.58582067489624
2025-12-09 12:37:38.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00297542014190211 Training loss: 6.790731906890869
2025-12-09 12:37:39.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0029752513847828162 Training loss: 6.95964241027832
2025-12-09 12:37:39.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0029750820551472617 Training loss: 6.954017162322998
2025-12-09 12:37:40.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0029749121530611602 Training loss: 7.008285045623779
2025-12-09 12:37:40.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0029747416785904472 Training loss: 8.700797080993652
2025-12-09 12:37:41.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.002974570631801281 Training loss: 7.1125102043151855
2025-12-09 12:37:41.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0029743990127600413 Training loss: 7.087706089019775
2025-12-09 12:37:42.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.002974226821533329 Training loss: 7.003636360168457
2025-12-09 12:37:42.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0029740540581879703 Training loss: 6.9135942459106445
2025-12-09 12:37:43.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0029738807227910093 Training loss: 6.1534881591796875
2025-12-09 12:37:43.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.002973706815409715 Training loss: 6.785772800445557
2025-12-09 12:37:44.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0029735323361115775 Training loss: 6.801372528076172
2025-12-09 12:37:44.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.002973357284964309 Training loss: 6.649014949798584
2025-12-09 12:37:45.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0029731816620358425 Training loss: 7.086587905883789
2025-12-09 12:37:45.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.002973005467394334 Training loss: 6.706903457641602
2025-12-09 12:37:46.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0029728287011081627 Training loss: 6.618128299713135
2025-12-09 12:37:46.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.002972651363245927 Training loss: 7.428765773773193
2025-12-09 12:37:47.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.002972473453876448 Training loss: 6.6439619064331055
2025-12-09 12:37:47.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0029722949730687687 Training loss: 6.677760124206543
2025-12-09 12:37:48.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0029721159208921546 Training loss: 7.00337553024292
2025-12-09 12:37:48.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.0029719362974160927 Training loss: 6.575199127197266
2025-12-09 12:37:49.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0029717561027102907 Training loss: 7.038753032684326
2025-12-09 12:37:49.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.002971575336844679 Training loss: 6.997184753417969
2025-12-09 12:37:50.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.002971393999889409 Training loss: 7.592228889465332
2025-12-09 12:37:50.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.002971212091914854 Training loss: 6.936263561248779
2025-12-09 12:37:51.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0029710296129916093 Training loss: 6.748647689819336
2025-12-09 12:37:51.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0029708465631904913 Training loss: 6.81915807723999
2025-12-09 12:37:52.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.002970662942582538 Training loss: 6.620545864105225
2025-12-09 12:37:52.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.002970478751239009 Training loss: 6.957313060760498
2025-12-09 12:37:53.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0029702939892313853 Training loss: 7.762955665588379
2025-12-09 12:37:53.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.002970108656631369 Training loss: 6.698115348815918
2025-12-09 12:37:54.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.002969922753510885 Training loss: 7.333410739898682
2025-12-09 12:37:54.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.002969736279942078 Training loss: 6.837406635284424
2025-12-09 12:37:55.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.002969549235997315 Training loss: 6.744839668273926
2025-12-09 12:37:55.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.002969361621749184 Training loss: 6.8378987312316895
2025-12-09 12:37:56.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.002969173437270495 Training loss: 6.629708766937256
2025-12-09 12:37:56.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0029689846826342773 Training loss: 6.974034786224365
2025-12-09 12:37:57.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.002968795357913784 Training loss: 6.874568462371826
2025-12-09 12:37:57.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0029686054631824885 Training loss: 6.707359790802002
2025-12-09 12:37:58.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0029684149985140847 Training loss: 6.7080864906311035
2025-12-09 12:37:58.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0029682239639824883 Training loss: 7.887223720550537
2025-12-09 12:37:59.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.002968032359661836 Training loss: 7.660674095153809
2025-12-09 12:37:59.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.002967840185626486 Training loss: 7.206418991088867
2025-12-09 12:38:00.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0029676474419510174 Training loss: 7.124353885650635
2025-12-09 12:38:00.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0029674541287102296 Training loss: 6.769044876098633
2025-12-09 12:38:01.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.002967260245979144 Training loss: 6.578869819641113
2025-12-09 12:38:01.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.002967065793833003 Training loss: 6.832899570465088
2025-12-09 12:38:02.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.002966870772347269 Training loss: 6.9586052894592285
2025-12-09 12:38:02.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0029666751815976273 Training loss: 6.806642055511475
2025-12-09 12:38:03.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0029664790216599813 Training loss: 6.776961803436279
2025-12-09 12:38:03.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0029662822926104578 Training loss: 6.831850051879883
2025-12-09 12:38:04.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0029660849945254038 Training loss: 6.9903645515441895
2025-12-09 12:38:04.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0029658871274813856 Training loss: 7.433866500854492
2025-12-09 12:38:05.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0029656886915551926 Training loss: 6.566164493560791
2025-12-09 12:38:05.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0029654896868238335 Training loss: 6.949564456939697
2025-12-09 12:38:06.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0029652901133645384 Training loss: 6.544183731079102
2025-12-09 12:38:06.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0029650899712547574 Training loss: 6.439774990081787
2025-12-09 12:38:07.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0029648892605721624 Training loss: 6.6423516273498535
2025-12-09 12:38:07.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0029646879813946445 Training loss: 6.686071395874023
2025-12-09 12:38:08.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.002964486133800317 Training loss: 7.162867069244385
2025-12-09 12:38:08.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0029642837178675122 Training loss: 6.811307907104492
2025-12-09 12:38:09.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.002964080733674784 Training loss: 6.939168930053711
2025-12-09 12:38:09.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0029638771813009076 Training loss: 6.66300106048584
2025-12-09 12:38:10.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.002963673060824877 Training loss: 6.423875331878662
2025-12-09 12:38:10.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0029634683723259066 Training loss: 7.169328212738037
2025-12-09 12:38:11.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0029632631158834333 Training loss: 7.231806755065918
2025-12-09 12:38:11.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0029630572915771117 Training loss: 6.6983489990234375
2025-12-09 12:38:12.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.00296285089948682 Training loss: 6.7316575050354
2025-12-09 12:38:12.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0029626439396926536 Training loss: 6.884973049163818
2025-12-09 12:38:13.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0029624364122749296 Training loss: 6.809420108795166
2025-12-09 12:38:13.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0029622283173141866 Training loss: 7.123557090759277
2025-12-09 12:38:14.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00296201965489118 Training loss: 7.013304710388184
2025-12-09 12:38:14.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00296181042508689 Training loss: 7.219738006591797
2025-12-09 12:38:15.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.0029616006279825128 Training loss: 6.57124662399292
2025-12-09 12:38:15.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.002961390263659467 Training loss: 7.200765132904053
2025-12-09 12:38:16.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0029611793321993912 Training loss: 6.881858825683594
2025-12-09 12:38:16.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.0029609678336841444 Training loss: 6.73156213760376
2025-12-09 12:38:17.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0029607557681958037 Training loss: 6.840055465698242
2025-12-09 12:38:17.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0029605431358166686 Training loss: 6.751256942749023
2025-12-09 12:38:18.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.002960329936629257 Training loss: 6.8547282218933105
2025-12-09 12:38:18.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.002960116170716308 Training loss: 6.606610298156738
2025-12-09 12:38:19.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0029599018381607787 Training loss: 6.895498275756836
2025-12-09 12:38:19.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0029596869390458485 Training loss: 6.400299072265625
2025-12-09 12:38:20.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.002959471473454915 Training loss: 6.948471546173096
2025-12-09 12:38:20.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0029592554414715967 Training loss: 6.668764591217041
2025-12-09 12:38:21.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.002959038843179731 Training loss: 6.665197849273682
2025-12-09 12:38:21.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0029588216786633763 Training loss: 7.633956432342529
2025-12-09 12:38:22.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0029586039480068087 Training loss: 6.521892070770264
2025-12-09 12:38:22.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0029583856512945257 Training loss: 6.637174129486084
2025-12-09 12:38:23.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0029581667886112435 Training loss: 7.057545185089111
2025-12-09 12:38:23.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0029579473600418998 Training loss: 6.81433629989624
2025-12-09 12:38:24.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0029577273656716495 Training loss: 6.692862510681152
2025-12-09 12:38:24.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0029575068055858675 Training loss: 7.2000651359558105
2025-12-09 12:38:25.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.0029572856798701507 Training loss: 6.900725364685059
2025-12-09 12:38:25.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0029570639886103123 Training loss: 6.882081508636475
2025-12-09 12:38:26.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0029568417318923865 Training loss: 6.7578043937683105
2025-12-09 12:38:26.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.002956618909802627 Training loss: 6.776646137237549
2025-12-09 12:38:27.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0029563955224275068 Training loss: 8.183306694030762
2025-12-09 12:38:27.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0029561715698537185 Training loss: 6.944852828979492
2025-12-09 12:38:28.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0029559470521681726 Training loss: 6.794253826141357
2025-12-09 12:38:28.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.002955721969458001 Training loss: 6.882577419281006
2025-12-09 12:38:29.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0029554963218105536 Training loss: 6.814934730529785
2025-12-09 12:38:29.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0029552701093133998 Training loss: 6.5126953125
2025-12-09 12:38:30.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0029550433320543286 Training loss: 6.43874454498291
2025-12-09 12:38:30.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0029548159901213473 Training loss: 6.6571574211120605
2025-12-09 12:38:31.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.0029545880836026835 Training loss: 6.974604606628418
2025-12-09 12:38:31.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0029543596125867827 Training loss: 6.627739429473877
2025-12-09 12:38:32.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00295413057716231 Training loss: 6.822383403778076
2025-12-09 12:38:32.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.00295390097741815 Training loss: 6.724428653717041
2025-12-09 12:38:33.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0029536708134434058 Training loss: 6.50963020324707
2025-12-09 12:38:33.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.002953440085327399 Training loss: 6.650629997253418
2025-12-09 12:38:34.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0029532087931596718 Training loss: 6.979325771331787
2025-12-09 12:38:34.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0029529769370299826 Training loss: 6.9570393562316895
2025-12-09 12:38:35.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0029527445170283114 Training loss: 7.357726097106934
2025-12-09 12:38:35.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.002952511533244856 Training loss: 6.760787010192871
2025-12-09 12:38:36.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0029522779857700326 Training loss: 7.033881664276123
2025-12-09 12:38:36.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0029520438746944754 Training loss: 7.388489246368408
2025-12-09 12:38:37.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00295180920010904 Training loss: 6.303997993469238
2025-12-09 12:38:37.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.0029515739621047976 Training loss: 6.9268364906311035
2025-12-09 12:38:38.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0029513381607730402 Training loss: 6.815152645111084
2025-12-09 12:38:38.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.002951101796205278 Training loss: 6.741970062255859
2025-12-09 12:38:39.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0029508648684932392 Training loss: 6.633419036865234
2025-12-09 12:38:39.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0029506273777288703 Training loss: 7.198648452758789
2025-12-09 12:38:40.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.002950389324004337 Training loss: 6.433346271514893
2025-12-09 12:38:40.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.002950150707412024 Training loss: 6.720265865325928
2025-12-09 12:38:41.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.002949911528044533 Training loss: 6.816489219665527
2025-12-09 12:38:41.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0029496717859946848 Training loss: 6.293900966644287
2025-12-09 12:38:42.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0029494314813555194 Training loss: 6.863730430603027
2025-12-09 12:38:42.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.002949190614220294 Training loss: 6.492696285247803
2025-12-09 12:38:43.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.002948949184682484 Training loss: 6.945675849914551
2025-12-09 12:38:43.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0029487071928357834 Training loss: 6.776034832000732
2025-12-09 12:38:44.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0029484646387741057 Training loss: 6.680861949920654
2025-12-09 12:38:44.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0029482215225915803 Training loss: 7.314239501953125
2025-12-09 12:38:45.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0029479778443825553 Training loss: 7.203658103942871
2025-12-09 12:38:45.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.002947733604241599 Training loss: 6.773258209228516
2025-12-09 12:38:46.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.002947488802263496 Training loss: 6.556902885437012
2025-12-09 12:38:46.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0029472434385432477 Training loss: 6.769341945648193
2025-12-09 12:38:47.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0029469975131760765 Training loss: 6.948805809020996
2025-12-09 12:38:47.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0029467510262574203 Training loss: 6.69655179977417
2025-12-09 12:38:48.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0029465039778829366 Training loss: 7.1875901222229
2025-12-09 12:38:48.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0029462563681484995 Training loss: 6.670177936553955
2025-12-09 12:38:49.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.002946008197150202 Training loss: 7.32779598236084
2025-12-09 12:38:49.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0029457594649843536 Training loss: 7.497650623321533
2025-12-09 12:38:50.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0029455101717474836 Training loss: 7.024377822875977
2025-12-09 12:38:50.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0029452603175363365 Training loss: 6.8800554275512695
2025-12-09 12:38:51.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0029450099024478766 Training loss: 6.701206207275391
2025-12-09 12:38:51.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.002944758926579285 Training loss: 6.64777946472168
2025-12-09 12:38:52.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.002944507390027961 Training loss: 6.462193965911865
2025-12-09 12:38:52.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0029442552928915203 Training loss: 6.796241760253906
2025-12-09 12:38:53.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.002944002635267797 Training loss: 6.478754997253418
2025-12-09 12:38:53.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.002943749417254843 Training loss: 6.521158218383789
2025-12-09 12:38:54.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0029434956389509264 Training loss: 7.124505519866943
2025-12-09 12:38:54.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0029432413004545346 Training loss: 6.608340740203857
2025-12-09 12:38:55.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.002942986401864371 Training loss: 6.965118408203125
2025-12-09 12:38:55.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.002942730943279357 Training loss: 6.665293216705322
2025-12-09 12:38:56.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0029424749247986305 Training loss: 6.513140678405762
2025-12-09 12:38:56.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.002942218346521548 Training loss: 6.560479640960693
2025-12-09 12:38:57.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.0029419612085476816 Training loss: 6.494155406951904
2025-12-09 12:38:57.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0029417035109768224 Training loss: 6.778360366821289
2025-12-09 12:38:58.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.002941445253908978 Training loss: 6.668017387390137
2025-12-09 12:38:58.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.0029411864374443717 Training loss: 6.670901775360107
2025-12-09 12:38:59.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.002940927061683446 Training loss: 7.537123680114746
2025-12-09 12:38:59.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.002940667126726859 Training loss: 6.589272975921631
2025-12-09 12:39:00.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0029404066326754875 Training loss: 6.531635761260986
2025-12-09 12:39:00.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0029401455796304234 Training loss: 6.877689361572266
2025-12-09 12:39:01.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0029398839676929756 Training loss: 7.01445198059082
2025-12-09 12:39:01.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.002939621796964672 Training loss: 7.210219383239746
2025-12-09 12:39:02.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.002939359067547255 Training loss: 6.912619113922119
2025-12-09 12:39:02.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.002939095779542685 Training loss: 6.652957439422607
2025-12-09 12:39:03.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0029388319330531385 Training loss: 7.162081241607666
2025-12-09 12:39:03.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0029385675281810106 Training loss: 6.568964958190918
2025-12-09 12:39:04.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.00293830256502891 Training loss: 6.588897228240967
2025-12-09 12:39:04.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0029380370436996642 Training loss: 5.9884233474731445
2025-12-09 12:39:05.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0029377709642963174 Training loss: 6.9790167808532715
2025-12-09 12:39:05.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0029375043269221296 Training loss: 6.8766560554504395
2025-12-09 12:39:06.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.002937237131680577 Training loss: 7.125431537628174
2025-12-09 12:39:06.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.002936969378675354 Training loss: 6.790423393249512
2025-12-09 12:39:07.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0029367010680103685 Training loss: 7.119289398193359
2025-12-09 12:39:07.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0029364321997897482 Training loss: 6.169449329376221
2025-12-09 12:39:08.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0029361627741178358 Training loss: 6.7724528312683105
2025-12-09 12:39:08.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.002935892791099189 Training loss: 6.91564416885376
2025-12-09 12:39:09.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.002935622250838583 Training loss: 6.958348751068115
2025-12-09 12:39:09.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0029353511534410104 Training loss: 6.818078517913818
2025-12-09 12:39:10.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.002935079499011677 Training loss: 6.5077338218688965
2025-12-09 12:39:10.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0029348072876560086 Training loss: 6.7684125900268555
2025-12-09 12:39:11.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0029345345194796437 Training loss: 6.776096820831299
2025-12-09 12:39:11.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0029342611945884388 Training loss: 6.69453763961792
2025-12-09 12:39:12.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0029339873130884656 Training loss: 7.012258052825928
2025-12-09 12:39:12.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0029337128750860125 Training loss: 6.732998847961426
2025-12-09 12:39:13.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0029334378806875837 Training loss: 6.895816326141357
2025-12-09 12:39:13.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.002933162329999899 Training loss: 7.049030303955078
2025-12-09 12:39:14.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.002932886223129894 Training loss: 6.48617696762085
2025-12-09 12:39:14.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0029326095601847203 Training loss: 7.391583442687988
2025-12-09 12:39:15.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0029323323412717463 Training loss: 6.214669704437256
2025-12-09 12:39:15.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0029320545664985537 Training loss: 6.508408546447754
2025-12-09 12:39:16.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0029317762359729427 Training loss: 6.680190563201904
2025-12-09 12:39:16.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.002931497349802928 Training loss: 6.536350727081299
2025-12-09 12:39:17.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.002931217908096739 Training loss: 6.5452375411987305
2025-12-09 12:39:17.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0029309379109628223 Training loss: 6.714616775512695
2025-12-09 12:39:18.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0029306573585098387 Training loss: 6.722191333770752
2025-12-09 12:39:18.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0029303762508466656 Training loss: 7.076136112213135
2025-12-09 12:39:19.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0029300945880823956 Training loss: 6.355973243713379
2025-12-09 12:39:19.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0029298123703263364 Training loss: 6.594247341156006
2025-12-09 12:39:20.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.002929529597688011 Training loss: 6.593389987945557
2025-12-09 12:39:20.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0029292462702771574 Training loss: 6.64848518371582
2025-12-09 12:39:21.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0029289623882037302 Training loss: 6.90720272064209
2025-12-09 12:39:21.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0029286779515778983 Training loss: 7.0003886222839355
2025-12-09 12:39:22.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0029283929605100458 Training loss: 6.83302640914917
2025-12-09 12:39:22.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0029281074151107727 Training loss: 6.658051013946533
2025-12-09 12:39:23.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.002927821315490893 Training loss: 6.630795955657959
2025-12-09 12:39:23.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0029275346617614363 Training loss: 6.514383316040039
2025-12-09 12:39:24.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.002927247454033648 Training loss: 6.695343017578125
2025-12-09 12:39:24.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.002926959692418988 Training loss: 6.870854377746582
2025-12-09 12:39:25.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0029266713770291293 Training loss: 6.871775150299072
2025-12-09 12:39:26.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0029263825079759638 Training loss: 6.98901891708374
2025-12-09 12:39:26.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0029260930853715937 Training loss: 6.6783647537231445
2025-12-09 12:39:27.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0029258031093283396 Training loss: 6.986059188842773
2025-12-09 12:39:27.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0029255125799587355 Training loss: 6.992969512939453
2025-12-09 12:39:28.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.002925221497375529 Training loss: 7.036327838897705
2025-12-09 12:39:28.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0029249298616916856 Training loss: 6.266313552856445
2025-12-09 12:39:29.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.002924637673020382 Training loss: 6.653051853179932
2025-12-09 12:39:29.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.002924344931475011 Training loss: 7.093968391418457
2025-12-09 12:39:30.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0029240516371691803 Training loss: 6.80993127822876
2025-12-09 12:39:30.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.002923757790216711 Training loss: 6.2562456130981445
2025-12-09 12:39:31.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0029234633907316405 Training loss: 6.774490833282471
2025-12-09 12:39:31.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0029231684388282184 Training loss: 6.726069927215576
2025-12-09 12:39:32.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00292287293462091 Training loss: 6.645171642303467
2025-12-09 12:39:32.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0029225768782243956 Training loss: 6.831369400024414
2025-12-09 12:39:33.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0029222802697535678 Training loss: 7.307094097137451
2025-12-09 12:39:33.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.002921983109323535 Training loss: 7.31612491607666
2025-12-09 12:39:34.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0029216853970496196 Training loss: 6.951561450958252
2025-12-09 12:39:34.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0029213871330473575 Training loss: 6.4504618644714355
2025-12-09 12:39:35.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.002921088317432499 Training loss: 6.382967948913574
2025-12-09 12:39:35.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0029207889503210095 Training loss: 6.603499412536621
2025-12-09 12:39:36.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.002920489031829067 Training loss: 6.756002426147461
2025-12-09 12:39:36.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.002920188562073063 Training loss: 6.83304500579834
2025-12-09 12:39:37.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0029198875411696056 Training loss: 6.501041889190674
2025-12-09 12:39:37.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0029195859692355145 Training loss: 6.282521724700928
2025-12-09 12:39:38.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.002919283846387824 Training loss: 7.052137851715088
2025-12-09 12:39:38.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0029189811727437813 Training loss: 6.50005578994751
2025-12-09 12:39:39.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.002918677948420849 Training loss: 7.853137969970703
2025-12-09 12:39:39.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0029183741735367024 Training loss: 6.588970184326172
2025-12-09 12:39:40.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0029180698482092304 Training loss: 6.546586036682129
2025-12-09 12:39:40.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0029177649725565355 Training loss: 6.558159351348877
2025-12-09 12:39:41.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0029174595466969345 Training loss: 6.303953170776367
2025-12-09 12:39:41.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0029171535707489572 Training loss: 7.106457710266113
2025-12-09 12:39:42.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0029168470448313463 Training loss: 6.632439613342285
2025-12-09 12:39:42.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.002916539969063059 Training loss: 6.854753017425537
2025-12-09 12:39:43.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0029162323435632654 Training loss: 6.632949352264404
2025-12-09 12:39:43.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.002915924168451349 Training loss: 6.618130683898926
2025-12-09 12:39:44.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.002915615443846906 Training loss: 6.761684417724609
2025-12-09 12:39:44.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0029153061698697475 Training loss: 6.305741786956787
2025-12-09 12:39:45.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0029149963466398956 Training loss: 6.553879261016846
2025-12-09 12:39:45.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.002914685974277587 Training loss: 6.633080959320068
2025-12-09 12:39:46.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.002914375052903271 Training loss: 6.975075721740723
2025-12-09 12:39:46.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.002914063582637611 Training loss: 7.526063919067383
2025-12-09 12:39:47.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.002913751563601481 Training loss: 6.293928146362305
2025-12-09 12:39:47.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0029134389959159708 Training loss: 6.794498443603516
2025-12-09 12:39:48.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0029131258797023816 Training loss: 6.401054859161377
2025-12-09 12:39:48.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0029128122150822266 Training loss: 6.6570329666137695
2025-12-09 12:39:49.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0029124980021772344 Training loss: 7.192142009735107
2025-12-09 12:39:49.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0029121832411093443 Training loss: 6.719921112060547
2025-12-09 12:39:50.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0029118679320007087 Training loss: 7.010032653808594
2025-12-09 12:39:50.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0029115520749736935 Training loss: 7.00280237197876
2025-12-09 12:39:51.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0029112356701508756 Training loss: 6.795374870300293
2025-12-09 12:39:51.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0029109187176550463 Training loss: 7.066009044647217
2025-12-09 12:39:52.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0029106012176092085 Training loss: 7.212573528289795
2025-12-09 12:39:52.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0029102831701365785 Training loss: 6.565499305725098
2025-12-09 12:39:53.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0029099645753605827 Training loss: 7.900394439697266
2025-12-09 12:39:53.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.002909645433404863 Training loss: 6.6616129875183105
2025-12-09 12:39:54.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0029093257443932713 Training loss: 6.961087226867676
2025-12-09 12:39:54.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0029090055084498734 Training loss: 6.35503625869751
2025-12-09 12:39:55.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0029086847256989457 Training loss: 6.698184013366699
2025-12-09 12:39:55.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0029083633962649785 Training loss: 6.639029502868652
2025-12-09 12:39:56.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0029080415202726727 Training loss: 6.50530481338501
2025-12-09 12:39:56.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0029077190978469432 Training loss: 6.560400009155273
2025-12-09 12:39:57.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0029073961291129153 Training loss: 7.127096176147461
2025-12-09 12:39:57.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0029070726141959265 Training loss: 6.615300178527832
2025-12-09 12:39:58.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.002906748553221527 Training loss: 7.103059768676758
2025-12-09 12:39:58.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0029064239463154782 Training loss: 6.558083534240723
2025-12-09 12:39:59.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.002906098793603754 Training loss: 6.948427677154541
2025-12-09 12:39:59.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.00290577309521254 Training loss: 6.6057353019714355
2025-12-09 12:40:00.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.002905446851268233 Training loss: 6.519852161407471
2025-12-09 12:40:00.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.002905120061897442 Training loss: 6.774648189544678
2025-12-09 12:40:01.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.002904792727226987 Training loss: 6.489950180053711
2025-12-09 12:40:01.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.002904464847383902 Training loss: 6.545602798461914
2025-12-09 12:40:02.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.002904136422495429 Training loss: 6.567081451416016
2025-12-09 12:40:02.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0029038074526890243 Training loss: 6.539454460144043
2025-12-09 12:40:03.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.002903477938092354 Training loss: 7.093264579772949
2025-12-09 12:40:03.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.002903147878833296 Training loss: 6.701242923736572
2025-12-09 12:40:04.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.002902817275039941 Training loss: 6.875256538391113
2025-12-09 12:40:04.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0029024861268405894 Training loss: 6.484633922576904
2025-12-09 12:40:05.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0029021544343637525 Training loss: 6.320034027099609
2025-12-09 12:40:05.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0029018221977381554 Training loss: 6.436117172241211
2025-12-09 12:40:06.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0029014894170927307 Training loss: 6.794443130493164
2025-12-09 12:40:06.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0029011560925566253 Training loss: 6.554365158081055
2025-12-09 12:40:07.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.002900822224259196 Training loss: 6.6497483253479
2025-12-09 12:40:07.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0029004878123300095 Training loss: 6.412826061248779
2025-12-09 12:40:08.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0029001528568988457 Training loss: 6.993167400360107
2025-12-09 12:40:08.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.0028998173580956936 Training loss: 6.7214226722717285
2025-12-09 12:40:09.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.002899481316050754 Training loss: 6.256387710571289
2025-12-09 12:40:09.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0028991447308944385 Training loss: 6.82022762298584
2025-12-09 12:40:10.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.002898807602757369 Training loss: 6.663837909698486
2025-12-09 12:40:10.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0028984699317703777 Training loss: 6.661301612854004
2025-12-09 12:40:11.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0028981317180645093 Training loss: 7.111856460571289
2025-12-09 12:40:11.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.002897792961771017 Training loss: 6.805153846740723
2025-12-09 12:40:12.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.002897453663021366 Training loss: 6.574553966522217
2025-12-09 12:40:12.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.002897113821947231 Training loss: 6.717278480529785
2025-12-09 12:40:13.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.0028967734386804982 Training loss: 6.4227142333984375
2025-12-09 12:40:13.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.002896432513353264 Training loss: 6.592960834503174
2025-12-09 12:40:14.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.002896091046097834 Training loss: 6.316410064697266
2025-12-09 12:40:14.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0028957490370467255 Training loss: 6.56683349609375
2025-12-09 12:40:15.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0028954064863326652 Training loss: 6.743598461151123
2025-12-09 12:40:15.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.002895063394088591 Training loss: 7.205443382263184
2025-12-09 12:40:16.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0028947197604476493 Training loss: 6.4570088386535645
2025-12-09 12:40:16.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.002894375585543199 Training loss: 6.493547439575195
2025-12-09 12:40:17.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0028940308695088062 Training loss: 7.190380096435547
2025-12-09 12:40:17.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.002893685612478249 Training loss: 6.648975372314453
2025-12-09 12:40:18.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0028933398145855158 Training loss: 6.552767276763916
2025-12-09 12:40:18.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0028929934759648022 Training loss: 7.376919746398926
2025-12-09 12:40:19.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0028926465967505175 Training loss: 6.416475772857666
2025-12-09 12:40:19.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.002892299177077277 Training loss: 6.026702404022217
2025-12-09 12:40:20.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0028919512170799085 Training loss: 6.801113128662109
2025-12-09 12:40:20.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0028916027168934483 Training loss: 6.985145568847656
2025-12-09 12:40:21.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0028912536766531423 Training loss: 6.595853328704834
2025-12-09 12:40:21.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.0028909040964944462 Training loss: 6.681329250335693
2025-12-09 12:40:22.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.002890553976553025 Training loss: 6.955514430999756
2025-12-09 12:40:22.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.002890203316964755 Training loss: 6.3320465087890625
2025-12-09 12:40:23.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.002889852117865718 Training loss: 6.70095157623291
2025-12-09 12:40:23.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.002889500379392209 Training loss: 6.809141635894775
2025-12-09 12:40:24.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.0028891481016807305 Training loss: 6.555219650268555
2025-12-09 12:40:24.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.0028887952848679946 Training loss: 6.810851573944092
2025-12-09 12:40:25.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.002888441929090922 Training loss: 6.509577751159668
2025-12-09 12:40:25.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.002888088034486645 Training loss: 6.851680755615234
2025-12-09 12:40:26.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0028877336011925007 Training loss: 6.532039642333984
2025-12-09 12:40:26.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.00288737862934604 Training loss: 7.507720947265625
2025-12-09 12:40:27.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.002887023119085019 Training loss: 6.798548698425293
2025-12-09 12:40:27.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.002886667070547405 Training loss: 7.1508612632751465
2025-12-09 12:40:28.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.002886310483871373 Training loss: 6.9926981925964355
2025-12-09 12:40:28.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.0028859533591953077 Training loss: 6.188762664794922
2025-12-09 12:40:29.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.0028855956966578025 Training loss: 6.445092678070068
2025-12-09 12:40:29.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.002885237496397659 Training loss: 6.565018177032471
2025-12-09 12:40:30.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.0028848787585538872 Training loss: 6.2560625076293945
2025-12-09 12:40:30.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.0028845194832657064 Training loss: 6.556583881378174
2025-12-09 12:40:31.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.002884159670672545 Training loss: 6.6207380294799805
2025-12-09 12:40:31.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.002883799320914039 Training loss: 6.391359329223633
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 2.73 GiB is free. Including non-PyTorch memory, this process has 91.28 GiB memory in use. Of the allocated memory 90.25 GiB is allocated by PyTorch, and 278.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   1%|          | 51/10000 [00:00<00:19, 509.40it/s]Tokenizing texts:   1%|▏         | 136/10000 [00:00<00:15, 633.47it/s]Tokenizing texts:   2%|▏         | 205/10000 [00:00<00:15, 649.18it/s]Tokenizing texts:   3%|▎         | 274/10000 [00:00<00:14, 659.78it/s]Tokenizing texts:   3%|▎         | 340/10000 [00:00<00:15, 627.39it/s]Tokenizing texts:   4%|▍         | 403/10000 [00:00<00:15, 618.31it/s]Tokenizing texts:   5%|▍         | 465/10000 [00:00<00:16, 586.15it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 586.71it/s]Tokenizing texts:   6%|▌         | 599/10000 [00:00<00:15, 615.61it/s]Tokenizing texts:   7%|▋         | 661/10000 [00:01<00:15, 600.97it/s]Tokenizing texts:   7%|▋         | 722/10000 [00:01<00:16, 579.44it/s]Tokenizing texts:   8%|▊         | 781/10000 [00:01<00:15, 577.95it/s]Tokenizing texts:   8%|▊         | 843/10000 [00:01<00:15, 586.71it/s]Tokenizing texts:   9%|▉         | 911/10000 [00:01<00:14, 611.20it/s]Tokenizing texts:  10%|▉         | 994/10000 [00:01<00:13, 674.93it/s]Tokenizing texts:  11%|█         | 1062/10000 [00:01<00:13, 647.03it/s]Tokenizing texts:  11%|█▏        | 1128/10000 [00:01<00:13, 636.49it/s]Tokenizing texts:  12%|█▏        | 1192/10000 [00:01<00:14, 619.43it/s]Tokenizing texts:  13%|█▎        | 1255/10000 [00:02<00:14, 587.04it/s]Tokenizing texts:  13%|█▎        | 1323/10000 [00:02<00:14, 608.69it/s]Tokenizing texts:  14%|█▍        | 1385/10000 [00:02<00:14, 607.90it/s]Tokenizing texts:  15%|█▍        | 1458/10000 [00:02<00:13, 642.36it/s]Tokenizing texts:  15%|█▌        | 1528/10000 [00:02<00:12, 658.32it/s]Tokenizing texts:  16%|█▌        | 1603/10000 [00:02<00:12, 674.28it/s]Tokenizing texts:  17%|█▋        | 1671/10000 [00:02<00:13, 618.29it/s]Tokenizing texts:  17%|█▋        | 1744/10000 [00:02<00:12, 640.03it/s]Tokenizing texts:  18%|█▊        | 1815/10000 [00:02<00:12, 657.80it/s]Tokenizing texts:  19%|█▉        | 1882/10000 [00:03<00:13, 602.57it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:03<00:14, 571.22it/s]Tokenizing texts:  20%|██        | 2015/10000 [00:03<00:13, 602.19it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:03<00:13, 577.92it/s]Tokenizing texts:  21%|██▏       | 2141/10000 [00:03<00:13, 572.91it/s]Tokenizing texts:  22%|██▏       | 2212/10000 [00:03<00:12, 609.19it/s]Tokenizing texts:  23%|██▎       | 2284/10000 [00:03<00:12, 639.14it/s]Tokenizing texts:  23%|██▎       | 2349/10000 [00:03<00:12, 605.09it/s]Tokenizing texts:  24%|██▍       | 2426/10000 [00:03<00:11, 638.18it/s]Tokenizing texts:  25%|██▍       | 2491/10000 [00:04<00:11, 635.38it/s]Tokenizing texts:  26%|██▌       | 2561/10000 [00:04<00:11, 624.58it/s]Tokenizing texts:  26%|██▌       | 2624/10000 [00:04<00:12, 588.25it/s]Tokenizing texts:  27%|██▋       | 2684/10000 [00:04<00:12, 582.37it/s]Tokenizing texts:  28%|██▊       | 2750/10000 [00:04<00:12, 603.75it/s]Tokenizing texts:  28%|██▊       | 2811/10000 [00:04<00:12, 554.23it/s]Tokenizing texts:  29%|██▊       | 2868/10000 [00:04<00:12, 557.84it/s]Tokenizing texts:  29%|██▉       | 2943/10000 [00:04<00:11, 610.36it/s]Tokenizing texts:  30%|███       | 3007/10000 [00:04<00:11, 618.57it/s]Tokenizing texts:  31%|███       | 3070/10000 [00:05<00:11, 606.85it/s]Tokenizing texts:  31%|███▏      | 3139/10000 [00:05<00:11, 622.43it/s]Tokenizing texts:  32%|███▏      | 3205/10000 [00:05<00:10, 621.88it/s]Tokenizing texts:  33%|███▎      | 3280/10000 [00:05<00:10, 655.63it/s]Tokenizing texts:  33%|███▎      | 3348/10000 [00:05<00:10, 654.74it/s]Tokenizing texts:  34%|███▍      | 3414/10000 [00:05<00:10, 646.74it/s]Tokenizing texts:  35%|███▍      | 3493/10000 [00:05<00:09, 678.36it/s]Tokenizing texts:  36%|███▌      | 3561/10000 [00:05<00:09, 660.04it/s]Tokenizing texts:  36%|███▋      | 3628/10000 [00:05<00:09, 657.64it/s]Tokenizing texts:  37%|███▋      | 3694/10000 [00:05<00:09, 650.30it/s]Tokenizing texts:  38%|███▊      | 3760/10000 [00:06<00:10, 622.04it/s]Tokenizing texts:  38%|███▊      | 3823/10000 [00:06<00:10, 592.00it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:06<00:09, 623.55it/s]Tokenizing texts:  40%|███▉      | 3958/10000 [00:06<00:09, 610.27it/s]Tokenizing texts:  40%|████      | 4020/10000 [00:06<00:09, 598.20it/s]Tokenizing texts:  41%|████      | 4097/10000 [00:06<00:09, 645.95it/s]Tokenizing texts:  42%|████▏     | 4170/10000 [00:06<00:08, 669.21it/s]Tokenizing texts:  42%|████▏     | 4238/10000 [00:06<00:08, 652.56it/s]Tokenizing texts:  43%|████▎     | 4304/10000 [00:06<00:08, 643.78it/s]Tokenizing texts:  44%|████▍     | 4384/10000 [00:07<00:08, 687.84it/s]Tokenizing texts:  45%|████▍     | 4454/10000 [00:07<00:08, 680.80it/s]Tokenizing texts:  45%|████▌     | 4523/10000 [00:07<00:08, 652.81it/s]Tokenizing texts:  46%|████▌     | 4589/10000 [00:07<00:08, 651.29it/s]Tokenizing texts:  47%|████▋     | 4665/10000 [00:07<00:07, 682.47it/s]Tokenizing texts:  47%|████▋     | 4740/10000 [00:07<00:07, 697.50it/s]Tokenizing texts:  48%|████▊     | 4810/10000 [00:07<00:07, 672.14it/s]Tokenizing texts:  49%|████▉     | 4884/10000 [00:07<00:07, 688.95it/s]Tokenizing texts:  50%|████▉     | 4956/10000 [00:07<00:07, 687.42it/s]Tokenizing texts:  50%|█████     | 5031/10000 [00:07<00:07, 704.38it/s]Tokenizing texts:  51%|█████     | 5102/10000 [00:08<00:07, 665.17it/s]Tokenizing texts:  52%|█████▏    | 5170/10000 [00:08<00:07, 642.82it/s]Tokenizing texts:  52%|█████▏    | 5235/10000 [00:08<00:07, 640.48it/s]Tokenizing texts:  53%|█████▎    | 5306/10000 [00:08<00:07, 658.00it/s]Tokenizing texts:  54%|█████▍    | 5390/10000 [00:08<00:06, 708.59it/s]Tokenizing texts:  55%|█████▍    | 5462/10000 [00:08<00:06, 703.26it/s]Tokenizing texts:  55%|█████▌    | 5533/10000 [00:08<00:06, 638.85it/s]Tokenizing texts:  56%|█████▌    | 5601/10000 [00:08<00:06, 648.57it/s]Tokenizing texts:  57%|█████▋    | 5689/10000 [00:08<00:06, 711.33it/s]Tokenizing texts:  58%|█████▊    | 5762/10000 [00:09<00:06, 673.64it/s]Tokenizing texts:  58%|█████▊    | 5831/10000 [00:09<00:06, 601.58it/s]Tokenizing texts:  59%|█████▉    | 5901/10000 [00:09<00:06, 627.02it/s]Tokenizing texts:  60%|█████▉    | 5966/10000 [00:09<00:06, 629.84it/s]Tokenizing texts:  60%|██████    | 6031/10000 [00:09<00:06, 628.62it/s]Tokenizing texts:  61%|██████    | 6101/10000 [00:09<00:06, 647.80it/s]Tokenizing texts:  62%|██████▏   | 6177/10000 [00:09<00:05, 673.59it/s]Tokenizing texts:  62%|██████▏   | 6245/10000 [00:09<00:05, 654.26it/s]Tokenizing texts:  63%|██████▎   | 6311/10000 [00:09<00:05, 640.13it/s]Tokenizing texts:  64%|██████▍   | 6376/10000 [00:10<00:05, 627.42it/s]Tokenizing texts:  64%|██████▍   | 6442/10000 [00:10<00:05, 634.99it/s]Tokenizing texts:  65%|██████▌   | 6506/10000 [00:10<00:05, 605.91it/s]Tokenizing texts:  66%|██████▌   | 6589/10000 [00:10<00:05, 668.78it/s]Tokenizing texts:  67%|██████▋   | 6657/10000 [00:10<00:05, 646.93it/s]Tokenizing texts:  67%|██████▋   | 6723/10000 [00:10<00:05, 645.29it/s]Tokenizing texts:  68%|██████▊   | 6798/10000 [00:10<00:04, 673.65it/s]Tokenizing texts:  69%|██████▊   | 6870/10000 [00:10<00:04, 684.26it/s]Tokenizing texts:  69%|██████▉   | 6939/10000 [00:10<00:04, 622.83it/s]Tokenizing texts:  70%|███████   | 7003/10000 [00:11<00:04, 623.10it/s]Tokenizing texts:  71%|███████   | 7067/10000 [00:11<00:05, 580.14it/s]Tokenizing texts:  71%|███████▏  | 7129/10000 [00:11<00:04, 590.83it/s]Tokenizing texts:  72%|███████▏  | 7192/10000 [00:11<00:04, 601.43it/s]Tokenizing texts:  73%|███████▎  | 7255/10000 [00:11<00:04, 608.87it/s]Tokenizing texts:  73%|███████▎  | 7336/10000 [00:11<00:04, 665.16it/s]Tokenizing texts:  74%|███████▍  | 7404/10000 [00:11<00:03, 653.29it/s]Tokenizing texts:  75%|███████▍  | 7470/10000 [00:11<00:04, 592.74it/s]Tokenizing texts:  75%|███████▌  | 7545/10000 [00:11<00:03, 633.44it/s]Tokenizing texts:  76%|███████▌  | 7618/10000 [00:12<00:03, 655.10it/s]Tokenizing texts:  77%|███████▋  | 7696/10000 [00:12<00:03, 683.00it/s]Tokenizing texts:  78%|███████▊  | 7766/10000 [00:12<00:03, 637.52it/s]Tokenizing texts:  78%|███████▊  | 7831/10000 [00:12<00:03, 629.22it/s]Tokenizing texts:  79%|███████▉  | 7895/10000 [00:12<00:03, 622.41it/s]Tokenizing texts:  80%|███████▉  | 7975/10000 [00:12<00:03, 670.03it/s]Tokenizing texts:  80%|████████  | 8043/10000 [00:12<00:03, 644.06it/s]Tokenizing texts:  81%|████████  | 8108/10000 [00:12<00:03, 592.41it/s]Tokenizing texts:  82%|████████▏ | 8171/10000 [00:12<00:03, 599.72it/s]Tokenizing texts:  82%|████████▏ | 8232/10000 [00:13<00:03, 587.03it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:13<00:02, 615.39it/s]Tokenizing texts:  84%|████████▎ | 8364/10000 [00:13<00:02, 617.07it/s]Tokenizing texts:  84%|████████▍ | 8427/10000 [00:13<00:02, 600.51it/s]Tokenizing texts:  85%|████████▌ | 8505/10000 [00:13<00:02, 651.38it/s]Tokenizing texts:  86%|████████▌ | 8571/10000 [00:13<00:02, 639.30it/s]Tokenizing texts:  86%|████████▋ | 8649/10000 [00:13<00:01, 677.73it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:13<00:01, 680.98it/s]Tokenizing texts:  88%|████████▊ | 8794/10000 [00:13<00:01, 674.81it/s]Tokenizing texts:  89%|████████▊ | 8862/10000 [00:14<00:01, 613.42it/s]Tokenizing texts:  89%|████████▉ | 8946/10000 [00:14<00:01, 673.23it/s]Tokenizing texts:  90%|█████████ | 9015/10000 [00:14<00:01, 640.66it/s]Tokenizing texts:  91%|█████████ | 9081/10000 [00:14<00:01, 639.34it/s]Tokenizing texts:  92%|█████████▏| 9154/10000 [00:14<00:01, 663.36it/s]Tokenizing texts:  92%|█████████▏| 9225/10000 [00:14<00:01, 666.52it/s]Tokenizing texts:  93%|█████████▎| 9293/10000 [00:14<00:01, 640.89it/s]Tokenizing texts:  94%|█████████▎| 9358/10000 [00:14<00:01, 575.04it/s]Tokenizing texts:  94%|█████████▍| 9422/10000 [00:14<00:00, 584.69it/s]Tokenizing texts:  95%|█████████▍| 9482/10000 [00:15<00:00, 544.75it/s]Tokenizing texts:  96%|█████████▌| 9557/10000 [00:15<00:00, 589.53it/s]Tokenizing texts:  96%|█████████▌| 9618/10000 [00:15<00:00, 582.06it/s]Tokenizing texts:  97%|█████████▋| 9680/10000 [00:15<00:00, 591.82it/s]Tokenizing texts:  97%|█████████▋| 9740/10000 [00:15<00:00, 587.37it/s]Tokenizing texts:  98%|█████████▊| 9800/10000 [00:15<00:00, 583.94it/s]Tokenizing texts:  99%|█████████▊| 9869/10000 [00:15<00:00, 611.47it/s]Tokenizing texts:  99%|█████████▉| 9945/10000 [00:15<00:00, 653.44it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:15<00:00, 631.91it/s]
2025-12-09 12:41:21.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 12.170351028442383
2025-12-09 12:41:21.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 12.115022659301758
2025-12-09 12:41:22.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 12.136765480041504
2025-12-09 12:41:22.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 12.105313301086426
2025-12-09 12:41:23.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 12.0382719039917
2025-12-09 12:41:24.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 11.724254608154297
2025-12-09 12:41:24.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 11.699348449707031
2025-12-09 12:41:25.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 11.32693862915039
2025-12-09 12:41:25.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 10.357242584228516
2025-12-09 12:41:26.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 9.782731056213379
2025-12-09 12:41:26.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 8.96280288696289
2025-12-09 12:41:27.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 9.345754623413086
2025-12-09 12:41:27.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 8.736298561096191
2025-12-09 12:41:28.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 8.561903953552246
2025-12-09 12:41:28.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 8.478850364685059
2025-12-09 12:41:29.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 9.146171569824219
2025-12-09 12:41:29.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 8.515888214111328
2025-12-09 12:41:30.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 9.202595710754395
2025-12-09 12:41:30.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 8.505722045898438
2025-12-09 12:41:31.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 9.404093742370605
2025-12-09 12:41:31.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 9.329771041870117
2025-12-09 12:41:32.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 9.33764934539795
2025-12-09 12:41:32.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 9.133565902709961
2025-12-09 12:41:33.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 9.874764442443848
2025-12-09 12:41:33.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 9.056174278259277
2025-12-09 12:41:33.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 9.369487762451172
2025-12-09 12:41:34.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 9.244820594787598
2025-12-09 12:41:34.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 9.88313102722168
2025-12-09 12:41:35.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 9.330126762390137
2025-12-09 12:41:35.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 9.169276237487793
2025-12-09 12:41:36.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 9.677091598510742
2025-12-09 12:41:36.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 8.987390518188477
2025-12-09 12:41:37.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 9.408859252929688
2025-12-09 12:41:37.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 9.06749153137207
2025-12-09 12:41:38.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 10.785601615905762
2025-12-09 12:41:38.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 8.660170555114746
2025-12-09 12:41:39.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 9.177667617797852
2025-12-09 12:41:39.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 8.496101379394531
2025-12-09 12:41:40.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 9.14905834197998
2025-12-09 12:41:40.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 8.667450904846191
2025-12-09 12:41:41.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 8.640336990356445
2025-12-09 12:41:41.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 9.03247356414795
2025-12-09 12:41:42.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 9.406944274902344
2025-12-09 12:41:42.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 9.051220893859863
2025-12-09 12:41:43.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 8.862008094787598
2025-12-09 12:41:43.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 8.104360580444336
2025-12-09 12:41:44.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 8.636046409606934
2025-12-09 12:41:44.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 8.648259162902832
2025-12-09 12:41:45.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 8.776143074035645
2025-12-09 12:41:45.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 8.685002326965332
2025-12-09 12:41:46.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 8.731597900390625
2025-12-09 12:41:46.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 8.18691349029541
2025-12-09 12:41:47.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 8.423876762390137
2025-12-09 12:41:47.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 8.70719051361084
2025-12-09 12:41:48.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 8.839229583740234
2025-12-09 12:41:48.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 8.728604316711426
2025-12-09 12:41:49.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 8.007814407348633
2025-12-09 12:41:49.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 8.653824806213379
2025-12-09 12:41:50.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 8.158750534057617
2025-12-09 12:41:50.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 8.165692329406738
2025-12-09 12:41:51.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 7.857446670532227
2025-12-09 12:41:51.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 7.980341911315918
2025-12-09 12:41:52.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 8.464825630187988
2025-12-09 12:41:52.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 8.186655044555664
2025-12-09 12:41:53.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 8.214642524719238
2025-12-09 12:41:53.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 7.9993696212768555
2025-12-09 12:41:54.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 7.882334232330322
2025-12-09 12:41:54.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 8.345382690429688
2025-12-09 12:41:55.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 8.35860538482666
2025-12-09 12:41:55.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 7.529832363128662
2025-12-09 12:41:56.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 7.9425883293151855
2025-12-09 12:41:56.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 7.982936382293701
2025-12-09 12:41:57.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 8.67192554473877
2025-12-09 12:41:57.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 7.682762622833252
2025-12-09 12:41:58.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 8.303510665893555
2025-12-09 12:41:58.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 8.275867462158203
2025-12-09 12:41:59.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 7.6717400550842285
2025-12-09 12:41:59.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 7.668149471282959
2025-12-09 12:42:00.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 8.24330997467041
2025-12-09 12:42:00.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 7.734947681427002
2025-12-09 12:42:01.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 7.738102436065674
2025-12-09 12:42:01.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 7.754218578338623
2025-12-09 12:42:02.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 8.431215286254883
2025-12-09 12:42:02.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 8.32417106628418
2025-12-09 12:42:03.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 8.476675033569336
2025-12-09 12:42:03.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 8.495378494262695
2025-12-09 12:42:04.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 8.242676734924316
2025-12-09 12:42:04.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 7.7237091064453125
2025-12-09 12:42:05.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 8.228574752807617
2025-12-09 12:42:05.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 7.998266220092773
2025-12-09 12:42:06.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 7.490607261657715
2025-12-09 12:42:06.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 8.07024097442627
2025-12-09 12:42:07.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 8.388936996459961
2025-12-09 12:42:07.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 7.8942389488220215
2025-12-09 12:42:08.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 8.027433395385742
2025-12-09 12:42:08.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 7.442301273345947
2025-12-09 12:42:09.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 7.90503454208374
2025-12-09 12:42:09.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 7.430388927459717
2025-12-09 12:42:10.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 7.697278022766113
2025-12-09 12:42:10.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 7.494693279266357
2025-12-09 12:42:11.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999999029798808 Training loss: 7.323974609375
2025-12-09 12:42:11.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00999999611919561 Training loss: 7.566224575042725
2025-12-09 12:42:12.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009999991268191535 Training loss: 7.832344055175781
2025-12-09 12:42:12.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009999984476788465 Training loss: 7.841397285461426
2025-12-09 12:42:13.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009999975744989035 Training loss: 7.418730735778809
2025-12-09 12:42:13.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009999965072796636 Training loss: 8.402981758117676
2025-12-09 12:42:14.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009999952460215409 Training loss: 7.79241943359375
2025-12-09 12:42:14.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.009999937907250246 Training loss: 7.763362407684326
2025-12-09 12:42:15.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.009999921413906798 Training loss: 8.298759460449219
2025-12-09 12:42:15.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.009999902980191464 Training loss: 7.489439010620117
2025-12-09 12:42:16.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009999882606111399 Training loss: 8.022537231445312
2025-12-09 12:42:16.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.009999860291674507 Training loss: 7.592624187469482
2025-12-09 12:42:17.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009999836036889453 Training loss: 7.181353569030762
2025-12-09 12:42:17.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009999809841765645 Training loss: 9.04453182220459
2025-12-09 12:42:18.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00999978170631325 Training loss: 8.321451187133789
2025-12-09 12:42:18.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009999751630543188 Training loss: 7.604058742523193
2025-12-09 12:42:19.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00999971961446713 Training loss: 7.5996503829956055
2025-12-09 12:42:19.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.009999685658097501 Training loss: 7.66883659362793
2025-12-09 12:42:20.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.009999649761447477 Training loss: 7.743205547332764
2025-12-09 12:42:20.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009999611924530994 Training loss: 7.4949445724487305
2025-12-09 12:42:21.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.00999957214736273 Training loss: 7.221380710601807
2025-12-09 12:42:21.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.009999530429958124 Training loss: 7.519382953643799
2025-12-09 12:42:22.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.009999486772333366 Training loss: 7.547115325927734
2025-12-09 12:42:22.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0099994411745054 Training loss: 8.369566917419434
2025-12-09 12:42:23.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.009999393636491919 Training loss: 7.47849178314209
2025-12-09 12:42:23.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00999934415831137 Training loss: 7.774749279022217
2025-12-09 12:42:24.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009999292739982958 Training loss: 7.326566219329834
2025-12-09 12:42:24.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009999239381526638 Training loss: 8.2337064743042
2025-12-09 12:42:25.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009999184082963117 Training loss: 7.713865756988525
2025-12-09 12:42:25.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009999126844313852 Training loss: 7.628945350646973
2025-12-09 12:42:26.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00999906766560106 Training loss: 7.804828643798828
2025-12-09 12:42:26.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009999006546847707 Training loss: 7.23618745803833
2025-12-09 12:42:27.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.009998943488077507 Training loss: 7.083462238311768
2025-12-09 12:42:27.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009998878489314937 Training loss: 7.491841793060303
2025-12-09 12:42:28.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009998811550585221 Training loss: 7.698217868804932
2025-12-09 12:42:28.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009998742671914335 Training loss: 7.309119701385498
2025-12-09 12:42:29.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.00999867185332901 Training loss: 6.936102390289307
2025-12-09 12:42:29.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00999859909485673 Training loss: 7.9234771728515625
2025-12-09 12:42:30.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00999852439652573 Training loss: 7.16434907913208
2025-12-09 12:42:30.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009998447758365002 Training loss: 7.664010047912598
2025-12-09 12:42:31.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009998369180404282 Training loss: 7.658667087554932
2025-12-09 12:42:31.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00999828866267407 Training loss: 7.40318489074707
2025-12-09 12:42:32.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.00999820620520561 Training loss: 7.8492631912231445
2025-12-09 12:42:32.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009998121808030905 Training loss: 7.456338405609131
2025-12-09 12:42:33.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.009998035471182706 Training loss: 7.5387959480285645
2025-12-09 12:42:33.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00999794719469452 Training loss: 7.403537273406982
2025-12-09 12:42:34.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.009997856978600603 Training loss: 7.581790924072266
2025-12-09 12:42:34.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009997764822935967 Training loss: 7.718843936920166
2025-12-09 12:42:35.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.009997670727736378 Training loss: 7.856426239013672
2025-12-09 12:42:35.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009997574693038351 Training loss: 7.549262523651123
2025-12-09 12:42:36.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009997476718879152 Training loss: 7.3941802978515625
2025-12-09 12:42:36.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009997376805296809 Training loss: 7.518932342529297
2025-12-09 12:42:37.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009997274952330094 Training loss: 7.105958938598633
2025-12-09 12:42:37.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00999717116001853 Training loss: 7.096723556518555
2025-12-09 12:42:38.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009997065428402403 Training loss: 7.444065093994141
2025-12-09 12:42:38.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009996957757522741 Training loss: 7.703850269317627
2025-12-09 12:42:39.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009996848147421333 Training loss: 7.699076175689697
2025-12-09 12:42:39.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009996736598140715 Training loss: 7.450543403625488
2025-12-09 12:42:40.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.009996623109724174 Training loss: 7.008502960205078
2025-12-09 12:42:40.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.009996507682215754 Training loss: 7.684058666229248
2025-12-09 12:42:41.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.009996390315660254 Training loss: 7.467440128326416
2025-12-09 12:42:41.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.009996271010103216 Training loss: 7.805999279022217
2025-12-09 12:42:42.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.009996149765590946 Training loss: 7.523479461669922
2025-12-09 12:42:42.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00999602658217049 Training loss: 7.330877780914307
2025-12-09 12:42:43.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.009995901459889657 Training loss: 8.091928482055664
2025-12-09 12:42:43.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.009995774398797007 Training loss: 7.746781826019287
2025-12-09 12:42:44.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.009995645398941846 Training loss: 6.613270282745361
2025-12-09 12:42:44.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.009995514460374237 Training loss: 7.839529514312744
2025-12-09 12:42:45.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.009995381583144995 Training loss: 7.408051013946533
2025-12-09 12:42:45.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.009995246767305689 Training loss: 7.421999931335449
2025-12-09 12:42:46.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.009995110012908634 Training loss: 7.431645393371582
2025-12-09 12:42:46.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.009994971320006905 Training loss: 7.3372344970703125
2025-12-09 12:42:47.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.009994830688654326 Training loss: 7.770325660705566
2025-12-09 12:42:47.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.009994688118905472 Training loss: 7.365876197814941
2025-12-09 12:42:48.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.009994543610815672 Training loss: 7.358321189880371
2025-12-09 12:42:48.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.009994397164441006 Training loss: 7.200776100158691
2025-12-09 12:42:49.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.009994248779838311 Training loss: 6.917827606201172
2025-12-09 12:42:49.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.009994098457065167 Training loss: 7.186360836029053
2025-12-09 12:42:50.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.009993946196179913 Training loss: 7.120929718017578
2025-12-09 12:42:50.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.009993791997241638 Training loss: 7.361905097961426
2025-12-09 12:42:51.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.009993635860310187 Training loss: 7.424858570098877
2025-12-09 12:42:51.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00999347778544615 Training loss: 7.161836624145508
2025-12-09 12:42:52.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.009993317772710874 Training loss: 7.08610200881958
2025-12-09 12:42:52.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.009993155822166457 Training loss: 7.236615180969238
2025-12-09 12:42:53.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.009992991933875747 Training loss: 7.594260215759277
2025-12-09 12:42:53.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.009992826107902348 Training loss: 7.166801452636719
2025-12-09 12:42:54.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.009992658344310614 Training loss: 7.0483269691467285
2025-12-09 12:42:54.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00999248864316565 Training loss: 6.895529747009277
2025-12-09 12:42:55.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.009992317004533314 Training loss: 7.575483322143555
2025-12-09 12:42:55.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.009992143428480213 Training loss: 7.165027618408203
2025-12-09 12:42:56.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.009991967915073714 Training loss: 7.417134761810303
2025-12-09 12:42:56.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.009991790464381926 Training loss: 7.196866989135742
2025-12-09 12:42:57.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.009991611076473714 Training loss: 6.968716144561768
2025-12-09 12:42:57.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.009991429751418698 Training loss: 7.555809497833252
2025-12-09 12:42:58.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.009991246489287245 Training loss: 7.0888190269470215
2025-12-09 12:42:58.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.009991061290150474 Training loss: 6.847196102142334
2025-12-09 12:42:59.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.009990874154080258 Training loss: 7.640418529510498
2025-12-09 12:42:59.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.009990685081149222 Training loss: 7.093740463256836
2025-12-09 12:43:00.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.009990494071430742 Training loss: 7.166308879852295
2025-12-09 12:43:00.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.009990301124998944 Training loss: 6.607621669769287
2025-12-09 12:43:01.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.009990106241928705 Training loss: 6.873034954071045
2025-12-09 12:43:01.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.009989909422295658 Training loss: 7.2411932945251465
2025-12-09 12:43:02.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.009989710666176184 Training loss: 6.900091171264648
2025-12-09 12:43:02.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.009989509973647417 Training loss: 6.674678802490234
2025-12-09 12:43:03.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.009989307344787242 Training loss: 7.245971202850342
2025-12-09 12:43:03.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.009989102779674294 Training loss: 7.068427562713623
2025-12-09 12:43:04.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00998889627838796 Training loss: 6.8079142570495605
2025-12-09 12:43:04.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00998868784100838 Training loss: 7.071592330932617
2025-12-09 12:43:05.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.009988477467616446 Training loss: 7.181904315948486
2025-12-09 12:43:05.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0099882651582938 Training loss: 7.025269508361816
2025-12-09 12:43:06.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.009988050913122831 Training loss: 7.837571144104004
2025-12-09 12:43:06.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.009987834732186687 Training loss: 7.479762554168701
2025-12-09 12:43:07.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.009987616615569263 Training loss: 7.270496368408203
2025-12-09 12:43:07.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.009987396563355204 Training loss: 7.113997459411621
2025-12-09 12:43:08.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.009987174575629911 Training loss: 7.690854549407959
2025-12-09 12:43:08.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.009986950652479532 Training loss: 7.0354390144348145
2025-12-09 12:43:09.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.009986724793990967 Training loss: 6.854018211364746
2025-12-09 12:43:09.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.009986497000251867 Training loss: 6.821666240692139
2025-12-09 12:43:10.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.009986267271350633 Training loss: 7.497819900512695
2025-12-09 12:43:10.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.00998603560737642 Training loss: 6.860793113708496
2025-12-09 12:43:11.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.009985802008419132 Training loss: 6.903954982757568
2025-12-09 12:43:11.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.009985566474569425 Training loss: 6.870201587677002
2025-12-09 12:43:12.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.009985329005918702 Training loss: 7.027231693267822
2025-12-09 12:43:12.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.009985089602559125 Training loss: 7.270394802093506
2025-12-09 12:43:13.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.009984848264583597 Training loss: 7.13750696182251
2025-12-09 12:43:13.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00998460499208578 Training loss: 7.153573989868164
2025-12-09 12:43:14.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00998435978516008 Training loss: 7.6044020652771
2025-12-09 12:43:14.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00998411264390166 Training loss: 7.5101189613342285
2025-12-09 12:43:15.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.009983863568406429 Training loss: 7.171511173248291
2025-12-09 12:43:15.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.009983612558771048 Training loss: 7.186981678009033
2025-12-09 12:43:16.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00998335961509293 Training loss: 7.219714641571045
2025-12-09 12:43:16.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.009983104737470239 Training loss: 7.060098171234131
2025-12-09 12:43:17.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.009982847926001886 Training loss: 7.315389633178711
2025-12-09 12:43:17.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.009982589180787534 Training loss: 7.050279140472412
2025-12-09 12:43:18.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.009982328501927597 Training loss: 7.318639278411865
2025-12-09 12:43:18.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.009982065889523242 Training loss: 7.119284629821777
2025-12-09 12:43:19.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00998180134367638 Training loss: 7.187157154083252
2025-12-09 12:43:19.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.009981534864489678 Training loss: 6.820873260498047
2025-12-09 12:43:20.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.009981266452066553 Training loss: 7.1109819412231445
2025-12-09 12:43:20.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.009980996106511169 Training loss: 7.395974159240723
2025-12-09 12:43:21.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.009980723827928441 Training loss: 7.428614139556885
2025-12-09 12:43:21.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.009980449616424037 Training loss: 6.603586196899414
2025-12-09 12:43:22.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00998017347210437 Training loss: 7.666954517364502
2025-12-09 12:43:22.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.009979895395076608 Training loss: 7.033082485198975
2025-12-09 12:43:23.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.009979615385448668 Training loss: 7.113259792327881
2025-12-09 12:43:23.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.009979333443329217 Training loss: 7.45045280456543
2025-12-09 12:43:24.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00997904956882767 Training loss: 7.145931243896484
2025-12-09 12:43:24.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.009978763762054192 Training loss: 6.697127819061279
2025-12-09 12:43:25.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0099784760231197 Training loss: 7.678164005279541
2025-12-09 12:43:25.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.00997818635213586 Training loss: 7.484626293182373
2025-12-09 12:43:26.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.009977894749215089 Training loss: 7.4503912925720215
2025-12-09 12:43:26.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00997760121447055 Training loss: 7.224595546722412
2025-12-09 12:43:27.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.009977305748016158 Training loss: 7.292801856994629
2025-12-09 12:43:27.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00997700834996658 Training loss: 6.854926109313965
2025-12-09 12:43:28.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.009976709020437229 Training loss: 7.494583606719971
2025-12-09 12:43:28.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.00997640775954427 Training loss: 8.171175956726074
2025-12-09 12:43:29.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.009976104567404616 Training loss: 7.103387355804443
2025-12-09 12:43:29.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.009975799444135928 Training loss: 7.291759014129639
2025-12-09 12:43:30.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.009975492389856622 Training loss: 7.233290672302246
2025-12-09 12:43:30.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.009975183404685856 Training loss: 7.309473037719727
2025-12-09 12:43:31.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.009974872488743543 Training loss: 7.269880294799805
2025-12-09 12:43:31.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.009974559642150344 Training loss: 6.976792812347412
2025-12-09 12:43:32.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.009974244865027668 Training loss: 7.023744583129883
2025-12-09 12:43:32.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.009973928157497673 Training loss: 7.247967720031738
2025-12-09 12:43:33.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.009973609519683268 Training loss: 6.443594932556152
2025-12-09 12:43:33.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.009973288951708112 Training loss: 7.238213539123535
2025-12-09 12:43:34.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.009972966453696608 Training loss: 7.095010757446289
2025-12-09 12:43:34.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.009972642025773911 Training loss: 7.194639205932617
2025-12-09 12:43:35.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.009972315668065928 Training loss: 7.279710292816162
2025-12-09 12:43:35.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00997198738069931 Training loss: 7.349696636199951
2025-12-09 12:43:36.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.009971657163801459 Training loss: 7.208883285522461
2025-12-09 12:43:36.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.009971325017500525 Training loss: 6.739727973937988
2025-12-09 12:43:37.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00997099094192541 Training loss: 6.992547988891602
2025-12-09 12:43:37.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.009970654937205762 Training loss: 7.162473201751709
2025-12-09 12:43:38.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.009970317003471976 Training loss: 7.396735668182373
2025-12-09 12:43:38.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.009969977140855197 Training loss: 7.509301662445068
2025-12-09 12:43:39.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.009969635349487322 Training loss: 6.751310348510742
2025-12-09 12:43:39.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.009969291629500991 Training loss: 7.292081832885742
2025-12-09 12:43:40.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.009968945981029596 Training loss: 7.8093581199646
2025-12-09 12:43:40.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.009968598404207276 Training loss: 7.492965221405029
2025-12-09 12:43:41.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.009968248899168919 Training loss: 7.130009651184082
2025-12-09 12:43:41.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.00996789746605016 Training loss: 7.164113521575928
2025-12-09 12:43:42.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.009967544104987387 Training loss: 7.726715087890625
2025-12-09 12:43:42.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.009967188816117727 Training loss: 7.7236104011535645
2025-12-09 12:43:43.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.009966831599579066 Training loss: 7.127745628356934
2025-12-09 12:43:43.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00996647245551003 Training loss: 7.321977615356445
2025-12-09 12:43:44.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.009966111384049996 Training loss: 7.045519828796387
2025-12-09 12:43:44.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.009965748385339089 Training loss: 7.179984092712402
2025-12-09 12:43:45.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.00996538345951818 Training loss: 7.23288106918335
2025-12-09 12:43:45.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.009965016606728895 Training loss: 6.952159404754639
2025-12-09 12:43:46.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.009964647827113595 Training loss: 7.306218147277832
2025-12-09 12:43:46.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.009964277120815402 Training loss: 7.350615501403809
2025-12-09 12:43:47.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.009963904487978178 Training loss: 7.298469066619873
2025-12-09 12:43:47.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.009963529928746533 Training loss: 7.178114891052246
2025-12-09 12:43:48.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.009963153443265827 Training loss: 6.801499843597412
2025-12-09 12:43:48.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.00996277503168217 Training loss: 7.510593891143799
2025-12-09 12:43:49.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00996239469414241 Training loss: 7.242609977722168
2025-12-09 12:43:49.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.009962012430794153 Training loss: 7.951026439666748
2025-12-09 12:43:50.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.009961628241785746 Training loss: 7.192435264587402
2025-12-09 12:43:50.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.009961242127266288 Training loss: 7.265941619873047
2025-12-09 12:43:51.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.009960854087385618 Training loss: 7.19232702255249
2025-12-09 12:43:51.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.00996046412229433 Training loss: 7.035221576690674
2025-12-09 12:43:52.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.009960072232143761 Training loss: 7.27801513671875
2025-12-09 12:43:52.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.009959678417085997 Training loss: 6.87296724319458
2025-12-09 12:43:53.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.009959282677273869 Training loss: 7.468862056732178
2025-12-09 12:43:53.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.009958885012860954 Training loss: 6.946866035461426
2025-12-09 12:43:54.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.009958485424001582 Training loss: 7.359542369842529
2025-12-09 12:43:54.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.009958083910850821 Training loss: 7.1967973709106445
2025-12-09 12:43:55.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.009957680473564495 Training loss: 7.247198581695557
2025-12-09 12:43:55.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.009957275112299165 Training loss: 7.0409393310546875
2025-12-09 12:43:56.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.009956867827212149 Training loss: 7.179038047790527
2025-12-09 12:43:56.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.009956458618461502 Training loss: 7.09916877746582
2025-12-09 12:43:57.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.009956047486206033 Training loss: 7.0428619384765625
2025-12-09 12:43:57.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.00995563443060529 Training loss: 6.683989524841309
2025-12-09 12:43:58.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00995521945181958 Training loss: 7.401625156402588
2025-12-09 12:43:58.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00995480255000994 Training loss: 6.920444965362549
2025-12-09 12:43:59.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.009954383725338167 Training loss: 7.248817443847656
2025-12-09 12:43:59.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.009953962977966795 Training loss: 6.999892234802246
2025-12-09 12:44:00.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00995354030805911 Training loss: 7.043951034545898
2025-12-09 12:44:00.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.009953115715779141 Training loss: 7.037802219390869
2025-12-09 12:44:01.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.009952689201291663 Training loss: 6.550197601318359
2025-12-09 12:44:01.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0099522607647622 Training loss: 7.1680006980896
2025-12-09 12:44:02.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00995183040635702 Training loss: 6.845522403717041
2025-12-09 12:44:02.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.009951398126243134 Training loss: 7.282987594604492
2025-12-09 12:44:03.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.009950963924588304 Training loss: 7.004817008972168
2025-12-09 12:44:03.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.009950527801561034 Training loss: 7.416269302368164
2025-12-09 12:44:04.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.009950089757330574 Training loss: 7.126711845397949
2025-12-09 12:44:04.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.009949649792066922 Training loss: 6.766808032989502
2025-12-09 12:44:05.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00994920790594082 Training loss: 7.107395172119141
2025-12-09 12:44:05.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.009948764099123755 Training loss: 7.116005897521973
2025-12-09 12:44:06.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.00994831837178796 Training loss: 7.015739917755127
2025-12-09 12:44:06.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.009947870724106411 Training loss: 6.648236274719238
2025-12-09 12:44:07.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.009947421156252837 Training loss: 7.218929767608643
2025-12-09 12:44:07.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.009946969668401697 Training loss: 6.95290994644165
2025-12-09 12:44:08.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.009946516260728214 Training loss: 6.673097133636475
2025-12-09 12:44:08.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.009946060933408342 Training loss: 6.702050685882568
2025-12-09 12:44:09.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.009945603686618785 Training loss: 7.226283073425293
2025-12-09 12:44:09.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.009945144520536991 Training loss: 7.263240337371826
2025-12-09 12:44:10.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.009944683435341155 Training loss: 6.702829360961914
2025-12-09 12:44:10.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.009944220431210215 Training loss: 7.007819175720215
2025-12-09 12:44:11.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.009943755508323854 Training loss: 7.232687950134277
2025-12-09 12:44:11.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.009943288666862497 Training loss: 6.909900188446045
2025-12-09 12:44:12.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.00994281990700732 Training loss: 7.074356555938721
2025-12-09 12:44:12.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.009942349228940238 Training loss: 6.79145622253418
2025-12-09 12:44:13.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00994187663284391 Training loss: 6.665388584136963
2025-12-09 12:44:13.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.009941402118901743 Training loss: 7.210897922515869
2025-12-09 12:44:14.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.009940925687297887 Training loss: 7.2491068840026855
2025-12-09 12:44:14.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.009940447338217234 Training loss: 7.480123043060303
2025-12-09 12:44:15.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.009939967071845425 Training loss: 6.401063442230225
2025-12-09 12:44:15.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.009939484888368837 Training loss: 7.444952011108398
2025-12-09 12:44:16.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.009939000787974602 Training loss: 7.498937129974365
2025-12-09 12:44:16.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.009938514770850585 Training loss: 7.168690204620361
2025-12-09 12:44:17.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.009938026837185403 Training loss: 7.241108417510986
2025-12-09 12:44:17.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.009937536987168413 Training loss: 7.098414421081543
2025-12-09 12:44:18.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.009937045220989716 Training loss: 7.327215194702148
2025-12-09 12:44:18.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.009936551538840153 Training loss: 6.772062301635742
2025-12-09 12:44:19.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.009936055940911319 Training loss: 6.914669513702393
2025-12-09 12:44:19.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.009935558427395541 Training loss: 6.795671463012695
2025-12-09 12:44:20.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.009935058998485898 Training loss: 6.982972145080566
2025-12-09 12:44:20.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.009934557654376204 Training loss: 7.222626686096191
2025-12-09 12:44:21.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.009934054395261025 Training loss: 7.170962333679199
2025-12-09 12:44:21.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.009933549221335665 Training loss: 6.899124622344971
2025-12-09 12:44:22.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.009933042132796171 Training loss: 6.897152900695801
2025-12-09 12:44:22.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.009932533129839334 Training loss: 7.114818572998047
2025-12-09 12:44:23.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00993202221266269 Training loss: 6.715932846069336
2025-12-09 12:44:23.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.009931509381464514 Training loss: 7.302417755126953
2025-12-09 12:44:24.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.009930994636443828 Training loss: 6.840936660766602
2025-12-09 12:44:24.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.009930477977800391 Training loss: 6.763546943664551
2025-12-09 12:44:25.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.009929959405734712 Training loss: 7.349926948547363
2025-12-09 12:44:25.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.009929438920448038 Training loss: 7.017276287078857
2025-12-09 12:44:26.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.009928916522142357 Training loss: 6.900688171386719
2025-12-09 12:44:26.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0099283922110204 Training loss: 7.132015705108643
2025-12-09 12:44:27.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.009927865987285648 Training loss: 7.50365686416626
2025-12-09 12:44:27.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.009927337851142314 Training loss: 7.870297908782959
2025-12-09 12:44:28.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.009926807802795359 Training loss: 6.882498264312744
2025-12-09 12:44:28.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.009926275842450481 Training loss: 6.968447685241699
2025-12-09 12:44:29.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.009925741970314128 Training loss: 6.895899772644043
2025-12-09 12:44:29.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.009925206186593483 Training loss: 7.0632643699646
2025-12-09 12:44:30.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.009924668491496473 Training loss: 7.025409698486328
2025-12-09 12:44:30.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.009924128885231769 Training loss: 7.281083583831787
2025-12-09 12:44:31.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.009923587368008779 Training loss: 6.960066318511963
2025-12-09 12:44:31.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.009923043940037657 Training loss: 7.231663227081299
2025-12-09 12:44:32.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.009922498601529295 Training loss: 6.651087284088135
2025-12-09 12:44:32.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.00992195135269533 Training loss: 6.757845401763916
2025-12-09 12:44:33.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.009921402193748138 Training loss: 6.989624500274658
2025-12-09 12:44:33.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.009920851124900838 Training loss: 6.702486038208008
2025-12-09 12:44:34.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.009920298146367286 Training loss: 6.913912296295166
2025-12-09 12:44:34.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.009919743258362085 Training loss: 8.019289016723633
2025-12-09 12:44:35.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.009919186461100576 Training loss: 7.129539966583252
2025-12-09 12:44:35.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.009918627754798839 Training loss: 7.040722370147705
2025-12-09 12:44:36.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0099180671396737 Training loss: 6.7806572914123535
2025-12-09 12:44:36.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.009917504615942721 Training loss: 6.9883880615234375
2025-12-09 12:44:37.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.009916940183824205 Training loss: 6.780886173248291
2025-12-09 12:44:37.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.009916373843537201 Training loss: 7.033484935760498
2025-12-09 12:44:38.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.009915805595301492 Training loss: 6.858616828918457
2025-12-09 12:44:38.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.009915235439337602 Training loss: 7.096217155456543
2025-12-09 12:44:39.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.009914663375866804 Training loss: 7.0482306480407715
2025-12-09 12:44:39.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.009914089405111097 Training loss: 6.995991230010986
2025-12-09 12:44:40.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.009913513527293234 Training loss: 6.93223762512207
2025-12-09 12:44:40.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.009912935742636698 Training loss: 7.169092655181885
2025-12-09 12:44:41.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.009912356051365718 Training loss: 7.5624003410339355
2025-12-09 12:44:41.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.009911774453705257 Training loss: 6.992119312286377
2025-12-09 12:44:42.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00991119094988103 Training loss: 6.204849720001221
2025-12-09 12:44:42.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.009910605540119475 Training loss: 6.128866672515869
2025-12-09 12:44:43.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.009910018224647781 Training loss: 8.966459274291992
2025-12-09 12:44:43.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.009909429003693876 Training loss: 7.048325538635254
2025-12-09 12:44:44.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.009908837877486422 Training loss: 7.598023891448975
2025-12-09 12:44:44.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.009908244846254825 Training loss: 7.130185604095459
2025-12-09 12:44:45.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.009907649910229228 Training loss: 7.201645374298096
2025-12-09 12:44:45.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.009907053069640516 Training loss: 6.812040328979492
2025-12-09 12:44:46.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.009906454324720308 Training loss: 6.911349296569824
2025-12-09 12:44:46.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.009905853675700968 Training loss: 7.606995582580566
2025-12-09 12:44:47.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.009905251122815597 Training loss: 7.1157307624816895
2025-12-09 12:44:47.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00990464666629803 Training loss: 7.215779781341553
2025-12-09 12:44:48.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.009904040306382847 Training loss: 7.004698753356934
2025-12-09 12:44:48.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.009903432043305365 Training loss: 6.808079719543457
2025-12-09 12:44:49.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.009902821877301638 Training loss: 7.077596187591553
2025-12-09 12:44:49.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00990220980860846 Training loss: 7.117050647735596
2025-12-09 12:44:50.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.009901595837463363 Training loss: 7.057714939117432
2025-12-09 12:44:50.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.009900979964104618 Training loss: 7.130357265472412
2025-12-09 12:44:51.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00990036218877123 Training loss: 6.7648725509643555
2025-12-09 12:44:51.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.00989974251170295 Training loss: 7.730785846710205
2025-12-09 12:44:52.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00989912093314026 Training loss: 7.043667793273926
2025-12-09 12:44:52.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.009898497453324384 Training loss: 6.979488372802734
2025-12-09 12:44:53.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.009897872072497281 Training loss: 7.548293113708496
2025-12-09 12:44:53.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.00989724479090165 Training loss: 6.485877513885498
2025-12-09 12:44:54.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.009896615608780924 Training loss: 7.247476577758789
2025-12-09 12:44:54.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.00989598452637928 Training loss: 7.548773288726807
2025-12-09 12:44:55.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.009895351543941628 Training loss: 7.258933067321777
2025-12-09 12:44:55.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.009894716661713616 Training loss: 7.020913600921631
2025-12-09 12:44:56.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.009894079879941628 Training loss: 6.778873920440674
2025-12-09 12:44:56.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.009893441198872787 Training loss: 6.958953857421875
2025-12-09 12:44:57.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.009892800618754954 Training loss: 6.836483001708984
2025-12-09 12:44:57.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.009892158139836724 Training loss: 6.834100723266602
2025-12-09 12:44:58.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.00989151376236743 Training loss: 7.409610271453857
2025-12-09 12:44:58.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.009890867486597146 Training loss: 7.246042251586914
2025-12-09 12:44:59.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.009890219312776677 Training loss: 7.029992580413818
2025-12-09 12:44:59.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.009889569241157564 Training loss: 6.817840099334717
2025-12-09 12:45:00.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.00988891727199209 Training loss: 7.0679240226745605
2025-12-09 12:45:00.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.009888263405533271 Training loss: 6.930448532104492
2025-12-09 12:45:01.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.009887607642034859 Training loss: 7.0561137199401855
2025-12-09 12:45:01.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.009886949981751346 Training loss: 6.794604778289795
2025-12-09 12:45:02.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.009886290424937952 Training loss: 6.894570827484131
2025-12-09 12:45:02.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.009885628971850642 Training loss: 8.138226509094238
2025-12-09 12:45:03.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.009884965622746112 Training loss: 7.922671794891357
2025-12-09 12:45:03.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.009884300377881794 Training loss: 7.7036614418029785
2025-12-09 12:45:04.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.009883633237515857 Training loss: 7.2094407081604
2025-12-09 12:45:04.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.009882964201907207 Training loss: 7.157731533050537
2025-12-09 12:45:05.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.00988229327131548 Training loss: 6.697330951690674
2025-12-09 12:45:05.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.009881620446001056 Training loss: 6.984570503234863
2025-12-09 12:45:06.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.00988094572622504 Training loss: 6.028395175933838
2025-12-09 12:45:06.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.00988026911224928 Training loss: 7.10154390335083
2025-12-09 12:45:07.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.00987959060433636 Training loss: 6.8543853759765625
2025-12-09 12:45:07.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.009878910202749589 Training loss: 6.771678924560547
2025-12-09 12:45:08.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.009878227907753022 Training loss: 7.785422325134277
2025-12-09 12:45:08.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.009877543719611444 Training loss: 7.689386367797852
2025-12-09 12:45:09.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.009876857638590373 Training loss: 7.7121148109436035
2025-12-09 12:45:09.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.009876169664956067 Training loss: 6.9222187995910645
2025-12-09 12:45:10.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.009875479798975512 Training loss: 6.841021537780762
2025-12-09 12:45:10.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.009874788040916432 Training loss: 6.775393486022949
2025-12-09 12:45:11.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.009874094391047288 Training loss: 8.094632148742676
2025-12-09 12:45:11.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.009873398849637267 Training loss: 6.784125804901123
2025-12-09 12:45:12.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.009872701416956299 Training loss: 6.944911003112793
2025-12-09 12:45:12.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.009872002093275042 Training loss: 7.2089762687683105
2025-12-09 12:45:13.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.00987130087886489 Training loss: 7.846539497375488
2025-12-09 12:45:13.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.009870597773997972 Training loss: 6.784156322479248
2025-12-09 12:45:14.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.009869892778947148 Training loss: 6.9719319343566895
2025-12-09 12:45:14.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.009869185893986013 Training loss: 7.23166561126709
2025-12-09 12:45:15.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.009868477119388895 Training loss: 6.915422439575195
2025-12-09 12:45:15.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.009867766455430856 Training loss: 7.049798965454102
2025-12-09 12:45:16.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.009867053902387693 Training loss: 7.383526802062988
2025-12-09 12:45:16.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.009866339460535929 Training loss: 7.522879600524902
2025-12-09 12:45:17.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.009865623130152828 Training loss: 6.76742696762085
2025-12-09 12:45:17.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.009864904911516384 Training loss: 6.900379180908203
2025-12-09 12:45:18.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.009864184804905323 Training loss: 7.574166297912598
2025-12-09 12:45:18.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.009863462810599103 Training loss: 6.989035606384277
2025-12-09 12:45:19.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.009862738928877922 Training loss: 6.883431911468506
2025-12-09 12:45:19.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.009862013160022696 Training loss: 6.795194149017334
2025-12-09 12:45:20.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.009861285504315085 Training loss: 7.415529251098633
2025-12-09 12:45:20.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00986055596203748 Training loss: 7.345175266265869
2025-12-09 12:45:21.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.009859824533472998 Training loss: 6.899064540863037
2025-12-09 12:45:21.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.009859091218905498 Training loss: 6.876798152923584
2025-12-09 12:45:22.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.00985835601861956 Training loss: 6.860148906707764
2025-12-09 12:45:22.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.009857618932900504 Training loss: 6.679032325744629
2025-12-09 12:45:23.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.009856879962034375 Training loss: 6.904329776763916
2025-12-09 12:45:23.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.009856139106307955 Training loss: 7.067889213562012
2025-12-09 12:45:24.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.009855396366008757 Training loss: 7.481867790222168
2025-12-09 12:45:24.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.009854651741425023 Training loss: 7.074429512023926
2025-12-09 12:45:25.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.009853905232845728 Training loss: 7.068796157836914
2025-12-09 12:45:25.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.009853156840560576 Training loss: 7.398283004760742
2025-12-09 12:45:26.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.009852406564860004 Training loss: 6.976293563842773
2025-12-09 12:45:26.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.009851654406035179 Training loss: 7.004825592041016
2025-12-09 12:45:27.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.009850900364378 Training loss: 6.872819900512695
2025-12-09 12:45:27.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.009850144440181096 Training loss: 6.775216579437256
2025-12-09 12:45:28.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.009849386633737824 Training loss: 6.821646213531494
2025-12-09 12:45:28.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.009848626945342278 Training loss: 6.844836235046387
2025-12-09 12:45:29.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.009847865375289276 Training loss: 6.973506927490234
2025-12-09 12:45:29.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.009847101923874366 Training loss: 7.046770095825195
2025-12-09 12:45:30.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.009846336591393832 Training loss: 7.102224826812744
2025-12-09 12:45:30.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.009845569378144686 Training loss: 7.05117654800415
2025-12-09 12:45:31.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.009844800284424663 Training loss: 6.9765706062316895
2025-12-09 12:45:31.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.00984402931053224 Training loss: 7.59237003326416
2025-12-09 12:45:32.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.009843256456766609 Training loss: 6.805094242095947
2025-12-09 12:45:32.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.009842481723427705 Training loss: 7.033079624176025
2025-12-09 12:45:33.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.009841705110816185 Training loss: 6.671841621398926
2025-12-09 12:45:33.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00984092661923344 Training loss: 7.167298316955566
2025-12-09 12:45:34.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.009840146248981585 Training loss: 6.961639881134033
2025-12-09 12:45:34.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.009839364000363466 Training loss: 6.824223518371582
2025-12-09 12:45:35.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00983857987368266 Training loss: 7.233497619628906
2025-12-09 12:45:35.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.009837793869243468 Training loss: 6.828888893127441
2025-12-09 12:45:36.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.009837005987350926 Training loss: 6.855590343475342
2025-12-09 12:45:36.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.009836216228310797 Training loss: 6.886682033538818
2025-12-09 12:45:37.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.009835424592429568 Training loss: 6.7446441650390625
2025-12-09 12:45:37.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.009834631080014457 Training loss: 7.682192802429199
2025-12-09 12:45:38.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.009833835691373412 Training loss: 7.075412750244141
2025-12-09 12:45:38.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.00983303842681511 Training loss: 6.9431233406066895
2025-12-09 12:45:39.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.009832239286648949 Training loss: 6.69774055480957
2025-12-09 12:45:39.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.009831438271185065 Training loss: 7.452398777008057
2025-12-09 12:45:40.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.009830635380734313 Training loss: 7.036809921264648
2025-12-09 12:45:40.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.009829830615608279 Training loss: 7.163064479827881
2025-12-09 12:45:41.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.009829023976119278 Training loss: 7.057746410369873
2025-12-09 12:45:41.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.009828215462580352 Training loss: 7.280259132385254
2025-12-09 12:45:42.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.009827405075305266 Training loss: 7.139554977416992
2025-12-09 12:45:42.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.009826592814608518 Training loss: 7.105749607086182
2025-12-09 12:45:43.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.00982577868080533 Training loss: 6.689247131347656
2025-12-09 12:45:43.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.009824962674211653 Training loss: 7.355653762817383
2025-12-09 12:45:44.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.009824144795144159 Training loss: 6.792923450469971
2025-12-09 12:45:44.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.009823325043920255 Training loss: 7.354985237121582
2025-12-09 12:45:45.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.009822503420858067 Training loss: 7.209160327911377
2025-12-09 12:45:45.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.009821679926276456 Training loss: 6.657129764556885
2025-12-09 12:45:46.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.009820854560494998 Training loss: 7.073250770568848
2025-12-09 12:45:46.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.009820027323834007 Training loss: 7.09492301940918
2025-12-09 12:45:47.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.009819198216614512 Training loss: 6.9752020835876465
2025-12-09 12:45:47.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.009818367239158278 Training loss: 7.031611919403076
2025-12-09 12:45:48.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.009817534391787787 Training loss: 6.626362323760986
2025-12-09 12:45:48.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.009816699674826256 Training loss: 6.96261739730835
2025-12-09 12:45:49.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.009815863088597618 Training loss: 7.255606174468994
2025-12-09 12:45:49.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.009815024633426537 Training loss: 6.774003982543945
2025-12-09 12:45:50.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0098141843096384 Training loss: 7.1764702796936035
2025-12-09 12:45:50.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.009813342117559323 Training loss: 6.889792442321777
2025-12-09 12:45:51.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.009812498057516142 Training loss: 6.769056797027588
2025-12-09 12:45:51.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.009811652129836422 Training loss: 6.893653392791748
2025-12-09 12:45:52.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.009810804334848449 Training loss: 6.624764442443848
2025-12-09 12:45:52.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.009809954672881238 Training loss: 7.18032169342041
2025-12-09 12:45:53.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.009809103144264524 Training loss: 7.044913291931152
2025-12-09 12:45:53.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.009808249749328769 Training loss: 6.815013408660889
2025-12-09 12:45:54.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.009807394488405159 Training loss: 6.902885437011719
2025-12-09 12:45:54.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.009806537361825607 Training loss: 6.694851398468018
2025-12-09 12:45:55.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.009805678369922742 Training loss: 7.03082275390625
2025-12-09 12:45:55.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.009804817513029926 Training loss: 6.874844551086426
2025-12-09 12:45:56.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.009803954791481238 Training loss: 6.866637706756592
2025-12-09 12:45:56.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.009803090205611487 Training loss: 6.9855170249938965
2025-12-09 12:45:57.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.009802223755756198 Training loss: 7.213197708129883
2025-12-09 12:45:57.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.009801355442251625 Training loss: 6.963052749633789
2025-12-09 12:45:58.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.009800485265434745 Training loss: 6.727146625518799
2025-12-09 12:45:58.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.009799613225643253 Training loss: 6.846096038818359
2025-12-09 12:45:59.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.009798739323215573 Training loss: 6.682992458343506
2025-12-09 12:45:59.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00979786355849085 Training loss: 6.924779415130615
2025-12-09 12:46:00.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.009796985931808949 Training loss: 7.61607027053833
2025-12-09 12:46:00.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.009796106443510462 Training loss: 7.165246486663818
2025-12-09 12:46:01.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.009795225093936702 Training loss: 7.56447696685791
2025-12-09 12:46:01.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.009794341883429699 Training loss: 6.727937698364258
2025-12-09 12:46:02.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.009793456812332214 Training loss: 7.218570232391357
2025-12-09 12:46:02.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.009792569880987725 Training loss: 7.072213649749756
2025-12-09 12:46:03.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.009791681089740432 Training loss: 6.857078552246094
2025-12-09 12:46:03.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.009790790438935257 Training loss: 6.990890026092529
2025-12-09 12:46:04.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.009789897928917846 Training loss: 6.991334915161133
2025-12-09 12:46:04.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.009789003560034561 Training loss: 6.816929817199707
2025-12-09 12:46:05.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.009788107332632493 Training loss: 6.953880786895752
2025-12-09 12:46:05.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.009787209247059453 Training loss: 7.184940338134766
2025-12-09 12:46:06.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.009786309303663962 Training loss: 7.171700477600098
2025-12-09 12:46:06.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.009785407502795277 Training loss: 7.10505485534668
2025-12-09 12:46:07.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.009784503844803368 Training loss: 6.988964080810547
2025-12-09 12:46:07.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.009783598330038924 Training loss: 6.481257915496826
2025-12-09 12:46:08.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.009782690958853361 Training loss: 7.128876686096191
2025-12-09 12:46:08.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.009781781731598813 Training loss: 7.120203495025635
2025-12-09 12:46:09.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.00978087064862813 Training loss: 6.952917575836182
2025-12-09 12:46:09.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.009779957710294886 Training loss: 7.383388519287109
2025-12-09 12:46:10.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.009779042916953376 Training loss: 6.605326175689697
2025-12-09 12:46:10.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.009778126268958612 Training loss: 7.111917972564697
2025-12-09 12:46:11.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.009777207766666329 Training loss: 6.864431858062744
2025-12-09 12:46:11.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.00977628741043298 Training loss: 7.009601593017578
2025-12-09 12:46:12.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.009775365200615735 Training loss: 6.771430492401123
2025-12-09 12:46:12.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.009774441137572488 Training loss: 6.862941265106201
2025-12-09 12:46:13.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.009773515221661847 Training loss: 7.211367130279541
2025-12-09 12:46:13.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.009772587453243142 Training loss: 7.087372779846191
2025-12-09 12:46:14.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.009771657832676426 Training loss: 7.1196489334106445
2025-12-09 12:46:14.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.009770726360322463 Training loss: 7.670738697052002
2025-12-09 12:46:15.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.009769793036542742 Training loss: 6.6284918785095215
2025-12-09 12:46:15.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.009768857861699462 Training loss: 7.503938674926758
2025-12-09 12:46:16.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.009767920836155552 Training loss: 6.690702438354492
2025-12-09 12:46:16.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.009766981960274652 Training loss: 6.508106231689453
2025-12-09 12:46:17.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.009766041234421121 Training loss: 6.947000980377197
2025-12-09 12:46:17.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.009765098658960036 Training loss: 6.8759355545043945
2025-12-09 12:46:18.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.00976415423425719 Training loss: 6.650432109832764
2025-12-09 12:46:18.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0097632079606791 Training loss: 7.093774795532227
2025-12-09 12:46:19.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.009762259838592994 Training loss: 6.955671787261963
2025-12-09 12:46:19.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.009761309868366819 Training loss: 7.328284740447998
2025-12-09 12:46:20.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.009760358050369242 Training loss: 7.22489070892334
2025-12-09 12:46:20.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.009759404384969644 Training loss: 6.639915943145752
2025-12-09 12:46:21.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.009758448872538121 Training loss: 7.117063045501709
2025-12-09 12:46:21.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.009757491513445493 Training loss: 6.846997261047363
2025-12-09 12:46:22.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.009756532308063294 Training loss: 7.3877410888671875
2025-12-09 12:46:22.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.009755571256763764 Training loss: 6.873599529266357
2025-12-09 12:46:23.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.009754608359919878 Training loss: 6.59032678604126
2025-12-09 12:46:23.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.009753643617905313 Training loss: 6.715965270996094
2025-12-09 12:46:24.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.009752677031094465 Training loss: 7.014976501464844
2025-12-09 12:46:24.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.009751708599862451 Training loss: 7.382742881774902
2025-12-09 12:46:25.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.009750738324585098 Training loss: 7.03806209564209
2025-12-09 12:46:25.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.009749766205638952 Training loss: 7.343587398529053
2025-12-09 12:46:26.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.009748792243401274 Training loss: 6.814128398895264
2025-12-09 12:46:26.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.009747816438250036 Training loss: 6.9510650634765625
2025-12-09 12:46:27.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.009746838790563934 Training loss: 7.188531875610352
2025-12-09 12:46:27.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.009745859300722371 Training loss: 7.305503845214844
2025-12-09 12:46:28.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.009744877969105468 Training loss: 6.852449893951416
2025-12-09 12:46:28.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.009743894796094062 Training loss: 6.9766716957092285
2025-12-09 12:46:29.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.009742909782069702 Training loss: 7.937979698181152
2025-12-09 12:46:29.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.009741922927414652 Training loss: 7.1059393882751465
2025-12-09 12:46:30.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.009740934232511893 Training loss: 7.553287982940674
2025-12-09 12:46:30.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.009739943697745118 Training loss: 6.499965190887451
2025-12-09 12:46:31.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.009738951323498732 Training loss: 7.011884689331055
2025-12-09 12:46:31.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.009737957110157859 Training loss: 7.085821628570557
2025-12-09 12:46:32.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.009736961058108331 Training loss: 6.5639753341674805
2025-12-09 12:46:32.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.009735963167736698 Training loss: 6.6356682777404785
2025-12-09 12:46:33.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.009734963439430222 Training loss: 6.263533115386963
2025-12-09 12:46:33.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.009733961873576877 Training loss: 6.536466598510742
2025-12-09 12:46:34.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.009732958470565352 Training loss: 6.434812068939209
2025-12-09 12:46:34.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.009731953230785049 Training loss: 7.029122352600098
2025-12-09 12:46:35.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.009730946154626078 Training loss: 6.695685863494873
2025-12-09 12:46:35.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.00972993724247927 Training loss: 6.680463790893555
2025-12-09 12:46:36.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.009728926494736164 Training loss: 6.623676300048828
2025-12-09 12:46:36.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.009727913911789008 Training loss: 6.5822224617004395
2025-12-09 12:46:37.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.009726899494030768 Training loss: 6.499149322509766
2025-12-09 12:46:37.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.009725883241855119 Training loss: 6.988124847412109
2025-12-09 12:46:38.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.009724865155656449 Training loss: 6.486441135406494
2025-12-09 12:46:38.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.009723845235829857 Training loss: 7.53091287612915
2025-12-09 12:46:39.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.009722823482771155 Training loss: 6.8653669357299805
2025-12-09 12:46:39.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.009721799896876864 Training loss: 6.918842792510986
2025-12-09 12:46:40.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.009720774478544218 Training loss: 7.134929180145264
2025-12-09 12:46:40.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.009719747228171163 Training loss: 6.6206583976745605
2025-12-09 12:46:41.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.009718718146156354 Training loss: 6.876285076141357
2025-12-09 12:46:41.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.00971768723289916 Training loss: 6.899353504180908
2025-12-09 12:46:42.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.009716654488799652 Training loss: 6.819641590118408
2025-12-09 12:46:42.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.009715619914258624 Training loss: 6.8068037033081055
2025-12-09 12:46:43.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00971458350967757 Training loss: 6.964560031890869
2025-12-09 12:46:43.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.009713545275458703 Training loss: 7.100711345672607
2025-12-09 12:46:44.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.009712505212004938 Training loss: 6.286995887756348
2025-12-09 12:46:44.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.009711463319719903 Training loss: 6.906829357147217
2025-12-09 12:46:45.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.009710419599007938 Training loss: 6.850066661834717
2025-12-09 12:46:45.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.009709374050274088 Training loss: 6.973238468170166
2025-12-09 12:46:46.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.009708326673924114 Training loss: 7.4830522537231445
2025-12-09 12:46:46.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.009707277470364482 Training loss: 7.652136325836182
2025-12-09 12:46:47.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.009706226440002363 Training loss: 6.757518291473389
2025-12-09 12:46:47.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.009705173583245644 Training loss: 7.340140342712402
2025-12-09 12:46:48.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.009704118900502918 Training loss: 7.601779937744141
2025-12-09 12:46:48.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.009703062392183489 Training loss: 6.8100481033325195
2025-12-09 12:46:49.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.009702004058697363 Training loss: 6.849939823150635
2025-12-09 12:46:49.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.00970094390045526 Training loss: 6.528460502624512
2025-12-09 12:46:50.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00969988191786861 Training loss: 6.7826080322265625
2025-12-09 12:46:50.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.009698818111349544 Training loss: 7.063129901885986
2025-12-09 12:46:51.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.009697752481310905 Training loss: 6.7569193840026855
2025-12-09 12:46:51.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.009696685028166244 Training loss: 6.804859638214111
2025-12-09 12:46:52.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.00969561575232982 Training loss: 7.349950313568115
2025-12-09 12:46:52.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.009694544654216595 Training loss: 7.048714637756348
2025-12-09 12:46:53.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.009693471734242244 Training loss: 7.397242069244385
2025-12-09 12:46:53.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.009692396992823146 Training loss: 6.779159069061279
2025-12-09 12:46:54.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.009691320430376385 Training loss: 6.604461669921875
2025-12-09 12:46:54.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.009690242047319756 Training loss: 7.05154275894165
2025-12-09 12:46:55.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.009689161844071757 Training loss: 7.159306049346924
2025-12-09 12:46:55.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.009688079821051594 Training loss: 6.9615559577941895
2025-12-09 12:46:56.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.009686995978679181 Training loss: 6.80505895614624
2025-12-09 12:46:56.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.009685910317375132 Training loss: 6.794353485107422
2025-12-09 12:46:57.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.009684822837560777 Training loss: 7.0748066902160645
2025-12-09 12:46:57.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.00968373353965814 Training loss: 6.898841381072998
2025-12-09 12:46:58.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.009682642424089958 Training loss: 6.772862434387207
2025-12-09 12:46:58.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.009681549491279673 Training loss: 7.249188423156738
2025-12-09 12:46:59.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.00968045474165143 Training loss: 6.85060453414917
2025-12-09 12:46:59.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.00967935817563008 Training loss: 6.699539661407471
2025-12-09 12:47:00.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00967825979364118 Training loss: 6.208042144775391
2025-12-09 12:47:00.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.009677159596110986 Training loss: 6.649138927459717
2025-12-09 12:47:01.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.009676057583466471 Training loss: 7.252166748046875
2025-12-09 12:47:01.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.009674953756135297 Training loss: 7.106296539306641
2025-12-09 12:47:02.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.009673848114545842 Training loss: 7.241261005401611
2025-12-09 12:47:02.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.009672740659127184 Training loss: 7.276160717010498
2025-12-09 12:47:03.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.009671631390309103 Training loss: 6.9707350730896
2025-12-09 12:47:03.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.009670520308522083 Training loss: 7.080225467681885
2025-12-09 12:47:04.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.009669407414197318 Training loss: 6.103435039520264
2025-12-09 12:47:04.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.009668292707766698 Training loss: 7.082108497619629
2025-12-09 12:47:05.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.009667176189662818 Training loss: 6.2090044021606445
2025-12-09 12:47:05.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.009666057860318978 Training loss: 6.79522705078125
2025-12-09 12:47:06.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00966493772016918 Training loss: 6.930951118469238
2025-12-09 12:47:06.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.009663815769648127 Training loss: 7.07249641418457
2025-12-09 12:47:07.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00966269200919123 Training loss: 6.803319931030273
2025-12-09 12:47:07.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.009661566439234593 Training loss: 7.294331073760986
2025-12-09 12:47:08.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.00966043906021503 Training loss: 7.429880619049072
2025-12-09 12:47:08.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.009659309872570057 Training loss: 6.696549892425537
2025-12-09 12:47:09.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.009658178876737887 Training loss: 6.543484687805176
2025-12-09 12:47:09.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.009657046073157436 Training loss: 7.347438335418701
2025-12-09 12:47:10.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.009655911462268327 Training loss: 6.931416988372803
2025-12-09 12:47:10.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.00965477504451088 Training loss: 6.827164649963379
2025-12-09 12:47:11.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.009653636820326113 Training loss: 6.814478397369385
2025-12-09 12:47:11.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.009652496790155752 Training loss: 6.763557434082031
2025-12-09 12:47:12.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.009651354954442217 Training loss: 6.743243217468262
2025-12-09 12:47:12.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.009650211313628636 Training loss: 6.577255725860596
2025-12-09 12:47:13.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.009649065868158831 Training loss: 6.5556111335754395
2025-12-09 12:47:13.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.009647918618477328 Training loss: 6.799378395080566
2025-12-09 12:47:14.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.009646769565029354 Training loss: 6.710133075714111
2025-12-09 12:47:14.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00964561870826083 Training loss: 6.859903335571289
2025-12-09 12:47:15.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.009644466048618386 Training loss: 6.303062915802002
2025-12-09 12:47:15.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.009643311586549342 Training loss: 6.769557476043701
2025-12-09 12:47:16.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.009642155322501724 Training loss: 6.9284868240356445
2025-12-09 12:47:16.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.009640997256924256 Training loss: 7.220927715301514
2025-12-09 12:47:17.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.009639837390266361 Training loss: 7.465236663818359
2025-12-09 12:47:17.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.009638675722978161 Training loss: 6.917313098907471
2025-12-09 12:47:18.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.009637512255510475 Training loss: 6.930831432342529
2025-12-09 12:47:18.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.009636346988314821 Training loss: 6.9777984619140625
2025-12-09 12:47:19.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.009635179921843418 Training loss: 7.504559516906738
2025-12-09 12:47:19.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.009634011056549182 Training loss: 6.980488300323486
2025-12-09 12:47:20.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.009632840392885726 Training loss: 6.70923376083374
2025-12-09 12:47:20.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.009631667931307365 Training loss: 6.970551013946533
2025-12-09 12:47:21.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.009630493672269102 Training loss: 6.969278335571289
2025-12-09 12:47:21.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.009629317616226648 Training loss: 6.9733357429504395
2025-12-09 12:47:22.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.009628139763636408 Training loss: 7.232212066650391
2025-12-09 12:47:22.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.009626960114955483 Training loss: 7.351742744445801
2025-12-09 12:47:23.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.009625778670641669 Training loss: 7.0244951248168945
2025-12-09 12:47:23.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.009624595431153467 Training loss: 7.823200225830078
2025-12-09 12:47:24.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.009623410396950064 Training loss: 6.690030097961426
2025-12-09 12:47:24.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.009622223568491349 Training loss: 7.730260848999023
2025-12-09 12:47:25.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.00962103494623791 Training loss: 6.795243740081787
2025-12-09 12:47:25.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.009619844530651026 Training loss: 6.926873683929443
2025-12-09 12:47:26.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.009618652322192675 Training loss: 6.904688358306885
2025-12-09 12:47:26.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.00961745832132553 Training loss: 6.86143684387207
2025-12-09 12:47:27.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.009616262528512956 Training loss: 7.063655853271484
2025-12-09 12:47:27.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.009615064944219022 Training loss: 7.114069938659668
2025-12-09 12:47:28.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.009613865568908484 Training loss: 6.534045696258545
2025-12-09 12:47:28.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.009612664403046797 Training loss: 7.032340049743652
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 2.72 GiB is free. Including non-PyTorch memory, this process has 91.28 GiB memory in use. Of the allocated memory 90.25 GiB is allocated by PyTorch, and 278.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   1%|          | 52/10000 [00:00<00:19, 518.25it/s]Tokenizing texts:   1%|▏         | 136/10000 [00:00<00:15, 631.08it/s]Tokenizing texts:   2%|▏         | 204/10000 [00:00<00:15, 651.77it/s]Tokenizing texts:   3%|▎         | 269/10000 [00:00<00:14, 648.83it/s]Tokenizing texts:   3%|▎         | 334/10000 [00:00<00:15, 620.81it/s]Tokenizing texts:   4%|▍         | 397/10000 [00:00<00:15, 610.85it/s]Tokenizing texts:   5%|▍         | 459/10000 [00:00<00:16, 588.84it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 578.77it/s]Tokenizing texts:   6%|▌         | 597/10000 [00:00<00:15, 603.00it/s]Tokenizing texts:   7%|▋         | 658/10000 [00:01<00:15, 596.13it/s]Tokenizing texts:   7%|▋         | 718/10000 [00:01<00:15, 580.63it/s]Tokenizing texts:   8%|▊         | 777/10000 [00:01<00:15, 581.83it/s]Tokenizing texts:   8%|▊         | 836/10000 [00:01<00:15, 577.85it/s]Tokenizing texts:   9%|▉         | 899/10000 [00:01<00:15, 591.37it/s]Tokenizing texts:  10%|▉         | 981/10000 [00:01<00:13, 657.20it/s]Tokenizing texts:  11%|█         | 1051/10000 [00:01<00:14, 638.07it/s]Tokenizing texts:  11%|█         | 1116/10000 [00:01<00:13, 635.44it/s]Tokenizing texts:  12%|█▏        | 1180/10000 [00:01<00:14, 601.13it/s]Tokenizing texts:  12%|█▏        | 1241/10000 [00:02<00:14, 593.71it/s]Tokenizing texts:  13%|█▎        | 1308/10000 [00:02<00:14, 614.51it/s]Tokenizing texts:  14%|█▎        | 1370/10000 [00:02<00:14, 587.47it/s]Tokenizing texts:  14%|█▍        | 1440/10000 [00:02<00:13, 617.71it/s]Tokenizing texts:  15%|█▌        | 1518/10000 [00:02<00:12, 654.45it/s]Tokenizing texts:  16%|█▌        | 1594/10000 [00:02<00:12, 683.54it/s]Tokenizing texts:  17%|█▋        | 1663/10000 [00:02<00:13, 616.06it/s]Tokenizing texts:  17%|█▋        | 1727/10000 [00:02<00:13, 616.48it/s]Tokenizing texts:  18%|█▊        | 1794/10000 [00:02<00:13, 630.12it/s]Tokenizing texts:  19%|█▊        | 1858/10000 [00:03<00:13, 612.41it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:03<00:13, 600.07it/s]Tokenizing texts:  20%|█▉        | 1981/10000 [00:03<00:13, 578.81it/s]Tokenizing texts:  21%|██        | 2052/10000 [00:03<00:12, 612.35it/s]Tokenizing texts:  21%|██        | 2114/10000 [00:03<00:14, 560.16it/s]Tokenizing texts:  22%|██▏       | 2172/10000 [00:03<00:13, 560.44it/s]Tokenizing texts:  22%|██▏       | 2249/10000 [00:03<00:12, 616.89it/s]Tokenizing texts:  23%|██▎       | 2312/10000 [00:03<00:12, 599.53it/s]Tokenizing texts:  24%|██▍       | 2381/10000 [00:03<00:12, 624.33it/s]Tokenizing texts:  24%|██▍       | 2445/10000 [00:04<00:12, 628.27it/s]Tokenizing texts:  25%|██▌       | 2509/10000 [00:04<00:11, 624.63it/s]Tokenizing texts:  26%|██▌       | 2572/10000 [00:04<00:12, 612.36it/s]Tokenizing texts:  26%|██▋       | 2634/10000 [00:04<00:12, 574.72it/s]Tokenizing texts:  27%|██▋       | 2694/10000 [00:04<00:13, 553.83it/s]Tokenizing texts:  28%|██▊       | 2768/10000 [00:04<00:11, 602.80it/s]Tokenizing texts:  28%|██▊       | 2830/10000 [00:04<00:12, 554.75it/s]Tokenizing texts:  29%|██▉       | 2889/10000 [00:04<00:12, 559.72it/s]Tokenizing texts:  30%|██▉       | 2952/10000 [00:04<00:12, 577.44it/s]Tokenizing texts:  30%|███       | 3024/10000 [00:05<00:11, 599.89it/s]Tokenizing texts:  31%|███       | 3086/10000 [00:05<00:11, 599.11it/s]Tokenizing texts:  32%|███▏      | 3153/10000 [00:05<00:11, 611.21it/s]Tokenizing texts:  32%|███▏      | 3227/10000 [00:05<00:10, 647.39it/s]Tokenizing texts:  33%|███▎      | 3294/10000 [00:05<00:10, 647.20it/s]Tokenizing texts:  34%|███▎      | 3361/10000 [00:05<00:10, 649.50it/s]Tokenizing texts:  34%|███▍      | 3427/10000 [00:05<00:10, 650.50it/s]Tokenizing texts:  35%|███▍      | 3497/10000 [00:05<00:09, 651.57it/s]Tokenizing texts:  36%|███▌      | 3565/10000 [00:05<00:09, 659.62it/s]Tokenizing texts:  36%|███▋      | 3632/10000 [00:05<00:09, 659.26it/s]Tokenizing texts:  37%|███▋      | 3698/10000 [00:06<00:09, 631.83it/s]Tokenizing texts:  38%|███▊      | 3762/10000 [00:06<00:10, 617.46it/s]Tokenizing texts:  38%|███▊      | 3824/10000 [00:06<00:10, 585.38it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:06<00:09, 616.73it/s]Tokenizing texts:  40%|███▉      | 3958/10000 [00:06<00:10, 603.25it/s]Tokenizing texts:  40%|████      | 4019/10000 [00:06<00:10, 589.97it/s]Tokenizing texts:  41%|████      | 4096/10000 [00:06<00:09, 637.68it/s]Tokenizing texts:  42%|████▏     | 4169/10000 [00:06<00:08, 662.34it/s]Tokenizing texts:  42%|████▏     | 4236/10000 [00:06<00:08, 661.91it/s]Tokenizing texts:  43%|████▎     | 4303/10000 [00:07<00:09, 629.86it/s]Tokenizing texts:  44%|████▍     | 4382/10000 [00:07<00:08, 674.57it/s]Tokenizing texts:  45%|████▍     | 4451/10000 [00:07<00:08, 670.12it/s]Tokenizing texts:  45%|████▌     | 4519/10000 [00:07<00:08, 640.68it/s]Tokenizing texts:  46%|████▌     | 4585/10000 [00:07<00:08, 641.91it/s]Tokenizing texts:  47%|████▋     | 4660/10000 [00:07<00:07, 669.98it/s]Tokenizing texts:  47%|████▋     | 4739/10000 [00:07<00:07, 703.07it/s]Tokenizing texts:  48%|████▊     | 4810/10000 [00:07<00:07, 661.36it/s]Tokenizing texts:  49%|████▉     | 4882/10000 [00:07<00:07, 676.98it/s]Tokenizing texts:  50%|████▉     | 4953/10000 [00:07<00:07, 683.54it/s]Tokenizing texts:  50%|█████     | 5029/10000 [00:08<00:07, 700.19it/s]Tokenizing texts:  51%|█████     | 5100/10000 [00:08<00:07, 649.80it/s]Tokenizing texts:  52%|█████▏    | 5166/10000 [00:08<00:07, 630.28it/s]Tokenizing texts:  52%|█████▏    | 5230/10000 [00:08<00:07, 630.55it/s]Tokenizing texts:  53%|█████▎    | 5299/10000 [00:08<00:07, 646.87it/s]Tokenizing texts:  54%|█████▍    | 5383/10000 [00:08<00:06, 701.57it/s]Tokenizing texts:  55%|█████▍    | 5454/10000 [00:08<00:06, 687.42it/s]Tokenizing texts:  55%|█████▌    | 5524/10000 [00:08<00:07, 627.04it/s]Tokenizing texts:  56%|█████▌    | 5593/10000 [00:08<00:06, 640.13it/s]Tokenizing texts:  57%|█████▋    | 5680/10000 [00:09<00:06, 703.71it/s]Tokenizing texts:  58%|█████▊    | 5752/10000 [00:09<00:06, 672.99it/s]Tokenizing texts:  58%|█████▊    | 5821/10000 [00:09<00:06, 604.06it/s]Tokenizing texts:  59%|█████▉    | 5890/10000 [00:09<00:06, 624.81it/s]Tokenizing texts:  60%|█████▉    | 5955/10000 [00:09<00:06, 622.13it/s]Tokenizing texts:  60%|██████    | 6019/10000 [00:09<00:06, 611.83it/s]Tokenizing texts:  61%|██████    | 6092/10000 [00:09<00:06, 644.24it/s]Tokenizing texts:  62%|██████▏   | 6166/10000 [00:09<00:05, 668.86it/s]Tokenizing texts:  62%|██████▏   | 6234/10000 [00:09<00:05, 650.31it/s]Tokenizing texts:  63%|██████▎   | 6300/10000 [00:10<00:05, 631.76it/s]Tokenizing texts:  64%|██████▎   | 6364/10000 [00:10<00:05, 617.14it/s]Tokenizing texts:  64%|██████▍   | 6427/10000 [00:10<00:05, 617.67it/s]Tokenizing texts:  65%|██████▍   | 6492/10000 [00:10<00:05, 626.37it/s]Tokenizing texts:  66%|██████▌   | 6557/10000 [00:10<00:05, 632.54it/s]Tokenizing texts:  66%|██████▋   | 6627/10000 [00:10<00:05, 651.94it/s]Tokenizing texts:  67%|██████▋   | 6693/10000 [00:10<00:05, 618.00it/s]Tokenizing texts:  68%|██████▊   | 6767/10000 [00:10<00:04, 651.76it/s]Tokenizing texts:  68%|██████▊   | 6850/10000 [00:10<00:04, 700.16it/s]Tokenizing texts:  69%|██████▉   | 6921/10000 [00:11<00:04, 617.39it/s]Tokenizing texts:  70%|██████▉   | 6987/10000 [00:11<00:04, 627.35it/s]Tokenizing texts:  71%|███████   | 7052/10000 [00:11<00:04, 607.97it/s]Tokenizing texts:  71%|███████   | 7114/10000 [00:11<00:04, 581.97it/s]Tokenizing texts:  72%|███████▏  | 7176/10000 [00:11<00:04, 592.23it/s]Tokenizing texts:  72%|███████▏  | 7244/10000 [00:11<00:04, 610.92it/s]Tokenizing texts:  73%|███████▎  | 7330/10000 [00:11<00:03, 680.29it/s]Tokenizing texts:  74%|███████▍  | 7399/10000 [00:11<00:04, 635.67it/s]Tokenizing texts:  75%|███████▍  | 7464/10000 [00:11<00:04, 582.23it/s]Tokenizing texts:  75%|███████▌  | 7543/10000 [00:12<00:03, 637.15it/s]Tokenizing texts:  76%|███████▌  | 7617/10000 [00:12<00:03, 664.88it/s]Tokenizing texts:  77%|███████▋  | 7689/10000 [00:12<00:03, 677.78it/s]Tokenizing texts:  78%|███████▊  | 7758/10000 [00:12<00:03, 623.35it/s]Tokenizing texts:  78%|███████▊  | 7822/10000 [00:12<00:03, 621.14it/s]Tokenizing texts:  79%|███████▉  | 7886/10000 [00:12<00:03, 615.79it/s]Tokenizing texts:  80%|███████▉  | 7966/10000 [00:12<00:03, 665.27it/s]Tokenizing texts:  80%|████████  | 8036/10000 [00:12<00:02, 674.63it/s]Tokenizing texts:  81%|████████  | 8105/10000 [00:12<00:03, 580.92it/s]Tokenizing texts:  82%|████████▏ | 8169/10000 [00:13<00:03, 595.58it/s]Tokenizing texts:  82%|████████▏ | 8231/10000 [00:13<00:02, 598.47it/s]Tokenizing texts:  83%|████████▎ | 8293/10000 [00:13<00:02, 593.10it/s]Tokenizing texts:  84%|████████▎ | 8357/10000 [00:13<00:02, 605.70it/s]Tokenizing texts:  84%|████████▍ | 8419/10000 [00:13<00:02, 585.98it/s]Tokenizing texts:  85%|████████▍ | 8497/10000 [00:13<00:02, 637.69it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:13<00:02, 638.89it/s]Tokenizing texts:  86%|████████▋ | 8636/10000 [00:13<00:02, 662.45it/s]Tokenizing texts:  87%|████████▋ | 8713/10000 [00:13<00:01, 693.39it/s]Tokenizing texts:  88%|████████▊ | 8783/10000 [00:13<00:01, 693.24it/s]Tokenizing texts:  89%|████████▊ | 8853/10000 [00:14<00:01, 600.21it/s]Tokenizing texts:  89%|████████▉ | 8934/10000 [00:14<00:01, 653.57it/s]Tokenizing texts:  90%|█████████ | 9002/10000 [00:14<00:01, 619.01it/s]Tokenizing texts:  91%|█████████ | 9067/10000 [00:14<00:01, 627.25it/s]Tokenizing texts:  91%|█████████▏| 9139/10000 [00:14<00:01, 649.56it/s]Tokenizing texts:  92%|█████████▏| 9214/10000 [00:14<00:01, 676.96it/s]Tokenizing texts:  93%|█████████▎| 9283/10000 [00:14<00:01, 627.43it/s]Tokenizing texts:  93%|█████████▎| 9348/10000 [00:14<00:01, 628.82it/s]Tokenizing texts:  94%|█████████▍| 9412/10000 [00:15<00:00, 597.88it/s]Tokenizing texts:  95%|█████████▍| 9474/10000 [00:15<00:00, 598.49it/s]Tokenizing texts:  95%|█████████▌| 9535/10000 [00:15<00:00, 561.56it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:15<00:00, 573.25it/s]Tokenizing texts:  97%|█████████▋| 9655/10000 [00:15<00:00, 576.89it/s]Tokenizing texts:  97%|█████████▋| 9714/10000 [00:15<00:00, 570.64it/s]Tokenizing texts:  98%|█████████▊| 9772/10000 [00:15<00:00, 549.77it/s]Tokenizing texts:  98%|█████████▊| 9849/10000 [00:15<00:00, 608.96it/s]Tokenizing texts:  99%|█████████▉| 9922/10000 [00:15<00:00, 642.70it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:15<00:00, 682.41it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:15<00:00, 625.96it/s]
2025-12-09 12:48:18.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 12.162163734436035
2025-12-09 12:48:18.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 12.169829368591309
2025-12-09 12:48:19.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 12.117663383483887
2025-12-09 12:48:19.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 12.185525894165039
2025-12-09 12:48:19.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 12.1953763961792
2025-12-09 12:48:20.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 12.197092056274414
2025-12-09 12:48:20.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 12.210591316223145
2025-12-09 12:48:20.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 12.167391777038574
2025-12-09 12:48:21.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 12.128413200378418
2025-12-09 12:48:21.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 12.162223815917969
2025-12-09 12:48:21.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 12.205605506896973
2025-12-09 12:48:22.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 12.210490226745605
2025-12-09 12:48:22.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 12.200961112976074
2025-12-09 12:48:23.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 12.34389591217041
2025-12-09 12:48:23.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 12.184263229370117
2025-12-09 12:48:23.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 12.220465660095215
2025-12-09 12:48:24.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 12.151910781860352
2025-12-09 12:48:24.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 12.155423164367676
2025-12-09 12:48:24.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 12.157703399658203
2025-12-09 12:48:25.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 12.123184204101562
2025-12-09 12:48:25.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 12.190208435058594
2025-12-09 12:48:26.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 12.191192626953125
2025-12-09 12:48:26.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 12.175613403320312
2025-12-09 12:48:26.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 12.164671897888184
2025-12-09 12:48:27.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 12.17802906036377
2025-12-09 12:48:27.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 12.211603164672852
2025-12-09 12:48:27.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 12.106077194213867
2025-12-09 12:48:28.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 12.187079429626465
2025-12-09 12:48:28.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 12.170389175415039
2025-12-09 12:48:29.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 12.175522804260254
2025-12-09 12:48:29.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 12.136961936950684
2025-12-09 12:48:29.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 12.165750503540039
2025-12-09 12:48:30.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 12.166057586669922
2025-12-09 12:48:30.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 12.176685333251953
2025-12-09 12:48:30.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 12.136297225952148
2025-12-09 12:48:31.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 12.123641014099121
2025-12-09 12:48:31.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 12.153851509094238
2025-12-09 12:48:32.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 12.186870574951172
2025-12-09 12:48:32.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 12.195305824279785
2025-12-09 12:48:32.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 12.132088661193848
2025-12-09 12:48:33.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 12.129196166992188
2025-12-09 12:48:33.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 12.16373348236084
2025-12-09 12:48:33.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 12.172629356384277
2025-12-09 12:48:34.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 12.117392539978027
2025-12-09 12:48:34.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 12.110901832580566
2025-12-09 12:48:35.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 12.122503280639648
2025-12-09 12:48:35.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 12.200176239013672
2025-12-09 12:48:35.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 12.120745658874512
2025-12-09 12:48:36.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 12.109186172485352
2025-12-09 12:48:36.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 12.117300033569336
2025-12-09 12:48:36.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 12.087366104125977
2025-12-09 12:48:37.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 12.124190330505371
2025-12-09 12:48:37.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 12.117071151733398
2025-12-09 12:48:37.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 12.043821334838867
2025-12-09 12:48:38.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 12.091883659362793
2025-12-09 12:48:38.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 12.135543823242188
2025-12-09 12:48:39.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 12.097591400146484
2025-12-09 12:48:39.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 12.065223693847656
2025-12-09 12:48:39.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 12.068614959716797
2025-12-09 12:48:40.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 12.076142311096191
2025-12-09 12:48:40.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 12.08559513092041
2025-12-09 12:48:40.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 12.07158088684082
2025-12-09 12:48:41.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 12.044792175292969
2025-12-09 12:48:41.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 12.036609649658203
2025-12-09 12:48:42.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 12.049824714660645
2025-12-09 12:48:42.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 12.030176162719727
2025-12-09 12:48:42.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 12.01749324798584
2025-12-09 12:48:43.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 12.025797843933105
2025-12-09 12:48:43.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 12.055620193481445
2025-12-09 12:48:43.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 11.999513626098633
2025-12-09 12:48:44.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 12.038586616516113
2025-12-09 12:48:44.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 12.065753936767578
2025-12-09 12:48:45.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 12.028143882751465
2025-12-09 12:48:45.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 11.998446464538574
2025-12-09 12:48:45.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 12.04477310180664
2025-12-09 12:48:46.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 12.027569770812988
2025-12-09 12:48:46.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 12.03686237335205
2025-12-09 12:48:46.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 11.943374633789062
2025-12-09 12:48:47.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 11.944486618041992
2025-12-09 12:48:47.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 12.011140823364258
2025-12-09 12:48:48.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 11.990104675292969
2025-12-09 12:48:48.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 12.073492050170898
2025-12-09 12:48:48.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 11.925308227539062
2025-12-09 12:48:49.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 12.015758514404297
2025-12-09 12:48:49.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 11.947731971740723
2025-12-09 12:48:49.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 11.966586112976074
2025-12-09 12:48:50.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 11.907608032226562
2025-12-09 12:48:50.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 12.017390251159668
2025-12-09 12:48:51.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 11.975567817687988
2025-12-09 12:48:51.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 11.971487045288086
2025-12-09 12:48:51.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 11.99289321899414
2025-12-09 12:48:52.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 11.943201065063477
2025-12-09 12:48:52.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 11.95628547668457
2025-12-09 12:48:52.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 11.913899421691895
2025-12-09 12:48:53.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 11.979228973388672
2025-12-09 12:48:53.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 11.806804656982422
2025-12-09 12:48:54.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 12.013786315917969
2025-12-09 12:48:54.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 11.846590995788574
2025-12-09 12:48:54.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 11.79616641998291
2025-12-09 12:48:55.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 11.924983024597168
2025-12-09 12:48:55.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.999999029798809e-05 Training loss: 11.785237312316895
2025-12-09 12:48:55.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 9.999996119195611e-05 Training loss: 11.793326377868652
2025-12-09 12:48:56.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 9.999991268191536e-05 Training loss: 11.78632926940918
2025-12-09 12:48:56.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 9.999984476788465e-05 Training loss: 11.872175216674805
2025-12-09 12:48:57.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 9.999975744989037e-05 Training loss: 11.68947982788086
2025-12-09 12:48:57.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 9.999965072796636e-05 Training loss: 11.742691993713379
2025-12-09 12:48:57.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 9.999952460215408e-05 Training loss: 11.727685928344727
2025-12-09 12:48:58.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.999937907250246e-05 Training loss: 11.890458106994629
2025-12-09 12:48:58.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 9.999921413906798e-05 Training loss: 11.798478126525879
2025-12-09 12:48:58.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 9.999902980191464e-05 Training loss: 11.844319343566895
2025-12-09 12:48:59.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 9.999882606111399e-05 Training loss: 11.754762649536133
2025-12-09 12:48:59.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 9.999860291674508e-05 Training loss: 11.770105361938477
2025-12-09 12:48:59.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 9.999836036889453e-05 Training loss: 11.803543090820312
2025-12-09 12:49:00.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 9.999809841765644e-05 Training loss: 11.60687255859375
2025-12-09 12:49:00.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 9.999781706313251e-05 Training loss: 11.7212553024292
2025-12-09 12:49:01.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 9.999751630543188e-05 Training loss: 11.663324356079102
2025-12-09 12:49:01.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 9.99971961446713e-05 Training loss: 11.760815620422363
2025-12-09 12:49:01.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 9.999685658097502e-05 Training loss: 11.27832317352295
2025-12-09 12:49:02.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 9.999649761447478e-05 Training loss: 11.837480545043945
2025-12-09 12:49:02.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 9.999611924530994e-05 Training loss: 11.699317932128906
2025-12-09 12:49:02.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 9.999572147362731e-05 Training loss: 11.49933910369873
2025-12-09 12:49:03.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 9.999530429958124e-05 Training loss: 11.674701690673828
2025-12-09 12:49:03.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 9.999486772333366e-05 Training loss: 11.405783653259277
2025-12-09 12:49:04.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 9.999441174505399e-05 Training loss: 11.496513366699219
2025-12-09 12:49:04.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 9.999393636491918e-05 Training loss: 11.54464340209961
2025-12-09 12:49:04.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 9.99934415831137e-05 Training loss: 11.570005416870117
2025-12-09 12:49:05.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 9.99929273998296e-05 Training loss: 11.548973083496094
2025-12-09 12:49:05.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 9.99923938152664e-05 Training loss: 11.491233825683594
2025-12-09 12:49:05.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 9.999184082963118e-05 Training loss: 11.5483980178833
2025-12-09 12:49:06.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 9.999126844313853e-05 Training loss: 11.461623191833496
2025-12-09 12:49:06.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 9.999067665601061e-05 Training loss: 11.368895530700684
2025-12-09 12:49:07.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 9.999006546847707e-05 Training loss: 11.5049467086792
2025-12-09 12:49:07.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 9.998943488077508e-05 Training loss: 11.328349113464355
2025-12-09 12:49:07.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 9.998878489314938e-05 Training loss: 11.217138290405273
2025-12-09 12:49:08.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 9.99881155058522e-05 Training loss: 11.523008346557617
2025-12-09 12:49:08.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 9.998742671914335e-05 Training loss: 11.212780952453613
2025-12-09 12:49:08.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 9.99867185332901e-05 Training loss: 11.339944839477539
2025-12-09 12:49:09.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 9.998599094856732e-05 Training loss: 11.431185722351074
2025-12-09 12:49:09.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 9.99852439652573e-05 Training loss: 11.356016159057617
2025-12-09 12:49:10.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 9.998447758365002e-05 Training loss: 11.381332397460938
2025-12-09 12:49:10.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 9.998369180404283e-05 Training loss: 11.1334867477417
2025-12-09 12:49:10.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 9.99828866267407e-05 Training loss: 10.872836112976074
2025-12-09 12:49:11.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 9.998206205205611e-05 Training loss: 11.176006317138672
2025-12-09 12:49:11.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 9.998121808030906e-05 Training loss: 10.914996147155762
2025-12-09 12:49:11.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 9.998035471182708e-05 Training loss: 11.098191261291504
2025-12-09 12:49:12.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 9.997947194694519e-05 Training loss: 11.32523250579834
2025-12-09 12:49:12.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 9.997856978600604e-05 Training loss: 11.141162872314453
2025-12-09 12:49:13.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 9.997764822935967e-05 Training loss: 11.160271644592285
2025-12-09 12:49:13.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 9.997670727736378e-05 Training loss: 11.056584358215332
2025-12-09 12:49:13.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 9.99757469303835e-05 Training loss: 11.218456268310547
2025-12-09 12:49:14.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 9.997476718879153e-05 Training loss: 10.86037540435791
2025-12-09 12:49:14.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 9.99737680529681e-05 Training loss: 10.938823699951172
2025-12-09 12:49:14.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 9.997274952330094e-05 Training loss: 11.18699836730957
2025-12-09 12:49:15.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 9.997171160018531e-05 Training loss: 11.072670936584473
2025-12-09 12:49:15.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 9.997065428402403e-05 Training loss: 10.96828556060791
2025-12-09 12:49:16.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 9.996957757522742e-05 Training loss: 10.784357070922852
2025-12-09 12:49:16.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 9.996848147421334e-05 Training loss: 10.850643157958984
2025-12-09 12:49:16.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 9.996736598140714e-05 Training loss: 11.127063751220703
2025-12-09 12:49:17.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 9.996623109724174e-05 Training loss: 11.332084655761719
2025-12-09 12:49:17.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 9.996507682215754e-05 Training loss: 10.828466415405273
2025-12-09 12:49:17.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 9.996390315660253e-05 Training loss: 10.884355545043945
2025-12-09 12:49:18.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 9.996271010103216e-05 Training loss: 10.752790451049805
2025-12-09 12:49:18.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 9.996149765590946e-05 Training loss: 10.669111251831055
2025-12-09 12:49:19.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 9.99602658217049e-05 Training loss: 10.964224815368652
2025-12-09 12:49:19.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 9.995901459889658e-05 Training loss: 10.811060905456543
2025-12-09 12:49:19.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 9.995774398797007e-05 Training loss: 10.82894229888916
2025-12-09 12:49:20.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 9.995645398941846e-05 Training loss: 10.648233413696289
2025-12-09 12:49:20.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 9.995514460374238e-05 Training loss: 11.038290977478027
2025-12-09 12:49:20.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 9.995381583144996e-05 Training loss: 10.721672058105469
2025-12-09 12:49:21.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 9.995246767305688e-05 Training loss: 11.01842975616455
2025-12-09 12:49:21.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 9.995110012908634e-05 Training loss: 10.874443054199219
2025-12-09 12:49:21.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 9.994971320006905e-05 Training loss: 10.608304977416992
2025-12-09 12:49:22.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 9.994830688654326e-05 Training loss: 10.775552749633789
2025-12-09 12:49:22.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 9.994688118905472e-05 Training loss: 10.617237091064453
2025-12-09 12:49:23.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 9.994543610815671e-05 Training loss: 10.585933685302734
2025-12-09 12:49:23.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 9.994397164441007e-05 Training loss: 10.836801528930664
2025-12-09 12:49:23.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 9.994248779838311e-05 Training loss: 10.343914985656738
2025-12-09 12:49:24.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 9.994098457065166e-05 Training loss: 10.733258247375488
2025-12-09 12:49:24.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 9.993946196179913e-05 Training loss: 10.963654518127441
2025-12-09 12:49:24.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 9.993791997241639e-05 Training loss: 11.047673225402832
2025-12-09 12:49:25.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 9.993635860310187e-05 Training loss: 10.39225959777832
2025-12-09 12:49:25.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 9.99347778544615e-05 Training loss: 10.722264289855957
2025-12-09 12:49:26.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 9.993317772710874e-05 Training loss: 10.512035369873047
2025-12-09 12:49:26.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 9.993155822166457e-05 Training loss: 10.51222038269043
2025-12-09 12:49:26.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 9.992991933875748e-05 Training loss: 10.749431610107422
2025-12-09 12:49:27.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 9.99282610790235e-05 Training loss: 10.561431884765625
2025-12-09 12:49:27.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 9.992658344310614e-05 Training loss: 10.424931526184082
2025-12-09 12:49:27.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 9.992488643165651e-05 Training loss: 10.634074211120605
2025-12-09 12:49:28.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 9.992317004533313e-05 Training loss: 10.624419212341309
2025-12-09 12:49:28.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 9.992143428480214e-05 Training loss: 10.497724533081055
2025-12-09 12:49:29.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 9.991967915073714e-05 Training loss: 10.415751457214355
2025-12-09 12:49:29.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 9.991790464381926e-05 Training loss: 10.223017692565918
2025-12-09 12:49:29.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 9.991611076473714e-05 Training loss: 10.524879455566406
2025-12-09 12:49:30.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 9.991429751418697e-05 Training loss: 10.331722259521484
2025-12-09 12:49:30.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 9.991246489287245e-05 Training loss: 10.394624710083008
2025-12-09 12:49:30.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 9.991061290150475e-05 Training loss: 11.004168510437012
2025-12-09 12:49:31.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 9.990874154080259e-05 Training loss: 10.250880241394043
2025-12-09 12:49:31.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 9.990685081149222e-05 Training loss: 10.408140182495117
2025-12-09 12:49:32.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 9.990494071430742e-05 Training loss: 10.846076011657715
2025-12-09 12:49:32.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 9.990301124998945e-05 Training loss: 10.147920608520508
2025-12-09 12:49:32.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 9.990106241928706e-05 Training loss: 10.473099708557129
2025-12-09 12:49:33.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 9.989909422295659e-05 Training loss: 10.364727020263672
2025-12-09 12:49:33.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 9.989710666176186e-05 Training loss: 10.595335006713867
2025-12-09 12:49:33.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 9.989509973647417e-05 Training loss: 10.423338890075684
2025-12-09 12:49:34.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 9.989307344787242e-05 Training loss: 10.27683162689209
2025-12-09 12:49:34.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 9.989102779674293e-05 Training loss: 10.577385902404785
2025-12-09 12:49:35.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 9.98889627838796e-05 Training loss: 10.5608491897583
2025-12-09 12:49:35.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 9.98868784100838e-05 Training loss: 10.508788108825684
2025-12-09 12:49:35.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 9.988477467616447e-05 Training loss: 10.329981803894043
2025-12-09 12:49:36.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 9.988265158293799e-05 Training loss: 10.477877616882324
2025-12-09 12:49:36.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 9.98805091312283e-05 Training loss: 10.44042682647705
2025-12-09 12:49:36.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 9.987834732186687e-05 Training loss: 10.770843505859375
2025-12-09 12:49:37.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 9.987616615569263e-05 Training loss: 10.451589584350586
2025-12-09 12:49:37.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 9.987396563355205e-05 Training loss: 10.197437286376953
2025-12-09 12:49:38.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 9.987174575629911e-05 Training loss: 10.880026817321777
2025-12-09 12:49:38.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 9.986950652479532e-05 Training loss: 10.350839614868164
2025-12-09 12:49:38.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 9.986724793990966e-05 Training loss: 10.376651763916016
2025-12-09 12:49:39.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 9.986497000251866e-05 Training loss: 10.097819328308105
2025-12-09 12:49:39.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 9.986267271350633e-05 Training loss: 10.10232162475586
2025-12-09 12:49:39.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 9.98603560737642e-05 Training loss: 10.252894401550293
2025-12-09 12:49:40.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 9.985802008419131e-05 Training loss: 10.191271781921387
2025-12-09 12:49:40.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 9.985566474569424e-05 Training loss: 10.397116661071777
2025-12-09 12:49:41.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 9.985329005918702e-05 Training loss: 10.26093578338623
2025-12-09 12:49:41.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 9.985089602559125e-05 Training loss: 10.137243270874023
2025-12-09 12:49:41.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 9.984848264583597e-05 Training loss: 10.2694730758667
2025-12-09 12:49:42.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 9.98460499208578e-05 Training loss: 10.355692863464355
2025-12-09 12:49:42.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 9.98435978516008e-05 Training loss: 10.391196250915527
2025-12-09 12:49:42.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 9.98411264390166e-05 Training loss: 10.209521293640137
2025-12-09 12:49:43.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 9.983863568406428e-05 Training loss: 10.08398151397705
2025-12-09 12:49:43.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 9.983612558771049e-05 Training loss: 9.888911247253418
2025-12-09 12:49:43.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 9.983359615092931e-05 Training loss: 10.456385612487793
2025-12-09 12:49:44.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 9.983104737470239e-05 Training loss: 10.258344650268555
2025-12-09 12:49:44.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 9.982847926001886e-05 Training loss: 10.110895156860352
2025-12-09 12:49:45.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 9.982589180787534e-05 Training loss: 10.13536262512207
2025-12-09 12:49:45.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 9.982328501927599e-05 Training loss: 10.067292213439941
2025-12-09 12:49:45.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 9.982065889523242e-05 Training loss: 10.046698570251465
2025-12-09 12:49:46.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 9.98180134367638e-05 Training loss: 10.147855758666992
2025-12-09 12:49:46.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 9.981534864489679e-05 Training loss: 10.08233642578125
2025-12-09 12:49:46.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 9.981266452066553e-05 Training loss: 10.205440521240234
2025-12-09 12:49:47.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 9.98099610651117e-05 Training loss: 10.07536792755127
2025-12-09 12:49:47.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 9.980723827928441e-05 Training loss: 10.184678077697754
2025-12-09 12:49:48.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 9.980449616424037e-05 Training loss: 10.2174711227417
2025-12-09 12:49:48.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 9.98017347210437e-05 Training loss: 10.157844543457031
2025-12-09 12:49:48.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 9.979895395076609e-05 Training loss: 10.139946937561035
2025-12-09 12:49:49.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 9.979615385448669e-05 Training loss: 10.261917114257812
2025-12-09 12:49:49.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 9.979333443329217e-05 Training loss: 10.498883247375488
2025-12-09 12:49:49.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 9.97904956882767e-05 Training loss: 10.019021034240723
2025-12-09 12:49:50.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 9.978763762054194e-05 Training loss: 10.060608863830566
2025-12-09 12:49:50.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 9.978476023119701e-05 Training loss: 10.243354797363281
2025-12-09 12:49:51.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 9.978186352135861e-05 Training loss: 10.408341407775879
2025-12-09 12:49:51.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 9.977894749215089e-05 Training loss: 10.457671165466309
2025-12-09 12:49:51.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 9.97760121447055e-05 Training loss: 9.964212417602539
2025-12-09 12:49:52.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 9.977305748016159e-05 Training loss: 9.940801620483398
2025-12-09 12:49:52.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 9.977008349966582e-05 Training loss: 10.297362327575684
2025-12-09 12:49:52.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 9.976709020437229e-05 Training loss: 10.096936225891113
2025-12-09 12:49:53.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 9.97640775954427e-05 Training loss: 10.061444282531738
2025-12-09 12:49:53.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 9.976104567404617e-05 Training loss: 10.536656379699707
2025-12-09 12:49:54.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 9.97579944413593e-05 Training loss: 10.07320785522461
2025-12-09 12:49:54.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 9.975492389856622e-05 Training loss: 10.160550117492676
2025-12-09 12:49:54.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 9.975183404685856e-05 Training loss: 9.849785804748535
2025-12-09 12:49:55.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 9.974872488743543e-05 Training loss: 10.349806785583496
2025-12-09 12:49:55.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 9.974559642150345e-05 Training loss: 10.366107940673828
2025-12-09 12:49:55.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 9.974244865027669e-05 Training loss: 10.031368255615234
2025-12-09 12:49:56.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 9.973928157497674e-05 Training loss: 10.26887321472168
2025-12-09 12:49:56.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 9.973609519683268e-05 Training loss: 10.074053764343262
2025-12-09 12:49:57.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 9.973288951708111e-05 Training loss: 10.027859687805176
2025-12-09 12:49:57.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 9.972966453696608e-05 Training loss: 9.995098114013672
2025-12-09 12:49:57.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 9.972642025773912e-05 Training loss: 10.173632621765137
2025-12-09 12:49:58.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 9.972315668065929e-05 Training loss: 10.178361892700195
2025-12-09 12:49:58.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 9.97198738069931e-05 Training loss: 10.266975402832031
2025-12-09 12:49:58.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 9.971657163801458e-05 Training loss: 10.065520286560059
2025-12-09 12:49:59.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 9.971325017500526e-05 Training loss: 9.983501434326172
2025-12-09 12:49:59.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 9.970990941925411e-05 Training loss: 9.870549201965332
2025-12-09 12:50:00.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 9.970654937205762e-05 Training loss: 10.128117561340332
2025-12-09 12:50:00.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 9.970317003471976e-05 Training loss: 9.85637092590332
2025-12-09 12:50:00.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 9.969977140855198e-05 Training loss: 9.887116432189941
2025-12-09 12:50:01.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 9.969635349487321e-05 Training loss: 9.901044845581055
2025-12-09 12:50:01.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 9.969291629500991e-05 Training loss: 9.95906925201416
2025-12-09 12:50:01.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 9.968945981029596e-05 Training loss: 10.438660621643066
2025-12-09 12:50:02.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 9.968598404207275e-05 Training loss: 10.181575775146484
2025-12-09 12:50:02.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 9.96824889916892e-05 Training loss: 10.395589828491211
2025-12-09 12:50:03.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 9.96789746605016e-05 Training loss: 9.981450080871582
2025-12-09 12:50:03.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 9.967544104987387e-05 Training loss: 9.855002403259277
2025-12-09 12:50:03.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 9.967188816117727e-05 Training loss: 10.045783042907715
2025-12-09 12:50:04.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 9.966831599579066e-05 Training loss: 10.10616397857666
2025-12-09 12:50:04.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 9.96647245551003e-05 Training loss: 9.820018768310547
2025-12-09 12:50:04.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 9.966111384049997e-05 Training loss: 9.814043045043945
2025-12-09 12:50:05.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 9.965748385339089e-05 Training loss: 10.015721321105957
2025-12-09 12:50:05.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 9.96538345951818e-05 Training loss: 10.225654602050781
2025-12-09 12:50:05.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 9.965016606728894e-05 Training loss: 9.828672409057617
2025-12-09 12:50:06.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 9.964647827113595e-05 Training loss: 10.007301330566406
2025-12-09 12:50:06.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 9.964277120815401e-05 Training loss: 9.890606880187988
2025-12-09 12:50:07.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 9.963904487978177e-05 Training loss: 10.057378768920898
2025-12-09 12:50:07.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 9.963529928746534e-05 Training loss: 9.981829643249512
2025-12-09 12:50:07.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 9.963153443265828e-05 Training loss: 9.781253814697266
2025-12-09 12:50:08.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 9.96277503168217e-05 Training loss: 10.182222366333008
2025-12-09 12:50:08.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 9.96239469414241e-05 Training loss: 9.852587699890137
2025-12-09 12:50:08.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 9.962012430794153e-05 Training loss: 9.94367790222168
2025-12-09 12:50:09.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 9.961628241785747e-05 Training loss: 9.65950870513916
2025-12-09 12:50:09.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 9.961242127266288e-05 Training loss: 9.958202362060547
2025-12-09 12:50:10.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 9.960854087385619e-05 Training loss: 9.951903343200684
2025-12-09 12:50:10.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 9.96046412229433e-05 Training loss: 10.109512329101562
2025-12-09 12:50:10.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 9.960072232143762e-05 Training loss: 9.813610076904297
2025-12-09 12:50:11.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 9.959678417085997e-05 Training loss: 9.827508926391602
2025-12-09 12:50:11.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 9.95928267727387e-05 Training loss: 9.97535228729248
2025-12-09 12:50:11.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 9.958885012860954e-05 Training loss: 9.825493812561035
2025-12-09 12:50:12.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 9.958485424001583e-05 Training loss: 10.40023136138916
2025-12-09 12:50:12.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 9.958083910850821e-05 Training loss: 9.683938980102539
2025-12-09 12:50:13.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 9.957680473564495e-05 Training loss: 9.874008178710938
2025-12-09 12:50:13.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 9.957275112299165e-05 Training loss: 9.771635055541992
2025-12-09 12:50:13.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 9.956867827212148e-05 Training loss: 9.915602684020996
2025-12-09 12:50:14.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 9.956458618461502e-05 Training loss: 9.912557601928711
2025-12-09 12:50:14.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 9.956047486206032e-05 Training loss: 10.074220657348633
2025-12-09 12:50:14.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 9.955634430605291e-05 Training loss: 9.921175956726074
2025-12-09 12:50:15.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 9.955219451819579e-05 Training loss: 9.751479148864746
2025-12-09 12:50:15.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 9.954802550009942e-05 Training loss: 9.960739135742188
2025-12-09 12:50:16.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 9.954383725338167e-05 Training loss: 10.669637680053711
2025-12-09 12:50:16.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 9.953962977966795e-05 Training loss: 9.982176780700684
2025-12-09 12:50:16.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 9.953540308059111e-05 Training loss: 9.943882942199707
2025-12-09 12:50:17.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 9.953115715779141e-05 Training loss: 9.946135520935059
2025-12-09 12:50:17.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 9.952689201291664e-05 Training loss: 9.883417129516602
2025-12-09 12:50:17.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 9.9522607647622e-05 Training loss: 9.901617050170898
2025-12-09 12:50:18.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 9.951830406357019e-05 Training loss: 9.832561492919922
2025-12-09 12:50:18.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 9.951398126243134e-05 Training loss: 9.746139526367188
2025-12-09 12:50:19.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 9.950963924588303e-05 Training loss: 9.905734062194824
2025-12-09 12:50:19.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 9.950527801561033e-05 Training loss: 9.757115364074707
2025-12-09 12:50:19.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 9.950089757330574e-05 Training loss: 9.830517768859863
2025-12-09 12:50:20.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 9.949649792066922e-05 Training loss: 9.771347045898438
2025-12-09 12:50:20.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 9.94920790594082e-05 Training loss: 9.741057395935059
2025-12-09 12:50:20.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 9.948764099123755e-05 Training loss: 9.838644027709961
2025-12-09 12:50:21.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 9.94831837178796e-05 Training loss: 10.045063018798828
2025-12-09 12:50:21.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.947870724106412e-05 Training loss: 9.756109237670898
2025-12-09 12:50:22.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.947421156252836e-05 Training loss: 9.391104698181152
2025-12-09 12:50:22.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.946969668401697e-05 Training loss: 9.616451263427734
2025-12-09 12:50:22.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 9.946516260728214e-05 Training loss: 9.990531921386719
2025-12-09 12:50:23.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 9.946060933408341e-05 Training loss: 10.263059616088867
2025-12-09 12:50:23.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 9.945603686618785e-05 Training loss: 9.789401054382324
2025-12-09 12:50:23.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 9.945144520536992e-05 Training loss: 9.98781967163086
2025-12-09 12:50:24.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 9.944683435341155e-05 Training loss: 9.780316352844238
2025-12-09 12:50:24.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 9.944220431210216e-05 Training loss: 9.869556427001953
2025-12-09 12:50:24.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 9.943755508323855e-05 Training loss: 9.678277015686035
2025-12-09 12:50:25.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 9.943288666862498e-05 Training loss: 10.055224418640137
2025-12-09 12:50:25.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 9.942819907007321e-05 Training loss: 9.719404220581055
2025-12-09 12:50:26.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 9.942349228940237e-05 Training loss: 9.562967300415039
2025-12-09 12:50:26.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 9.941876632843909e-05 Training loss: 9.707161903381348
2025-12-09 12:50:26.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 9.941402118901744e-05 Training loss: 9.876967430114746
2025-12-09 12:50:27.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 9.940925687297886e-05 Training loss: 10.284820556640625
2025-12-09 12:50:27.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 9.940447338217234e-05 Training loss: 9.884064674377441
2025-12-09 12:50:27.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 9.939967071845423e-05 Training loss: 9.968877792358398
2025-12-09 12:50:28.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 9.939484888368838e-05 Training loss: 9.717201232910156
2025-12-09 12:50:28.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 9.939000787974602e-05 Training loss: 9.807687759399414
2025-12-09 12:50:29.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 9.938514770850587e-05 Training loss: 9.908910751342773
2025-12-09 12:50:29.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 9.938026837185404e-05 Training loss: 10.633213996887207
2025-12-09 12:50:29.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 9.937536987168413e-05 Training loss: 9.916866302490234
2025-12-09 12:50:30.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 9.937045220989715e-05 Training loss: 9.690810203552246
2025-12-09 12:50:30.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 9.936551538840155e-05 Training loss: 9.83810806274414
2025-12-09 12:50:30.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.93605594091132e-05 Training loss: 9.649495124816895
2025-12-09 12:50:31.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.935558427395542e-05 Training loss: 9.768267631530762
2025-12-09 12:50:31.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 9.935058998485897e-05 Training loss: 9.540467262268066
2025-12-09 12:50:32.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 9.934557654376205e-05 Training loss: 9.947766304016113
2025-12-09 12:50:32.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 9.934054395261026e-05 Training loss: 9.867445945739746
2025-12-09 12:50:32.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 9.933549221335664e-05 Training loss: 9.817309379577637
2025-12-09 12:50:33.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 9.933042132796171e-05 Training loss: 9.735108375549316
2025-12-09 12:50:33.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 9.932533129839334e-05 Training loss: 10.131185531616211
2025-12-09 12:50:33.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 9.932022212662691e-05 Training loss: 10.114134788513184
2025-12-09 12:50:34.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 9.931509381464515e-05 Training loss: 9.668691635131836
2025-12-09 12:50:34.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 9.930994636443829e-05 Training loss: 9.586180686950684
2025-12-09 12:50:35.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 9.930477977800392e-05 Training loss: 9.737403869628906
2025-12-09 12:50:35.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 9.929959405734712e-05 Training loss: 9.946925163269043
2025-12-09 12:50:35.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 9.929438920448037e-05 Training loss: 9.7499418258667
2025-12-09 12:50:36.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 9.928916522142357e-05 Training loss: 9.71851921081543
2025-12-09 12:50:36.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 9.928392211020401e-05 Training loss: 9.903003692626953
2025-12-09 12:50:36.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.927865987285649e-05 Training loss: 9.69002628326416
2025-12-09 12:50:37.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 9.927337851142314e-05 Training loss: 9.58450698852539
2025-12-09 12:50:37.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 9.926807802795359e-05 Training loss: 9.940970420837402
2025-12-09 12:50:38.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 9.926275842450483e-05 Training loss: 9.586408615112305
2025-12-09 12:50:38.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 9.925741970314129e-05 Training loss: 9.682319641113281
2025-12-09 12:50:38.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 9.925206186593484e-05 Training loss: 9.923629760742188
2025-12-09 12:50:39.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 9.924668491496474e-05 Training loss: 9.672581672668457
2025-12-09 12:50:39.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 9.92412888523177e-05 Training loss: 9.912446022033691
2025-12-09 12:50:39.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 9.923587368008778e-05 Training loss: 10.494706153869629
2025-12-09 12:50:40.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 9.923043940037657e-05 Training loss: 9.816356658935547
2025-12-09 12:50:40.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 9.922498601529296e-05 Training loss: 9.781228065490723
2025-12-09 12:50:41.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 9.92195135269533e-05 Training loss: 9.728202819824219
2025-12-09 12:50:41.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 9.921402193748139e-05 Training loss: 9.801891326904297
2025-12-09 12:50:41.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 9.920851124900837e-05 Training loss: 9.888652801513672
2025-12-09 12:50:42.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 9.920298146367286e-05 Training loss: 9.836540222167969
2025-12-09 12:50:42.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 9.919743258362085e-05 Training loss: 9.897258758544922
2025-12-09 12:50:42.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 9.919186461100576e-05 Training loss: 10.09568977355957
2025-12-09 12:50:43.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 9.91862775479884e-05 Training loss: 9.856160163879395
2025-12-09 12:50:43.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 9.9180671396737e-05 Training loss: 9.856157302856445
2025-12-09 12:50:44.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 9.91750461594272e-05 Training loss: 9.666570663452148
2025-12-09 12:50:44.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 9.916940183824206e-05 Training loss: 9.672353744506836
2025-12-09 12:50:44.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 9.916373843537201e-05 Training loss: 9.613789558410645
2025-12-09 12:50:45.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 9.915805595301491e-05 Training loss: 10.07406234741211
2025-12-09 12:50:45.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 9.915235439337603e-05 Training loss: 9.520806312561035
2025-12-09 12:50:45.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 9.914663375866804e-05 Training loss: 9.89579963684082
2025-12-09 12:50:46.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 9.914089405111098e-05 Training loss: 9.707418441772461
2025-12-09 12:50:46.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 9.913513527293235e-05 Training loss: 9.470094680786133
2025-12-09 12:50:46.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 9.912935742636698e-05 Training loss: 9.984273910522461
2025-12-09 12:50:47.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 9.912356051365718e-05 Training loss: 9.585844039916992
2025-12-09 12:50:47.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 9.911774453705258e-05 Training loss: 9.630552291870117
2025-12-09 12:50:48.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 9.91119094988103e-05 Training loss: 9.80911636352539
2025-12-09 12:50:48.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 9.910605540119475e-05 Training loss: 9.893495559692383
2025-12-09 12:50:48.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 9.91001822464778e-05 Training loss: 9.946226119995117
2025-12-09 12:50:49.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 9.909429003693876e-05 Training loss: 9.513421058654785
2025-12-09 12:50:49.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 9.908837877486423e-05 Training loss: 9.805570602416992
2025-12-09 12:50:49.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 9.908244846254826e-05 Training loss: 9.652641296386719
2025-12-09 12:50:50.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 9.907649910229229e-05 Training loss: 9.70641803741455
2025-12-09 12:50:50.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 9.907053069640517e-05 Training loss: 9.834307670593262
2025-12-09 12:50:51.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 9.90645432472031e-05 Training loss: 9.741867065429688
2025-12-09 12:50:51.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 9.905853675700969e-05 Training loss: 9.74886703491211
2025-12-09 12:50:51.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 9.905251122815596e-05 Training loss: 9.705796241760254
2025-12-09 12:50:52.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 9.90464666629803e-05 Training loss: 9.749080657958984
2025-12-09 12:50:52.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 9.904040306382846e-05 Training loss: 9.584628105163574
2025-12-09 12:50:52.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 9.903432043305365e-05 Training loss: 9.83012866973877
2025-12-09 12:50:53.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 9.902821877301637e-05 Training loss: 9.536869049072266
2025-12-09 12:50:53.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 9.90220980860846e-05 Training loss: 9.806961059570312
2025-12-09 12:50:54.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 9.901595837463363e-05 Training loss: 9.578662872314453
2025-12-09 12:50:54.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 9.900979964104617e-05 Training loss: 9.989110946655273
2025-12-09 12:50:54.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 9.900362188771231e-05 Training loss: 9.602302551269531
2025-12-09 12:50:55.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 9.899742511702951e-05 Training loss: 9.518097877502441
2025-12-09 12:50:55.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 9.89912093314026e-05 Training loss: 10.17345142364502
2025-12-09 12:50:55.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 9.898497453324384e-05 Training loss: 9.821386337280273
2025-12-09 12:50:56.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 9.897872072497281e-05 Training loss: 9.717072486877441
2025-12-09 12:50:56.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 9.897244790901649e-05 Training loss: 9.635443687438965
2025-12-09 12:50:57.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 9.896615608780925e-05 Training loss: 9.559080123901367
2025-12-09 12:50:57.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 9.895984526379281e-05 Training loss: 9.659337997436523
2025-12-09 12:50:57.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 9.895351543941629e-05 Training loss: 9.669271469116211
2025-12-09 12:50:58.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 9.894716661713617e-05 Training loss: 9.293913841247559
2025-12-09 12:50:58.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 9.894079879941627e-05 Training loss: 9.61565113067627
2025-12-09 12:50:58.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 9.893441198872787e-05 Training loss: 9.683829307556152
2025-12-09 12:50:59.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 9.892800618754954e-05 Training loss: 9.730870246887207
2025-12-09 12:50:59.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 9.892158139836725e-05 Training loss: 9.56859302520752
2025-12-09 12:51:00.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 9.891513762367431e-05 Training loss: 9.849347114562988
2025-12-09 12:51:00.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 9.890867486597146e-05 Training loss: 9.573612213134766
2025-12-09 12:51:00.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 9.890219312776676e-05 Training loss: 9.528392791748047
2025-12-09 12:51:01.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 9.889569241157563e-05 Training loss: 9.696592330932617
2025-12-09 12:51:01.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 9.888917271992091e-05 Training loss: 9.354167938232422
2025-12-09 12:51:01.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 9.888263405533271e-05 Training loss: 9.535327911376953
2025-12-09 12:51:02.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 9.88760764203486e-05 Training loss: 9.835514068603516
2025-12-09 12:51:02.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 9.886949981751346e-05 Training loss: 9.720569610595703
2025-12-09 12:51:03.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 9.886290424937952e-05 Training loss: 9.875960350036621
2025-12-09 12:51:03.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 9.885628971850642e-05 Training loss: 9.380355834960938
2025-12-09 12:51:03.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 9.884965622746111e-05 Training loss: 9.942689895629883
2025-12-09 12:51:04.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 9.884300377881795e-05 Training loss: 9.750895500183105
2025-12-09 12:51:04.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 9.883633237515858e-05 Training loss: 9.826451301574707
2025-12-09 12:51:04.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 9.882964201907207e-05 Training loss: 9.646209716796875
2025-12-09 12:51:05.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 9.882293271315481e-05 Training loss: 9.741631507873535
2025-12-09 12:51:05.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 9.881620446001056e-05 Training loss: 9.726459503173828
2025-12-09 12:51:05.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 9.88094572622504e-05 Training loss: 9.89570426940918
2025-12-09 12:51:06.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 9.88026911224928e-05 Training loss: 10.356729507446289
2025-12-09 12:51:06.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 9.879590604336359e-05 Training loss: 9.699478149414062
2025-12-09 12:51:07.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 9.87891020274959e-05 Training loss: 9.273527145385742
2025-12-09 12:51:07.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 9.878227907753021e-05 Training loss: 9.73816204071045
2025-12-09 12:51:07.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 9.877543719611444e-05 Training loss: 9.822915077209473
2025-12-09 12:51:08.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 9.876857638590373e-05 Training loss: 9.459753036499023
2025-12-09 12:51:08.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 9.876169664956067e-05 Training loss: 9.51450252532959
2025-12-09 12:51:08.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 9.875479798975512e-05 Training loss: 9.716841697692871
2025-12-09 12:51:09.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 9.874788040916432e-05 Training loss: 9.46215534210205
2025-12-09 12:51:09.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 9.874094391047289e-05 Training loss: 10.056185722351074
2025-12-09 12:51:10.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 9.873398849637268e-05 Training loss: 9.69058895111084
2025-12-09 12:51:10.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 9.872701416956299e-05 Training loss: 9.995893478393555
2025-12-09 12:51:10.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 9.872002093275042e-05 Training loss: 9.597497940063477
2025-12-09 12:51:11.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 9.871300878864891e-05 Training loss: 9.403817176818848
2025-12-09 12:51:11.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 9.870597773997972e-05 Training loss: 9.705315589904785
2025-12-09 12:51:11.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 9.869892778947148e-05 Training loss: 9.585073471069336
2025-12-09 12:51:12.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 9.869185893986012e-05 Training loss: 9.693126678466797
2025-12-09 12:51:12.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 9.868477119388896e-05 Training loss: 9.638091087341309
2025-12-09 12:51:13.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 9.867766455430857e-05 Training loss: 9.5562105178833
2025-12-09 12:51:13.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 9.867053902387693e-05 Training loss: 9.539185523986816
2025-12-09 12:51:13.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 9.86633946053593e-05 Training loss: 9.548482894897461
2025-12-09 12:51:14.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 9.865623130152828e-05 Training loss: 9.456801414489746
2025-12-09 12:51:14.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 9.864904911516384e-05 Training loss: 9.697822570800781
2025-12-09 12:51:14.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 9.864184804905323e-05 Training loss: 9.671807289123535
2025-12-09 12:51:15.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 9.863462810599105e-05 Training loss: 9.270270347595215
2025-12-09 12:51:15.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 9.862738928877922e-05 Training loss: 9.622878074645996
2025-12-09 12:51:16.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 9.862013160022696e-05 Training loss: 9.413186073303223
2025-12-09 12:51:16.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 9.861285504315085e-05 Training loss: 9.618906021118164
2025-12-09 12:51:16.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 9.860555962037479e-05 Training loss: 9.546331405639648
2025-12-09 12:51:17.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 9.859824533472998e-05 Training loss: 9.57211685180664
2025-12-09 12:51:17.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 9.859091218905498e-05 Training loss: 9.780566215515137
2025-12-09 12:51:17.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 9.85835601861956e-05 Training loss: 10.113210678100586
2025-12-09 12:51:18.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 9.857618932900503e-05 Training loss: 9.957412719726562
2025-12-09 12:51:18.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 9.856879962034374e-05 Training loss: 9.383747100830078
2025-12-09 12:51:19.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 9.856139106307955e-05 Training loss: 9.758645057678223
2025-12-09 12:51:19.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 9.855396366008758e-05 Training loss: 9.707764625549316
2025-12-09 12:51:19.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 9.854651741425023e-05 Training loss: 9.76783275604248
2025-12-09 12:51:20.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 9.853905232845728e-05 Training loss: 9.285840034484863
2025-12-09 12:51:20.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 9.853156840560575e-05 Training loss: 9.297219276428223
2025-12-09 12:51:20.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 9.852406564860003e-05 Training loss: 9.416653633117676
2025-12-09 12:51:21.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 9.851654406035179e-05 Training loss: 9.684832572937012
2025-12-09 12:51:21.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 9.850900364378e-05 Training loss: 9.366354942321777
2025-12-09 12:51:22.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 9.850144440181096e-05 Training loss: 9.712989807128906
2025-12-09 12:51:22.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 9.849386633737825e-05 Training loss: 10.221723556518555
2025-12-09 12:51:22.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 9.848626945342278e-05 Training loss: 9.814480781555176
2025-12-09 12:51:23.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 9.847865375289275e-05 Training loss: 9.987215995788574
2025-12-09 12:51:23.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 9.847101923874367e-05 Training loss: 9.729350090026855
2025-12-09 12:51:23.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 9.846336591393833e-05 Training loss: 9.680825233459473
2025-12-09 12:51:24.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 9.845569378144686e-05 Training loss: 9.792119026184082
2025-12-09 12:51:24.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 9.844800284424664e-05 Training loss: 9.564379692077637
2025-12-09 12:51:24.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 9.844029310532239e-05 Training loss: 9.725458145141602
2025-12-09 12:51:25.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 9.843256456766609e-05 Training loss: 9.554132461547852
2025-12-09 12:51:25.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 9.842481723427705e-05 Training loss: 9.799630165100098
2025-12-09 12:51:26.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 9.841705110816187e-05 Training loss: 9.751659393310547
2025-12-09 12:51:26.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 9.840926619233441e-05 Training loss: 9.560064315795898
2025-12-09 12:51:26.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 9.840146248981585e-05 Training loss: 9.792757034301758
2025-12-09 12:51:27.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 9.839364000363467e-05 Training loss: 9.694703102111816
2025-12-09 12:51:27.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 9.83857987368266e-05 Training loss: 9.674386024475098
2025-12-09 12:51:27.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 9.837793869243468e-05 Training loss: 9.668373107910156
2025-12-09 12:51:28.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 9.837005987350926e-05 Training loss: 9.964110374450684
2025-12-09 12:51:28.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 9.836216228310798e-05 Training loss: 9.514908790588379
2025-12-09 12:51:29.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 9.835424592429567e-05 Training loss: 9.436443328857422
2025-12-09 12:51:29.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 9.834631080014457e-05 Training loss: 9.625385284423828
2025-12-09 12:51:29.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 9.833835691373413e-05 Training loss: 9.554601669311523
2025-12-09 12:51:30.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 9.83303842681511e-05 Training loss: 9.509346008300781
2025-12-09 12:51:30.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 9.83223928664895e-05 Training loss: 9.742769241333008
2025-12-09 12:51:30.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 9.831438271185065e-05 Training loss: 9.539152145385742
2025-12-09 12:51:31.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 9.830635380734313e-05 Training loss: 9.614425659179688
2025-12-09 12:51:31.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 9.82983061560828e-05 Training loss: 9.376667976379395
2025-12-09 12:51:32.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 9.829023976119279e-05 Training loss: 9.300714492797852
2025-12-09 12:51:32.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 9.828215462580353e-05 Training loss: 9.604120254516602
2025-12-09 12:51:32.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 9.827405075305267e-05 Training loss: 9.361040115356445
2025-12-09 12:51:33.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 9.826592814608518e-05 Training loss: 9.596179008483887
2025-12-09 12:51:33.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 9.825778680805331e-05 Training loss: 9.85051155090332
2025-12-09 12:51:33.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 9.824962674211653e-05 Training loss: 9.592777252197266
2025-12-09 12:51:34.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 9.824144795144159e-05 Training loss: 9.65841293334961
2025-12-09 12:51:34.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 9.823325043920254e-05 Training loss: 9.888496398925781
2025-12-09 12:51:35.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 9.822503420858069e-05 Training loss: 9.607723236083984
2025-12-09 12:51:35.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 9.821679926276456e-05 Training loss: 9.409372329711914
2025-12-09 12:51:35.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 9.820854560494999e-05 Training loss: 9.536983489990234
2025-12-09 12:51:36.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 9.820027323834006e-05 Training loss: 9.46429443359375
2025-12-09 12:51:36.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 9.819198216614512e-05 Training loss: 9.478227615356445
2025-12-09 12:51:36.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 9.818367239158278e-05 Training loss: 9.713602066040039
2025-12-09 12:51:37.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 9.817534391787789e-05 Training loss: 9.826409339904785
2025-12-09 12:51:37.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 9.816699674826255e-05 Training loss: 9.529056549072266
2025-12-09 12:51:38.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 9.815863088597618e-05 Training loss: 9.591349601745605
2025-12-09 12:51:38.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 9.815024633426538e-05 Training loss: 9.435553550720215
2025-12-09 12:51:38.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 9.814184309638402e-05 Training loss: 9.590415000915527
2025-12-09 12:51:39.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 9.813342117559323e-05 Training loss: 9.3385591506958
2025-12-09 12:51:39.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 9.812498057516143e-05 Training loss: 9.66147232055664
2025-12-09 12:51:39.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 9.811652129836421e-05 Training loss: 9.697051048278809
2025-12-09 12:51:40.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 9.810804334848449e-05 Training loss: 9.391319274902344
2025-12-09 12:51:40.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 9.809954672881238e-05 Training loss: 9.645404815673828
2025-12-09 12:51:41.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 9.809103144264525e-05 Training loss: 9.61992359161377
2025-12-09 12:51:41.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 9.808249749328768e-05 Training loss: 9.40546989440918
2025-12-09 12:51:41.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 9.80739448840516e-05 Training loss: 9.890178680419922
2025-12-09 12:51:42.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 9.806537361825606e-05 Training loss: 9.635104179382324
2025-12-09 12:51:42.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 9.805678369922742e-05 Training loss: 9.676231384277344
2025-12-09 12:51:42.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 9.804817513029927e-05 Training loss: 9.620488166809082
2025-12-09 12:51:43.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 9.803954791481239e-05 Training loss: 9.630942344665527
2025-12-09 12:51:43.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 9.803090205611487e-05 Training loss: 9.535919189453125
2025-12-09 12:51:43.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 9.802223755756198e-05 Training loss: 9.353310585021973
2025-12-09 12:51:44.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 9.801355442251625e-05 Training loss: 9.56106948852539
2025-12-09 12:51:44.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 9.800485265434744e-05 Training loss: 9.725973129272461
2025-12-09 12:51:45.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 9.799613225643253e-05 Training loss: 9.60973072052002
2025-12-09 12:51:45.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 9.798739323215574e-05 Training loss: 9.5874605178833
2025-12-09 12:51:45.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 9.797863558490849e-05 Training loss: 9.528306007385254
2025-12-09 12:51:46.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 9.79698593180895e-05 Training loss: 9.43315601348877
2025-12-09 12:51:46.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 9.796106443510462e-05 Training loss: 9.499345779418945
2025-12-09 12:51:46.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 9.795225093936702e-05 Training loss: 9.479406356811523
2025-12-09 12:51:47.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 9.794341883429699e-05 Training loss: 9.256861686706543
2025-12-09 12:51:47.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 9.793456812332215e-05 Training loss: 9.671095848083496
2025-12-09 12:51:48.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 9.792569880987726e-05 Training loss: 9.451074600219727
2025-12-09 12:51:48.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 9.791681089740432e-05 Training loss: 10.172457695007324
2025-12-09 12:51:48.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 9.790790438935256e-05 Training loss: 9.545124053955078
2025-12-09 12:51:49.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 9.789897928917847e-05 Training loss: 9.291988372802734
2025-12-09 12:51:49.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 9.789003560034561e-05 Training loss: 9.604820251464844
2025-12-09 12:51:49.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 9.788107332632495e-05 Training loss: 9.666090965270996
2025-12-09 12:51:50.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 9.787209247059452e-05 Training loss: 9.355164527893066
2025-12-09 12:51:50.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 9.786309303663963e-05 Training loss: 9.389183044433594
2025-12-09 12:51:51.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 9.785407502795278e-05 Training loss: 9.539765357971191
2025-12-09 12:51:51.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 9.784503844803368e-05 Training loss: 9.453765869140625
2025-12-09 12:51:51.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 9.783598330038925e-05 Training loss: 9.541646957397461
2025-12-09 12:51:52.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 9.782690958853362e-05 Training loss: 9.402876853942871
2025-12-09 12:51:52.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 9.781781731598812e-05 Training loss: 9.527145385742188
2025-12-09 12:51:52.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 9.780870648628128e-05 Training loss: 9.412039756774902
2025-12-09 12:51:53.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 9.779957710294886e-05 Training loss: 9.85310173034668
2025-12-09 12:51:53.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 9.779042916953376e-05 Training loss: 9.626404762268066
2025-12-09 12:51:54.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 9.778126268958613e-05 Training loss: 9.555002212524414
2025-12-09 12:51:54.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 9.77720776666633e-05 Training loss: 9.385469436645508
2025-12-09 12:51:54.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 9.77628741043298e-05 Training loss: 9.410762786865234
2025-12-09 12:51:55.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 9.775365200615735e-05 Training loss: 9.45082950592041
2025-12-09 12:51:55.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 9.774441137572487e-05 Training loss: 9.546320915222168
2025-12-09 12:51:55.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 9.773515221661846e-05 Training loss: 10.040142059326172
2025-12-09 12:51:56.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 9.772587453243143e-05 Training loss: 9.79920768737793
2025-12-09 12:51:56.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 9.771657832676427e-05 Training loss: 9.546760559082031
2025-12-09 12:51:57.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 9.770726360322463e-05 Training loss: 9.572799682617188
2025-12-09 12:51:57.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 9.769793036542741e-05 Training loss: 9.462076187133789
2025-12-09 12:51:57.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 9.768857861699463e-05 Training loss: 9.401036262512207
2025-12-09 12:51:58.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 9.767920836155553e-05 Training loss: 9.586628913879395
2025-12-09 12:51:58.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 9.766981960274653e-05 Training loss: 9.321526527404785
2025-12-09 12:51:58.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 9.766041234421122e-05 Training loss: 9.292618751525879
2025-12-09 12:51:59.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 9.765098658960036e-05 Training loss: 9.464165687561035
2025-12-09 12:51:59.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 9.764154234257192e-05 Training loss: 9.3021879196167
2025-12-09 12:52:00.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 9.763207960679101e-05 Training loss: 9.445879936218262
2025-12-09 12:52:00.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 9.762259838592994e-05 Training loss: 9.54053020477295
2025-12-09 12:52:00.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 9.761309868366819e-05 Training loss: 9.416152954101562
2025-12-09 12:52:01.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 9.760358050369243e-05 Training loss: 9.487835884094238
2025-12-09 12:52:01.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 9.759404384969643e-05 Training loss: 9.311820030212402
2025-12-09 12:52:01.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 9.758448872538122e-05 Training loss: 9.55954647064209
2025-12-09 12:52:02.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 9.757491513445493e-05 Training loss: 9.71151065826416
2025-12-09 12:52:02.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 9.756532308063293e-05 Training loss: 9.623552322387695
2025-12-09 12:52:02.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 9.755571256763765e-05 Training loss: 9.418081283569336
2025-12-09 12:52:03.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 9.754608359919879e-05 Training loss: 9.285870552062988
2025-12-09 12:52:03.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 9.753643617905313e-05 Training loss: 9.477951049804688
2025-12-09 12:52:04.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 9.752677031094466e-05 Training loss: 9.704615592956543
2025-12-09 12:52:04.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 9.751708599862452e-05 Training loss: 9.31175422668457
2025-12-09 12:52:04.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 9.750738324585098e-05 Training loss: 10.118170738220215
2025-12-09 12:52:05.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 9.749766205638952e-05 Training loss: 9.418313980102539
2025-12-09 12:52:05.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 9.748792243401273e-05 Training loss: 9.388702392578125
2025-12-09 12:52:05.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 9.747816438250037e-05 Training loss: 9.446166038513184
2025-12-09 12:52:06.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 9.746838790563934e-05 Training loss: 9.469132423400879
2025-12-09 12:52:06.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 9.74585930072237e-05 Training loss: 9.538459777832031
2025-12-09 12:52:07.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 9.744877969105469e-05 Training loss: 9.544912338256836
2025-12-09 12:52:07.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 9.743894796094062e-05 Training loss: 9.499017715454102
2025-12-09 12:52:07.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 9.742909782069701e-05 Training loss: 9.500014305114746
2025-12-09 12:52:08.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 9.741922927414651e-05 Training loss: 9.698504447937012
2025-12-09 12:52:08.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 9.740934232511894e-05 Training loss: 9.798140525817871
2025-12-09 12:52:08.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 9.739943697745118e-05 Training loss: 9.554183006286621
2025-12-09 12:52:09.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 9.738951323498732e-05 Training loss: 9.387386322021484
2025-12-09 12:52:09.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 9.737957110157858e-05 Training loss: 9.46336841583252
2025-12-09 12:52:10.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 9.736961058108332e-05 Training loss: 9.723668098449707
2025-12-09 12:52:10.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 9.735963167736698e-05 Training loss: 9.448946952819824
2025-12-09 12:52:10.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 9.734963439430222e-05 Training loss: 9.99081802368164
2025-12-09 12:52:11.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 9.733961873576878e-05 Training loss: 9.478556632995605
2025-12-09 12:52:11.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 9.732958470565353e-05 Training loss: 9.571450233459473
2025-12-09 12:52:11.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 9.731953230785049e-05 Training loss: 9.59181022644043
2025-12-09 12:52:12.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 9.730946154626079e-05 Training loss: 9.342340469360352
2025-12-09 12:52:12.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 9.729937242479271e-05 Training loss: 9.569840431213379
2025-12-09 12:52:13.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 9.728926494736164e-05 Training loss: 9.532388687133789
2025-12-09 12:52:13.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 9.727913911789009e-05 Training loss: 9.555962562561035
2025-12-09 12:52:13.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 9.726899494030768e-05 Training loss: 9.515738487243652
2025-12-09 12:52:14.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 9.725883241855119e-05 Training loss: 9.637195587158203
2025-12-09 12:52:14.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 9.724865155656448e-05 Training loss: 9.566598892211914
2025-12-09 12:52:14.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 9.723845235829857e-05 Training loss: 9.366483688354492
2025-12-09 12:52:15.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 9.722823482771155e-05 Training loss: 9.461221694946289
2025-12-09 12:52:15.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 9.721799896876864e-05 Training loss: 9.448797225952148
2025-12-09 12:52:16.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 9.720774478544219e-05 Training loss: 9.460980415344238
2025-12-09 12:52:16.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 9.719747228171163e-05 Training loss: 9.353549003601074
2025-12-09 12:52:16.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 9.718718146156355e-05 Training loss: 9.807722091674805
2025-12-09 12:52:17.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 9.717687232899159e-05 Training loss: 9.59178352355957
2025-12-09 12:52:17.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 9.716654488799652e-05 Training loss: 9.345242500305176
2025-12-09 12:52:17.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 9.715619914258624e-05 Training loss: 9.622410774230957
2025-12-09 12:52:18.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 9.71458350967757e-05 Training loss: 9.337639808654785
2025-12-09 12:52:18.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 9.713545275458703e-05 Training loss: 9.290398597717285
2025-12-09 12:52:19.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 9.712505212004938e-05 Training loss: 9.363354682922363
2025-12-09 12:52:19.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 9.711463319719904e-05 Training loss: 9.449009895324707
2025-12-09 12:52:19.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 9.710419599007939e-05 Training loss: 9.42398738861084
2025-12-09 12:52:20.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 9.70937405027409e-05 Training loss: 9.206798553466797
2025-12-09 12:52:20.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 9.708326673924115e-05 Training loss: 9.418911933898926
2025-12-09 12:52:20.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 9.707277470364482e-05 Training loss: 9.452187538146973
2025-12-09 12:52:21.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 9.706226440002363e-05 Training loss: 9.235246658325195
2025-12-09 12:52:21.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 9.705173583245645e-05 Training loss: 9.613638877868652
2025-12-09 12:52:21.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 9.704118900502919e-05 Training loss: 9.483325958251953
2025-12-09 12:52:22.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 9.703062392183489e-05 Training loss: 9.776250839233398
2025-12-09 12:52:22.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 9.702004058697363e-05 Training loss: 9.343948364257812
2025-12-09 12:52:23.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 9.700943900455262e-05 Training loss: 9.254881858825684
2025-12-09 12:52:23.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 9.69988191786861e-05 Training loss: 9.639301300048828
2025-12-09 12:52:23.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 9.698818111349543e-05 Training loss: 9.404766082763672
2025-12-09 12:52:24.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 9.697752481310904e-05 Training loss: 9.669953346252441
2025-12-09 12:52:24.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 9.696685028166244e-05 Training loss: 9.657225608825684
2025-12-09 12:52:24.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 9.69561575232982e-05 Training loss: 9.179664611816406
2025-12-09 12:52:25.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 9.694544654216596e-05 Training loss: 9.502224922180176
2025-12-09 12:52:25.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 9.693471734242243e-05 Training loss: 9.734192848205566
2025-12-09 12:52:26.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 9.692396992823145e-05 Training loss: 9.488277435302734
2025-12-09 12:52:26.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 9.691320430376385e-05 Training loss: 9.568770408630371
2025-12-09 12:52:26.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 9.690242047319755e-05 Training loss: 9.273031234741211
2025-12-09 12:52:27.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 9.689161844071757e-05 Training loss: 9.349818229675293
2025-12-09 12:52:27.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 9.688079821051595e-05 Training loss: 9.309855461120605
2025-12-09 12:52:27.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 9.68699597867918e-05 Training loss: 9.510257720947266
2025-12-09 12:52:28.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 9.685910317375134e-05 Training loss: 9.417768478393555
2025-12-09 12:52:28.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 9.684822837560776e-05 Training loss: 9.564151763916016
2025-12-09 12:52:29.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 9.683733539658139e-05 Training loss: 9.44261360168457
2025-12-09 12:52:29.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 9.682642424089958e-05 Training loss: 9.703060150146484
2025-12-09 12:52:29.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 9.681549491279674e-05 Training loss: 9.19759464263916
2025-12-09 12:52:30.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 9.68045474165143e-05 Training loss: 9.43382453918457
2025-12-09 12:52:30.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 9.679358175630081e-05 Training loss: 9.276407241821289
2025-12-09 12:52:30.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 9.67825979364118e-05 Training loss: 9.416631698608398
2025-12-09 12:52:31.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 9.677159596110987e-05 Training loss: 9.441117286682129
2025-12-09 12:52:31.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 9.676057583466472e-05 Training loss: 9.333950996398926
2025-12-09 12:52:32.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 9.674953756135298e-05 Training loss: 9.461149215698242
2025-12-09 12:52:32.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 9.673848114545843e-05 Training loss: 9.280868530273438
2025-12-09 12:52:32.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 9.672740659127184e-05 Training loss: 9.284035682678223
2025-12-09 12:52:33.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 9.671631390309102e-05 Training loss: 9.296886444091797
2025-12-09 12:52:33.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 9.670520308522084e-05 Training loss: 9.246354103088379
2025-12-09 12:52:33.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 9.66940741419732e-05 Training loss: 9.30221176147461
2025-12-09 12:52:34.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 9.668292707766699e-05 Training loss: 9.582077026367188
2025-12-09 12:52:34.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 9.667176189662818e-05 Training loss: 9.000906944274902
2025-12-09 12:52:35.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 9.666057860318979e-05 Training loss: 9.589763641357422
2025-12-09 12:52:35.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 9.66493772016918e-05 Training loss: 9.45382308959961
2025-12-09 12:52:35.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 9.663815769648129e-05 Training loss: 9.583028793334961
2025-12-09 12:52:36.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 9.66269200919123e-05 Training loss: 9.477340698242188
2025-12-09 12:52:36.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 9.661566439234593e-05 Training loss: 9.460004806518555
2025-12-09 12:52:36.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 9.660439060215031e-05 Training loss: 9.517366409301758
2025-12-09 12:52:37.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 9.659309872570058e-05 Training loss: 9.312129974365234
2025-12-09 12:52:37.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 9.658178876737886e-05 Training loss: 9.376846313476562
2025-12-09 12:52:38.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 9.657046073157436e-05 Training loss: 9.346946716308594
2025-12-09 12:52:38.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 9.655911462268327e-05 Training loss: 9.739412307739258
2025-12-09 12:52:38.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 9.65477504451088e-05 Training loss: 9.76757526397705
2025-12-09 12:52:39.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 9.653636820326113e-05 Training loss: 9.679176330566406
2025-12-09 12:52:39.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 9.652496790155751e-05 Training loss: 9.076191902160645
2025-12-09 12:52:39.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 9.651354954442218e-05 Training loss: 9.369670867919922
2025-12-09 12:52:40.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 9.650211313628637e-05 Training loss: 9.569026947021484
2025-12-09 12:52:40.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 9.649065868158832e-05 Training loss: 9.42232608795166
2025-12-09 12:52:41.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 9.64791861847733e-05 Training loss: 9.325969696044922
2025-12-09 12:52:41.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 9.646769565029354e-05 Training loss: 9.343382835388184
2025-12-09 12:52:41.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 9.64561870826083e-05 Training loss: 9.340557098388672
2025-12-09 12:52:42.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 9.644466048618385e-05 Training loss: 9.411357879638672
2025-12-09 12:52:42.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 9.643311586549342e-05 Training loss: 9.268568992614746
2025-12-09 12:52:42.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 9.642155322501725e-05 Training loss: 9.469903945922852
2025-12-09 12:52:43.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 9.640997256924257e-05 Training loss: 9.289041519165039
2025-12-09 12:52:43.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 9.639837390266361e-05 Training loss: 9.573980331420898
2025-12-09 12:52:43.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 9.638675722978161e-05 Training loss: 9.424476623535156
2025-12-09 12:52:44.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 9.637512255510475e-05 Training loss: 9.4098539352417
2025-12-09 12:52:44.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 9.63634698831482e-05 Training loss: 9.30477523803711
2025-12-09 12:52:45.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 9.635179921843418e-05 Training loss: 9.441316604614258
2025-12-09 12:52:45.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 9.634011056549182e-05 Training loss: 9.237652778625488
2025-12-09 12:52:45.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 9.632840392885727e-05 Training loss: 9.588113784790039
2025-12-09 12:52:46.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 9.631667931307364e-05 Training loss: 9.461227416992188
2025-12-09 12:52:46.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 9.630493672269102e-05 Training loss: 9.44157886505127
2025-12-09 12:52:46.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 9.629317616226649e-05 Training loss: 9.465545654296875
2025-12-09 12:52:47.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 9.628139763636408e-05 Training loss: 9.480231285095215
2025-12-09 12:52:47.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 9.626960114955483e-05 Training loss: 9.46264934539795
2025-12-09 12:52:48.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 9.62577867064167e-05 Training loss: 9.514487266540527
2025-12-09 12:52:48.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 9.624595431153467e-05 Training loss: 9.590744018554688
2025-12-09 12:52:48.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 9.623410396950064e-05 Training loss: 9.65434455871582
2025-12-09 12:52:49.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 9.62222356849135e-05 Training loss: 9.433405876159668
2025-12-09 12:52:49.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 9.621034946237911e-05 Training loss: 9.720502853393555
2025-12-09 12:52:49.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 9.619844530651027e-05 Training loss: 9.477336883544922
2025-12-09 12:52:50.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 9.618652322192675e-05 Training loss: 9.382425308227539
2025-12-09 12:52:50.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 9.61745832132553e-05 Training loss: 9.746809005737305
2025-12-09 12:52:51.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 9.616262528512957e-05 Training loss: 9.48220443725586
2025-12-09 12:52:51.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 9.615064944219022e-05 Training loss: 9.259673118591309
2025-12-09 12:52:51.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 9.613865568908485e-05 Training loss: 9.361222267150879
2025-12-09 12:52:52.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 9.612664403046797e-05 Training loss: 9.193299293518066
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 3.23 GiB is free. Including non-PyTorch memory, this process has 90.80 GiB memory in use. Of the allocated memory 89.27 GiB is allocated by PyTorch, and 792.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 50/10000 [00:00<00:19, 499.17it/s]Tokenizing texts:   1%|▏         | 135/10000 [00:00<00:13, 705.17it/s]Tokenizing texts:   2%|▏         | 206/10000 [00:00<00:15, 627.67it/s]Tokenizing texts:   3%|▎         | 275/10000 [00:00<00:15, 645.90it/s]Tokenizing texts:   3%|▎         | 341/10000 [00:00<00:15, 617.66it/s]Tokenizing texts:   4%|▍         | 404/10000 [00:00<00:15, 608.69it/s]Tokenizing texts:   5%|▍         | 466/10000 [00:00<00:16, 577.77it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 577.41it/s]Tokenizing texts:   6%|▌         | 597/10000 [00:00<00:15, 603.16it/s]Tokenizing texts:   7%|▋         | 658/10000 [00:01<00:15, 596.39it/s]Tokenizing texts:   7%|▋         | 718/10000 [00:01<00:15, 580.81it/s]Tokenizing texts:   8%|▊         | 777/10000 [00:01<00:15, 581.94it/s]Tokenizing texts:   8%|▊         | 836/10000 [00:01<00:15, 577.83it/s]Tokenizing texts:   9%|▉         | 899/10000 [00:01<00:15, 591.63it/s]Tokenizing texts:  10%|▉         | 981/10000 [00:01<00:13, 657.45it/s]Tokenizing texts:  11%|█         | 1051/10000 [00:01<00:14, 637.99it/s]Tokenizing texts:  11%|█         | 1116/10000 [00:01<00:13, 635.84it/s]Tokenizing texts:  12%|█▏        | 1180/10000 [00:01<00:14, 601.93it/s]Tokenizing texts:  12%|█▏        | 1241/10000 [00:02<00:14, 593.86it/s]Tokenizing texts:  13%|█▎        | 1308/10000 [00:02<00:14, 614.20it/s]Tokenizing texts:  14%|█▎        | 1370/10000 [00:02<00:14, 586.86it/s]Tokenizing texts:  14%|█▍        | 1440/10000 [00:02<00:13, 617.22it/s]Tokenizing texts:  15%|█▌        | 1518/10000 [00:02<00:12, 653.31it/s]Tokenizing texts:  16%|█▌        | 1594/10000 [00:02<00:12, 682.56it/s]Tokenizing texts:  17%|█▋        | 1663/10000 [00:02<00:13, 614.92it/s]Tokenizing texts:  17%|█▋        | 1726/10000 [00:02<00:13, 613.11it/s]Tokenizing texts:  18%|█▊        | 1793/10000 [00:02<00:13, 628.23it/s]Tokenizing texts:  19%|█▊        | 1857/10000 [00:03<00:13, 609.87it/s]Tokenizing texts:  19%|█▉        | 1919/10000 [00:03<00:13, 609.63it/s]Tokenizing texts:  20%|█▉        | 1981/10000 [00:03<00:13, 578.02it/s]Tokenizing texts:  21%|██        | 2052/10000 [00:03<00:12, 612.04it/s]Tokenizing texts:  21%|██        | 2114/10000 [00:03<00:14, 559.63it/s]Tokenizing texts:  22%|██▏       | 2172/10000 [00:03<00:13, 560.14it/s]Tokenizing texts:  22%|██▏       | 2249/10000 [00:03<00:12, 616.79it/s]Tokenizing texts:  23%|██▎       | 2312/10000 [00:03<00:12, 599.59it/s]Tokenizing texts:  24%|██▍       | 2381/10000 [00:03<00:12, 624.79it/s]Tokenizing texts:  24%|██▍       | 2445/10000 [00:04<00:12, 628.84it/s]Tokenizing texts:  25%|██▌       | 2509/10000 [00:04<00:11, 625.08it/s]Tokenizing texts:  26%|██▌       | 2572/10000 [00:04<00:12, 612.01it/s]Tokenizing texts:  26%|██▋       | 2634/10000 [00:04<00:12, 574.50it/s]Tokenizing texts:  27%|██▋       | 2694/10000 [00:04<00:13, 553.77it/s]Tokenizing texts:  28%|██▊       | 2768/10000 [00:04<00:11, 603.13it/s]Tokenizing texts:  28%|██▊       | 2830/10000 [00:04<00:12, 555.28it/s]Tokenizing texts:  29%|██▉       | 2889/10000 [00:04<00:12, 559.51it/s]Tokenizing texts:  30%|██▉       | 2952/10000 [00:04<00:12, 577.36it/s]Tokenizing texts:  30%|███       | 3024/10000 [00:05<00:11, 599.66it/s]Tokenizing texts:  31%|███       | 3086/10000 [00:05<00:11, 598.40it/s]Tokenizing texts:  32%|███▏      | 3153/10000 [00:05<00:11, 610.24it/s]Tokenizing texts:  32%|███▏      | 3227/10000 [00:05<00:10, 646.50it/s]Tokenizing texts:  33%|███▎      | 3294/10000 [00:05<00:10, 646.20it/s]Tokenizing texts:  34%|███▎      | 3361/10000 [00:05<00:10, 649.02it/s]Tokenizing texts:  34%|███▍      | 3427/10000 [00:05<00:10, 650.58it/s]Tokenizing texts:  35%|███▍      | 3497/10000 [00:05<00:09, 651.38it/s]Tokenizing texts:  36%|███▌      | 3565/10000 [00:05<00:09, 659.60it/s]Tokenizing texts:  36%|███▋      | 3632/10000 [00:05<00:09, 659.00it/s]Tokenizing texts:  37%|███▋      | 3698/10000 [00:06<00:09, 631.35it/s]Tokenizing texts:  38%|███▊      | 3762/10000 [00:06<00:10, 616.84it/s]Tokenizing texts:  38%|███▊      | 3824/10000 [00:06<00:10, 584.82it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:06<00:09, 616.57it/s]Tokenizing texts:  40%|███▉      | 3958/10000 [00:06<00:10, 603.12it/s]Tokenizing texts:  40%|████      | 4019/10000 [00:06<00:10, 588.93it/s]Tokenizing texts:  41%|████      | 4096/10000 [00:06<00:09, 636.81it/s]Tokenizing texts:  42%|████▏     | 4169/10000 [00:06<00:08, 661.94it/s]Tokenizing texts:  42%|████▏     | 4236/10000 [00:06<00:08, 661.19it/s]Tokenizing texts:  43%|████▎     | 4303/10000 [00:07<00:09, 629.60it/s]Tokenizing texts:  44%|████▍     | 4382/10000 [00:07<00:08, 674.32it/s]Tokenizing texts:  45%|████▍     | 4451/10000 [00:07<00:08, 669.64it/s]Tokenizing texts:  45%|████▌     | 4519/10000 [00:07<00:08, 640.10it/s]Tokenizing texts:  46%|████▌     | 4585/10000 [00:07<00:08, 641.57it/s]Tokenizing texts:  47%|████▋     | 4660/10000 [00:07<00:07, 670.01it/s]Tokenizing texts:  47%|████▋     | 4739/10000 [00:07<00:07, 702.61it/s]Tokenizing texts:  48%|████▊     | 4810/10000 [00:07<00:07, 661.74it/s]Tokenizing texts:  49%|████▉     | 4882/10000 [00:07<00:07, 677.03it/s]Tokenizing texts:  50%|████▉     | 4953/10000 [00:07<00:07, 683.48it/s]Tokenizing texts:  50%|█████     | 5029/10000 [00:08<00:07, 701.64it/s]Tokenizing texts:  51%|█████     | 5100/10000 [00:08<00:07, 651.00it/s]Tokenizing texts:  52%|█████▏    | 5166/10000 [00:08<00:07, 631.85it/s]Tokenizing texts:  52%|█████▏    | 5230/10000 [00:08<00:07, 632.57it/s]Tokenizing texts:  53%|█████▎    | 5299/10000 [00:08<00:07, 648.50it/s]Tokenizing texts:  54%|█████▍    | 5383/10000 [00:08<00:06, 702.74it/s]Tokenizing texts:  55%|█████▍    | 5454/10000 [00:08<00:06, 687.78it/s]Tokenizing texts:  55%|█████▌    | 5524/10000 [00:08<00:07, 626.84it/s]Tokenizing texts:  56%|█████▌    | 5592/10000 [00:08<00:06, 641.16it/s]Tokenizing texts:  57%|█████▋    | 5677/10000 [00:09<00:06, 699.23it/s]Tokenizing texts:  57%|█████▋    | 5749/10000 [00:09<00:06, 671.52it/s]Tokenizing texts:  58%|█████▊    | 5818/10000 [00:09<00:06, 609.25it/s]Tokenizing texts:  59%|█████▉    | 5888/10000 [00:09<00:06, 629.10it/s]Tokenizing texts:  60%|█████▉    | 5953/10000 [00:09<00:06, 620.13it/s]Tokenizing texts:  60%|██████    | 6016/10000 [00:09<00:06, 607.65it/s]Tokenizing texts:  61%|██████    | 6090/10000 [00:09<00:06, 643.83it/s]Tokenizing texts:  62%|██████▏   | 6161/10000 [00:09<00:05, 661.21it/s]Tokenizing texts:  62%|██████▏   | 6228/10000 [00:09<00:05, 643.74it/s]Tokenizing texts:  63%|██████▎   | 6293/10000 [00:10<00:05, 629.58it/s]Tokenizing texts:  64%|██████▎   | 6357/10000 [00:10<00:05, 612.94it/s]Tokenizing texts:  64%|██████▍   | 6424/10000 [00:10<00:05, 628.71it/s]Tokenizing texts:  65%|██████▍   | 6488/10000 [00:10<00:05, 623.17it/s]Tokenizing texts:  66%|██████▌   | 6552/10000 [00:10<00:05, 626.72it/s]Tokenizing texts:  66%|██████▋   | 6625/10000 [00:10<00:05, 650.09it/s]Tokenizing texts:  67%|██████▋   | 6691/10000 [00:10<00:05, 622.12it/s]Tokenizing texts:  68%|██████▊   | 6763/10000 [00:10<00:04, 648.71it/s]Tokenizing texts:  68%|██████▊   | 6847/10000 [00:10<00:04, 702.94it/s]Tokenizing texts:  69%|██████▉   | 6918/10000 [00:11<00:04, 617.75it/s]Tokenizing texts:  70%|██████▉   | 6985/10000 [00:11<00:04, 625.83it/s]Tokenizing texts:  70%|███████   | 7050/10000 [00:11<00:04, 611.80it/s]Tokenizing texts:  71%|███████   | 7113/10000 [00:11<00:04, 582.06it/s]Tokenizing texts:  72%|███████▏  | 7176/10000 [00:11<00:04, 593.64it/s]Tokenizing texts:  72%|███████▏  | 7244/10000 [00:11<00:04, 611.92it/s]Tokenizing texts:  73%|███████▎  | 7330/10000 [00:11<00:03, 680.34it/s]Tokenizing texts:  74%|███████▍  | 7399/10000 [00:11<00:04, 635.80it/s]Tokenizing texts:  75%|███████▍  | 7464/10000 [00:11<00:04, 581.26it/s]Tokenizing texts:  75%|███████▌  | 7543/10000 [00:12<00:03, 636.35it/s]Tokenizing texts:  76%|███████▌  | 7617/10000 [00:12<00:03, 664.54it/s]Tokenizing texts:  77%|███████▋  | 7689/10000 [00:12<00:03, 678.07it/s]Tokenizing texts:  78%|███████▊  | 7758/10000 [00:12<00:03, 623.26it/s]Tokenizing texts:  78%|███████▊  | 7822/10000 [00:12<00:03, 620.30it/s]Tokenizing texts:  79%|███████▉  | 7886/10000 [00:12<00:03, 614.95it/s]Tokenizing texts:  80%|███████▉  | 7966/10000 [00:12<00:03, 665.59it/s]Tokenizing texts:  80%|████████  | 8035/10000 [00:12<00:02, 672.36it/s]Tokenizing texts:  81%|████████  | 8103/10000 [00:12<00:03, 582.19it/s]Tokenizing texts:  82%|████████▏ | 8166/10000 [00:13<00:03, 593.83it/s]Tokenizing texts:  82%|████████▏ | 8229/10000 [00:13<00:02, 598.30it/s]Tokenizing texts:  83%|████████▎ | 8291/10000 [00:13<00:02, 588.54it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:13<00:02, 604.37it/s]Tokenizing texts:  84%|████████▍ | 8418/10000 [00:13<00:02, 582.93it/s]Tokenizing texts:  85%|████████▍ | 8495/10000 [00:13<00:02, 635.36it/s]Tokenizing texts:  86%|████████▌ | 8561/10000 [00:13<00:02, 637.08it/s]Tokenizing texts:  86%|████████▋ | 8634/10000 [00:13<00:02, 661.31it/s]Tokenizing texts:  87%|████████▋ | 8706/10000 [00:13<00:01, 677.80it/s]Tokenizing texts:  88%|████████▊ | 8779/10000 [00:13<00:01, 691.81it/s]Tokenizing texts:  88%|████████▊ | 8849/10000 [00:14<00:01, 601.66it/s]Tokenizing texts:  89%|████████▉ | 8930/10000 [00:14<00:01, 652.54it/s]Tokenizing texts:  90%|████████▉ | 8998/10000 [00:14<00:01, 627.55it/s]Tokenizing texts:  91%|█████████ | 9063/10000 [00:14<00:01, 631.85it/s]Tokenizing texts:  91%|█████████▏| 9133/10000 [00:14<00:01, 648.11it/s]Tokenizing texts:  92%|█████████▏| 9210/10000 [00:14<00:01, 682.58it/s]Tokenizing texts:  93%|█████████▎| 9280/10000 [00:14<00:01, 636.78it/s]Tokenizing texts:  93%|█████████▎| 9345/10000 [00:14<00:01, 633.14it/s]Tokenizing texts:  94%|█████████▍| 9410/10000 [00:15<00:00, 600.74it/s]Tokenizing texts:  95%|█████████▍| 9474/10000 [00:15<00:00, 604.76it/s]Tokenizing texts:  95%|█████████▌| 9536/10000 [00:15<00:00, 566.28it/s]Tokenizing texts:  96%|█████████▌| 9597/10000 [00:15<00:00, 571.81it/s]Tokenizing texts:  97%|█████████▋| 9659/10000 [00:15<00:00, 582.44it/s]Tokenizing texts:  97%|█████████▋| 9718/10000 [00:15<00:00, 579.22it/s]Tokenizing texts:  98%|█████████▊| 9777/10000 [00:15<00:00, 551.84it/s]Tokenizing texts:  99%|█████████▊| 9856/10000 [00:15<00:00, 617.72it/s]Tokenizing texts:  99%|█████████▉| 9926/10000 [00:15<00:00, 640.83it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:15<00:00, 626.01it/s]
2025-12-09 12:53:43.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 12.203877449035645
2025-12-09 12:53:44.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 12.191946983337402
2025-12-09 12:53:44.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 12.120243072509766
2025-12-09 12:53:45.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 12.237682342529297
2025-12-09 12:53:45.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 12.169525146484375
2025-12-09 12:53:45.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 12.21381950378418
2025-12-09 12:53:46.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 12.136531829833984
2025-12-09 12:53:46.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 12.173645973205566
2025-12-09 12:53:46.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 12.227005958557129
2025-12-09 12:53:47.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 12.248133659362793
2025-12-09 12:53:47.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 12.178958892822266
2025-12-09 12:53:48.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 12.216499328613281
2025-12-09 12:53:48.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 12.20379638671875
2025-12-09 12:53:48.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 12.182133674621582
2025-12-09 12:53:49.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 12.173523902893066
2025-12-09 12:53:49.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 12.212797164916992
2025-12-09 12:53:49.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 12.161209106445312
2025-12-09 12:53:50.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 12.255350112915039
2025-12-09 12:53:50.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 12.266639709472656
2025-12-09 12:53:50.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 12.12193489074707
2025-12-09 12:53:51.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 12.137989044189453
2025-12-09 12:53:51.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 12.195158004760742
2025-12-09 12:53:52.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 12.150286674499512
2025-12-09 12:53:52.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 12.177170753479004
2025-12-09 12:53:52.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 12.124739646911621
2025-12-09 12:53:53.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 12.147971153259277
2025-12-09 12:53:53.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 12.183771133422852
2025-12-09 12:53:53.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 12.139851570129395
2025-12-09 12:53:54.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 12.124486923217773
2025-12-09 12:53:54.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 12.168231964111328
2025-12-09 12:53:55.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 12.168315887451172
2025-12-09 12:53:55.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 12.111981391906738
2025-12-09 12:53:55.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 12.11130428314209
2025-12-09 12:53:56.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 12.182177543640137
2025-12-09 12:53:56.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 12.096768379211426
2025-12-09 12:53:56.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 12.045478820800781
2025-12-09 12:53:57.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 12.11401081085205
2025-12-09 12:53:57.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 12.106844902038574
2025-12-09 12:53:58.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 12.083673477172852
2025-12-09 12:53:58.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 12.095328330993652
2025-12-09 12:53:58.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 12.057110786437988
2025-12-09 12:53:59.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 12.0422945022583
2025-12-09 12:53:59.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 12.040175437927246
2025-12-09 12:53:59.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 12.014174461364746
2025-12-09 12:54:00.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 12.033540725708008
2025-12-09 12:54:00.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 12.046425819396973
2025-12-09 12:54:01.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 11.996696472167969
2025-12-09 12:54:01.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 12.042367935180664
2025-12-09 12:54:01.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 11.950074195861816
2025-12-09 12:54:02.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 11.988748550415039
2025-12-09 12:54:02.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 11.980616569519043
2025-12-09 12:54:02.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 12.026400566101074
2025-12-09 12:54:03.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 11.98784065246582
2025-12-09 12:54:03.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 11.955266952514648
2025-12-09 12:54:04.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 11.91154670715332
2025-12-09 12:54:04.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 12.019571304321289
2025-12-09 12:54:04.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 12.011927604675293
2025-12-09 12:54:05.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 11.887373924255371
2025-12-09 12:54:05.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 11.82070541381836
2025-12-09 12:54:05.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 11.820520401000977
2025-12-09 12:54:06.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 11.801372528076172
2025-12-09 12:54:06.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 11.889659881591797
2025-12-09 12:54:07.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 11.804450035095215
2025-12-09 12:54:07.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 11.867417335510254
2025-12-09 12:54:07.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 11.780656814575195
2025-12-09 12:54:08.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 11.580550193786621
2025-12-09 12:54:08.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 11.803400993347168
2025-12-09 12:54:08.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 11.63109016418457
2025-12-09 12:54:09.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 11.78532886505127
2025-12-09 12:54:09.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 11.728583335876465
2025-12-09 12:54:10.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 11.661027908325195
2025-12-09 12:54:10.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 11.751740455627441
2025-12-09 12:54:10.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 11.506093978881836
2025-12-09 12:54:11.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 11.647452354431152
2025-12-09 12:54:11.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 11.613743782043457
2025-12-09 12:54:11.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 11.73453140258789
2025-12-09 12:54:12.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 11.476470947265625
2025-12-09 12:54:12.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 11.784992218017578
2025-12-09 12:54:12.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 11.302520751953125
2025-12-09 12:54:13.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 11.42880630493164
2025-12-09 12:54:13.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 11.326547622680664
2025-12-09 12:54:14.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 11.255294799804688
2025-12-09 12:54:14.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 11.239041328430176
2025-12-09 12:54:14.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 11.223502159118652
2025-12-09 12:54:15.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 11.151359558105469
2025-12-09 12:54:15.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 11.159197807312012
2025-12-09 12:54:15.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 11.091970443725586
2025-12-09 12:54:16.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 10.967589378356934
2025-12-09 12:54:16.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 10.885013580322266
2025-12-09 12:54:17.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 10.869625091552734
2025-12-09 12:54:17.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 10.823784828186035
2025-12-09 12:54:17.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 10.650283813476562
2025-12-09 12:54:18.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 10.938807487487793
2025-12-09 12:54:18.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 10.703385353088379
2025-12-09 12:54:18.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 10.443281173706055
2025-12-09 12:54:19.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 11.064423561096191
2025-12-09 12:54:19.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 10.65998649597168
2025-12-09 12:54:20.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 10.609807014465332
2025-12-09 12:54:20.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 10.61587142944336
2025-12-09 12:54:20.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 10.869775772094727
2025-12-09 12:54:21.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999997089396425 Training loss: 10.365324020385742
2025-12-09 12:54:21.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029999988357586825 Training loss: 10.497885704040527
2025-12-09 12:54:21.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000299999738045746 Training loss: 10.383729934692383
2025-12-09 12:54:22.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0002999995343036539 Training loss: 10.799601554870605
2025-12-09 12:54:22.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00029999927234967104 Training loss: 11.08602237701416
2025-12-09 12:54:23.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.00029999895218389905 Training loss: 10.65307331085205
2025-12-09 12:54:23.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0002999985738064622 Training loss: 10.39223861694336
2025-12-09 12:54:23.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00029999813721750737 Training loss: 10.444615364074707
2025-12-09 12:54:24.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.00029999764241720394 Training loss: 10.457887649536133
2025-12-09 12:54:24.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0002999970894057439 Training loss: 10.492886543273926
2025-12-09 12:54:24.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029999647818334195 Training loss: 10.306450843811035
2025-12-09 12:54:25.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0002999958087502352 Training loss: 10.295165061950684
2025-12-09 12:54:25.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.00029999508110668355 Training loss: 10.299060821533203
2025-12-09 12:54:26.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0002999942952529693 Training loss: 10.415765762329102
2025-12-09 12:54:26.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00029999345118939745 Training loss: 10.070266723632812
2025-12-09 12:54:26.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0002999925489162956 Training loss: 10.279041290283203
2025-12-09 12:54:27.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029999158843401386 Training loss: 10.264524459838867
2025-12-09 12:54:27.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.000299990569742925 Training loss: 10.459352493286133
2025-12-09 12:54:27.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002999894928434243 Training loss: 10.345303535461426
2025-12-09 12:54:28.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.00029998835773592975 Training loss: 10.643854141235352
2025-12-09 12:54:28.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.00029998716442088184 Training loss: 9.987205505371094
2025-12-09 12:54:29.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0002999859128987437 Training loss: 10.181700706481934
2025-12-09 12:54:29.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00029998460317000097 Training loss: 10.088730812072754
2025-12-09 12:54:29.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00029998323523516195 Training loss: 10.06936264038086
2025-12-09 12:54:30.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002999818090947575 Training loss: 9.919434547424316
2025-12-09 12:54:30.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00029998032474934106 Training loss: 10.162863731384277
2025-12-09 12:54:30.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0002999787821994887 Training loss: 9.946142196655273
2025-12-09 12:54:31.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00029997718144579913 Training loss: 10.2642183303833
2025-12-09 12:54:31.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0002999755224888935 Training loss: 10.157005310058594
2025-12-09 12:54:32.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029997380532941555 Training loss: 10.28645133972168
2025-12-09 12:54:32.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00029997202996803177 Training loss: 10.008413314819336
2025-12-09 12:54:32.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0002999701964054312 Training loss: 10.215729713439941
2025-12-09 12:54:33.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0002999683046423252 Training loss: 10.282170295715332
2025-12-09 12:54:33.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0002999663546794481 Training loss: 10.469746589660645
2025-12-09 12:54:33.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.00029996434651755657 Training loss: 10.454437255859375
2025-12-09 12:54:34.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.00029996228015743 Training loss: 10.460734367370605
2025-12-09 12:54:34.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002999601555998703 Training loss: 9.90141773223877
2025-12-09 12:54:34.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0002999579728457019 Training loss: 9.976826667785645
2025-12-09 12:54:35.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0002999557318957719 Training loss: 10.374045372009277
2025-12-09 12:54:35.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.00029995343275095003 Training loss: 10.068692207336426
2025-12-09 12:54:36.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.00029995107541212843 Training loss: 9.908307075500488
2025-12-09 12:54:36.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00029994865988022205 Training loss: 9.801227569580078
2025-12-09 12:54:36.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0002999461861561683 Training loss: 9.921012878417969
2025-12-09 12:54:37.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0002999436542409271 Training loss: 10.405194282531738
2025-12-09 12:54:37.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0002999410641354812 Training loss: 9.794981002807617
2025-12-09 12:54:37.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00029993841584083553 Training loss: 9.849342346191406
2025-12-09 12:54:38.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00029993570935801805 Training loss: 10.263554573059082
2025-12-09 12:54:38.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.000299932944688079 Training loss: 9.743358612060547
2025-12-09 12:54:39.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.00029993012183209135 Training loss: 10.04559326171875
2025-12-09 12:54:39.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002999272407911505 Training loss: 10.03580093383789
2025-12-09 12:54:39.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.00029992430156637454 Training loss: 10.00192928314209
2025-12-09 12:54:40.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00029992130415890426 Training loss: 9.77278995513916
2025-12-09 12:54:40.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.00029991824856990276 Training loss: 10.072216033935547
2025-12-09 12:54:40.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0002999151348005559 Training loss: 9.466947555541992
2025-12-09 12:54:41.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0002999119628520721 Training loss: 9.843805313110352
2025-12-09 12:54:41.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.00029990873272568226 Training loss: 9.865580558776855
2025-12-09 12:54:42.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.00029990544442263996 Training loss: 10.221102714538574
2025-12-09 12:54:42.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002999020979442214 Training loss: 9.741836547851562
2025-12-09 12:54:42.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0002998986932917252 Training loss: 9.891409873962402
2025-12-09 12:54:43.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.00029989523046647257 Training loss: 9.882384300231934
2025-12-09 12:54:43.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00029989170946980755 Training loss: 9.904751777648926
2025-12-09 12:54:43.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00029988813030309644 Training loss: 9.982861518859863
2025-12-09 12:54:44.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0002998844929677283 Training loss: 10.038582801818848
2025-12-09 12:54:44.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00029988079746511465 Training loss: 10.689109802246094
2025-12-09 12:54:45.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.00029987704379668973 Training loss: 9.894947052001953
2025-12-09 12:54:45.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0002998732319639102 Training loss: 9.716365814208984
2025-12-09 12:54:45.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.00029986936196825536 Training loss: 10.026769638061523
2025-12-09 12:54:46.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0002998654338112271 Training loss: 10.057333946228027
2025-12-09 12:54:46.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00029986144749434985 Training loss: 9.74780559539795
2025-12-09 12:54:46.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0002998574030191706 Training loss: 9.833230972290039
2025-12-09 12:54:47.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.000299853300387259 Training loss: 9.817741394042969
2025-12-09 12:54:47.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00029984913960020714 Training loss: 10.075234413146973
2025-12-09 12:54:48.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.00029984492065962976 Training loss: 9.659953117370605
2025-12-09 12:54:48.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00029984064356716414 Training loss: 9.959181785583496
2025-12-09 12:54:48.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0002998363083244701 Training loss: 10.499865531921387
2025-12-09 12:54:49.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00029983191493323017 Training loss: 9.521554946899414
2025-12-09 12:54:49.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0002998274633951493 Training loss: 9.720048904418945
2025-12-09 12:54:49.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00029982295371195494 Training loss: 9.770681381225586
2025-12-09 12:54:50.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00029981838588539735 Training loss: 9.644274711608887
2025-12-09 12:54:50.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.00029981375991724915 Training loss: 9.706631660461426
2025-12-09 12:54:50.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0002998090758093056 Training loss: 9.807294845581055
2025-12-09 12:54:51.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00029980433356338447 Training loss: 9.671335220336914
2025-12-09 12:54:51.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0002997995331813262 Training loss: 9.656155586242676
2025-12-09 12:54:52.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.00029979467466499367 Training loss: 9.573551177978516
2025-12-09 12:54:52.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00029978975801627243 Training loss: 9.899996757507324
2025-12-09 12:54:52.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0002997847832370704 Training loss: 9.636026382446289
2025-12-09 12:54:53.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0002997797503293184 Training loss: 9.946096420288086
2025-12-09 12:54:53.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00029977465929496947 Training loss: 10.012309074401855
2025-12-09 12:54:53.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0002997695101359994 Training loss: 9.570425987243652
2025-12-09 12:54:54.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0002997643028544064 Training loss: 9.519295692443848
2025-12-09 12:54:54.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0002997590374522114 Training loss: 9.60371208190918
2025-12-09 12:54:55.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0002997537139314577 Training loss: 9.574197769165039
2025-12-09 12:54:55.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0002997483322942114 Training loss: 9.505603790283203
2025-12-09 12:54:55.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0002997428925425609 Training loss: 9.809460639953613
2025-12-09 12:54:56.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0002997373946786173 Training loss: 9.683867454528809
2025-12-09 12:54:56.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.00029973183870451417 Training loss: 9.710411071777344
2025-12-09 12:54:56.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0002997262246224077 Training loss: 9.725656509399414
2025-12-09 12:54:57.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00029972055243447665 Training loss: 9.83613395690918
2025-12-09 12:54:57.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.00029971482214292223 Training loss: 9.710294723510742
2025-12-09 12:54:58.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.00029970903374996826 Training loss: 9.759239196777344
2025-12-09 12:54:58.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0002997031872578611 Training loss: 9.389281272888184
2025-12-09 12:54:58.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00029969728266886973 Training loss: 9.617593765258789
2025-12-09 12:54:59.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00029969131998528554 Training loss: 9.326614379882812
2025-12-09 12:54:59.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0002996852992094225 Training loss: 9.400396347045898
2025-12-09 12:54:59.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.00029967922034361723 Training loss: 9.542825698852539
2025-12-09 12:55:00.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0002996730833902287 Training loss: 9.540346145629883
2025-12-09 12:55:00.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00029966688835163875 Training loss: 9.762099266052246
2025-12-09 12:55:01.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00029966063523025136 Training loss: 9.478100776672363
2025-12-09 12:55:01.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00029965432402849333 Training loss: 9.395551681518555
2025-12-09 12:55:01.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0002996479547488139 Training loss: 9.478132247924805
2025-12-09 12:55:02.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0002996415273936849 Training loss: 9.523140907287598
2025-12-09 12:55:02.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00029963504196560056 Training loss: 9.769227027893066
2025-12-09 12:55:02.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.00029962849846707786 Training loss: 9.206005096435547
2025-12-09 12:55:03.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0002996218969006561 Training loss: 9.492788314819336
2025-12-09 12:55:03.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00029961523726889733 Training loss: 9.445049285888672
2025-12-09 12:55:04.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00029960851957438594 Training loss: 9.643376350402832
2025-12-09 12:55:04.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.000299601743819729 Training loss: 9.716588973999023
2025-12-09 12:55:04.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00029959491000755594 Training loss: 9.622349739074707
2025-12-09 12:55:05.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00029958801814051897 Training loss: 9.663907051086426
2025-12-09 12:55:05.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0002995810682212926 Training loss: 9.946918487548828
2025-12-09 12:55:05.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0002995740602525739 Training loss: 9.718887329101562
2025-12-09 12:55:06.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0002995669942370827 Training loss: 9.646048545837402
2025-12-09 12:55:06.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.000299559870177561 Training loss: 9.694482803344727
2025-12-09 12:55:07.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0002995526880767737 Training loss: 9.539823532104492
2025-12-09 12:55:07.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.00029954544793750785 Training loss: 9.50555419921875
2025-12-09 12:55:07.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00029953814976257335 Training loss: 9.455615997314453
2025-12-09 12:55:08.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0002995307935548024 Training loss: 9.34410285949707
2025-12-09 12:55:08.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0002995233793170498 Training loss: 9.332966804504395
2025-12-09 12:55:08.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.00029951590705219283 Training loss: 9.322219848632812
2025-12-09 12:55:09.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0002995083767631314 Training loss: 9.24411392211914
2025-12-09 12:55:09.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0002995007884527879 Training loss: 9.693151473999023
2025-12-09 12:55:09.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.00029949314212410715 Training loss: 9.551342964172363
2025-12-09 12:55:10.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.00029948543778005656 Training loss: 9.30289077758789
2025-12-09 12:55:10.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00029947767542362597 Training loss: 9.459869384765625
2025-12-09 12:55:11.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0002994698550578279 Training loss: 9.850378036499023
2025-12-09 12:55:11.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0002994619766856972 Training loss: 10.126166343688965
2025-12-09 12:55:11.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00029945404031029134 Training loss: 9.499249458312988
2025-12-09 12:55:12.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.00029944604593469033 Training loss: 9.712681770324707
2025-12-09 12:55:12.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00029943799356199656 Training loss: 9.400822639465332
2025-12-09 12:55:12.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.00029942988319533504 Training loss: 9.268694877624512
2025-12-09 12:55:13.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0002994217148378532 Training loss: 9.456790924072266
2025-12-09 12:55:13.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00029941348849272105 Training loss: 10.06268310546875
2025-12-09 12:55:14.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0002994052041631311 Training loss: 9.3966064453125
2025-12-09 12:55:14.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0002993968618522982 Training loss: 9.702396392822266
2025-12-09 12:55:14.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0002993884615634601 Training loss: 9.595876693725586
2025-12-09 12:55:15.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.00029938000329987645 Training loss: 9.474395751953125
2025-12-09 12:55:15.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00029937148706483003 Training loss: 9.241410255432129
2025-12-09 12:55:15.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.00029936291286162577 Training loss: 9.689478874206543
2025-12-09 12:55:16.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.000299354280693591 Training loss: 9.503055572509766
2025-12-09 12:55:16.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0002993455905640758 Training loss: 9.350439071655273
2025-12-09 12:55:17.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0002993368424764526 Training loss: 9.530940055847168
2025-12-09 12:55:17.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0002993280364341165 Training loss: 9.29137897491455
2025-12-09 12:55:17.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00029931917244048473 Training loss: 10.15758228302002
2025-12-09 12:55:18.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0002993102504989974 Training loss: 9.476675987243652
2025-12-09 12:55:18.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00029930127061311685 Training loss: 9.928210258483887
2025-12-09 12:55:18.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0002992922327863281 Training loss: 9.40673542022705
2025-12-09 12:55:19.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00029928313702213844 Training loss: 9.243611335754395
2025-12-09 12:55:19.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.00029927398332407784 Training loss: 9.67699909210205
2025-12-09 12:55:20.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00029926477169569865 Training loss: 9.561589241027832
2025-12-09 12:55:20.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00029925550214057565 Training loss: 10.006404876708984
2025-12-09 12:55:20.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.00029924617466230624 Training loss: 9.518387794494629
2025-12-09 12:55:21.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.00029923678926451034 Training loss: 9.282245635986328
2025-12-09 12:55:21.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00029922734595083005 Training loss: 9.44579792022705
2025-12-09 12:55:21.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0002992178447249302 Training loss: 9.423521995544434
2025-12-09 12:55:22.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.00029920828559049805 Training loss: 9.666796684265137
2025-12-09 12:55:22.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0002991986685512433 Training loss: 9.435577392578125
2025-12-09 12:55:23.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0002991889936108982 Training loss: 9.372468948364258
2025-12-09 12:55:23.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0002991792607732173 Training loss: 9.665238380432129
2025-12-09 12:55:23.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0002991694700419778 Training loss: 9.255369186401367
2025-12-09 12:55:24.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00029915962142097925 Training loss: 9.31079387664795
2025-12-09 12:55:24.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00029914971491404373 Training loss: 9.18508529663086
2025-12-09 12:55:24.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00029913975052501575 Training loss: 9.444372177124023
2025-12-09 12:55:25.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0002991297282577623 Training loss: 9.576423645019531
2025-12-09 12:55:25.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00029911964811617285 Training loss: 9.31807804107666
2025-12-09 12:55:26.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00029910951010415926 Training loss: 9.634868621826172
2025-12-09 12:55:26.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0002990993142256559 Training loss: 9.385967254638672
2025-12-09 12:55:26.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0002990890604846196 Training loss: 9.23659896850586
2025-12-09 12:55:27.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00029907874888502966 Training loss: 9.315237998962402
2025-12-09 12:55:27.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.00029906837943088785 Training loss: 10.020289421081543
2025-12-09 12:55:27.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.00029905795212621823 Training loss: 9.52686882019043
2025-12-09 12:55:28.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.00029904746697506754 Training loss: 9.682336807250977
2025-12-09 12:55:28.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0002990369239815048 Training loss: 9.427224159240723
2025-12-09 12:55:28.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.00029902632314962157 Training loss: 9.122123718261719
2025-12-09 12:55:29.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002990156644835318 Training loss: 9.399689674377441
2025-12-09 12:55:29.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.00029900494798737194 Training loss: 9.548599243164062
2025-12-09 12:55:30.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00029899417366530085 Training loss: 10.00610065460205
2025-12-09 12:55:30.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.00029898334152149984 Training loss: 9.302172660827637
2025-12-09 12:55:30.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0002989724515601726 Training loss: 9.895065307617188
2025-12-09 12:55:31.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0002989615037855454 Training loss: 9.217927932739258
2025-12-09 12:55:31.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0002989504982018668 Training loss: 9.219483375549316
2025-12-09 12:55:31.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.00029893943481340785 Training loss: 9.467292785644531
2025-12-09 12:55:32.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.000298928313624462 Training loss: 9.241996765136719
2025-12-09 12:55:32.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0002989171346393453 Training loss: 9.256806373596191
2025-12-09 12:55:33.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00029890589786239595 Training loss: 9.514920234680176
2025-12-09 12:55:33.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0002988946032979748 Training loss: 9.147130012512207
2025-12-09 12:55:33.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.000298883250950465 Training loss: 10.179108619689941
2025-12-09 12:55:34.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0002988718408242722 Training loss: 9.455885887145996
2025-12-09 12:55:34.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.00029886037292382455 Training loss: 9.328792572021484
2025-12-09 12:55:34.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00029884884725357236 Training loss: 9.405986785888672
2025-12-09 12:55:35.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0002988372638179886 Training loss: 9.508146286010742
2025-12-09 12:55:35.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002988256226215685 Training loss: 9.200616836547852
2025-12-09 12:55:36.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0002988139236688299 Training loss: 9.536526679992676
2025-12-09 12:55:36.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.00029880216696431285 Training loss: 9.270835876464844
2025-12-09 12:55:36.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0002987903525125799 Training loss: 9.203550338745117
2025-12-09 12:55:37.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.000298778480318216 Training loss: 9.56382942199707
2025-12-09 12:55:37.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0002987665503858286 Training loss: 9.776069641113281
2025-12-09 12:55:37.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0002987545627200474 Training loss: 9.252485275268555
2025-12-09 12:55:38.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0002987425173255246 Training loss: 9.397527694702148
2025-12-09 12:55:38.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0002987304142069348 Training loss: 9.323119163513184
2025-12-09 12:55:39.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0002987182533689749 Training loss: 9.379033088684082
2025-12-09 12:55:39.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0002987060348163644 Training loss: 9.379796028137207
2025-12-09 12:55:39.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.000298693758553845 Training loss: 8.833736419677734
2025-12-09 12:55:40.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.00029868142458618096 Training loss: 9.455320358276367
2025-12-09 12:55:40.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0002986690329181587 Training loss: 9.777547836303711
2025-12-09 12:55:40.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00029865658355458736 Training loss: 9.509038925170898
2025-12-09 12:55:41.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0002986440765002982 Training loss: 9.274248123168945
2025-12-09 12:55:41.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.000298631511760145 Training loss: 8.982203483581543
2025-12-09 12:55:42.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0002986188893390038 Training loss: 9.25153636932373
2025-12-09 12:55:42.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0002986062092417733 Training loss: 9.34914493560791
2025-12-09 12:55:42.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.00029859347147337417 Training loss: 9.30716323852539
2025-12-09 12:55:43.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0002985806760387499 Training loss: 9.395532608032227
2025-12-09 12:55:43.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00029856782294286594 Training loss: 8.718557357788086
2025-12-09 12:55:43.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00029855491219071053 Training loss: 9.373625755310059
2025-12-09 12:55:44.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.000298541943787294 Training loss: 9.365925788879395
2025-12-09 12:55:44.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00029852891773764906 Training loss: 9.375944137573242
2025-12-09 12:55:44.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00029851583404683096 Training loss: 9.321771621704102
2025-12-09 12:55:45.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0002985026927199172 Training loss: 9.41252613067627
2025-12-09 12:55:45.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.00029848949376200766 Training loss: 9.366135597229004
2025-12-09 12:55:46.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0002984762371782246 Training loss: 9.557685852050781
2025-12-09 12:55:46.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00029846292297371264 Training loss: 9.203336715698242
2025-12-09 12:55:46.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0002984495511536388 Training loss: 9.29262638092041
2025-12-09 12:55:47.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0002984361217231923 Training loss: 9.261770248413086
2025-12-09 12:55:47.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.00029842263468758505 Training loss: 9.190497398376465
2025-12-09 12:55:47.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0002984090900520509 Training loss: 9.240849494934082
2025-12-09 12:55:48.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00029839548782184636 Training loss: 9.135854721069336
2025-12-09 12:55:48.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.00029838182800225017 Training loss: 9.524638175964355
2025-12-09 12:55:49.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00029836811059856354 Training loss: 9.302785873413086
2025-12-09 12:55:49.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.00029835433561610974 Training loss: 9.433793067932129
2025-12-09 12:55:49.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0002983405030602346 Training loss: 9.109074592590332
2025-12-09 12:55:50.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.00029832661293630644 Training loss: 9.250844955444336
2025-12-09 12:55:50.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.00029831266524971557 Training loss: 9.278060913085938
2025-12-09 12:55:50.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0002982986600058749 Training loss: 9.404057502746582
2025-12-09 12:55:51.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0002982845972102196 Training loss: 9.373224258422852
2025-12-09 12:55:51.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0002982704768682071 Training loss: 9.511934280395508
2025-12-09 12:55:52.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00029825629898531724 Training loss: 9.044544219970703
2025-12-09 12:55:52.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0002982420635670523 Training loss: 9.623822212219238
2025-12-09 12:55:52.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.00029822777061893653 Training loss: 9.219868659973145
2025-12-09 12:55:53.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00029821342014651694 Training loss: 9.127206802368164
2025-12-09 12:55:53.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0002981990121553627 Training loss: 9.257619857788086
2025-12-09 12:55:53.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0002981845466510651 Training loss: 9.20839786529541
2025-12-09 12:55:54.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00029817002363923803 Training loss: 9.188211441040039
2025-12-09 12:55:54.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00029815544312551754 Training loss: 9.233882904052734
2025-12-09 12:55:55.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00029814080511556207 Training loss: 9.16540241241455
2025-12-09 12:55:55.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00029812610961505234 Training loss: 9.302229881286621
2025-12-09 12:55:55.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.00029811135662969143 Training loss: 9.381011009216309
2025-12-09 12:55:56.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.00029809654616520456 Training loss: 9.223825454711914
2025-12-09 12:55:56.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00029808167822733953 Training loss: 9.117572784423828
2025-12-09 12:55:56.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0002980667528218662 Training loss: 9.28296184539795
2025-12-09 12:55:57.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002980517699545769 Training loss: 9.393363952636719
2025-12-09 12:55:57.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0002980367296312861 Training loss: 9.39069652557373
2025-12-09 12:55:58.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00029802163185783074 Training loss: 9.256458282470703
2025-12-09 12:55:58.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.00029800647664006993 Training loss: 9.360499382019043
2025-12-09 12:55:58.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0002979912639838851 Training loss: 9.291915893554688
2025-12-09 12:55:59.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.00029797599389518 Training loss: 9.371424674987793
2025-12-09 12:55:59.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0002979606663798807 Training loss: 9.111937522888184
2025-12-09 12:55:59.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0002979452814439354 Training loss: 9.230886459350586
2025-12-09 12:56:00.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.00029792983909331485 Training loss: 9.795951843261719
2025-12-09 12:56:00.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0002979143393340117 Training loss: 9.052502632141113
2025-12-09 12:56:01.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.00029789878217204133 Training loss: 9.426223754882812
2025-12-09 12:56:01.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00029788316761344106 Training loss: 9.372576713562012
2025-12-09 12:56:01.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00029786749566427064 Training loss: 9.234742164611816
2025-12-09 12:56:02.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.000297851766330612 Training loss: 9.361167907714844
2025-12-09 12:56:02.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.00029783597961856946 Training loss: 9.048430442810059
2025-12-09 12:56:02.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00029782013553426937 Training loss: 9.898022651672363
2025-12-09 12:56:03.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.00029780423408386073 Training loss: 9.338711738586426
2025-12-09 12:56:03.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.00029778827527351443 Training loss: 9.228965759277344
2025-12-09 12:56:03.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0002977722591094238 Training loss: 9.191498756408691
2025-12-09 12:56:04.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.00029775618559780447 Training loss: 9.145196914672852
2025-12-09 12:56:04.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.00029774005474489417 Training loss: 9.311731338500977
2025-12-09 12:56:05.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.00029772386655695305 Training loss: 9.016215324401855
2025-12-09 12:56:05.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0002977076210402633 Training loss: 9.696050643920898
2025-12-09 12:56:05.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.00029769131820112966 Training loss: 9.084351539611816
2025-12-09 12:56:06.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.00029767495804587885 Training loss: 9.044660568237305
2025-12-09 12:56:06.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.0002976585405808599 Training loss: 9.277303695678711
2025-12-09 12:56:06.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0002976420658124441 Training loss: 9.42293643951416
2025-12-09 12:56:07.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0002976255337470251 Training loss: 9.26070785522461
2025-12-09 12:56:07.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.00029760894439101855 Training loss: 9.177753448486328
2025-12-09 12:56:08.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0002975922977508625 Training loss: 9.171820640563965
2025-12-09 12:56:08.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0002975755938330172 Training loss: 9.166797637939453
2025-12-09 12:56:08.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.00029755883264396513 Training loss: 9.88825798034668
2025-12-09 12:56:09.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00029754201419021094 Training loss: 9.2849702835083
2025-12-09 12:56:09.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0002975251384782816 Training loss: 8.903902053833008
2025-12-09 12:56:09.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.00029750820551472615 Training loss: 9.00967788696289
2025-12-09 12:56:10.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.00029749121530611597 Training loss: 9.207409858703613
2025-12-09 12:56:10.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0002974741678590447 Training loss: 9.776676177978516
2025-12-09 12:56:11.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.00029745706318012806 Training loss: 9.27531623840332
2025-12-09 12:56:11.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.00029743990127600406 Training loss: 9.20093822479248
2025-12-09 12:56:11.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0002974226821533329 Training loss: 9.30087947845459
2025-12-09 12:56:12.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.000297405405818797 Training loss: 9.555909156799316
2025-12-09 12:56:12.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0002973880722791009 Training loss: 9.21084976196289
2025-12-09 12:56:12.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0002973706815409715 Training loss: 9.965738296508789
2025-12-09 12:56:13.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0002973532336111577 Training loss: 9.172026634216309
2025-12-09 12:56:13.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00029733572849643085 Training loss: 9.29140853881836
2025-12-09 12:56:14.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.00029731816620358424 Training loss: 9.219646453857422
2025-12-09 12:56:14.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0002973005467394334 Training loss: 9.013885498046875
2025-12-09 12:56:14.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.00029728287011081625 Training loss: 9.378066062927246
2025-12-09 12:56:15.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0002972651363245927 Training loss: 9.229005813598633
2025-12-09 12:56:15.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00029724734538764475 Training loss: 8.925328254699707
2025-12-09 12:56:15.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0002972294973068768 Training loss: 9.074576377868652
2025-12-09 12:56:16.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.00029721159208921546 Training loss: 9.256260871887207
2025-12-09 12:56:16.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.00029719362974160924 Training loss: 8.908428192138672
2025-12-09 12:56:17.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.000297175610271029 Training loss: 9.10437297821045
2025-12-09 12:56:17.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.00029715753368446786 Training loss: 9.242813110351562
2025-12-09 12:56:17.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00029713939998894087 Training loss: 9.339975357055664
2025-12-09 12:56:18.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0002971212091914854 Training loss: 9.225467681884766
2025-12-09 12:56:18.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0002971029612991609 Training loss: 9.560355186462402
2025-12-09 12:56:18.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0002970846563190491 Training loss: 9.00498104095459
2025-12-09 12:56:19.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00029706629425825374 Training loss: 9.211400985717773
2025-12-09 12:56:19.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.00029704787512390085 Training loss: 9.286423683166504
2025-12-09 12:56:20.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0002970293989231385 Training loss: 9.624122619628906
2025-12-09 12:56:20.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.0002970108656631369 Training loss: 9.143610954284668
2025-12-09 12:56:20.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0002969922753510885 Training loss: 9.211831092834473
2025-12-09 12:56:21.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00029697362799420776 Training loss: 9.109807014465332
2025-12-09 12:56:21.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0002969549235997315 Training loss: 9.142949104309082
2025-12-09 12:56:21.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0002969361621749184 Training loss: 9.063187599182129
2025-12-09 12:56:22.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.00029691734372704943 Training loss: 9.130542755126953
2025-12-09 12:56:22.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0002968984682634277 Training loss: 9.084634780883789
2025-12-09 12:56:23.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0002968795357913784 Training loss: 9.313718795776367
2025-12-09 12:56:23.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0002968605463182488 Training loss: 9.320759773254395
2025-12-09 12:56:23.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0002968414998514085 Training loss: 8.798468589782715
2025-12-09 12:56:24.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0002968223963982488 Training loss: 9.325185775756836
2025-12-09 12:56:24.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.00029680323596618355 Training loss: 9.878939628601074
2025-12-09 12:56:24.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00029678401856264857 Training loss: 9.10901165008545
2025-12-09 12:56:25.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0002967647441951017 Training loss: 9.428182601928711
2025-12-09 12:56:25.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0002967454128710229 Training loss: 8.828875541687012
2025-12-09 12:56:25.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.00029672602459791434 Training loss: 9.285505294799805
2025-12-09 12:56:26.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0002967065793833002 Training loss: 9.226536750793457
2025-12-09 12:56:26.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0002966870772347269 Training loss: 9.051285743713379
2025-12-09 12:56:27.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0002966675181597627 Training loss: 9.60312557220459
2025-12-09 12:56:27.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0002966479021659981 Training loss: 9.125343322753906
2025-12-09 12:56:27.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.00029662822926104576 Training loss: 9.17833137512207
2025-12-09 12:56:28.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0002966084994525403 Training loss: 9.002692222595215
2025-12-09 12:56:28.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.00029658871274813853 Training loss: 9.396143913269043
2025-12-09 12:56:28.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.00029656886915551924 Training loss: 8.982531547546387
2025-12-09 12:56:29.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0002965489686823833 Training loss: 9.282485008239746
2025-12-09 12:56:29.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.00029652901133645377 Training loss: 8.957088470458984
2025-12-09 12:56:30.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0002965089971254757 Training loss: 9.403698921203613
2025-12-09 12:56:30.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0002964889260572162 Training loss: 9.31277084350586
2025-12-09 12:56:30.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0002964687981394644 Training loss: 9.116092681884766
2025-12-09 12:56:31.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.00029644861338003165 Training loss: 9.083768844604492
2025-12-09 12:56:31.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0002964283717867512 Training loss: 9.579399108886719
2025-12-09 12:56:31.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0002964080733674784 Training loss: 9.10669231414795
2025-12-09 12:56:32.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0002963877181300907 Training loss: 9.07272720336914
2025-12-09 12:56:32.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00029636730608248766 Training loss: 8.878406524658203
2025-12-09 12:56:33.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0002963468372325906 Training loss: 9.336051940917969
2025-12-09 12:56:33.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.00029632631158834326 Training loss: 9.014948844909668
2025-12-09 12:56:33.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.00029630572915771117 Training loss: 9.067937850952148
2025-12-09 12:56:34.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.000296285089948682 Training loss: 8.72986125946045
2025-12-09 12:56:34.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.00029626439396926533 Training loss: 9.244791030883789
2025-12-09 12:56:34.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.00029624364122749294 Training loss: 9.114219665527344
2025-12-09 12:56:35.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0002962228317314186 Training loss: 9.102987289428711
2025-12-09 12:56:35.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00029620196548911797 Training loss: 9.139708518981934
2025-12-09 12:56:36.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.0002961810425086889 Training loss: 9.295684814453125
2025-12-09 12:56:36.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00029616006279825126 Training loss: 9.185779571533203
2025-12-09 12:56:36.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0002961390263659467 Training loss: 9.090341567993164
2025-12-09 12:56:37.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0002961179332199391 Training loss: 9.203798294067383
2025-12-09 12:56:37.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00029609678336841444 Training loss: 9.195960998535156
2025-12-09 12:56:37.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.00029607557681958035 Training loss: 9.034750938415527
2025-12-09 12:56:38.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.00029605431358166684 Training loss: 9.205110549926758
2025-12-09 12:56:38.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.00029603299366292565 Training loss: 8.76011848449707
2025-12-09 12:56:39.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00029601161707163077 Training loss: 9.317877769470215
2025-12-09 12:56:39.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.00029599018381607785 Training loss: 9.066627502441406
2025-12-09 12:56:39.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0002959686939045848 Training loss: 9.183294296264648
2025-12-09 12:56:40.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.00029594714734549146 Training loss: 9.117441177368164
2025-12-09 12:56:40.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0002959255441471597 Training loss: 9.178250312805176
2025-12-09 12:56:40.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0002959038843179731 Training loss: 9.14087200164795
2025-12-09 12:56:41.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0002958821678663376 Training loss: 9.312933921813965
2025-12-09 12:56:41.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.00029586039480068087 Training loss: 9.039546012878418
2025-12-09 12:56:41.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0002958385651294525 Training loss: 9.300651550292969
2025-12-09 12:56:42.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00029581667886112434 Training loss: 9.565918922424316
2025-12-09 12:56:42.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.00029579473600418993 Training loss: 9.113941192626953
2025-12-09 12:56:43.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0002957727365671649 Training loss: 9.202398300170898
2025-12-09 12:56:43.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0002957506805585867 Training loss: 9.102546691894531
2025-12-09 12:56:43.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00029572856798701504 Training loss: 9.370545387268066
2025-12-09 12:56:44.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0002957063988610312 Training loss: 8.890585899353027
2025-12-09 12:56:44.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0002956841731892386 Training loss: 9.192434310913086
2025-12-09 12:56:44.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0002956618909802627 Training loss: 9.198614120483398
2025-12-09 12:56:45.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.00029563955224275065 Training loss: 9.149543762207031
2025-12-09 12:56:45.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.00029561715698537183 Training loss: 8.978593826293945
2025-12-09 12:56:46.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0002955947052168172 Training loss: 9.122238159179688
2025-12-09 12:56:46.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0002955721969458001 Training loss: 9.170381546020508
2025-12-09 12:56:46.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0002955496321810553 Training loss: 9.098953247070312
2025-12-09 12:56:47.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.00029552701093133994 Training loss: 9.19636058807373
2025-12-09 12:56:47.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.00029550433320543284 Training loss: 9.124703407287598
2025-12-09 12:56:47.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0002954815990121347 Training loss: 9.018383026123047
2025-12-09 12:56:48.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00029545880836026833 Training loss: 9.027019500732422
2025-12-09 12:56:48.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0002954359612586782 Training loss: 9.186410903930664
2025-12-09 12:56:49.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00029541305771623095 Training loss: 9.374932289123535
2025-12-09 12:56:49.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.00029539009774181494 Training loss: 9.185455322265625
2025-12-09 12:56:49.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.00029536708134434054 Training loss: 9.109084129333496
2025-12-09 12:56:50.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.00029534400853273985 Training loss: 9.151520729064941
2025-12-09 12:56:50.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0002953208793159671 Training loss: 9.017729759216309
2025-12-09 12:56:50.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.00029529769370299823 Training loss: 8.900045394897461
2025-12-09 12:56:51.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0002952744517028311 Training loss: 9.308229446411133
2025-12-09 12:56:51.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00029525115332448555 Training loss: 8.925193786621094
2025-12-09 12:56:52.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0002952277985770032 Training loss: 8.975435256958008
2025-12-09 12:56:52.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0002952043874694475 Training loss: 9.107489585876465
2025-12-09 12:56:52.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00029518092001090397 Training loss: 9.486875534057617
2025-12-09 12:56:53.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00029515739621047973 Training loss: 9.066099166870117
2025-12-09 12:56:53.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.000295133816077304 Training loss: 9.218374252319336
2025-12-09 12:56:53.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.0002951101796205278 Training loss: 8.983139991760254
2025-12-09 12:56:54.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0002950864868493239 Training loss: 9.368581771850586
2025-12-09 12:56:54.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.00029506273777288696 Training loss: 9.370575904846191
2025-12-09 12:56:55.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0002950389324004337 Training loss: 9.51565170288086
2025-12-09 12:56:55.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.00029501507074120237 Training loss: 9.035727500915527
2025-12-09 12:56:55.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.00029499115280445326 Training loss: 9.088890075683594
2025-12-09 12:56:56.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0002949671785994685 Training loss: 9.119184494018555
2025-12-09 12:56:56.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.00029494314813555193 Training loss: 8.986550331115723
2025-12-09 12:56:56.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.00029491906142202934 Training loss: 9.207637786865234
2025-12-09 12:56:57.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.00029489491846824837 Training loss: 9.038305282592773
2025-12-09 12:56:57.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0002948707192835783 Training loss: 9.405960083007812
2025-12-09 12:56:58.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0002948464638774105 Training loss: 9.307050704956055
2025-12-09 12:56:58.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.00029482215225915795 Training loss: 9.307193756103516
2025-12-09 12:56:58.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0002947977844382555 Training loss: 9.067586898803711
2025-12-09 12:56:59.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0002947733604241599 Training loss: 9.139453887939453
2025-12-09 12:56:59.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.00029474888022634955 Training loss: 9.12491226196289
2025-12-09 12:56:59.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.00029472434385432474 Training loss: 9.085543632507324
2025-12-09 12:57:00.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0002946997513176076 Training loss: 9.173100471496582
2025-12-09 12:57:00.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.000294675102625742 Training loss: 9.046978950500488
2025-12-09 12:57:00.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.00029465039778829366 Training loss: 9.033607482910156
2025-12-09 12:57:01.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0002946256368148499 Training loss: 8.953560829162598
2025-12-09 12:57:01.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.00029460081971502015 Training loss: 9.179368019104004
2025-12-09 12:57:02.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.00029457594649843534 Training loss: 9.210073471069336
2025-12-09 12:57:02.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0002945510171747483 Training loss: 8.864801406860352
2025-12-09 12:57:02.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0002945260317536336 Training loss: 8.924651145935059
2025-12-09 12:57:03.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0002945009902447876 Training loss: 9.132740020751953
2025-12-09 12:57:03.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.00029447589265792847 Training loss: 9.099433898925781
2025-12-09 12:57:03.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.00029445073900279605 Training loss: 9.14075756072998
2025-12-09 12:57:04.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.000294425529289152 Training loss: 9.563340187072754
2025-12-09 12:57:04.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.00029440026352677966 Training loss: 9.114700317382812
2025-12-09 12:57:05.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.00029437494172548424 Training loss: 9.38036060333252
2025-12-09 12:57:05.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.00029434956389509263 Training loss: 9.24123477935791
2025-12-09 12:57:05.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0002943241300454534 Training loss: 9.346354484558105
2025-12-09 12:57:06.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.0002942986401864371 Training loss: 8.989336013793945
2025-12-09 12:57:06.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0002942730943279357 Training loss: 9.309723854064941
2025-12-09 12:57:06.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.000294247492479863 Training loss: 9.02142333984375
2025-12-09 12:57:07.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.00029422183465215474 Training loss: 9.01640510559082
2025-12-09 12:57:07.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00029419612085476813 Training loss: 9.337462425231934
2025-12-09 12:57:08.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0002941703510976822 Training loss: 9.41692066192627
2025-12-09 12:57:08.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.00029414452539089776 Training loss: 9.121394157409668
2025-12-09 12:57:08.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00029411864374443716 Training loss: 8.995660781860352
2025-12-09 12:57:09.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0002940927061683446 Training loss: 9.47302532196045
2025-12-09 12:57:09.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.0002940667126726859 Training loss: 9.014700889587402
2025-12-09 12:57:09.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0002940406632675487 Training loss: 9.141648292541504
2025-12-09 12:57:10.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0002940145579630423 Training loss: 9.942425727844238
2025-12-09 12:57:10.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.00029398839676929756 Training loss: 9.136452674865723
2025-12-09 12:57:11.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.00029396217969646717 Training loss: 8.476707458496094
2025-12-09 12:57:11.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00029393590675472545 Training loss: 8.986847877502441
2025-12-09 12:57:11.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00029390957795426845 Training loss: 9.553542137145996
2025-12-09 12:57:12.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0002938831933053138 Training loss: 9.35262680053711
2025-12-09 12:57:12.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.000293856752818101 Training loss: 9.069296836853027
2025-12-09 12:57:12.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.00029383025650289095 Training loss: 9.228521347045898
2025-12-09 12:57:13.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0002938037043699664 Training loss: 8.97943115234375
2025-12-09 12:57:13.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0002937770964296317 Training loss: 9.190155029296875
2025-12-09 12:57:14.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0002937504326922129 Training loss: 9.200661659240723
2025-12-09 12:57:14.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.00029372371316805767 Training loss: 8.83920955657959
2025-12-09 12:57:14.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.00029369693786753534 Training loss: 8.907999992370605
2025-12-09 12:57:15.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0002936701068010368 Training loss: 8.888679504394531
2025-12-09 12:57:15.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0002936432199789748 Training loss: 9.137838363647461
2025-12-09 12:57:15.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.00029361627741178356 Training loss: 9.161111831665039
2025-12-09 12:57:16.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.00029358927910991885 Training loss: 9.09048843383789
2025-12-09 12:57:16.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.00029356222508385827 Training loss: 9.285493850708008
2025-12-09 12:57:16.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.000293535115344101 Training loss: 8.85970401763916
2025-12-09 12:57:17.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0002935079499011677 Training loss: 9.201313018798828
2025-12-09 12:57:17.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0002934807287656008 Training loss: 9.284392356872559
2025-12-09 12:57:18.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.00029345345194796435 Training loss: 8.939413070678711
2025-12-09 12:57:18.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0002934261194588438 Training loss: 9.311909675598145
2025-12-09 12:57:18.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.00029339873130884654 Training loss: 9.117361068725586
2025-12-09 12:57:19.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.00029337128750860124 Training loss: 8.94629192352295
2025-12-09 12:57:19.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.00029334378806875836 Training loss: 9.138647079467773
2025-12-09 12:57:19.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.00029331623299998986 Training loss: 9.23121166229248
2025-12-09 12:57:20.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0002932886223129894 Training loss: 9.257522583007812
2025-12-09 12:57:20.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.000293260956018472 Training loss: 9.571898460388184
2025-12-09 12:57:21.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0002932332341271746 Training loss: 9.204678535461426
2025-12-09 12:57:21.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00029320545664985535 Training loss: 9.073266983032227
2025-12-09 12:57:21.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.00029317762359729423 Training loss: 9.098213195800781
2025-12-09 12:57:22.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.00029314973498029275 Training loss: 8.921882629394531
2025-12-09 12:57:22.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0002931217908096739 Training loss: 9.37714672088623
2025-12-09 12:57:22.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0002930937910962822 Training loss: 8.809865951538086
2025-12-09 12:57:23.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.00029306573585098384 Training loss: 9.147000312805176
2025-12-09 12:57:23.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.00029303762508466654 Training loss: 9.59249210357666
2025-12-09 12:57:24.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00029300945880823956 Training loss: 8.845988273620605
2025-12-09 12:57:24.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0002929812370326336 Training loss: 9.397126197814941
2025-12-09 12:57:24.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.000292952959768801 Training loss: 8.890685081481934
2025-12-09 12:57:25.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0002929246270277157 Training loss: 8.837602615356445
2025-12-09 12:57:25.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.000292896238820373 Training loss: 9.548345565795898
2025-12-09 12:57:25.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0002928677951577898 Training loss: 9.125741958618164
2025-12-09 12:57:26.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.00029283929605100455 Training loss: 9.381918907165527
2025-12-09 12:57:26.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0002928107415110772 Training loss: 9.257268905639648
2025-12-09 12:57:27.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0002927821315490893 Training loss: 8.80115032196045
2025-12-09 12:57:27.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0002927534661761436 Training loss: 9.163384437561035
2025-12-09 12:57:27.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.00029272474540336475 Training loss: 9.14631462097168
2025-12-09 12:57:28.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.00029269596924189875 Training loss: 9.144092559814453
2025-12-09 12:57:28.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0002926671377029129 Training loss: 9.25048542022705
2025-12-09 12:57:28.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.00029263825079759635 Training loss: 9.238311767578125
2025-12-09 12:57:29.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.00029260930853715935 Training loss: 8.925246238708496
2025-12-09 12:57:29.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0002925803109328339 Training loss: 9.196764945983887
2025-12-09 12:57:30.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0002925512579958735 Training loss: 9.281548500061035
2025-12-09 12:57:30.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0002925221497375529 Training loss: 8.911504745483398
2025-12-09 12:57:30.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.00029249298616916856 Training loss: 9.067721366882324
2025-12-09 12:57:31.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.00029246376730203817 Training loss: 9.807910919189453
2025-12-09 12:57:31.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0002924344931475011 Training loss: 8.997244834899902
2025-12-09 12:57:31.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.000292405163716918 Training loss: 8.851713180541992
2025-12-09 12:57:32.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0002923757790216711 Training loss: 9.093328475952148
2025-12-09 12:57:32.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.000292346339073164 Training loss: 8.867496490478516
2025-12-09 12:57:33.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.00029231684388282184 Training loss: 9.157648086547852
2025-12-09 12:57:33.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.000292287293462091 Training loss: 9.5057954788208
2025-12-09 12:57:33.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0002922576878224395 Training loss: 9.07571792602539
2025-12-09 12:57:34.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.00029222802697535674 Training loss: 8.866219520568848
2025-12-09 12:57:34.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0002921983109323535 Training loss: 9.487556457519531
2025-12-09 12:57:34.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0002921685397049619 Training loss: 9.260682106018066
2025-12-09 12:57:35.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0002921387133047357 Training loss: 9.649221420288086
2025-12-09 12:57:35.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0002921088317432499 Training loss: 8.99112319946289
2025-12-09 12:57:35.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0002920788950321009 Training loss: 9.228073120117188
2025-12-09 12:57:36.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.00029204890318290666 Training loss: 9.0889253616333
2025-12-09 12:57:36.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0002920188562073063 Training loss: 8.849380493164062
2025-12-09 12:57:37.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0002919887541169605 Training loss: 8.964150428771973
2025-12-09 12:57:37.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0002919585969235514 Training loss: 9.750072479248047
2025-12-09 12:57:37.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.00029192838463878236 Training loss: 9.227787971496582
2025-12-09 12:57:38.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0002918981172743781 Training loss: 9.03126335144043
2025-12-09 12:57:38.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.00029186779484208485 Training loss: 9.248800277709961
2025-12-09 12:57:38.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0002918374173536702 Training loss: 9.440632820129395
2025-12-09 12:57:39.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.000291806984820923 Training loss: 9.033592224121094
2025-12-09 12:57:39.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.00029177649725565353 Training loss: 8.941988945007324
2025-12-09 12:57:40.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.00029174595466969344 Training loss: 8.922493934631348
2025-12-09 12:57:40.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.00029171535707489565 Training loss: 9.109182357788086
2025-12-09 12:57:40.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0002916847044831346 Training loss: 9.333260536193848
2025-12-09 12:57:41.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0002916539969063059 Training loss: 9.567887306213379
2025-12-09 12:57:41.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0002916232343563265 Training loss: 9.172450065612793
2025-12-09 12:57:41.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0002915924168451349 Training loss: 9.468432426452637
2025-12-09 12:57:42.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0002915615443846906 Training loss: 9.082683563232422
2025-12-09 12:57:42.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0002915306169869747 Training loss: 9.124251365661621
2025-12-09 12:57:43.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0002914996346639895 Training loss: 9.153823852539062
2025-12-09 12:57:43.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.00029146859742775865 Training loss: 9.14303970336914
2025-12-09 12:57:43.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00029143750529032707 Training loss: 8.924274444580078
2025-12-09 12:57:44.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0002914063582637611 Training loss: 8.901034355163574
2025-12-09 12:57:44.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0002913751563601481 Training loss: 9.04995346069336
2025-12-09 12:57:44.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0002913438995915971 Training loss: 9.23519229888916
2025-12-09 12:57:45.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0002913125879702381 Training loss: 8.895977020263672
2025-12-09 12:57:45.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.00029128122150822263 Training loss: 9.085457801818848
2025-12-09 12:57:46.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0002912498002177234 Training loss: 8.996955871582031
2025-12-09 12:57:46.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0002912183241109344 Training loss: 8.834613800048828
2025-12-09 12:57:46.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.00029118679320007087 Training loss: 8.962486267089844
2025-12-09 12:57:47.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0002911552074973693 Training loss: 8.895822525024414
2025-12-09 12:57:47.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0002911235670150875 Training loss: 8.858377456665039
2025-12-09 12:57:47.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0002910918717655046 Training loss: 9.223718643188477
2025-12-09 12:57:48.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00029106012176092084 Training loss: 8.995407104492188
2025-12-09 12:57:48.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0002910283170136578 Training loss: 9.448935508728027
2025-12-09 12:57:49.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00029099645753605827 Training loss: 9.17835521697998
2025-12-09 12:57:49.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.00029096454334048627 Training loss: 9.121918678283691
2025-12-09 12:57:49.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0002909325744393271 Training loss: 9.263985633850098
2025-12-09 12:57:50.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0002909005508449873 Training loss: 9.145827293395996
2025-12-09 12:57:50.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0002908684725698946 Training loss: 9.526871681213379
2025-12-09 12:57:50.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0002908363396264978 Training loss: 9.110715866088867
2025-12-09 12:57:51.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.00029080415202726727 Training loss: 8.947153091430664
2025-12-09 12:57:51.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0002907719097846943 Training loss: 9.377164840698242
2025-12-09 12:57:51.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0002907396129112915 Training loss: 9.258947372436523
2025-12-09 12:57:52.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.00029070726141959265 Training loss: 8.919565200805664
2025-12-09 12:57:52.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.00029067485532215267 Training loss: 9.262027740478516
2025-12-09 12:57:53.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0002906423946315478 Training loss: 8.89858627319336
2025-12-09 12:57:53.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.00029060987936037536 Training loss: 9.141328811645508
2025-12-09 12:57:53.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.00029057730952125393 Training loss: 9.227574348449707
2025-12-09 12:57:54.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0002905446851268233 Training loss: 9.339180946350098
2025-12-09 12:57:54.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0002905120061897441 Training loss: 9.00328540802002
2025-12-09 12:57:54.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0002904792727226987 Training loss: 8.941805839538574
2025-12-09 12:57:55.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.00029044648473839014 Training loss: 9.00764274597168
2025-12-09 12:57:55.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0002904136422495429 Training loss: 9.123763084411621
2025-12-09 12:57:56.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0002903807452689024 Training loss: 8.996834754943848
2025-12-09 12:57:56.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00029034779380923535 Training loss: 9.229575157165527
2025-12-09 12:57:56.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.00029031478788332955 Training loss: 8.97766399383545
2025-12-09 12:57:57.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0002902817275039941 Training loss: 9.185547828674316
2025-12-09 12:57:57.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.00029024861268405887 Training loss: 8.963273048400879
2025-12-09 12:57:57.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.00029021544343637526 Training loss: 9.381844520568848
2025-12-09 12:57:58.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.00029018221977381546 Training loss: 9.212221145629883
2025-12-09 12:57:58.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.00029014894170927306 Training loss: 9.029964447021484
2025-12-09 12:57:59.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0002901156092556625 Training loss: 9.23085880279541
2025-12-09 12:57:59.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0002900822224259195 Training loss: 9.034636497497559
2025-12-09 12:57:59.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0002900487812330009 Training loss: 9.034056663513184
2025-12-09 12:58:00.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.00029001528568988453 Training loss: 9.086995124816895
2025-12-09 12:58:00.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.00028998173580956934 Training loss: 9.167346000671387
2025-12-09 12:58:00.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00028994813160507536 Training loss: 9.089946746826172
2025-12-09 12:58:01.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0002899144730894438 Training loss: 9.051314353942871
2025-12-09 12:58:01.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00028988076027573685 Training loss: 9.096094131469727
2025-12-09 12:58:02.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.00028984699317703775 Training loss: 9.733338356018066
2025-12-09 12:58:02.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0002898131718064509 Training loss: 8.93356990814209
2025-12-09 12:58:02.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.00028977929617710166 Training loss: 8.917332649230957
2025-12-09 12:58:03.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.00028974536630213657 Training loss: 9.04427433013916
2025-12-09 12:58:03.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.00028971138219472303 Training loss: 9.154071807861328
2025-12-09 12:58:03.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.00028967734386804977 Training loss: 8.972593307495117
2025-12-09 12:58:04.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.0002896432513353264 Training loss: 8.974359512329102
2025-12-09 12:58:04.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.00028960910460978337 Training loss: 8.929336547851562
2025-12-09 12:58:05.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0002895749037046725 Training loss: 8.85001277923584
2025-12-09 12:58:05.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0002895406486332665 Training loss: 9.837650299072266
2025-12-09 12:58:05.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.00028950633940885907 Training loss: 8.892995834350586
2025-12-09 12:58:06.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.00028947197604476494 Training loss: 9.081396102905273
2025-12-09 12:58:06.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.00028943755855431985 Training loss: 9.047819137573242
2025-12-09 12:58:06.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0002894030869508806 Training loss: 9.04812240600586
2025-12-09 12:58:07.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.0002893685612478249 Training loss: 9.44723129272461
2025-12-09 12:58:07.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.00028933398145855154 Training loss: 8.948987007141113
2025-12-09 12:58:08.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0002892993475964802 Training loss: 9.000654220581055
2025-12-09 12:58:08.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0002892646596750517 Training loss: 8.878006935119629
2025-12-09 12:58:08.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0002892299177077277 Training loss: 9.158896446228027
2025-12-09 12:58:09.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0002891951217079908 Training loss: 9.154838562011719
2025-12-09 12:58:09.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0002891602716893448 Training loss: 9.076445579528809
2025-12-09 12:58:09.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0002891253676653142 Training loss: 8.809889793395996
2025-12-09 12:58:10.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.00028909040964944456 Training loss: 8.930198669433594
2025-12-09 12:58:10.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.0002890553976553025 Training loss: 9.000362396240234
2025-12-09 12:58:10.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.00028902033169647543 Training loss: 8.755059242248535
2025-12-09 12:58:11.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.00028898521178657174 Training loss: 8.98930835723877
2025-12-09 12:58:11.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0002889500379392209 Training loss: 9.108779907226562
2025-12-09 12:58:12.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.000288914810168073 Training loss: 8.869787216186523
2025-12-09 12:58:12.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.00028887952848679943 Training loss: 9.203433990478516
2025-12-09 12:58:12.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.0002888441929090922 Training loss: 8.920679092407227
2025-12-09 12:58:13.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.00028880880344866447 Training loss: 8.74101734161377
2025-12-09 12:58:13.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.00028877336011925005 Training loss: 8.954344749450684
2025-12-09 12:58:13.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.000288737862934604 Training loss: 9.220269203186035
2025-12-09 12:58:14.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.00028870231190850185 Training loss: 9.295598983764648
2025-12-09 12:58:14.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.00028866670705474047 Training loss: 9.316584587097168
2025-12-09 12:58:15.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.00028863104838713726 Training loss: 9.02859878540039
2025-12-09 12:58:15.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.00028859533591953074 Training loss: 9.574012756347656
2025-12-09 12:58:15.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.00028855956966578023 Training loss: 8.896282196044922
2025-12-09 12:58:16.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.00028852374963976585 Training loss: 9.186266899108887
2025-12-09 12:58:16.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.0002884878758553887 Training loss: 9.258809089660645
2025-12-09 12:58:16.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.00028845194832657065 Training loss: 9.349954605102539
2025-12-09 12:58:17.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.0002884159670672545 Training loss: 8.986023902893066
2025-12-09 12:58:17.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.00028837993209140385 Training loss: 9.450884819030762
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 3.20 GiB is free. Including non-PyTorch memory, this process has 90.87 GiB memory in use. Of the allocated memory 89.27 GiB is allocated by PyTorch, and 856.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 49/10000 [00:00<00:20, 489.58it/s]Tokenizing texts:   1%|▏         | 131/10000 [00:00<00:14, 674.68it/s]Tokenizing texts:   2%|▏         | 199/10000 [00:00<00:15, 621.42it/s]Tokenizing texts:   3%|▎         | 262/10000 [00:00<00:15, 622.56it/s]Tokenizing texts:   3%|▎         | 328/10000 [00:00<00:15, 634.68it/s]Tokenizing texts:   4%|▍         | 392/10000 [00:00<00:16, 588.31it/s]Tokenizing texts:   5%|▍         | 452/10000 [00:00<00:16, 566.39it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:00<00:15, 613.53it/s]Tokenizing texts:   6%|▌         | 588/10000 [00:00<00:16, 572.16it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:01<00:16, 571.27it/s]Tokenizing texts:   7%|▋         | 709/10000 [00:01<00:16, 557.83it/s]Tokenizing texts:   8%|▊         | 770/10000 [00:01<00:16, 571.72it/s]Tokenizing texts:   8%|▊         | 828/10000 [00:01<00:16, 564.67it/s]Tokenizing texts:   9%|▉         | 889/10000 [00:01<00:15, 573.77it/s]Tokenizing texts:  10%|▉         | 973/10000 [00:01<00:13, 650.43it/s]Tokenizing texts:  10%|█         | 1048/10000 [00:01<00:13, 678.24it/s]Tokenizing texts:  11%|█         | 1117/10000 [00:01<00:14, 615.24it/s]Tokenizing texts:  12%|█▏        | 1180/10000 [00:01<00:15, 583.87it/s]Tokenizing texts:  12%|█▏        | 1240/10000 [00:02<00:15, 580.52it/s]Tokenizing texts:  13%|█▎        | 1305/10000 [00:02<00:14, 596.26it/s]Tokenizing texts:  14%|█▎        | 1366/10000 [00:02<00:14, 585.82it/s]Tokenizing texts:  14%|█▍        | 1426/10000 [00:02<00:14, 587.00it/s]Tokenizing texts:  15%|█▌        | 1503/10000 [00:02<00:13, 638.37it/s]Tokenizing texts:  16%|█▌        | 1577/10000 [00:02<00:12, 666.76it/s]Tokenizing texts:  16%|█▋        | 1645/10000 [00:02<00:13, 612.75it/s]Tokenizing texts:  17%|█▋        | 1708/10000 [00:02<00:13, 596.42it/s]Tokenizing texts:  18%|█▊        | 1782/10000 [00:02<00:12, 633.05it/s]Tokenizing texts:  18%|█▊        | 1847/10000 [00:03<00:13, 597.05it/s]Tokenizing texts:  19%|█▉        | 1911/10000 [00:03<00:13, 607.54it/s]Tokenizing texts:  20%|█▉        | 1973/10000 [00:03<00:14, 562.53it/s]Tokenizing texts:  20%|██        | 2042/10000 [00:03<00:13, 596.61it/s]Tokenizing texts:  21%|██        | 2103/10000 [00:03<00:14, 549.57it/s]Tokenizing texts:  22%|██▏       | 2160/10000 [00:03<00:14, 554.04it/s]Tokenizing texts:  22%|██▏       | 2233/10000 [00:03<00:12, 600.74it/s]Tokenizing texts:  23%|██▎       | 2301/10000 [00:03<00:12, 620.00it/s]Tokenizing texts:  24%|██▎       | 2364/10000 [00:03<00:12, 601.00it/s]Tokenizing texts:  24%|██▍       | 2429/10000 [00:04<00:12, 612.28it/s]Tokenizing texts:  25%|██▍       | 2492/10000 [00:04<00:12, 616.69it/s]Tokenizing texts:  26%|██▌       | 2561/10000 [00:04<00:12, 605.44it/s]Tokenizing texts:  26%|██▌       | 2622/10000 [00:04<00:12, 571.30it/s]Tokenizing texts:  27%|██▋       | 2680/10000 [00:04<00:12, 570.20it/s]Tokenizing texts:  27%|██▋       | 2738/10000 [00:04<00:12, 568.42it/s]Tokenizing texts:  28%|██▊       | 2796/10000 [00:04<00:13, 527.62it/s]Tokenizing texts:  29%|██▊       | 2854/10000 [00:04<00:13, 535.50it/s]Tokenizing texts:  29%|██▉       | 2930/10000 [00:04<00:11, 596.36it/s]Tokenizing texts:  30%|██▉       | 2991/10000 [00:05<00:11, 589.39it/s]Tokenizing texts:  31%|███       | 3051/10000 [00:05<00:11, 583.48it/s]Tokenizing texts:  31%|███       | 3123/10000 [00:05<00:11, 619.48it/s]Tokenizing texts:  32%|███▏      | 3186/10000 [00:05<00:11, 610.01it/s]Tokenizing texts:  33%|███▎      | 3254/10000 [00:05<00:10, 628.84it/s]Tokenizing texts:  33%|███▎      | 3318/10000 [00:05<00:10, 631.84it/s]Tokenizing texts:  34%|███▍      | 3383/10000 [00:05<00:10, 634.81it/s]Tokenizing texts:  35%|███▍      | 3465/10000 [00:05<00:09, 688.75it/s]Tokenizing texts:  35%|███▌      | 3535/10000 [00:05<00:10, 633.54it/s]Tokenizing texts:  36%|███▌      | 3600/10000 [00:05<00:10, 635.04it/s]Tokenizing texts:  37%|███▋      | 3665/10000 [00:06<00:09, 633.76it/s]Tokenizing texts:  37%|███▋      | 3729/10000 [00:06<00:10, 624.90it/s]Tokenizing texts:  38%|███▊      | 3792/10000 [00:06<00:10, 601.90it/s]Tokenizing texts:  39%|███▊      | 3853/10000 [00:06<00:10, 586.33it/s]Tokenizing texts:  39%|███▉      | 3912/10000 [00:06<00:10, 583.76it/s]Tokenizing texts:  40%|███▉      | 3975/10000 [00:06<00:10, 596.77it/s]Tokenizing texts:  40%|████      | 4035/10000 [00:06<00:10, 584.94it/s]Tokenizing texts:  41%|████      | 4112/10000 [00:06<00:09, 637.26it/s]Tokenizing texts:  42%|████▏     | 4184/10000 [00:06<00:08, 659.58it/s]Tokenizing texts:  43%|████▎     | 4251/10000 [00:07<00:09, 625.61it/s]Tokenizing texts:  43%|████▎     | 4316/10000 [00:07<00:09, 630.68it/s]Tokenizing texts:  44%|████▍     | 4393/10000 [00:07<00:08, 670.11it/s]Tokenizing texts:  45%|████▍     | 4461/10000 [00:07<00:08, 649.51it/s]Tokenizing texts:  45%|████▌     | 4527/10000 [00:07<00:08, 637.21it/s]Tokenizing texts:  46%|████▌     | 4592/10000 [00:07<00:08, 636.14it/s]Tokenizing texts:  47%|████▋     | 4665/10000 [00:07<00:08, 661.84it/s]Tokenizing texts:  47%|████▋     | 4740/10000 [00:07<00:07, 677.64it/s]Tokenizing texts:  48%|████▊     | 4808/10000 [00:07<00:07, 650.02it/s]Tokenizing texts:  49%|████▉     | 4878/10000 [00:07<00:07, 664.01it/s]Tokenizing texts:  49%|████▉     | 4948/10000 [00:08<00:07, 674.02it/s]Tokenizing texts:  50%|█████     | 5024/10000 [00:08<00:07, 698.70it/s]Tokenizing texts:  51%|█████     | 5095/10000 [00:08<00:07, 639.22it/s]Tokenizing texts:  52%|█████▏    | 5161/10000 [00:08<00:07, 632.32it/s]Tokenizing texts:  52%|█████▏    | 5225/10000 [00:08<00:07, 617.96it/s]Tokenizing texts:  53%|█████▎    | 5291/10000 [00:08<00:07, 628.76it/s]Tokenizing texts:  54%|█████▍    | 5379/10000 [00:08<00:06, 694.01it/s]Tokenizing texts:  54%|█████▍    | 5449/10000 [00:08<00:06, 680.60it/s]Tokenizing texts:  55%|█████▌    | 5518/10000 [00:09<00:07, 613.23it/s]Tokenizing texts:  56%|█████▌    | 5588/10000 [00:09<00:06, 634.46it/s]Tokenizing texts:  57%|█████▋    | 5672/10000 [00:09<00:06, 689.91it/s]Tokenizing texts:  57%|█████▋    | 5743/10000 [00:09<00:06, 671.60it/s]Tokenizing texts:  58%|█████▊    | 5812/10000 [00:09<00:07, 596.20it/s]Tokenizing texts:  59%|█████▉    | 5879/10000 [00:09<00:06, 614.87it/s]Tokenizing texts:  59%|█████▉    | 5943/10000 [00:09<00:06, 607.19it/s]Tokenizing texts:  60%|██████    | 6008/10000 [00:09<00:06, 617.55it/s]Tokenizing texts:  61%|██████    | 6076/10000 [00:09<00:06, 629.74it/s]Tokenizing texts:  61%|██████▏   | 6147/10000 [00:09<00:05, 652.54it/s]Tokenizing texts:  62%|██████▏   | 6213/10000 [00:10<00:05, 632.01it/s]Tokenizing texts:  63%|██████▎   | 6277/10000 [00:10<00:05, 633.29it/s]Tokenizing texts:  63%|██████▎   | 6341/10000 [00:10<00:06, 598.38it/s]Tokenizing texts:  64%|██████▍   | 6405/10000 [00:10<00:05, 609.76it/s]Tokenizing texts:  65%|██████▍   | 6469/10000 [00:10<00:05, 616.89it/s]Tokenizing texts:  65%|██████▌   | 6532/10000 [00:10<00:05, 594.04it/s]Tokenizing texts:  66%|██████▌   | 6610/10000 [00:10<00:05, 644.58it/s]Tokenizing texts:  67%|██████▋   | 6675/10000 [00:10<00:05, 643.75it/s]Tokenizing texts:  67%|██████▋   | 6740/10000 [00:10<00:05, 625.02it/s]Tokenizing texts:  68%|██████▊   | 6819/10000 [00:11<00:04, 671.83it/s]Tokenizing texts:  69%|██████▉   | 6889/10000 [00:11<00:04, 679.52it/s]Tokenizing texts:  70%|██████▉   | 6958/10000 [00:11<00:05, 591.50it/s]Tokenizing texts:  70%|███████   | 7022/10000 [00:11<00:04, 603.96it/s]Tokenizing texts:  71%|███████   | 7085/10000 [00:11<00:05, 565.87it/s]Tokenizing texts:  71%|███████▏  | 7148/10000 [00:11<00:04, 575.97it/s]Tokenizing texts:  72%|███████▏  | 7213/10000 [00:11<00:04, 594.45it/s]Tokenizing texts:  73%|███████▎  | 7290/10000 [00:11<00:04, 641.99it/s]Tokenizing texts:  74%|███████▎  | 7356/10000 [00:11<00:04, 643.35it/s]Tokenizing texts:  74%|███████▍  | 7422/10000 [00:12<00:04, 596.60it/s]Tokenizing texts:  75%|███████▍  | 7483/10000 [00:12<00:04, 584.82it/s]Tokenizing texts:  76%|███████▌  | 7557/10000 [00:12<00:03, 626.11it/s]Tokenizing texts:  76%|███████▋  | 7626/10000 [00:12<00:03, 638.61it/s]Tokenizing texts:  77%|███████▋  | 7702/10000 [00:12<00:03, 672.06it/s]Tokenizing texts:  78%|███████▊  | 7770/10000 [00:12<00:03, 625.35it/s]Tokenizing texts:  78%|███████▊  | 7834/10000 [00:12<00:03, 612.75it/s]Tokenizing texts:  79%|███████▉  | 7896/10000 [00:12<00:03, 612.38it/s]Tokenizing texts:  80%|███████▉  | 7974/10000 [00:12<00:03, 658.08it/s]Tokenizing texts:  80%|████████  | 8041/10000 [00:13<00:03, 625.44it/s]Tokenizing texts:  81%|████████  | 8105/10000 [00:13<00:03, 577.46it/s]Tokenizing texts:  82%|████████▏ | 8168/10000 [00:13<00:03, 589.06it/s]Tokenizing texts:  82%|████████▏ | 8230/10000 [00:13<00:02, 594.79it/s]Tokenizing texts:  83%|████████▎ | 8291/10000 [00:13<00:02, 584.06it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:13<00:02, 598.94it/s]Tokenizing texts:  84%|████████▍ | 8417/10000 [00:13<00:02, 576.37it/s]Tokenizing texts:  85%|████████▍ | 8492/10000 [00:13<00:02, 624.53it/s]Tokenizing texts:  86%|████████▌ | 8561/10000 [00:13<00:02, 631.13it/s]Tokenizing texts:  86%|████████▋ | 8632/10000 [00:14<00:02, 652.96it/s]Tokenizing texts:  87%|████████▋ | 8703/10000 [00:14<00:01, 669.38it/s]Tokenizing texts:  88%|████████▊ | 8775/10000 [00:14<00:01, 682.52it/s]Tokenizing texts:  88%|████████▊ | 8844/10000 [00:14<00:01, 589.48it/s]Tokenizing texts:  89%|████████▉ | 8923/10000 [00:14<00:01, 640.72it/s]Tokenizing texts:  90%|████████▉ | 8995/10000 [00:14<00:01, 661.05it/s]Tokenizing texts:  91%|█████████ | 9063/10000 [00:14<00:01, 615.44it/s]Tokenizing texts:  91%|█████████▏| 9130/10000 [00:14<00:01, 630.05it/s]Tokenizing texts:  92%|█████████▏| 9205/10000 [00:14<00:01, 660.87it/s]Tokenizing texts:  93%|█████████▎| 9273/10000 [00:15<00:01, 630.70it/s]Tokenizing texts:  93%|█████████▎| 9338/10000 [00:15<00:01, 623.84it/s]Tokenizing texts:  94%|█████████▍| 9402/10000 [00:15<00:00, 604.33it/s]Tokenizing texts:  95%|█████████▍| 9463/10000 [00:15<00:00, 589.33it/s]Tokenizing texts:  95%|█████████▌| 9523/10000 [00:15<00:00, 542.87it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:15<00:00, 570.73it/s]Tokenizing texts:  97%|█████████▋| 9654/10000 [00:15<00:00, 571.97it/s]Tokenizing texts:  97%|█████████▋| 9712/10000 [00:15<00:00, 567.27it/s]Tokenizing texts:  98%|█████████▊| 9770/10000 [00:15<00:00, 549.15it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:16<00:00, 593.36it/s]Tokenizing texts:  99%|█████████▉| 9915/10000 [00:16<00:00, 633.90it/s]Tokenizing texts: 100%|█████████▉| 9994/10000 [00:16<00:00, 671.53it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:16<00:00, 616.33it/s]
2025-12-09 12:59:06.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 12.137935638427734
2025-12-09 12:59:07.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 12.178789138793945
2025-12-09 12:59:07.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 12.173369407653809
2025-12-09 12:59:07.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 12.221269607543945
2025-12-09 12:59:08.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 12.188817024230957
2025-12-09 12:59:08.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 12.140146255493164
2025-12-09 12:59:08.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 12.264005661010742
2025-12-09 12:59:09.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 12.129694938659668
2025-12-09 12:59:09.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 12.141913414001465
2025-12-09 12:59:09.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 12.1560640335083
2025-12-09 12:59:10.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 12.198697090148926
2025-12-09 12:59:10.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 12.129362106323242
2025-12-09 12:59:11.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 12.184121131896973
2025-12-09 12:59:11.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 12.212296485900879
2025-12-09 12:59:11.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 12.201111793518066
2025-12-09 12:59:12.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 12.086747169494629
2025-12-09 12:59:12.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 12.147589683532715
2025-12-09 12:59:12.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 12.143519401550293
2025-12-09 12:59:13.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 12.134771347045898
2025-12-09 12:59:13.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 12.11362075805664
2025-12-09 12:59:14.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 12.102848052978516
2025-12-09 12:59:14.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 12.144946098327637
2025-12-09 12:59:14.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 12.062349319458008
2025-12-09 12:59:15.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 12.126548767089844
2025-12-09 12:59:15.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 12.119095802307129
2025-12-09 12:59:15.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 12.099289894104004
2025-12-09 12:59:16.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 12.072470664978027
2025-12-09 12:59:16.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 12.041607856750488
2025-12-09 12:59:17.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 12.024762153625488
2025-12-09 12:59:17.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 12.125921249389648
2025-12-09 12:59:17.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 11.980693817138672
2025-12-09 12:59:18.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 11.975509643554688
2025-12-09 12:59:18.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 11.896003723144531
2025-12-09 12:59:18.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 11.882841110229492
2025-12-09 12:59:19.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 11.93174934387207
2025-12-09 12:59:19.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 11.96211051940918
2025-12-09 12:59:20.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 11.8009033203125
2025-12-09 12:59:20.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 11.912639617919922
2025-12-09 12:59:20.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 11.76193618774414
2025-12-09 12:59:21.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 11.765238761901855
2025-12-09 12:59:21.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 11.7529878616333
2025-12-09 12:59:21.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 11.66698169708252
2025-12-09 12:59:22.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 11.677132606506348
2025-12-09 12:59:22.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 11.84012222290039
2025-12-09 12:59:22.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 11.457876205444336
2025-12-09 12:59:23.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 11.510994911193848
2025-12-09 12:59:23.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 11.328808784484863
2025-12-09 12:59:24.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 11.56597900390625
2025-12-09 12:59:24.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 11.345779418945312
2025-12-09 12:59:24.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 11.276752471923828
2025-12-09 12:59:25.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 11.047534942626953
2025-12-09 12:59:25.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 10.964014053344727
2025-12-09 12:59:25.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 10.786998748779297
2025-12-09 12:59:26.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 10.892854690551758
2025-12-09 12:59:26.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 10.733841896057129
2025-12-09 12:59:27.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 10.834040641784668
2025-12-09 12:59:27.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 10.73005485534668
2025-12-09 12:59:27.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 10.736184120178223
2025-12-09 12:59:28.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 10.494820594787598
2025-12-09 12:59:28.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 10.995670318603516
2025-12-09 12:59:28.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 10.652786254882812
2025-12-09 12:59:29.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 10.240823745727539
2025-12-09 12:59:29.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 10.492606163024902
2025-12-09 12:59:30.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 10.66700553894043
2025-12-09 12:59:30.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 10.230242729187012
2025-12-09 12:59:30.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 10.251350402832031
2025-12-09 12:59:31.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 10.24189567565918
2025-12-09 12:59:31.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 9.983833312988281
2025-12-09 12:59:31.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 10.078245162963867
2025-12-09 12:59:32.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 10.031977653503418
2025-12-09 12:59:32.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 10.262140274047852
2025-12-09 12:59:33.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 10.498931884765625
2025-12-09 12:59:33.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 10.02877140045166
2025-12-09 12:59:33.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 10.080604553222656
2025-12-09 12:59:34.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 10.002663612365723
2025-12-09 12:59:34.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 10.347408294677734
2025-12-09 12:59:34.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 9.941997528076172
2025-12-09 12:59:35.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 10.112570762634277
2025-12-09 12:59:35.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 9.862850189208984
2025-12-09 12:59:36.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 9.971816062927246
2025-12-09 12:59:36.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 9.785957336425781
2025-12-09 12:59:36.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 9.652910232543945
2025-12-09 12:59:37.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 9.953611373901367
2025-12-09 12:59:37.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 10.04456901550293
2025-12-09 12:59:37.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 9.905428886413574
2025-12-09 12:59:38.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 9.669769287109375
2025-12-09 12:59:38.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 10.500260353088379
2025-12-09 12:59:38.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 9.842129707336426
2025-12-09 12:59:39.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 10.151945114135742
2025-12-09 12:59:39.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 9.70368766784668
2025-12-09 12:59:40.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 9.912490844726562
2025-12-09 12:59:40.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 9.87569522857666
2025-12-09 12:59:40.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 9.754172325134277
2025-12-09 12:59:41.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 9.69719123840332
2025-12-09 12:59:41.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 9.67518424987793
2025-12-09 12:59:41.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 9.550548553466797
2025-12-09 12:59:42.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 9.582671165466309
2025-12-09 12:59:42.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 9.591779708862305
2025-12-09 12:59:43.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 9.760870933532715
2025-12-09 12:59:43.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 9.628135681152344
2025-12-09 12:59:43.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009999999029798809 Training loss: 9.455211639404297
2025-12-09 12:59:44.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.000999999611919561 Training loss: 9.57008171081543
2025-12-09 12:59:44.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0009999991268191536 Training loss: 9.706546783447266
2025-12-09 12:59:44.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0009999984476788465 Training loss: 9.448872566223145
2025-12-09 12:59:45.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0009999975744989036 Training loss: 9.487771987915039
2025-12-09 12:59:45.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0009999965072796635 Training loss: 9.604016304016113
2025-12-09 12:59:46.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0009999952460215409 Training loss: 9.23054313659668
2025-12-09 12:59:46.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0009999937907250245 Training loss: 9.433575630187988
2025-12-09 12:59:46.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0009999921413906799 Training loss: 9.394303321838379
2025-12-09 12:59:47.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0009999902980191463 Training loss: 9.884955406188965
2025-12-09 12:59:47.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00099998826061114 Training loss: 9.599754333496094
2025-12-09 12:59:47.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0009999860291674508 Training loss: 9.514817237854004
2025-12-09 12:59:48.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0009999836036889453 Training loss: 9.976180076599121
2025-12-09 12:59:48.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0009999809841765644 Training loss: 9.454767227172852
2025-12-09 12:59:49.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.000999978170631325 Training loss: 9.42126178741455
2025-12-09 12:59:49.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0009999751630543187 Training loss: 9.5495023727417
2025-12-09 12:59:49.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.000999971961446713 Training loss: 9.427550315856934
2025-12-09 12:59:50.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0009999685658097501 Training loss: 9.590789794921875
2025-12-09 12:59:50.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0009999649761447478 Training loss: 9.460347175598145
2025-12-09 12:59:50.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0009999611924530994 Training loss: 9.474245071411133
2025-12-09 12:59:51.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.000999957214736273 Training loss: 9.262328147888184
2025-12-09 12:59:51.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0009999530429958125 Training loss: 10.202817916870117
2025-12-09 12:59:51.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0009999486772333365 Training loss: 9.374048233032227
2025-12-09 12:59:52.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00099994411745054 Training loss: 9.228412628173828
2025-12-09 12:59:52.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0009999393636491917 Training loss: 9.781554222106934
2025-12-09 12:59:53.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.000999934415831137 Training loss: 9.422042846679688
2025-12-09 12:59:53.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.000999929273998296 Training loss: 9.244505882263184
2025-12-09 12:59:53.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0009999239381526638 Training loss: 9.458905220031738
2025-12-09 12:59:54.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0009999184082963117 Training loss: 9.292396545410156
2025-12-09 12:59:54.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0009999126844313852 Training loss: 9.42265796661377
2025-12-09 12:59:54.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.000999906766560106 Training loss: 9.383087158203125
2025-12-09 12:59:55.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0009999006546847706 Training loss: 9.310324668884277
2025-12-09 12:59:55.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0009998943488077508 Training loss: 9.384669303894043
2025-12-09 12:59:56.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0009998878489314938 Training loss: 9.277446746826172
2025-12-09 12:59:56.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.000999881155058522 Training loss: 9.268464088439941
2025-12-09 12:59:56.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0009998742671914335 Training loss: 9.32143783569336
2025-12-09 12:59:57.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.000999867185332901 Training loss: 9.104493141174316
2025-12-09 12:59:57.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.000999859909485673 Training loss: 9.63952922821045
2025-12-09 12:59:57.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.000999852439652573 Training loss: 9.468714714050293
2025-12-09 12:59:58.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0009998447758365 Training loss: 9.549199104309082
2025-12-09 12:59:58.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0009998369180404282 Training loss: 9.364264488220215
2025-12-09 12:59:59.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.000999828866267407 Training loss: 9.6237211227417
2025-12-09 12:59:59.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0009998206205205612 Training loss: 9.33860969543457
2025-12-09 12:59:59.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0009998121808030905 Training loss: 9.466333389282227
2025-12-09 13:00:00.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0009998035471182707 Training loss: 9.413479804992676
2025-12-09 13:00:00.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.000999794719469452 Training loss: 9.695045471191406
2025-12-09 13:00:00.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0009997856978600603 Training loss: 9.495341300964355
2025-12-09 13:00:01.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0009997764822935967 Training loss: 9.147696495056152
2025-12-09 13:00:01.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.000999767072773638 Training loss: 9.531203269958496
2025-12-09 13:00:02.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0009997574693038351 Training loss: 9.387727737426758
2025-12-09 13:00:02.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0009997476718879154 Training loss: 8.981892585754395
2025-12-09 13:00:02.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.000999737680529681 Training loss: 9.438444137573242
2025-12-09 13:00:03.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0009997274952330093 Training loss: 9.0440092086792
2025-12-09 13:00:03.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.000999717116001853 Training loss: 9.308989524841309
2025-12-09 13:00:03.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0009997065428402404 Training loss: 9.157726287841797
2025-12-09 13:00:04.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0009996957757522741 Training loss: 9.092020034790039
2025-12-09 13:00:04.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0009996848147421334 Training loss: 9.2090482711792
2025-12-09 13:00:04.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0009996736598140714 Training loss: 9.128933906555176
2025-12-09 13:00:05.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0009996623109724174 Training loss: 9.209988594055176
2025-12-09 13:00:05.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0009996507682215755 Training loss: 9.183797836303711
2025-12-09 13:00:06.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0009996390315660253 Training loss: 9.141694068908691
2025-12-09 13:00:06.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0009996271010103215 Training loss: 9.119421005249023
2025-12-09 13:00:06.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0009996149765590945 Training loss: 9.262201309204102
2025-12-09 13:00:07.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.000999602658217049 Training loss: 9.182042121887207
2025-12-09 13:00:07.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0009995901459889658 Training loss: 9.5828857421875
2025-12-09 13:00:07.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0009995774398797008 Training loss: 9.190921783447266
2025-12-09 13:00:08.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0009995645398941846 Training loss: 9.06434440612793
2025-12-09 13:00:08.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0009995514460374238 Training loss: 9.050759315490723
2025-12-09 13:00:09.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0009995381583144996 Training loss: 9.56811237335205
2025-12-09 13:00:09.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0009995246767305688 Training loss: 9.009742736816406
2025-12-09 13:00:09.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0009995110012908633 Training loss: 9.071126937866211
2025-12-09 13:00:10.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0009994971320006906 Training loss: 9.26402759552002
2025-12-09 13:00:10.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0009994830688654327 Training loss: 9.149270057678223
2025-12-09 13:00:10.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.000999468811890547 Training loss: 9.38447093963623
2025-12-09 13:00:11.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.000999454361081567 Training loss: 9.145365715026855
2025-12-09 13:00:11.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.0009994397164441006 Training loss: 9.4594087600708
2025-12-09 13:00:12.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.000999424877983831 Training loss: 9.119239807128906
2025-12-09 13:00:12.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0009994098457065167 Training loss: 9.080266952514648
2025-12-09 13:00:12.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0009993946196179913 Training loss: 9.167706489562988
2025-12-09 13:00:13.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.000999379199724164 Training loss: 9.26475715637207
2025-12-09 13:00:13.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0009993635860310187 Training loss: 9.191400527954102
2025-12-09 13:00:13.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.000999347778544615 Training loss: 9.151095390319824
2025-12-09 13:00:14.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0009993317772710873 Training loss: 9.002920150756836
2025-12-09 13:00:14.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0009993155822166457 Training loss: 9.137007713317871
2025-12-09 13:00:15.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0009992991933875748 Training loss: 8.96234130859375
2025-12-09 13:00:15.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0009992826107902348 Training loss: 9.338290214538574
2025-12-09 13:00:15.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0009992658344310614 Training loss: 8.956343650817871
2025-12-09 13:00:16.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.000999248864316565 Training loss: 9.199254035949707
2025-12-09 13:00:16.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0009992317004533314 Training loss: 9.195355415344238
2025-12-09 13:00:16.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0009992143428480215 Training loss: 9.153220176696777
2025-12-09 13:00:17.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0009991967915073715 Training loss: 8.87348461151123
2025-12-09 13:00:17.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0009991790464381925 Training loss: 9.144179344177246
2025-12-09 13:00:17.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0009991611076473714 Training loss: 9.43064022064209
2025-12-09 13:00:18.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0009991429751418698 Training loss: 8.9924955368042
2025-12-09 13:00:18.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0009991246489287244 Training loss: 9.314881324768066
2025-12-09 13:00:19.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0009991061290150474 Training loss: 9.020210266113281
2025-12-09 13:00:19.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0009990874154080258 Training loss: 9.28157901763916
2025-12-09 13:00:19.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0009990685081149222 Training loss: 9.131901741027832
2025-12-09 13:00:20.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0009990494071430741 Training loss: 9.299622535705566
2025-12-09 13:00:20.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0009990301124998943 Training loss: 9.137345314025879
2025-12-09 13:00:20.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0009990106241928704 Training loss: 9.228001594543457
2025-12-09 13:00:21.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0009989909422295658 Training loss: 8.982528686523438
2025-12-09 13:00:21.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0009989710666176185 Training loss: 8.974932670593262
2025-12-09 13:00:22.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0009989509973647418 Training loss: 9.214588165283203
2025-12-09 13:00:22.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0009989307344787242 Training loss: 8.97619342803955
2025-12-09 13:00:22.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0009989102779674292 Training loss: 8.985005378723145
2025-12-09 13:00:23.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.000998889627838796 Training loss: 9.286198616027832
2025-12-09 13:00:23.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.000998868784100838 Training loss: 9.199370384216309
2025-12-09 13:00:23.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0009988477467616447 Training loss: 8.929740905761719
2025-12-09 13:00:24.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0009988265158293798 Training loss: 8.998412132263184
2025-12-09 13:00:24.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.000998805091312283 Training loss: 9.062339782714844
2025-12-09 13:00:25.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0009987834732186687 Training loss: 9.31432056427002
2025-12-09 13:00:25.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0009987616615569263 Training loss: 9.271262168884277
2025-12-09 13:00:25.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0009987396563355204 Training loss: 9.281968116760254
2025-12-09 13:00:26.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.000998717457562991 Training loss: 9.102042198181152
2025-12-09 13:00:26.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0009986950652479533 Training loss: 9.003632545471191
2025-12-09 13:00:26.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0009986724793990967 Training loss: 9.013727188110352
2025-12-09 13:00:27.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0009986497000251866 Training loss: 9.273077011108398
2025-12-09 13:00:27.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0009986267271350634 Training loss: 8.901498794555664
2025-12-09 13:00:28.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0009986035607376421 Training loss: 9.047677040100098
2025-12-09 13:00:28.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0009985802008419132 Training loss: 8.894655227661133
2025-12-09 13:00:28.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0009985566474569425 Training loss: 9.08566665649414
2025-12-09 13:00:29.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0009985329005918703 Training loss: 9.049214363098145
2025-12-09 13:00:29.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0009985089602559125 Training loss: 8.84961986541748
2025-12-09 13:00:29.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0009984848264583597 Training loss: 9.065335273742676
2025-12-09 13:00:30.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.000998460499208578 Training loss: 9.127481460571289
2025-12-09 13:00:30.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.000998435978516008 Training loss: 8.890545845031738
2025-12-09 13:00:30.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0009984112643901658 Training loss: 9.075743675231934
2025-12-09 13:00:31.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0009983863568406427 Training loss: 9.134389877319336
2025-12-09 13:00:31.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0009983612558771048 Training loss: 9.046087265014648
2025-12-09 13:00:32.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0009983359615092931 Training loss: 9.31141471862793
2025-12-09 13:00:32.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.000998310473747024 Training loss: 8.935006141662598
2025-12-09 13:00:32.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0009982847926001885 Training loss: 8.974495887756348
2025-12-09 13:00:33.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0009982589180787533 Training loss: 9.480904579162598
2025-12-09 13:00:33.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0009982328501927599 Training loss: 8.948978424072266
2025-12-09 13:00:33.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0009982065889523242 Training loss: 8.758326530456543
2025-12-09 13:00:34.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.000998180134367638 Training loss: 9.322686195373535
2025-12-09 13:00:34.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0009981534864489678 Training loss: 8.984633445739746
2025-12-09 13:00:35.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0009981266452066553 Training loss: 8.975831985473633
2025-12-09 13:00:35.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0009980996106511168 Training loss: 8.946418762207031
2025-12-09 13:00:35.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.000998072382792844 Training loss: 8.887574195861816
2025-12-09 13:00:36.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.0009980449616424037 Training loss: 9.019353866577148
2025-12-09 13:00:36.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.000998017347210437 Training loss: 8.968107223510742
2025-12-09 13:00:36.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.000997989539507661 Training loss: 9.328836441040039
2025-12-09 13:00:37.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.000997961538544867 Training loss: 8.959345817565918
2025-12-09 13:00:37.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0009979333443329217 Training loss: 8.935186386108398
2025-12-09 13:00:38.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.000997904956882767 Training loss: 9.46858024597168
2025-12-09 13:00:38.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0009978763762054192 Training loss: 9.758927345275879
2025-12-09 13:00:38.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00099784760231197 Training loss: 8.943705558776855
2025-12-09 13:00:39.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.000997818635213586 Training loss: 9.093546867370605
2025-12-09 13:00:39.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0009977894749215088 Training loss: 9.067055702209473
2025-12-09 13:00:39.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.000997760121447055 Training loss: 9.072340965270996
2025-12-09 13:00:40.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0009977305748016159 Training loss: 8.979072570800781
2025-12-09 13:00:40.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.000997700834996658 Training loss: 9.488059043884277
2025-12-09 13:00:41.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.000997670902043723 Training loss: 8.852107048034668
2025-12-09 13:00:41.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.000997640775954427 Training loss: 8.896976470947266
2025-12-09 13:00:41.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.0009976104567404615 Training loss: 8.89536190032959
2025-12-09 13:00:42.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0009975799444135929 Training loss: 9.45689868927002
2025-12-09 13:00:42.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0009975492389856621 Training loss: 9.463274002075195
2025-12-09 13:00:42.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0009975183404685856 Training loss: 9.244111061096191
2025-12-09 13:00:43.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0009974872488743543 Training loss: 9.076839447021484
2025-12-09 13:00:43.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0009974559642150344 Training loss: 9.162982940673828
2025-12-09 13:00:43.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.000997424486502767 Training loss: 9.07947063446045
2025-12-09 13:00:44.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0009973928157497674 Training loss: 8.90294361114502
2025-12-09 13:00:44.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.000997360951968327 Training loss: 9.164362907409668
2025-12-09 13:00:45.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0009973288951708112 Training loss: 9.161715507507324
2025-12-09 13:00:45.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0009972966453696609 Training loss: 9.076441764831543
2025-12-09 13:00:45.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0009972642025773912 Training loss: 8.9668550491333
2025-12-09 13:00:46.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0009972315668065929 Training loss: 8.811701774597168
2025-12-09 13:00:46.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.000997198738069931 Training loss: 8.956802368164062
2025-12-09 13:00:46.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.000997165716380146 Training loss: 8.984185218811035
2025-12-09 13:00:47.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0009971325017500525 Training loss: 8.864418029785156
2025-12-09 13:00:47.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0009970990941925411 Training loss: 9.030268669128418
2025-12-09 13:00:48.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0009970654937205762 Training loss: 8.942254066467285
2025-12-09 13:00:48.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0009970317003471976 Training loss: 9.342601776123047
2025-12-09 13:00:48.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0009969977140855198 Training loss: 9.032618522644043
2025-12-09 13:00:49.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009969635349487322 Training loss: 9.606302261352539
2025-12-09 13:00:49.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.000996929162950099 Training loss: 9.114492416381836
2025-12-09 13:00:49.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0009968945981029596 Training loss: 8.843073844909668
2025-12-09 13:00:50.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009968598404207275 Training loss: 8.833215713500977
2025-12-09 13:00:50.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0009968248899168918 Training loss: 8.984464645385742
2025-12-09 13:00:51.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.000996789746605016 Training loss: 8.945331573486328
2025-12-09 13:00:51.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009967544104987386 Training loss: 8.67014217376709
2025-12-09 13:00:51.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0009967188816117727 Training loss: 8.919093132019043
2025-12-09 13:00:52.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0009966831599579067 Training loss: 8.843318939208984
2025-12-09 13:00:52.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.000996647245551003 Training loss: 9.001173973083496
2025-12-09 13:00:52.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0009966111384049996 Training loss: 9.406563758850098
2025-12-09 13:00:53.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0009965748385339088 Training loss: 8.601317405700684
2025-12-09 13:00:53.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0009965383459518181 Training loss: 9.138169288635254
2025-12-09 13:00:54.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0009965016606728893 Training loss: 9.142014503479004
2025-12-09 13:00:54.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0009964647827113596 Training loss: 9.076656341552734
2025-12-09 13:00:54.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0009964277120815403 Training loss: 9.18234920501709
2025-12-09 13:00:55.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0009963904487978177 Training loss: 8.68591594696045
2025-12-09 13:00:55.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0009963529928746534 Training loss: 8.85468864440918
2025-12-09 13:00:55.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0009963153443265829 Training loss: 8.896795272827148
2025-12-09 13:00:56.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0009962775031682168 Training loss: 8.891578674316406
2025-12-09 13:00:56.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0009962394694142409 Training loss: 9.101744651794434
2025-12-09 13:00:56.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0009962012430794153 Training loss: 9.061582565307617
2025-12-09 13:00:57.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0009961628241785747 Training loss: 8.944170951843262
2025-12-09 13:00:57.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0009961242127266288 Training loss: 9.121068000793457
2025-12-09 13:00:58.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0009960854087385617 Training loss: 8.850955963134766
2025-12-09 13:00:58.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.000996046412229433 Training loss: 8.71349811553955
2025-12-09 13:00:58.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0009960072232143762 Training loss: 9.220148086547852
2025-12-09 13:00:59.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0009959678417085997 Training loss: 8.849742889404297
2025-12-09 13:00:59.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0009959282677273868 Training loss: 8.831128120422363
2025-12-09 13:00:59.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0009958885012860954 Training loss: 8.813066482543945
2025-12-09 13:01:00.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0009958485424001581 Training loss: 8.68796443939209
2025-12-09 13:01:00.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0009958083910850822 Training loss: 9.08593463897705
2025-12-09 13:01:01.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0009957680473564494 Training loss: 8.999424934387207
2025-12-09 13:01:01.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0009957275112299165 Training loss: 8.867953300476074
2025-12-09 13:01:01.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0009956867827212148 Training loss: 8.992618560791016
2025-12-09 13:01:02.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.00099564586184615 Training loss: 8.729813575744629
2025-12-09 13:01:02.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0009956047486206032 Training loss: 9.299975395202637
2025-12-09 13:01:02.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0009955634430605291 Training loss: 8.918527603149414
2025-12-09 13:01:03.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.000995521945181958 Training loss: 8.654401779174805
2025-12-09 13:01:03.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0009954802550009943 Training loss: 9.100203514099121
2025-12-09 13:01:04.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.0009954383725338167 Training loss: 9.608113288879395
2025-12-09 13:01:04.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0009953962977966794 Training loss: 8.724231719970703
2025-12-09 13:01:04.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.000995354030805911 Training loss: 8.931002616882324
2025-12-09 13:01:05.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0009953115715779141 Training loss: 9.228940963745117
2025-12-09 13:01:05.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0009952689201291663 Training loss: 9.112866401672363
2025-12-09 13:01:05.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00099522607647622 Training loss: 8.879677772521973
2025-12-09 13:01:06.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0009951830406357018 Training loss: 9.07344913482666
2025-12-09 13:01:06.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0009951398126243135 Training loss: 8.696102142333984
2025-12-09 13:01:07.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0009950963924588304 Training loss: 9.086047172546387
2025-12-09 13:01:07.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0009950527801561033 Training loss: 8.759856224060059
2025-12-09 13:01:07.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0009950089757330574 Training loss: 8.936968803405762
2025-12-09 13:01:08.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0009949649792066922 Training loss: 8.868927955627441
2025-12-09 13:01:08.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.000994920790594082 Training loss: 9.713040351867676
2025-12-09 13:01:08.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0009948764099123755 Training loss: 8.864686012268066
2025-12-09 13:01:09.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.000994831837178796 Training loss: 8.806381225585938
2025-12-09 13:01:09.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.000994787072410641 Training loss: 8.93377685546875
2025-12-09 13:01:09.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0009947421156252835 Training loss: 8.82165813446045
2025-12-09 13:01:10.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0009946969668401698 Training loss: 9.31235408782959
2025-12-09 13:01:10.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.0009946516260728212 Training loss: 8.821179389953613
2025-12-09 13:01:11.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.000994606093340834 Training loss: 9.060606002807617
2025-12-09 13:01:11.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0009945603686618784 Training loss: 9.079970359802246
2025-12-09 13:01:11.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.000994514452053699 Training loss: 9.064461708068848
2025-12-09 13:01:12.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0009944683435341155 Training loss: 8.941243171691895
2025-12-09 13:01:12.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0009944220431210215 Training loss: 9.10112190246582
2025-12-09 13:01:12.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0009943755508323854 Training loss: 9.187788963317871
2025-12-09 13:01:13.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0009943288666862497 Training loss: 8.62945556640625
2025-12-09 13:01:13.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.000994281990700732 Training loss: 9.055447578430176
2025-12-09 13:01:14.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0009942349228940237 Training loss: 8.73198413848877
2025-12-09 13:01:14.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0009941876632843908 Training loss: 8.890374183654785
2025-12-09 13:01:14.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0009941402118901744 Training loss: 9.064692497253418
2025-12-09 13:01:15.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0009940925687297885 Training loss: 8.843194961547852
2025-12-09 13:01:15.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0009940447338217234 Training loss: 8.787732124328613
2025-12-09 13:01:15.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0009939967071845423 Training loss: 8.814430236816406
2025-12-09 13:01:16.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0009939484888368837 Training loss: 8.850788116455078
2025-12-09 13:01:16.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0009939000787974601 Training loss: 9.003788948059082
2025-12-09 13:01:17.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.0009938514770850585 Training loss: 8.906756401062012
2025-12-09 13:01:17.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.0009938026837185403 Training loss: 9.048709869384766
2025-12-09 13:01:17.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0009937536987168413 Training loss: 8.99294662475586
2025-12-09 13:01:18.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0009937045220989715 Training loss: 9.012145042419434
2025-12-09 13:01:18.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0009936551538840153 Training loss: 8.873952865600586
2025-12-09 13:01:18.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.000993605594091132 Training loss: 8.950578689575195
2025-12-09 13:01:19.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.000993555842739554 Training loss: 8.925192832946777
2025-12-09 13:01:19.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0009935058998485897 Training loss: 8.622367858886719
2025-12-09 13:01:20.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0009934557654376205 Training loss: 8.761558532714844
2025-12-09 13:01:20.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0009934054395261025 Training loss: 8.824812889099121
2025-12-09 13:01:20.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0009933549221335664 Training loss: 9.044198989868164
2025-12-09 13:01:21.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.000993304213279617 Training loss: 9.050637245178223
2025-12-09 13:01:21.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0009932533129839334 Training loss: 8.929473876953125
2025-12-09 13:01:21.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.000993202221266269 Training loss: 8.864115715026855
2025-12-09 13:01:22.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0009931509381464515 Training loss: 9.218267440795898
2025-12-09 13:01:22.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0009930994636443828 Training loss: 8.925861358642578
2025-12-09 13:01:22.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0009930477977800392 Training loss: 9.273713111877441
2025-12-09 13:01:23.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.000992995940573471 Training loss: 8.65691089630127
2025-12-09 13:01:23.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0009929438920448037 Training loss: 9.23355484008789
2025-12-09 13:01:24.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0009928916522142356 Training loss: 9.148368835449219
2025-12-09 13:01:24.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00099283922110204 Training loss: 8.965410232543945
2025-12-09 13:01:24.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.0009927865987285648 Training loss: 8.808692932128906
2025-12-09 13:01:25.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0009927337851142314 Training loss: 8.877184867858887
2025-12-09 13:01:25.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.000992680780279536 Training loss: 8.88927936553955
2025-12-09 13:01:25.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0009926275842450482 Training loss: 8.947260856628418
2025-12-09 13:01:26.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0009925741970314129 Training loss: 8.651219367980957
2025-12-09 13:01:26.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.0009925206186593484 Training loss: 8.713417053222656
2025-12-09 13:01:27.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.0009924668491496473 Training loss: 9.135381698608398
2025-12-09 13:01:27.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.000992412888523177 Training loss: 9.007896423339844
2025-12-09 13:01:27.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.000992358736800878 Training loss: 8.917159080505371
2025-12-09 13:01:28.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0009923043940037657 Training loss: 8.931578636169434
2025-12-09 13:01:28.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0009922498601529295 Training loss: 9.282635688781738
2025-12-09 13:01:28.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.000992195135269533 Training loss: 8.898107528686523
2025-12-09 13:01:29.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0009921402193748137 Training loss: 8.993722915649414
2025-12-09 13:01:29.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0009920851124900836 Training loss: 8.785175323486328
2025-12-09 13:01:30.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.0009920298146367287 Training loss: 8.819146156311035
2025-12-09 13:01:30.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0009919743258362086 Training loss: 8.893498420715332
2025-12-09 13:01:30.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0009919186461100577 Training loss: 8.785900115966797
2025-12-09 13:01:31.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.000991862775479884 Training loss: 9.634675979614258
2025-12-09 13:01:31.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00099180671396737 Training loss: 8.854145050048828
2025-12-09 13:01:31.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0009917504615942721 Training loss: 8.61108112335205
2025-12-09 13:01:32.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0009916940183824206 Training loss: 8.588184356689453
2025-12-09 13:01:32.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0009916373843537201 Training loss: 9.23566722869873
2025-12-09 13:01:33.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0009915805595301491 Training loss: 8.901957511901855
2025-12-09 13:01:33.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.0009915235439337602 Training loss: 8.72083854675293
2025-12-09 13:01:33.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0009914663375866803 Training loss: 9.04245662689209
2025-12-09 13:01:34.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0009914089405111098 Training loss: 8.819945335388184
2025-12-09 13:01:34.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0009913513527293235 Training loss: 8.947166442871094
2025-12-09 13:01:34.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0009912935742636697 Training loss: 8.850485801696777
2025-12-09 13:01:35.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0009912356051365717 Training loss: 9.018757820129395
2025-12-09 13:01:35.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0009911774453705258 Training loss: 8.903841972351074
2025-12-09 13:01:35.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.0009911190949881028 Training loss: 8.916704177856445
2025-12-09 13:01:36.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0009910605540119474 Training loss: 8.675870895385742
2025-12-09 13:01:36.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0009910018224647782 Training loss: 8.858406066894531
2025-12-09 13:01:37.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0009909429003693876 Training loss: 9.060410499572754
2025-12-09 13:01:37.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0009908837877486423 Training loss: 8.998579025268555
2025-12-09 13:01:37.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.0009908244846254825 Training loss: 8.785993576049805
2025-12-09 13:01:38.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.000990764991022923 Training loss: 8.819245338439941
2025-12-09 13:01:38.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0009907053069640515 Training loss: 8.976187705993652
2025-12-09 13:01:38.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.000990645432472031 Training loss: 8.581945419311523
2025-12-09 13:01:39.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.000990585367570097 Training loss: 9.028501510620117
2025-12-09 13:01:39.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.0009905251122815596 Training loss: 9.215211868286133
2025-12-09 13:01:40.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.000990464666629803 Training loss: 8.952473640441895
2025-12-09 13:01:40.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0009904040306382847 Training loss: 8.659180641174316
2025-12-09 13:01:40.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0009903432043305365 Training loss: 9.141386985778809
2025-12-09 13:01:41.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0009902821877301638 Training loss: 8.647421836853027
2025-12-09 13:01:41.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.000990220980860846 Training loss: 8.69855785369873
2025-12-09 13:01:41.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.0009901595837463362 Training loss: 8.827096939086914
2025-12-09 13:01:42.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0009900979964104616 Training loss: 8.985047340393066
2025-12-09 13:01:42.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.000990036218877123 Training loss: 9.076203346252441
2025-12-09 13:01:43.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.000989974251170295 Training loss: 8.866312026977539
2025-12-09 13:01:43.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.000989912093314026 Training loss: 8.769219398498535
2025-12-09 13:01:43.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0009898497453324385 Training loss: 9.281238555908203
2025-12-09 13:01:44.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.000989787207249728 Training loss: 8.889594078063965
2025-12-09 13:01:44.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0009897244790901649 Training loss: 8.806197166442871
2025-12-09 13:01:44.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0009896615608780924 Training loss: 9.150951385498047
2025-12-09 13:01:45.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.000989598452637928 Training loss: 8.769433975219727
2025-12-09 13:01:45.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0009895351543941628 Training loss: 8.732271194458008
2025-12-09 13:01:45.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0009894716661713616 Training loss: 8.910907745361328
2025-12-09 13:01:46.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0009894079879941627 Training loss: 8.937575340270996
2025-12-09 13:01:46.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0009893441198872788 Training loss: 8.971851348876953
2025-12-09 13:01:47.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.0009892800618754953 Training loss: 9.016982078552246
2025-12-09 13:01:47.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0009892158139836725 Training loss: 8.834563255310059
2025-12-09 13:01:47.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0009891513762367431 Training loss: 8.679845809936523
2025-12-09 13:01:48.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.0009890867486597146 Training loss: 8.740095138549805
2025-12-09 13:01:48.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0009890219312776677 Training loss: 8.839377403259277
2025-12-09 13:01:48.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0009889569241157564 Training loss: 8.65924072265625
2025-12-09 13:01:49.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.000988891727199209 Training loss: 8.941333770751953
2025-12-09 13:01:49.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.000988826340553327 Training loss: 8.638423919677734
2025-12-09 13:01:50.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0009887607642034859 Training loss: 8.854255676269531
2025-12-09 13:01:50.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0009886949981751346 Training loss: 9.050029754638672
2025-12-09 13:01:50.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0009886290424937951 Training loss: 8.818638801574707
2025-12-09 13:01:51.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0009885628971850642 Training loss: 9.258345603942871
2025-12-09 13:01:51.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0009884965622746112 Training loss: 9.464445114135742
2025-12-09 13:01:51.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0009884300377881795 Training loss: 8.724587440490723
2025-12-09 13:01:52.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0009883633237515858 Training loss: 8.841532707214355
2025-12-09 13:01:52.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0009882964201907208 Training loss: 8.742706298828125
2025-12-09 13:01:53.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0009882293271315482 Training loss: 8.978780746459961
2025-12-09 13:01:53.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0009881620446001056 Training loss: 8.746604919433594
2025-12-09 13:01:53.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.000988094572622504 Training loss: 9.059093475341797
2025-12-09 13:01:54.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0009880269112249281 Training loss: 8.894478797912598
2025-12-09 13:01:54.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.000987959060433636 Training loss: 8.845333099365234
2025-12-09 13:01:54.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.0009878910202749589 Training loss: 8.835260391235352
2025-12-09 13:01:55.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0009878227907753022 Training loss: 8.960169792175293
2025-12-09 13:01:55.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0009877543719611444 Training loss: 8.951156616210938
2025-12-09 13:01:56.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0009876857638590373 Training loss: 8.920156478881836
2025-12-09 13:01:56.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.0009876169664956068 Training loss: 9.175043106079102
2025-12-09 13:01:56.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0009875479798975512 Training loss: 8.919565200805664
2025-12-09 13:01:57.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0009874788040916433 Training loss: 8.946562767028809
2025-12-09 13:01:57.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0009874094391047288 Training loss: 8.76046085357666
2025-12-09 13:01:57.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.0009873398849637267 Training loss: 8.721996307373047
2025-12-09 13:01:58.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00098727014169563 Training loss: 8.756545066833496
2025-12-09 13:01:58.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.0009872002093275043 Training loss: 8.952746391296387
2025-12-09 13:01:58.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.000987130087886489 Training loss: 8.576114654541016
2025-12-09 13:01:59.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.000987059777399797 Training loss: 9.17602252960205
2025-12-09 13:01:59.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.0009869892778947148 Training loss: 9.229090690612793
2025-12-09 13:02:00.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0009869185893986011 Training loss: 8.697415351867676
2025-12-09 13:02:00.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0009868477119388895 Training loss: 8.76125717163086
2025-12-09 13:02:00.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.0009867766455430858 Training loss: 8.903395652770996
2025-12-09 13:02:01.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.0009867053902387693 Training loss: 8.831794738769531
2025-12-09 13:02:01.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0009866339460535929 Training loss: 8.953293800354004
2025-12-09 13:02:01.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0009865623130152828 Training loss: 8.81802749633789
2025-12-09 13:02:02.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.0009864904911516383 Training loss: 8.72712516784668
2025-12-09 13:02:02.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0009864184804905323 Training loss: 8.996217727661133
2025-12-09 13:02:03.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0009863462810599105 Training loss: 8.912375450134277
2025-12-09 13:02:03.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.000986273892887792 Training loss: 9.282248497009277
2025-12-09 13:02:03.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0009862013160022696 Training loss: 8.873257637023926
2025-12-09 13:02:04.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0009861285504315085 Training loss: 8.940634727478027
2025-12-09 13:02:04.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0009860555962037478 Training loss: 8.802968978881836
2025-12-09 13:02:04.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0009859824533472999 Training loss: 8.780756950378418
2025-12-09 13:02:05.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0009859091218905498 Training loss: 9.138632774353027
2025-12-09 13:02:05.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.000985835601861956 Training loss: 9.087044715881348
2025-12-09 13:02:06.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.0009857618932900504 Training loss: 9.315680503845215
2025-12-09 13:02:06.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0009856879962034375 Training loss: 8.896748542785645
2025-12-09 13:02:06.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0009856139106307956 Training loss: 8.76923942565918
2025-12-09 13:02:07.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0009855396366008756 Training loss: 9.007196426391602
2025-12-09 13:02:07.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0009854651741425023 Training loss: 8.72045612335205
2025-12-09 13:02:07.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0009853905232845728 Training loss: 8.75302505493164
2025-12-09 13:02:08.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0009853156840560575 Training loss: 9.15939712524414
2025-12-09 13:02:08.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0009852406564860004 Training loss: 8.897002220153809
2025-12-09 13:02:09.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.000985165440603518 Training loss: 9.015929222106934
2025-12-09 13:02:09.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0009850900364378 Training loss: 8.886028289794922
2025-12-09 13:02:09.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0009850144440181096 Training loss: 9.135725975036621
2025-12-09 13:02:10.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0009849386633737824 Training loss: 8.707448959350586
2025-12-09 13:02:10.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.0009848626945342278 Training loss: 8.934905052185059
2025-12-09 13:02:10.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0009847865375289275 Training loss: 8.770895004272461
2025-12-09 13:02:11.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0009847101923874367 Training loss: 8.786239624023438
2025-12-09 13:02:11.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.0009846336591393832 Training loss: 8.865350723266602
2025-12-09 13:02:12.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0009845569378144686 Training loss: 8.904621124267578
2025-12-09 13:02:12.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.0009844800284424663 Training loss: 8.92142105102539
2025-12-09 13:02:12.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0009844029310532238 Training loss: 8.859911918640137
2025-12-09 13:02:13.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0009843256456766609 Training loss: 9.043450355529785
2025-12-09 13:02:13.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0009842481723427705 Training loss: 8.651044845581055
2025-12-09 13:02:13.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.0009841705110816186 Training loss: 8.837430953979492
2025-12-09 13:02:14.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.000984092661923344 Training loss: 9.486786842346191
2025-12-09 13:02:14.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0009840146248981585 Training loss: 9.162250518798828
2025-12-09 13:02:14.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.0009839364000363466 Training loss: 8.941104888916016
2025-12-09 13:02:15.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.000983857987368266 Training loss: 8.758257865905762
2025-12-09 13:02:15.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0009837793869243467 Training loss: 8.920615196228027
2025-12-09 13:02:16.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.0009837005987350927 Training loss: 8.710097312927246
2025-12-09 13:02:16.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0009836216228310797 Training loss: 9.473055839538574
2025-12-09 13:02:16.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0009835424592429566 Training loss: 8.777423858642578
2025-12-09 13:02:17.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0009834631080014456 Training loss: 8.808911323547363
2025-12-09 13:02:17.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0009833835691373412 Training loss: 8.708501815795898
2025-12-09 13:02:17.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.000983303842681511 Training loss: 8.942031860351562
2025-12-09 13:02:18.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.000983223928664895 Training loss: 8.885597229003906
2025-12-09 13:02:18.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0009831438271185064 Training loss: 8.722382545471191
2025-12-09 13:02:19.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.0009830635380734312 Training loss: 8.735800743103027
2025-12-09 13:02:19.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.000982983061560828 Training loss: 8.885522842407227
2025-12-09 13:02:19.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0009829023976119279 Training loss: 9.104954719543457
2025-12-09 13:02:20.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0009828215462580352 Training loss: 8.862663269042969
2025-12-09 13:02:20.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0009827405075305267 Training loss: 9.146924018859863
2025-12-09 13:02:20.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.000982659281460852 Training loss: 8.805074691772461
2025-12-09 13:02:21.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0009825778680805331 Training loss: 9.192343711853027
2025-12-09 13:02:21.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.0009824962674211653 Training loss: 8.673962593078613
2025-12-09 13:02:22.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0009824144795144158 Training loss: 8.837141990661621
2025-12-09 13:02:22.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0009823325043920256 Training loss: 9.345657348632812
2025-12-09 13:02:22.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0009822503420858068 Training loss: 8.951546669006348
2025-12-09 13:02:23.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0009821679926276456 Training loss: 8.624842643737793
2025-12-09 13:02:23.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0009820854560494998 Training loss: 8.916516304016113
2025-12-09 13:02:23.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0009820027323834007 Training loss: 9.062616348266602
2025-12-09 13:02:24.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0009819198216614513 Training loss: 8.683043479919434
2025-12-09 13:02:24.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0009818367239158277 Training loss: 9.082542419433594
2025-12-09 13:02:24.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0009817534391787788 Training loss: 8.911630630493164
2025-12-09 13:02:25.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0009816699674826256 Training loss: 8.74827766418457
2025-12-09 13:02:25.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.0009815863088597618 Training loss: 8.785943031311035
2025-12-09 13:02:26.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.0009815024633426537 Training loss: 9.114813804626465
2025-12-09 13:02:26.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.00098141843096384 Training loss: 8.998416900634766
2025-12-09 13:02:26.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.0009813342117559324 Training loss: 9.060345649719238
2025-12-09 13:02:27.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.0009812498057516143 Training loss: 8.722627639770508
2025-12-09 13:02:27.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0009811652129836422 Training loss: 8.750787734985352
2025-12-09 13:02:27.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0009810804334848449 Training loss: 8.98681354522705
2025-12-09 13:02:28.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.0009809954672881237 Training loss: 8.7793607711792
2025-12-09 13:02:28.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0009809103144264523 Training loss: 8.746893882751465
2025-12-09 13:02:29.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0009808249749328768 Training loss: 8.962031364440918
2025-12-09 13:02:29.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.000980739448840516 Training loss: 8.766244888305664
2025-12-09 13:02:29.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.0009806537361825606 Training loss: 8.828676223754883
2025-12-09 13:02:30.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0009805678369922742 Training loss: 8.849570274353027
2025-12-09 13:02:30.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.0009804817513029928 Training loss: 9.438755989074707
2025-12-09 13:02:30.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.000980395479148124 Training loss: 8.790326118469238
2025-12-09 13:02:31.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0009803090205611487 Training loss: 8.776994705200195
2025-12-09 13:02:31.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.0009802223755756199 Training loss: 8.999738693237305
2025-12-09 13:02:32.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0009801355442251626 Training loss: 8.95058822631836
2025-12-09 13:02:32.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0009800485265434745 Training loss: 8.890241622924805
2025-12-09 13:02:32.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0009799613225643252 Training loss: 8.843713760375977
2025-12-09 13:02:33.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.0009798739323215572 Training loss: 8.934069633483887
2025-12-09 13:02:33.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.0009797863558490849 Training loss: 8.775749206542969
2025-12-09 13:02:33.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.000979698593180895 Training loss: 8.881163597106934
2025-12-09 13:02:34.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0009796106443510462 Training loss: 9.373701095581055
2025-12-09 13:02:34.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.00097952250939367 Training loss: 8.940766334533691
2025-12-09 13:02:35.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.0009794341883429699 Training loss: 8.731324195861816
2025-12-09 13:02:35.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0009793456812332215 Training loss: 9.34367847442627
2025-12-09 13:02:35.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0009792569880987725 Training loss: 9.04361629486084
2025-12-09 13:02:36.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0009791681089740432 Training loss: 9.028301239013672
2025-12-09 13:02:36.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.0009790790438935256 Training loss: 9.16904354095459
2025-12-09 13:02:36.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.0009789897928917846 Training loss: 8.919853210449219
2025-12-09 13:02:37.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.000978900356003456 Training loss: 9.050774574279785
2025-12-09 13:02:37.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0009788107332632495 Training loss: 8.99604606628418
2025-12-09 13:02:37.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0009787209247059453 Training loss: 8.91409969329834
2025-12-09 13:02:38.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.0009786309303663962 Training loss: 8.890934944152832
2025-12-09 13:02:38.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0009785407502795277 Training loss: 8.607489585876465
2025-12-09 13:02:39.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0009784503844803367 Training loss: 9.105460166931152
2025-12-09 13:02:39.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0009783598330038925 Training loss: 9.086908340454102
2025-12-09 13:02:39.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0009782690958853363 Training loss: 8.980643272399902
2025-12-09 13:02:40.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0009781781731598813 Training loss: 8.878005027770996
2025-12-09 13:02:40.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.000978087064862813 Training loss: 8.821627616882324
2025-12-09 13:02:40.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0009779957710294885 Training loss: 8.785542488098145
2025-12-09 13:02:41.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0009779042916953375 Training loss: 8.876693725585938
2025-12-09 13:02:41.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0009778126268958612 Training loss: 8.798321723937988
2025-12-09 13:02:42.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.000977720776666633 Training loss: 8.894777297973633
2025-12-09 13:02:42.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.000977628741043298 Training loss: 8.927213668823242
2025-12-09 13:02:42.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0009775365200615734 Training loss: 9.035512924194336
2025-12-09 13:02:43.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0009774441137572487 Training loss: 9.327749252319336
2025-12-09 13:02:43.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0009773515221661846 Training loss: 8.609233856201172
2025-12-09 13:02:43.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0009772587453243141 Training loss: 8.86060619354248
2025-12-09 13:02:44.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.0009771657832676427 Training loss: 9.287379264831543
2025-12-09 13:02:44.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0009770726360322465 Training loss: 8.839970588684082
2025-12-09 13:02:45.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.000976979303654274 Training loss: 8.781437873840332
2025-12-09 13:02:45.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0009768857861699462 Training loss: 8.85851764678955
2025-12-09 13:02:45.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0009767920836155552 Training loss: 9.171542167663574
2025-12-09 13:02:46.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0009766981960274653 Training loss: 8.918597221374512
2025-12-09 13:02:46.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.000976604123442112 Training loss: 8.956527709960938
2025-12-09 13:02:46.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.0009765098658960035 Training loss: 8.887020111083984
2025-12-09 13:02:47.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0009764154234257191 Training loss: 9.378649711608887
2025-12-09 13:02:47.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.00097632079606791 Training loss: 8.594223022460938
2025-12-09 13:02:48.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0009762259838592994 Training loss: 8.73393726348877
2025-12-09 13:02:48.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0009761309868366819 Training loss: 9.204139709472656
2025-12-09 13:02:48.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0009760358050369243 Training loss: 9.012548446655273
2025-12-09 13:02:49.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0009759404384969643 Training loss: 8.790596961975098
2025-12-09 13:02:49.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0009758448872538121 Training loss: 8.867982864379883
2025-12-09 13:02:49.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0009757491513445493 Training loss: 8.666529655456543
2025-12-09 13:02:50.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0009756532308063293 Training loss: 8.862868309020996
2025-12-09 13:02:50.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0009755571256763765 Training loss: 8.765416145324707
2025-12-09 13:02:50.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0009754608359919879 Training loss: 9.191608428955078
2025-12-09 13:02:51.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0009753643617905312 Training loss: 8.890125274658203
2025-12-09 13:02:51.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0009752677031094466 Training loss: 9.003271102905273
2025-12-09 13:02:52.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0009751708599862451 Training loss: 8.92955207824707
2025-12-09 13:02:52.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0009750738324585098 Training loss: 8.904447555541992
2025-12-09 13:02:52.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0009749766205638952 Training loss: 8.698989868164062
2025-12-09 13:02:53.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.0009748792243401273 Training loss: 8.963727951049805
2025-12-09 13:02:53.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0009747816438250037 Training loss: 9.20563793182373
2025-12-09 13:02:53.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0009746838790563935 Training loss: 8.95497989654541
2025-12-09 13:02:54.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.000974585930072237 Training loss: 8.934551239013672
2025-12-09 13:02:54.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0009744877969105468 Training loss: 9.067296028137207
2025-12-09 13:02:55.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0009743894796094062 Training loss: 8.776187896728516
2025-12-09 13:02:55.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.0009742909782069701 Training loss: 8.771040916442871
2025-12-09 13:02:55.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0009741922927414651 Training loss: 8.939224243164062
2025-12-09 13:02:56.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0009740934232511893 Training loss: 8.603645324707031
2025-12-09 13:02:56.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0009739943697745117 Training loss: 8.849344253540039
2025-12-09 13:02:56.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0009738951323498732 Training loss: 8.893131256103516
2025-12-09 13:02:57.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0009737957110157858 Training loss: 9.097920417785645
2025-12-09 13:02:57.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0009736961058108331 Training loss: 8.778083801269531
2025-12-09 13:02:58.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0009735963167736698 Training loss: 8.817008972167969
2025-12-09 13:02:58.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.0009734963439430222 Training loss: 9.174715042114258
2025-12-09 13:02:58.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0009733961873576877 Training loss: 9.001701354980469
2025-12-09 13:02:59.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0009732958470565352 Training loss: 8.62687873840332
2025-12-09 13:02:59.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0009731953230785049 Training loss: 8.899345397949219
2025-12-09 13:02:59.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0009730946154626079 Training loss: 8.78498649597168
2025-12-09 13:03:00.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.000972993724247927 Training loss: 8.85984992980957
2025-12-09 13:03:00.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0009728926494736163 Training loss: 9.151200294494629
2025-12-09 13:03:01.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0009727913911789008 Training loss: 9.027976989746094
2025-12-09 13:03:01.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0009726899494030768 Training loss: 9.13295841217041
2025-12-09 13:03:01.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0009725883241855118 Training loss: 9.150062561035156
2025-12-09 13:03:02.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0009724865155656448 Training loss: 9.785155296325684
2025-12-09 13:03:02.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0009723845235829856 Training loss: 9.047425270080566
2025-12-09 13:03:02.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0009722823482771155 Training loss: 9.113332748413086
2025-12-09 13:03:03.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0009721799896876864 Training loss: 8.992713928222656
2025-12-09 13:03:03.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0009720774478544219 Training loss: 8.711901664733887
2025-12-09 13:03:03.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0009719747228171163 Training loss: 8.630678176879883
2025-12-09 13:03:04.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0009718718146156355 Training loss: 8.971692085266113
2025-12-09 13:03:04.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0009717687232899158 Training loss: 8.920167922973633
2025-12-09 13:03:05.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0009716654488799652 Training loss: 9.093037605285645
2025-12-09 13:03:05.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0009715619914258623 Training loss: 9.12193775177002
2025-12-09 13:03:05.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.000971458350967757 Training loss: 8.75223159790039
2025-12-09 13:03:06.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0009713545275458703 Training loss: 8.774348258972168
2025-12-09 13:03:06.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0009712505212004937 Training loss: 9.122368812561035
2025-12-09 13:03:06.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0009711463319719904 Training loss: 9.150091171264648
2025-12-09 13:03:07.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0009710419599007938 Training loss: 8.98502254486084
2025-12-09 13:03:07.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0009709374050274089 Training loss: 8.976263999938965
2025-12-09 13:03:08.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0009708326673924114 Training loss: 8.620312690734863
2025-12-09 13:03:08.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0009707277470364482 Training loss: 8.982985496520996
2025-12-09 13:03:08.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0009706226440002363 Training loss: 8.883047103881836
2025-12-09 13:03:09.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0009705173583245644 Training loss: 8.98604965209961
2025-12-09 13:03:09.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0009704118900502918 Training loss: 8.998429298400879
2025-12-09 13:03:09.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0009703062392183488 Training loss: 9.028036117553711
2025-12-09 13:03:10.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0009702004058697362 Training loss: 9.316061973571777
2025-12-09 13:03:10.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0009700943900455262 Training loss: 8.81187915802002
2025-12-09 13:03:11.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0009699881917868609 Training loss: 8.790072441101074
2025-12-09 13:03:11.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.0009698818111349544 Training loss: 8.891671180725098
2025-12-09 13:03:11.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0009697752481310904 Training loss: 8.87205696105957
2025-12-09 13:03:12.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0009696685028166244 Training loss: 8.730527877807617
2025-12-09 13:03:12.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.000969561575232982 Training loss: 8.945183753967285
2025-12-09 13:03:12.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0009694544654216595 Training loss: 8.842991828918457
2025-12-09 13:03:13.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0009693471734242243 Training loss: 9.017873764038086
2025-12-09 13:03:13.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0009692396992823144 Training loss: 8.954875946044922
2025-12-09 13:03:14.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0009691320430376385 Training loss: 8.870152473449707
2025-12-09 13:03:14.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0009690242047319755 Training loss: 9.011482238769531
2025-12-09 13:03:14.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.0009689161844071756 Training loss: 9.076582908630371
2025-12-09 13:03:15.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0009688079821051594 Training loss: 8.617546081542969
2025-12-09 13:03:15.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.0009686995978679181 Training loss: 8.840387344360352
2025-12-09 13:03:15.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0009685910317375133 Training loss: 8.804596900939941
2025-12-09 13:03:16.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0009684822837560776 Training loss: 8.793456077575684
2025-12-09 13:03:16.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0009683733539658139 Training loss: 8.921491622924805
2025-12-09 13:03:16.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0009682642424089958 Training loss: 9.14338493347168
2025-12-09 13:03:17.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.0009681549491279673 Training loss: 8.917916297912598
2025-12-09 13:03:17.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.000968045474165143 Training loss: 9.242270469665527
2025-12-09 13:03:18.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0009679358175630081 Training loss: 8.956360816955566
2025-12-09 13:03:18.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.000967825979364118 Training loss: 8.831049919128418
2025-12-09 13:03:18.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.0009677159596110987 Training loss: 9.341041564941406
2025-12-09 13:03:19.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.000967605758346647 Training loss: 8.903855323791504
2025-12-09 13:03:19.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0009674953756135297 Training loss: 8.798447608947754
2025-12-09 13:03:19.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0009673848114545843 Training loss: 8.75668716430664
2025-12-09 13:03:20.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0009672740659127184 Training loss: 9.149629592895508
2025-12-09 13:03:20.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0009671631390309102 Training loss: 9.043027877807617
2025-12-09 13:03:21.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0009670520308522084 Training loss: 8.963472366333008
2025-12-09 13:03:21.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0009669407414197318 Training loss: 8.733806610107422
2025-12-09 13:03:21.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0009668292707766699 Training loss: 8.790458679199219
2025-12-09 13:03:22.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0009667176189662818 Training loss: 8.905228614807129
2025-12-09 13:03:22.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.0009666057860318978 Training loss: 8.849903106689453
2025-12-09 13:03:22.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.000966493772016918 Training loss: 9.324928283691406
2025-12-09 13:03:23.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0009663815769648127 Training loss: 9.030343055725098
2025-12-09 13:03:23.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.000966269200919123 Training loss: 8.846738815307617
2025-12-09 13:03:24.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0009661566439234593 Training loss: 8.968799591064453
2025-12-09 13:03:24.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.000966043906021503 Training loss: 8.858352661132812
2025-12-09 13:03:24.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.0009659309872570057 Training loss: 9.740279197692871
2025-12-09 13:03:25.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0009658178876737886 Training loss: 9.398697853088379
2025-12-09 13:03:25.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0009657046073157437 Training loss: 8.737801551818848
2025-12-09 13:03:25.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.0009655911462268327 Training loss: 8.661553382873535
2025-12-09 13:03:26.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.000965477504451088 Training loss: 8.784241676330566
2025-12-09 13:03:26.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.0009653636820326113 Training loss: 8.94445514678955
2025-12-09 13:03:27.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0009652496790155752 Training loss: 8.747698783874512
2025-12-09 13:03:27.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0009651354954442218 Training loss: 8.756053924560547
2025-12-09 13:03:27.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.0009650211313628637 Training loss: 8.846616744995117
2025-12-09 13:03:28.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0009649065868158832 Training loss: 8.881478309631348
2025-12-09 13:03:28.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.0009647918618477329 Training loss: 8.765130996704102
2025-12-09 13:03:28.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0009646769565029354 Training loss: 8.874812126159668
2025-12-09 13:03:29.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.0009645618708260831 Training loss: 8.875649452209473
2025-12-09 13:03:29.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0009644466048618386 Training loss: 8.741565704345703
2025-12-09 13:03:30.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0009643311586549342 Training loss: 9.591398239135742
2025-12-09 13:03:30.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0009642155322501725 Training loss: 9.06101131439209
2025-12-09 13:03:30.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0009640997256924257 Training loss: 8.864084243774414
2025-12-09 13:03:31.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0009639837390266362 Training loss: 8.868433952331543
2025-12-09 13:03:31.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0009638675722978161 Training loss: 9.146724700927734
2025-12-09 13:03:31.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0009637512255510475 Training loss: 8.98477840423584
2025-12-09 13:03:32.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.000963634698831482 Training loss: 9.378290176391602
2025-12-09 13:03:32.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.0009635179921843417 Training loss: 9.006269454956055
2025-12-09 13:03:32.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.0009634011056549182 Training loss: 8.94113540649414
2025-12-09 13:03:33.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.0009632840392885726 Training loss: 9.117043495178223
2025-12-09 13:03:33.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0009631667931307364 Training loss: 9.207880020141602
2025-12-09 13:03:34.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.0009630493672269101 Training loss: 8.87587833404541
2025-12-09 13:03:34.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.0009629317616226649 Training loss: 9.084671020507812
2025-12-09 13:03:34.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.0009628139763636407 Training loss: 8.922636985778809
2025-12-09 13:03:35.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.0009626960114955483 Training loss: 8.959832191467285
2025-12-09 13:03:35.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0009625778670641669 Training loss: 8.952259063720703
2025-12-09 13:03:35.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.0009624595431153467 Training loss: 9.090909957885742
2025-12-09 13:03:36.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.0009623410396950063 Training loss: 8.917102813720703
2025-12-09 13:03:36.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.000962222356849135 Training loss: 8.750676155090332
2025-12-09 13:03:37.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.000962103494623791 Training loss: 8.788788795471191
2025-12-09 13:03:37.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.0009619844530651026 Training loss: 8.832688331604004
2025-12-09 13:03:37.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.0009618652322192675 Training loss: 8.816078186035156
2025-12-09 13:03:38.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.0009617458321325529 Training loss: 9.150752067565918
2025-12-09 13:03:38.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.0009616262528512957 Training loss: 9.142777442932129
2025-12-09 13:03:38.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.0009615064944219022 Training loss: 9.32310676574707
2025-12-09 13:03:39.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.0009613865568908484 Training loss: 9.0405855178833
2025-12-09 13:03:39.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.0009612664403046797 Training loss: 8.94371509552002
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 3.32 GiB is free. Including non-PyTorch memory, this process has 90.80 GiB memory in use. Of the allocated memory 89.27 GiB is allocated by PyTorch, and 792.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 50/10000 [00:00<00:20, 497.29it/s]Tokenizing texts:   1%|▏         | 135/10000 [00:00<00:14, 701.02it/s]Tokenizing texts:   2%|▏         | 206/10000 [00:00<00:15, 621.26it/s]Tokenizing texts:   3%|▎         | 274/10000 [00:00<00:15, 639.35it/s]Tokenizing texts:   3%|▎         | 339/10000 [00:00<00:15, 611.25it/s]Tokenizing texts:   4%|▍         | 401/10000 [00:00<00:15, 601.65it/s]Tokenizing texts:   5%|▍         | 462/10000 [00:00<00:16, 573.00it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 575.06it/s]Tokenizing texts:   6%|▌         | 597/10000 [00:00<00:15, 600.00it/s]Tokenizing texts:   7%|▋         | 658/10000 [00:01<00:15, 593.12it/s]Tokenizing texts:   7%|▋         | 718/10000 [00:01<00:16, 577.88it/s]Tokenizing texts:   8%|▊         | 776/10000 [00:01<00:16, 576.22it/s]Tokenizing texts:   8%|▊         | 834/10000 [00:01<00:16, 571.41it/s]Tokenizing texts:   9%|▉         | 898/10000 [00:01<00:15, 590.31it/s]Tokenizing texts:  10%|▉         | 978/10000 [00:01<00:13, 650.46it/s]Tokenizing texts:  11%|█         | 1051/10000 [00:01<00:14, 635.89it/s]Tokenizing texts:  11%|█         | 1115/10000 [00:01<00:14, 632.58it/s]Tokenizing texts:  12%|█▏        | 1179/10000 [00:01<00:14, 597.68it/s]Tokenizing texts:  12%|█▏        | 1240/10000 [00:02<00:14, 594.47it/s]Tokenizing texts:  13%|█▎        | 1305/10000 [00:02<00:14, 608.76it/s]Tokenizing texts:  14%|█▎        | 1367/10000 [00:02<00:14, 597.41it/s]Tokenizing texts:  14%|█▍        | 1427/10000 [00:02<00:14, 596.15it/s]Tokenizing texts:  15%|█▌        | 1505/10000 [00:02<00:13, 647.83it/s]Tokenizing texts:  16%|█▌        | 1582/10000 [00:02<00:12, 682.09it/s]Tokenizing texts:  17%|█▋        | 1651/10000 [00:02<00:13, 617.18it/s]Tokenizing texts:  17%|█▋        | 1715/10000 [00:02<00:13, 607.12it/s]Tokenizing texts:  18%|█▊        | 1785/10000 [00:02<00:12, 632.61it/s]Tokenizing texts:  18%|█▊        | 1850/10000 [00:03<00:13, 605.07it/s]Tokenizing texts:  19%|█▉        | 1914/10000 [00:03<00:13, 613.21it/s]Tokenizing texts:  20%|█▉        | 1976/10000 [00:03<00:14, 568.56it/s]Tokenizing texts:  20%|██        | 2049/10000 [00:03<00:13, 610.69it/s]Tokenizing texts:  21%|██        | 2112/10000 [00:03<00:14, 555.03it/s]Tokenizing texts:  22%|██▏       | 2170/10000 [00:03<00:14, 556.97it/s]Tokenizing texts:  22%|██▏       | 2245/10000 [00:03<00:12, 609.34it/s]Tokenizing texts:  23%|██▎       | 2311/10000 [00:03<00:12, 599.52it/s]Tokenizing texts:  24%|██▍       | 2378/10000 [00:03<00:12, 619.06it/s]Tokenizing texts:  24%|██▍       | 2441/10000 [00:04<00:12, 619.43it/s]Tokenizing texts:  25%|██▌       | 2504/10000 [00:04<00:12, 620.95it/s]Tokenizing texts:  26%|██▌       | 2567/10000 [00:04<00:12, 596.28it/s]Tokenizing texts:  26%|██▋       | 2628/10000 [00:04<00:12, 575.74it/s]Tokenizing texts:  27%|██▋       | 2686/10000 [00:04<00:12, 566.50it/s]Tokenizing texts:  27%|██▋       | 2749/10000 [00:04<00:12, 582.29it/s]Tokenizing texts:  28%|██▊       | 2808/10000 [00:04<00:13, 537.53it/s]Tokenizing texts:  29%|██▊       | 2865/10000 [00:04<00:13, 545.16it/s]Tokenizing texts:  29%|██▉       | 2938/10000 [00:04<00:11, 595.49it/s]Tokenizing texts:  30%|███       | 3000/10000 [00:05<00:11, 601.52it/s]Tokenizing texts:  31%|███       | 3061/10000 [00:05<00:11, 593.93it/s]Tokenizing texts:  31%|███▏      | 3129/10000 [00:05<00:11, 617.32it/s]Tokenizing texts:  32%|███▏      | 3195/10000 [00:05<00:10, 627.62it/s]Tokenizing texts:  33%|███▎      | 3259/10000 [00:05<00:10, 622.70it/s]Tokenizing texts:  33%|███▎      | 3329/10000 [00:05<00:10, 644.88it/s]Tokenizing texts:  34%|███▍      | 3394/10000 [00:05<00:10, 636.82it/s]Tokenizing texts:  35%|███▍      | 3484/10000 [00:05<00:09, 708.08it/s]Tokenizing texts:  36%|███▌      | 3555/10000 [00:05<00:10, 627.92it/s]Tokenizing texts:  36%|███▌      | 3620/10000 [00:05<00:10, 626.86it/s]Tokenizing texts:  37%|███▋      | 3684/10000 [00:06<00:10, 629.36it/s]Tokenizing texts:  37%|███▋      | 3748/10000 [00:06<00:09, 628.49it/s]Tokenizing texts:  38%|███▊      | 3812/10000 [00:06<00:10, 566.78it/s]Tokenizing texts:  39%|███▉      | 3883/10000 [00:06<00:10, 604.02it/s]Tokenizing texts:  39%|███▉      | 3945/10000 [00:06<00:10, 580.83it/s]Tokenizing texts:  40%|████      | 4005/10000 [00:06<00:10, 585.49it/s]Tokenizing texts:  41%|████      | 4080/10000 [00:06<00:09, 626.62it/s]Tokenizing texts:  42%|████▏     | 4154/10000 [00:06<00:09, 643.73it/s]Tokenizing texts:  42%|████▏     | 4224/10000 [00:06<00:08, 650.66it/s]Tokenizing texts:  43%|████▎     | 4290/10000 [00:07<00:09, 632.06it/s]Tokenizing texts:  44%|████▎     | 4366/10000 [00:07<00:08, 665.42it/s]Tokenizing texts:  44%|████▍     | 4437/10000 [00:07<00:08, 675.31it/s]Tokenizing texts:  45%|████▌     | 4505/10000 [00:07<00:08, 635.79it/s]Tokenizing texts:  46%|████▌     | 4574/10000 [00:07<00:08, 650.36it/s]Tokenizing texts:  46%|████▋     | 4640/10000 [00:07<00:08, 642.52it/s]Tokenizing texts:  47%|████▋     | 4728/10000 [00:07<00:07, 707.40it/s]Tokenizing texts:  48%|████▊     | 4800/10000 [00:07<00:08, 641.17it/s]Tokenizing texts:  49%|████▉     | 4875/10000 [00:07<00:07, 663.55it/s]Tokenizing texts:  49%|████▉     | 4945/10000 [00:08<00:07, 673.61it/s]Tokenizing texts:  50%|█████     | 5023/10000 [00:08<00:07, 700.85it/s]Tokenizing texts:  51%|█████     | 5094/10000 [00:08<00:07, 641.46it/s]Tokenizing texts:  52%|█████▏    | 5160/10000 [00:08<00:07, 635.73it/s]Tokenizing texts:  52%|█████▏    | 5225/10000 [00:08<00:07, 623.72it/s]Tokenizing texts:  53%|█████▎    | 5291/10000 [00:08<00:07, 633.67it/s]Tokenizing texts:  54%|█████▍    | 5379/10000 [00:08<00:06, 697.47it/s]Tokenizing texts:  55%|█████▍    | 5450/10000 [00:08<00:06, 686.72it/s]Tokenizing texts:  55%|█████▌    | 5520/10000 [00:08<00:07, 619.06it/s]Tokenizing texts:  56%|█████▌    | 5590/10000 [00:09<00:06, 639.99it/s]Tokenizing texts:  57%|█████▋    | 5673/10000 [00:09<00:06, 688.50it/s]Tokenizing texts:  57%|█████▋    | 5744/10000 [00:09<00:06, 676.70it/s]Tokenizing texts:  58%|█████▊    | 5813/10000 [00:09<00:06, 600.45it/s]Tokenizing texts:  59%|█████▉    | 5881/10000 [00:09<00:06, 619.57it/s]Tokenizing texts:  59%|█████▉    | 5945/10000 [00:09<00:06, 608.71it/s]Tokenizing texts:  60%|██████    | 6010/10000 [00:09<00:06, 597.27it/s]Tokenizing texts:  61%|██████    | 6087/10000 [00:09<00:06, 643.68it/s]Tokenizing texts:  62%|██████▏   | 6157/10000 [00:09<00:05, 654.85it/s]Tokenizing texts:  62%|██████▏   | 6224/10000 [00:10<00:05, 640.17it/s]Tokenizing texts:  63%|██████▎   | 6289/10000 [00:10<00:05, 640.48it/s]Tokenizing texts:  64%|██████▎   | 6354/10000 [00:10<00:06, 602.94it/s]Tokenizing texts:  64%|██████▍   | 6423/10000 [00:10<00:05, 619.39it/s]Tokenizing texts:  65%|██████▍   | 6486/10000 [00:10<00:05, 616.89it/s]Tokenizing texts:  65%|██████▌   | 6549/10000 [00:10<00:05, 616.29it/s]Tokenizing texts:  66%|██████▌   | 6622/10000 [00:10<00:05, 647.41it/s]Tokenizing texts:  67%|██████▋   | 6688/10000 [00:10<00:05, 643.73it/s]Tokenizing texts:  68%|██████▊   | 6753/10000 [00:10<00:05, 625.96it/s]Tokenizing texts:  68%|██████▊   | 6836/10000 [00:10<00:04, 684.13it/s]Tokenizing texts:  69%|██████▉   | 6905/10000 [00:11<00:05, 609.63it/s]Tokenizing texts:  70%|██████▉   | 6970/10000 [00:11<00:04, 619.84it/s]Tokenizing texts:  70%|███████   | 7034/10000 [00:11<00:04, 620.51it/s]Tokenizing texts:  71%|███████   | 7097/10000 [00:11<00:05, 571.28it/s]Tokenizing texts:  72%|███████▏  | 7161/10000 [00:11<00:04, 588.45it/s]Tokenizing texts:  72%|███████▏  | 7225/10000 [00:11<00:04, 594.87it/s]Tokenizing texts:  73%|███████▎  | 7305/10000 [00:11<00:04, 650.90it/s]Tokenizing texts:  74%|███████▎  | 7371/10000 [00:11<00:04, 643.74it/s]Tokenizing texts:  74%|███████▍  | 7436/10000 [00:12<00:04, 597.19it/s]Tokenizing texts:  75%|███████▌  | 7500/10000 [00:12<00:04, 607.45it/s]Tokenizing texts:  76%|███████▌  | 7567/10000 [00:12<00:03, 622.66it/s]Tokenizing texts:  76%|███████▋  | 7639/10000 [00:12<00:03, 645.99it/s]Tokenizing texts:  77%|███████▋  | 7710/10000 [00:12<00:03, 659.05it/s]Tokenizing texts:  78%|███████▊  | 7777/10000 [00:12<00:03, 632.68it/s]Tokenizing texts:  78%|███████▊  | 7841/10000 [00:12<00:03, 625.47it/s]Tokenizing texts:  79%|███████▉  | 7904/10000 [00:12<00:03, 617.70it/s]Tokenizing texts:  80%|███████▉  | 7983/10000 [00:12<00:03, 667.01it/s]Tokenizing texts:  81%|████████  | 8051/10000 [00:12<00:03, 634.82it/s]Tokenizing texts:  81%|████████  | 8116/10000 [00:13<00:03, 566.92it/s]Tokenizing texts:  82%|████████▏ | 8184/10000 [00:13<00:03, 594.80it/s]Tokenizing texts:  82%|████████▏ | 8245/10000 [00:13<00:03, 561.99it/s]Tokenizing texts:  83%|████████▎ | 8318/10000 [00:13<00:02, 592.87it/s]Tokenizing texts:  84%|████████▍ | 8385/10000 [00:13<00:02, 612.70it/s]Tokenizing texts:  84%|████████▍ | 8448/10000 [00:13<00:02, 614.05it/s]Tokenizing texts:  85%|████████▌ | 8514/10000 [00:13<00:02, 626.43it/s]Tokenizing texts:  86%|████████▌ | 8578/10000 [00:13<00:02, 625.42it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:13<00:02, 662.11it/s]Tokenizing texts:  87%|████████▋ | 8726/10000 [00:14<00:01, 675.40it/s]Tokenizing texts:  88%|████████▊ | 8794/10000 [00:14<00:01, 665.33it/s]Tokenizing texts:  89%|████████▊ | 8861/10000 [00:14<00:01, 601.78it/s]Tokenizing texts:  89%|████████▉ | 8944/10000 [00:14<00:01, 663.60it/s]Tokenizing texts:  90%|█████████ | 9012/10000 [00:14<00:01, 624.07it/s]Tokenizing texts:  91%|█████████ | 9080/10000 [00:14<00:01, 628.64it/s]Tokenizing texts:  92%|█████████▏| 9151/10000 [00:14<00:01, 649.71it/s]Tokenizing texts:  92%|█████████▏| 9225/10000 [00:14<00:01, 656.54it/s]Tokenizing texts:  93%|█████████▎| 9292/10000 [00:14<00:01, 641.04it/s]Tokenizing texts:  94%|█████████▎| 9357/10000 [00:15<00:01, 621.69it/s]Tokenizing texts:  94%|█████████▍| 9420/10000 [00:15<00:00, 597.86it/s]Tokenizing texts:  95%|█████████▍| 9481/10000 [00:15<00:00, 548.54it/s]Tokenizing texts:  96%|█████████▌| 9555/10000 [00:15<00:00, 597.48it/s]Tokenizing texts:  96%|█████████▌| 9616/10000 [00:15<00:00, 578.21it/s]Tokenizing texts:  97%|█████████▋| 9676/10000 [00:15<00:00, 582.83it/s]Tokenizing texts:  97%|█████████▋| 9735/10000 [00:15<00:00, 572.08it/s]Tokenizing texts:  98%|█████████▊| 9793/10000 [00:15<00:00, 570.93it/s]Tokenizing texts:  99%|█████████▊| 9865/10000 [00:15<00:00, 613.39it/s]Tokenizing texts:  99%|█████████▉| 9937/10000 [00:16<00:00, 643.95it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:16<00:00, 620.72it/s]
2025-12-09 13:04:28.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 12.163979530334473
2025-12-09 13:04:28.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 12.187880516052246
2025-12-09 13:04:29.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 12.197481155395508
2025-12-09 13:04:29.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 12.192538261413574
2025-12-09 13:04:30.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 12.158312797546387
2025-12-09 13:04:30.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 12.183597564697266
2025-12-09 13:04:30.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 12.242986679077148
2025-12-09 13:04:31.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 12.140340805053711
2025-12-09 13:04:31.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 12.123865127563477
2025-12-09 13:04:31.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 12.141407012939453
2025-12-09 13:04:32.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 12.12791633605957
2025-12-09 13:04:32.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 12.158493041992188
2025-12-09 13:04:33.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 12.15687370300293
2025-12-09 13:04:33.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 12.133540153503418
2025-12-09 13:04:33.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 12.06472396850586
2025-12-09 13:04:34.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 12.127336502075195
2025-12-09 13:04:34.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 12.098105430603027
2025-12-09 13:04:34.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 12.104207038879395
2025-12-09 13:04:35.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 11.969590187072754
2025-12-09 13:04:35.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 12.01125717163086
2025-12-09 13:04:36.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 12.064716339111328
2025-12-09 13:04:36.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 11.90658950805664
2025-12-09 13:04:36.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 11.972935676574707
2025-12-09 13:04:37.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 11.906488418579102
2025-12-09 13:04:37.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 11.819367408752441
2025-12-09 13:04:37.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 11.82531452178955
2025-12-09 13:04:38.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 11.712775230407715
2025-12-09 13:04:38.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 11.538509368896484
2025-12-09 13:04:39.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 11.772583961486816
2025-12-09 13:04:39.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 11.324129104614258
2025-12-09 13:04:39.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 11.619688987731934
2025-12-09 13:04:40.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 11.256367683410645
2025-12-09 13:04:40.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 11.18386173248291
2025-12-09 13:04:40.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 10.844184875488281
2025-12-09 13:04:41.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 11.35335922241211
2025-12-09 13:04:41.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 10.721540451049805
2025-12-09 13:04:41.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 10.723678588867188
2025-12-09 13:04:42.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 10.806222915649414
2025-12-09 13:04:42.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 10.9032564163208
2025-12-09 13:04:43.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 10.29613971710205
2025-12-09 13:04:43.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 10.465530395507812
2025-12-09 13:04:43.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 10.581889152526855
2025-12-09 13:04:44.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 10.137469291687012
2025-12-09 13:04:44.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 10.076581001281738
2025-12-09 13:04:44.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 10.292071342468262
2025-12-09 13:04:45.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 9.92024040222168
2025-12-09 13:04:45.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 10.176229476928711
2025-12-09 13:04:46.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 10.199204444885254
2025-12-09 13:04:46.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 10.003252029418945
2025-12-09 13:04:46.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 10.032995223999023
2025-12-09 13:04:47.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 10.026782989501953
2025-12-09 13:04:47.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 10.020535469055176
2025-12-09 13:04:47.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 9.888640403747559
2025-12-09 13:04:48.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 9.584131240844727
2025-12-09 13:04:48.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 10.541848182678223
2025-12-09 13:04:49.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 9.836770057678223
2025-12-09 13:04:49.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 9.702540397644043
2025-12-09 13:04:49.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 9.863363265991211
2025-12-09 13:04:50.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 9.664637565612793
2025-12-09 13:04:50.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 9.542978286743164
2025-12-09 13:04:50.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 9.583080291748047
2025-12-09 13:04:51.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 9.578849792480469
2025-12-09 13:04:51.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 9.973373413085938
2025-12-09 13:04:52.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 9.79847526550293
2025-12-09 13:04:52.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 9.603888511657715
2025-12-09 13:04:52.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 9.375127792358398
2025-12-09 13:04:53.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 9.582247734069824
2025-12-09 13:04:53.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 9.478677749633789
2025-12-09 13:04:53.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 9.370415687561035
2025-12-09 13:04:54.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 9.338805198669434
2025-12-09 13:04:54.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 9.136085510253906
2025-12-09 13:04:54.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 9.720562934875488
2025-12-09 13:04:55.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 9.479307174682617
2025-12-09 13:04:55.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 9.296883583068848
2025-12-09 13:04:56.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 9.578920364379883
2025-12-09 13:04:56.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 9.212071418762207
2025-12-09 13:04:56.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 9.455487251281738
2025-12-09 13:04:57.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 9.378890991210938
2025-12-09 13:04:57.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 9.423171997070312
2025-12-09 13:04:57.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 9.19096565246582
2025-12-09 13:04:58.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 9.394083976745605
2025-12-09 13:04:58.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 9.272732734680176
2025-12-09 13:04:59.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 9.206514358520508
2025-12-09 13:04:59.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 9.094009399414062
2025-12-09 13:04:59.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 9.232072830200195
2025-12-09 13:05:00.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 9.244791984558105
2025-12-09 13:05:00.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 10.210721015930176
2025-12-09 13:05:00.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 9.478326797485352
2025-12-09 13:05:01.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 9.231222152709961
2025-12-09 13:05:01.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 9.198280334472656
2025-12-09 13:05:02.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 9.04789924621582
2025-12-09 13:05:02.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 9.097054481506348
2025-12-09 13:05:02.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 9.20063591003418
2025-12-09 13:05:03.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 9.328718185424805
2025-12-09 13:05:03.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 9.054551124572754
2025-12-09 13:05:03.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 9.218976974487305
2025-12-09 13:05:04.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 9.033333778381348
2025-12-09 13:05:04.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 9.105785369873047
2025-12-09 13:05:05.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 9.12907600402832
2025-12-09 13:05:05.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 9.071802139282227
2025-12-09 13:05:05.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999997089396424 Training loss: 9.212545394897461
2025-12-09 13:05:06.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002999998835758683 Training loss: 8.888249397277832
2025-12-09 13:05:06.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029999973804574606 Training loss: 9.123733520507812
2025-12-09 13:05:06.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0029999953430365394 Training loss: 9.005468368530273
2025-12-09 13:05:07.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.002999992723496711 Training loss: 9.03601360321045
2025-12-09 13:05:07.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.002999989521838991 Training loss: 9.147802352905273
2025-12-09 13:05:07.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029999857380646226 Training loss: 8.822027206420898
2025-12-09 13:05:08.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.002999981372175074 Training loss: 9.229202270507812
2025-12-09 13:05:08.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0029999764241720396 Training loss: 9.033212661743164
2025-12-09 13:05:09.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0029999708940574394 Training loss: 9.230890274047852
2025-12-09 13:05:09.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0029999647818334195 Training loss: 8.722346305847168
2025-12-09 13:05:09.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.002999958087502352 Training loss: 9.025964736938477
2025-12-09 13:05:10.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029999508110668356 Training loss: 9.147625923156738
2025-12-09 13:05:10.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029999429525296934 Training loss: 9.235187530517578
2025-12-09 13:05:10.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0029999345118939752 Training loss: 8.974577903747559
2025-12-09 13:05:11.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0029999254891629563 Training loss: 8.951959609985352
2025-12-09 13:05:11.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.002999915884340139 Training loss: 8.881974220275879
2025-12-09 13:05:12.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0029999056974292504 Training loss: 9.076031684875488
2025-12-09 13:05:12.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029998949284342435 Training loss: 9.049019813537598
2025-12-09 13:05:12.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.002999883577359298 Training loss: 8.9442777633667
2025-12-09 13:05:13.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.002999871644208819 Training loss: 9.138118743896484
2025-12-09 13:05:13.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002999859128987437 Training loss: 9.011968612670898
2025-12-09 13:05:13.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00299984603170001 Training loss: 9.166598320007324
2025-12-09 13:05:14.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0029998323523516197 Training loss: 8.844850540161133
2025-12-09 13:05:14.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029998180909475754 Training loss: 8.812294960021973
2025-12-09 13:05:15.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.002999803247493411 Training loss: 9.132085800170898
2025-12-09 13:05:15.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.002999787821994888 Training loss: 8.8753662109375
2025-12-09 13:05:15.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.0029997718144579915 Training loss: 9.183545112609863
2025-12-09 13:05:16.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0029997552248889354 Training loss: 8.715680122375488
2025-12-09 13:05:16.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.002999738053294156 Training loss: 9.530058860778809
2025-12-09 13:05:16.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0029997202996803183 Training loss: 8.8114595413208
2025-12-09 13:05:17.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002999701964054312 Training loss: 9.358261108398438
2025-12-09 13:05:17.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0029996830464232523 Training loss: 8.915806770324707
2025-12-09 13:05:18.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0029996635467944813 Training loss: 9.232075691223145
2025-12-09 13:05:18.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0029996434651755662 Training loss: 8.863179206848145
2025-12-09 13:05:18.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0029996228015743004 Training loss: 8.827253341674805
2025-12-09 13:05:19.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.002999601555998703 Training loss: 9.039045333862305
2025-12-09 13:05:19.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.002999579728457019 Training loss: 8.85445785522461
2025-12-09 13:05:19.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.002999557318957719 Training loss: 8.903572082519531
2025-12-09 13:05:20.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0029995343275095003 Training loss: 8.950624465942383
2025-12-09 13:05:20.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0029995107541212845 Training loss: 8.984630584716797
2025-12-09 13:05:20.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.002999486598802221 Training loss: 8.979615211486816
2025-12-09 13:05:21.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0029994618615616832 Training loss: 9.102749824523926
2025-12-09 13:05:21.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.002999436542409272 Training loss: 8.867372512817383
2025-12-09 13:05:22.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0029994106413548122 Training loss: 8.87325668334961
2025-12-09 13:05:22.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0029993841584083558 Training loss: 8.708415031433105
2025-12-09 13:05:22.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.002999357093580181 Training loss: 8.65829849243164
2025-12-09 13:05:23.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0029993294468807904 Training loss: 9.013705253601074
2025-12-09 13:05:23.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0029993012183209137 Training loss: 8.977921485900879
2025-12-09 13:05:23.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0029992724079115052 Training loss: 9.010900497436523
2025-12-09 13:05:24.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.002999243015663746 Training loss: 8.781085014343262
2025-12-09 13:05:24.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0029992130415890427 Training loss: 8.872142791748047
2025-12-09 13:05:25.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.002999182485699028 Training loss: 8.9130859375
2025-12-09 13:05:25.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0029991513480055595 Training loss: 8.966861724853516
2025-12-09 13:05:25.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002999119628520721 Training loss: 9.50719165802002
2025-12-09 13:05:26.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0029990873272568224 Training loss: 9.005294799804688
2025-12-09 13:05:26.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0029990544442264 Training loss: 8.955747604370117
2025-12-09 13:05:26.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.002999020979442214 Training loss: 9.077376365661621
2025-12-09 13:05:27.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.002998986932917252 Training loss: 9.007561683654785
2025-12-09 13:05:27.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.002998952304664726 Training loss: 8.786760330200195
2025-12-09 13:05:28.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.002998917094698076 Training loss: 8.780332565307617
2025-12-09 13:05:28.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.002998881303030965 Training loss: 8.626286506652832
2025-12-09 13:05:28.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0029988449296772836 Training loss: 8.782966613769531
2025-12-09 13:05:29.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.002998807974651147 Training loss: 8.759478569030762
2025-12-09 13:05:29.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0029987704379668976 Training loss: 8.983901023864746
2025-12-09 13:05:29.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002998732319639102 Training loss: 8.714378356933594
2025-12-09 13:05:30.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0029986936196825537 Training loss: 8.81999397277832
2025-12-09 13:05:30.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.002998654338112271 Training loss: 8.898782730102539
2025-12-09 13:05:30.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0029986144749434987 Training loss: 9.342625617980957
2025-12-09 13:05:31.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0029985740301917063 Training loss: 9.009204864501953
2025-12-09 13:05:31.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.00299853300387259 Training loss: 8.847990036010742
2025-12-09 13:05:32.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0029984913960020712 Training loss: 9.020313262939453
2025-12-09 13:05:32.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0029984492065962976 Training loss: 8.530567169189453
2025-12-09 13:05:32.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0029984064356716415 Training loss: 8.886796951293945
2025-12-09 13:05:33.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0029983630832447015 Training loss: 9.233400344848633
2025-12-09 13:05:33.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002998319149332302 Training loss: 8.944657325744629
2025-12-09 13:05:33.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0029982746339514933 Training loss: 8.680294036865234
2025-12-09 13:05:34.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0029982295371195496 Training loss: 8.78189754486084
2025-12-09 13:05:34.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.002998183858853974 Training loss: 8.879965782165527
2025-12-09 13:05:35.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0029981375991724917 Training loss: 8.910370826721191
2025-12-09 13:05:35.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0029980907580930563 Training loss: 8.94847297668457
2025-12-09 13:05:35.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.002998043335633845 Training loss: 8.89465618133545
2025-12-09 13:05:36.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002997995331813262 Training loss: 8.971650123596191
2025-12-09 13:05:36.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.002997946746649937 Training loss: 9.241476058959961
2025-12-09 13:05:36.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.0029978975801627245 Training loss: 8.834799766540527
2025-12-09 13:05:37.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0029978478323707046 Training loss: 8.793237686157227
2025-12-09 13:05:37.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0029977975032931844 Training loss: 9.75260066986084
2025-12-09 13:05:38.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.002997746592949695 Training loss: 8.817691802978516
2025-12-09 13:05:38.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.002997695101359994 Training loss: 8.723430633544922
2025-12-09 13:05:38.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.002997643028544064 Training loss: 8.867053985595703
2025-12-09 13:05:39.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0029975903745221143 Training loss: 8.848877906799316
2025-12-09 13:05:39.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.002997537139314578 Training loss: 9.375770568847656
2025-12-09 13:05:39.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0029974833229421145 Training loss: 9.093452453613281
2025-12-09 13:05:40.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002997428925425609 Training loss: 8.966575622558594
2025-12-09 13:05:40.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0029973739467861736 Training loss: 9.188523292541504
2025-12-09 13:05:41.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.002997318387045142 Training loss: 9.423455238342285
2025-12-09 13:05:41.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0029972622462240777 Training loss: 8.663501739501953
2025-12-09 13:05:41.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0029972055243447666 Training loss: 8.809893608093262
2025-12-09 13:05:42.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.002997148221429223 Training loss: 8.991144180297852
2025-12-09 13:05:42.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.002997090337499683 Training loss: 9.201077461242676
2025-12-09 13:05:42.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0029970318725786116 Training loss: 9.175832748413086
2025-12-09 13:05:43.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0029969728266886976 Training loss: 9.080050468444824
2025-12-09 13:05:43.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0029969131998528555 Training loss: 8.771710395812988
2025-12-09 13:05:43.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0029968529920942253 Training loss: 8.59328556060791
2025-12-09 13:05:44.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0029967922034361727 Training loss: 8.823962211608887
2025-12-09 13:05:44.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.002996730833902288 Training loss: 8.804986000061035
2025-12-09 13:05:45.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.002996668883516388 Training loss: 8.956547737121582
2025-12-09 13:05:45.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002996606352302514 Training loss: 8.864544868469238
2025-12-09 13:05:45.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.002996543240284934 Training loss: 8.970370292663574
2025-12-09 13:05:46.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0029964795474881397 Training loss: 8.702701568603516
2025-12-09 13:05:46.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.002996415273936849 Training loss: 8.979013442993164
2025-12-09 13:05:46.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.002996350419656006 Training loss: 8.707541465759277
2025-12-09 13:05:47.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0029962849846707786 Training loss: 8.840742111206055
2025-12-09 13:05:47.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0029962189690065613 Training loss: 8.887533187866211
2025-12-09 13:05:48.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.0029961523726889736 Training loss: 8.694497108459473
2025-12-09 13:05:48.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0029960851957438594 Training loss: 8.774003028869629
2025-12-09 13:05:48.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.00299601743819729 Training loss: 9.262372970581055
2025-12-09 13:05:49.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.0029959491000755597 Training loss: 8.94626235961914
2025-12-09 13:05:49.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0029958801814051897 Training loss: 9.649081230163574
2025-12-09 13:05:49.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.002995810682212926 Training loss: 8.654716491699219
2025-12-09 13:05:50.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0029957406025257396 Training loss: 8.900941848754883
2025-12-09 13:05:50.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.002995669942370827 Training loss: 8.654862403869629
2025-12-09 13:05:51.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0029955987017756106 Training loss: 8.94970989227295
2025-12-09 13:05:51.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0029955268807677375 Training loss: 9.372591018676758
2025-12-09 13:05:51.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.002995454479375079 Training loss: 9.402376174926758
2025-12-09 13:05:52.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0029953814976257337 Training loss: 8.592668533325195
2025-12-09 13:05:52.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.002995307935548024 Training loss: 9.206732749938965
2025-12-09 13:05:52.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.002995233793170498 Training loss: 8.711031913757324
2025-12-09 13:05:53.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0029951590705219284 Training loss: 8.96481990814209
2025-12-09 13:05:53.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0029950837676313144 Training loss: 8.679619789123535
2025-12-09 13:05:54.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0029950078845278794 Training loss: 9.033087730407715
2025-12-09 13:05:54.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0029949314212410717 Training loss: 8.729411125183105
2025-12-09 13:05:54.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0029948543778005655 Training loss: 8.863478660583496
2025-12-09 13:05:55.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00299477675423626 Training loss: 9.193373680114746
2025-12-09 13:05:55.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.002994698550578279 Training loss: 8.661051750183105
2025-12-09 13:05:55.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0029946197668569725 Training loss: 8.595980644226074
2025-12-09 13:05:56.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.002994540403102914 Training loss: 8.716485977172852
2025-12-09 13:05:56.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0029944604593469034 Training loss: 8.787408828735352
2025-12-09 13:05:56.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0029943799356199658 Training loss: 8.881027221679688
2025-12-09 13:05:57.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0029942988319533507 Training loss: 8.867186546325684
2025-12-09 13:05:57.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.002994217148378532 Training loss: 9.190165519714355
2025-12-09 13:05:58.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.002994134884927211 Training loss: 9.25216293334961
2025-12-09 13:05:58.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.002994052041631311 Training loss: 8.89947509765625
2025-12-09 13:05:58.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0029939686185229825 Training loss: 8.707476615905762
2025-12-09 13:05:59.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0029938846156346006 Training loss: 8.949732780456543
2025-12-09 13:05:59.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.002993800032998765 Training loss: 8.82975959777832
2025-12-09 13:05:59.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.002993714870648301 Training loss: 9.0465669631958
2025-12-09 13:06:00.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0029936291286162577 Training loss: 9.139333724975586
2025-12-09 13:06:00.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00299354280693591 Training loss: 9.107725143432617
2025-12-09 13:06:01.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.002993455905640758 Training loss: 8.981416702270508
2025-12-09 13:06:01.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0029933684247645267 Training loss: 9.011295318603516
2025-12-09 13:06:01.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.002993280364341165 Training loss: 8.797030448913574
2025-12-09 13:06:02.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.002993191724404848 Training loss: 8.932859420776367
2025-12-09 13:06:02.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0029931025049899744 Training loss: 8.805756568908691
2025-12-09 13:06:02.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.002993012706131169 Training loss: 9.111680030822754
2025-12-09 13:06:03.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.002992922327863281 Training loss: 8.933418273925781
2025-12-09 13:06:03.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.002992831370221385 Training loss: 8.863744735717773
2025-12-09 13:06:04.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.002992739833240779 Training loss: 9.20535945892334
2025-12-09 13:06:04.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0029926477169569866 Training loss: 8.938755989074707
2025-12-09 13:06:04.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0029925550214057565 Training loss: 9.00355052947998
2025-12-09 13:06:05.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.002992461746623063 Training loss: 9.324723243713379
2025-12-09 13:06:05.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0029923678926451034 Training loss: 8.620735168457031
2025-12-09 13:06:05.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.0029922734595083005 Training loss: 9.128188133239746
2025-12-09 13:06:06.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.002992178447249302 Training loss: 8.849163055419922
2025-12-09 13:06:06.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0029920828559049806 Training loss: 8.69640827178955
2025-12-09 13:06:07.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0029919866855124336 Training loss: 8.877288818359375
2025-12-09 13:06:07.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0029918899361089826 Training loss: 8.815183639526367
2025-12-09 13:06:07.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0029917926077321732 Training loss: 8.670852661132812
2025-12-09 13:06:08.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.002991694700419778 Training loss: 8.8988618850708
2025-12-09 13:06:08.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.002991596214209793 Training loss: 8.814602851867676
2025-12-09 13:06:08.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0029914971491404375 Training loss: 9.315323829650879
2025-12-09 13:06:09.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0029913975052501575 Training loss: 8.828681945800781
2025-12-09 13:06:09.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.002991297282577623 Training loss: 8.974746704101562
2025-12-09 13:06:09.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.0029911964811617287 Training loss: 8.946521759033203
2025-12-09 13:06:10.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0029910951010415927 Training loss: 9.178020477294922
2025-12-09 13:06:10.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0029909931422565593 Training loss: 8.492457389831543
2025-12-09 13:06:11.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0029908906048461965 Training loss: 9.260757446289062
2025-12-09 13:06:11.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.002990787488850297 Training loss: 8.985660552978516
2025-12-09 13:06:11.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0029906837943088787 Training loss: 8.761630058288574
2025-12-09 13:06:12.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0029905795212621824 Training loss: 8.969803810119629
2025-12-09 13:06:12.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.002990474669750676 Training loss: 8.887656211853027
2025-12-09 13:06:12.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.002990369239815048 Training loss: 9.203179359436035
2025-12-09 13:06:13.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.002990263231496216 Training loss: 8.892528533935547
2025-12-09 13:06:13.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029901566448353183 Training loss: 8.86751937866211
2025-12-09 13:06:14.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029900494798737196 Training loss: 8.917498588562012
2025-12-09 13:06:14.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002989941736653009 Training loss: 8.941184043884277
2025-12-09 13:06:14.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.002989833415214999 Training loss: 8.975016593933105
2025-12-09 13:06:15.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0029897245156017267 Training loss: 9.161107063293457
2025-12-09 13:06:15.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.002989615037855454 Training loss: 9.115171432495117
2025-12-09 13:06:15.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0029895049820186682 Training loss: 8.987137794494629
2025-12-09 13:06:16.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0029893943481340787 Training loss: 9.495384216308594
2025-12-09 13:06:16.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0029892831362446203 Training loss: 8.961274147033691
2025-12-09 13:06:17.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002989171346393453 Training loss: 8.829386711120605
2025-12-09 13:06:17.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00298905897862396 Training loss: 8.875685691833496
2025-12-09 13:06:17.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0029889460329797484 Training loss: 9.045869827270508
2025-12-09 13:06:18.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0029888325095046506 Training loss: 8.89115047454834
2025-12-09 13:06:18.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0029887184082427226 Training loss: 8.910202980041504
2025-12-09 13:06:18.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002988603729238246 Training loss: 8.929847717285156
2025-12-09 13:06:19.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0029884884725357237 Training loss: 9.128677368164062
2025-12-09 13:06:19.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0029883726381798865 Training loss: 8.948111534118652
2025-12-09 13:06:20.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0029882562262156854 Training loss: 9.048996925354004
2025-12-09 13:06:20.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.002988139236688299 Training loss: 8.968341827392578
2025-12-09 13:06:20.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0029880216696431287 Training loss: 9.263120651245117
2025-12-09 13:06:21.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0029879035251257993 Training loss: 9.001022338867188
2025-12-09 13:06:21.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.002987784803182161 Training loss: 8.946195602416992
2025-12-09 13:06:21.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0029876655038582863 Training loss: 9.155648231506348
2025-12-09 13:06:22.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0029875456272004746 Training loss: 9.32477855682373
2025-12-09 13:06:22.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0029874251732552462 Training loss: 9.244153022766113
2025-12-09 13:06:23.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.002987304142069348 Training loss: 9.200175285339355
2025-12-09 13:06:23.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.002987182533689749 Training loss: 8.719306945800781
2025-12-09 13:06:23.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0029870603481636443 Training loss: 8.931982040405273
2025-12-09 13:06:24.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0029869375855384505 Training loss: 9.15953254699707
2025-12-09 13:06:24.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0029868142458618096 Training loss: 9.001313209533691
2025-12-09 13:06:24.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0029866903291815875 Training loss: 8.985647201538086
2025-12-09 13:06:25.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.002986565835545874 Training loss: 8.889851570129395
2025-12-09 13:06:25.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0029864407650029823 Training loss: 9.018586158752441
2025-12-09 13:06:25.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00298631511760145 Training loss: 8.998501777648926
2025-12-09 13:06:26.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0029861888933900385 Training loss: 9.50135326385498
2025-12-09 13:06:26.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.002986062092417733 Training loss: 8.843461036682129
2025-12-09 13:06:27.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0029859347147337422 Training loss: 9.11274528503418
2025-12-09 13:06:27.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.002985806760387499 Training loss: 8.92713451385498
2025-12-09 13:06:27.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00298567822942866 Training loss: 8.67066478729248
2025-12-09 13:06:28.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0029855491219071056 Training loss: 9.010038375854492
2025-12-09 13:06:28.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00298541943787294 Training loss: 8.90786361694336
2025-12-09 13:06:28.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.002985289177376491 Training loss: 8.69865894317627
2025-12-09 13:06:29.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00298515834046831 Training loss: 9.029173851013184
2025-12-09 13:06:29.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.002985026927199172 Training loss: 9.043197631835938
2025-12-09 13:06:30.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0029848949376200767 Training loss: 9.000611305236816
2025-12-09 13:06:30.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0029847623717822462 Training loss: 8.838428497314453
2025-12-09 13:06:30.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0029846292297371264 Training loss: 8.941943168640137
2025-12-09 13:06:31.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.002984495511536388 Training loss: 9.332212448120117
2025-12-09 13:06:31.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0029843612172319235 Training loss: 9.05852222442627
2025-12-09 13:06:31.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.002984226346875851 Training loss: 9.320684432983398
2025-12-09 13:06:32.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0029840909005205093 Training loss: 9.097452163696289
2025-12-09 13:06:32.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.002983954878218464 Training loss: 9.418290138244629
2025-12-09 13:06:33.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.002983818280022502 Training loss: 9.337719917297363
2025-12-09 13:06:33.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0029836811059856354 Training loss: 9.073234558105469
2025-12-09 13:06:33.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0029835433561610975 Training loss: 8.895039558410645
2025-12-09 13:06:34.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0029834050306023468 Training loss: 8.970104217529297
2025-12-09 13:06:34.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0029832661293630646 Training loss: 9.048867225646973
2025-12-09 13:06:34.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.002983126652497156 Training loss: 9.036322593688965
2025-12-09 13:06:35.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.002982986600058749 Training loss: 9.050178527832031
2025-12-09 13:06:35.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0029828459721021965 Training loss: 9.309608459472656
2025-12-09 13:06:36.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.002982704768682071 Training loss: 8.89785385131836
2025-12-09 13:06:36.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0029825629898531728 Training loss: 9.44837760925293
2025-12-09 13:06:36.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.002982420635670523 Training loss: 8.806037902832031
2025-12-09 13:06:37.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.002982277706189366 Training loss: 9.19090747833252
2025-12-09 13:06:37.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00298213420146517 Training loss: 8.958671569824219
2025-12-09 13:06:37.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0029819901215536273 Training loss: 9.050148963928223
2025-12-09 13:06:38.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.002981845466510651 Training loss: 8.929128646850586
2025-12-09 13:06:38.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0029817002363923804 Training loss: 9.012980461120605
2025-12-09 13:06:38.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.002981554431255176 Training loss: 9.035378456115723
2025-12-09 13:06:39.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.002981408051155621 Training loss: 8.990767478942871
2025-12-09 13:06:39.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.002981261096150524 Training loss: 9.150976181030273
2025-12-09 13:06:40.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.002981113566296915 Training loss: 8.857691764831543
2025-12-09 13:06:40.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0029809654616520464 Training loss: 8.742859840393066
2025-12-09 13:06:40.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.0029808167822733956 Training loss: 8.941946983337402
2025-12-09 13:06:41.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0029806675282186626 Training loss: 9.03020191192627
2025-12-09 13:06:41.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.002980517699545769 Training loss: 8.924108505249023
2025-12-09 13:06:41.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0029803672963128617 Training loss: 9.200996398925781
2025-12-09 13:06:42.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0029802163185783073 Training loss: 9.008383750915527
2025-12-09 13:06:42.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0029800647664006996 Training loss: 9.210173606872559
2025-12-09 13:06:43.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.002979912639838851 Training loss: 9.291472434997559
2025-12-09 13:06:43.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0029797599389518002 Training loss: 8.961036682128906
2025-12-09 13:06:43.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0029796066637988072 Training loss: 8.910042762756348
2025-12-09 13:06:44.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.002979452814439354 Training loss: 9.268877029418945
2025-12-09 13:06:44.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0029792983909331487 Training loss: 9.256489753723145
2025-12-09 13:06:44.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0029791433933401175 Training loss: 9.224494934082031
2025-12-09 13:06:45.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0029789878217204137 Training loss: 8.729438781738281
2025-12-09 13:06:45.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0029788316761344114 Training loss: 8.97311019897461
2025-12-09 13:06:46.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0029786749566427066 Training loss: 9.481467247009277
2025-12-09 13:06:46.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00297851766330612 Training loss: 9.224102973937988
2025-12-09 13:06:46.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.002978359796185695 Training loss: 8.923099517822266
2025-12-09 13:06:47.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0029782013553426943 Training loss: 9.119357109069824
2025-12-09 13:06:47.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0029780423408386075 Training loss: 8.959244728088379
2025-12-09 13:06:47.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0029778827527351445 Training loss: 8.898934364318848
2025-12-09 13:06:48.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0029777225910942386 Training loss: 9.194144248962402
2025-12-09 13:06:48.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.002977561855978045 Training loss: 9.146998405456543
2025-12-09 13:06:49.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.002977400547448942 Training loss: 9.08555793762207
2025-12-09 13:06:49.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0029772386655695306 Training loss: 8.801037788391113
2025-12-09 13:06:49.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0029770762104026336 Training loss: 9.121355056762695
2025-12-09 13:06:50.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.002976913182011297 Training loss: 9.019593238830566
2025-12-09 13:06:50.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0029767495804587886 Training loss: 9.12291431427002
2025-12-09 13:06:50.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.002976585405808599 Training loss: 9.110084533691406
2025-12-09 13:06:51.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0029764206581244412 Training loss: 8.971714973449707
2025-12-09 13:06:51.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.002976255337470251 Training loss: 9.172344207763672
2025-12-09 13:06:51.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.002976089443910186 Training loss: 9.301168441772461
2025-12-09 13:06:52.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0029759229775086255 Training loss: 9.632884979248047
2025-12-09 13:06:52.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0029757559383301727 Training loss: 9.61117172241211
2025-12-09 13:06:53.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.0029755883264396517 Training loss: 9.042818069458008
2025-12-09 13:06:53.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00297542014190211 Training loss: 9.047318458557129
2025-12-09 13:06:53.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0029752513847828162 Training loss: 9.207773208618164
2025-12-09 13:06:54.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0029750820551472617 Training loss: 8.945716857910156
2025-12-09 13:06:54.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0029749121530611602 Training loss: 9.138672828674316
2025-12-09 13:06:54.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0029747416785904472 Training loss: 9.362045288085938
2025-12-09 13:06:55.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.002974570631801281 Training loss: 8.991950035095215
2025-12-09 13:06:55.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0029743990127600413 Training loss: 8.848807334899902
2025-12-09 13:06:56.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.002974226821533329 Training loss: 9.114556312561035
2025-12-09 13:06:56.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0029740540581879703 Training loss: 8.970256805419922
2025-12-09 13:06:56.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0029738807227910093 Training loss: 8.929178237915039
2025-12-09 13:06:57.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.002973706815409715 Training loss: 8.70960807800293
2025-12-09 13:06:57.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0029735323361115775 Training loss: 9.177693367004395
2025-12-09 13:06:57.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.002973357284964309 Training loss: 9.048670768737793
2025-12-09 13:06:58.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0029731816620358425 Training loss: 9.151190757751465
2025-12-09 13:06:58.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.002973005467394334 Training loss: 9.17270278930664
2025-12-09 13:06:59.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0029728287011081627 Training loss: 9.519801139831543
2025-12-09 13:06:59.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.002972651363245927 Training loss: 9.10861873626709
2025-12-09 13:06:59.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.002972473453876448 Training loss: 9.07662296295166
2025-12-09 13:07:00.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0029722949730687687 Training loss: 9.048737525939941
2025-12-09 13:07:00.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0029721159208921546 Training loss: 9.134045600891113
2025-12-09 13:07:00.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.0029719362974160927 Training loss: 9.167612075805664
2025-12-09 13:07:01.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0029717561027102907 Training loss: 9.137699127197266
2025-12-09 13:07:01.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.002971575336844679 Training loss: 9.109286308288574
2025-12-09 13:07:02.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.002971393999889409 Training loss: 9.455278396606445
2025-12-09 13:07:02.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.002971212091914854 Training loss: 9.373306274414062
2025-12-09 13:07:02.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0029710296129916093 Training loss: 9.793752670288086
2025-12-09 13:07:03.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0029708465631904913 Training loss: 9.184578895568848
2025-12-09 13:07:03.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.002970662942582538 Training loss: 9.237112045288086
2025-12-09 13:07:03.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.002970478751239009 Training loss: 9.122115135192871
2025-12-09 13:07:04.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0029702939892313853 Training loss: 9.431632995605469
2025-12-09 13:07:04.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.002970108656631369 Training loss: 9.137785911560059
2025-12-09 13:07:04.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.002969922753510885 Training loss: 9.32498550415039
2025-12-09 13:07:05.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.002969736279942078 Training loss: 9.570134162902832
2025-12-09 13:07:05.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.002969549235997315 Training loss: 9.01191520690918
2025-12-09 13:07:06.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.002969361621749184 Training loss: 9.132668495178223
2025-12-09 13:07:06.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.002969173437270495 Training loss: 9.167933464050293
2025-12-09 13:07:06.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0029689846826342773 Training loss: 9.21306324005127
2025-12-09 13:07:07.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.002968795357913784 Training loss: 9.103225708007812
2025-12-09 13:07:07.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0029686054631824885 Training loss: 9.221817970275879
2025-12-09 13:07:07.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0029684149985140847 Training loss: 9.294108390808105
2025-12-09 13:07:08.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0029682239639824883 Training loss: 9.236928939819336
2025-12-09 13:07:08.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.002968032359661836 Training loss: 9.33789348602295
2025-12-09 13:07:09.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.002967840185626486 Training loss: 9.134322166442871
2025-12-09 13:07:09.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0029676474419510174 Training loss: 9.120784759521484
2025-12-09 13:07:09.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0029674541287102296 Training loss: 9.390951156616211
2025-12-09 13:07:10.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.002967260245979144 Training loss: 9.687931060791016
2025-12-09 13:07:10.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.002967065793833003 Training loss: 9.139220237731934
2025-12-09 13:07:10.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.002966870772347269 Training loss: 9.29397201538086
2025-12-09 13:07:11.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0029666751815976273 Training loss: 8.958434104919434
2025-12-09 13:07:11.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0029664790216599813 Training loss: 9.373103141784668
2025-12-09 13:07:12.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0029662822926104578 Training loss: 9.215309143066406
2025-12-09 13:07:12.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0029660849945254038 Training loss: 9.072020530700684
2025-12-09 13:07:12.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0029658871274813856 Training loss: 9.638751983642578
2025-12-09 13:07:13.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0029656886915551926 Training loss: 9.081388473510742
2025-12-09 13:07:13.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0029654896868238335 Training loss: 9.059155464172363
2025-12-09 13:07:13.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0029652901133645384 Training loss: 9.252290725708008
2025-12-09 13:07:14.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0029650899712547574 Training loss: 9.367232322692871
2025-12-09 13:07:14.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0029648892605721624 Training loss: 9.335885047912598
2025-12-09 13:07:15.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0029646879813946445 Training loss: 9.181655883789062
2025-12-09 13:07:15.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.002964486133800317 Training loss: 9.37773609161377
2025-12-09 13:07:15.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0029642837178675122 Training loss: 9.125265121459961
2025-12-09 13:07:16.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.002964080733674784 Training loss: 9.191108703613281
2025-12-09 13:07:16.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0029638771813009076 Training loss: 9.036112785339355
2025-12-09 13:07:16.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.002963673060824877 Training loss: 9.295762062072754
2025-12-09 13:07:17.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0029634683723259066 Training loss: 9.072941780090332
2025-12-09 13:07:17.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0029632631158834333 Training loss: 9.732319831848145
2025-12-09 13:07:17.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0029630572915771117 Training loss: 9.108360290527344
2025-12-09 13:07:18.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.00296285089948682 Training loss: 9.447983741760254
2025-12-09 13:07:18.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0029626439396926536 Training loss: 9.148836135864258
2025-12-09 13:07:19.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0029624364122749296 Training loss: 9.248164176940918
2025-12-09 13:07:19.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0029622283173141866 Training loss: 9.188721656799316
2025-12-09 13:07:19.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00296201965489118 Training loss: 9.090373992919922
2025-12-09 13:07:20.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00296181042508689 Training loss: 9.712789535522461
2025-12-09 13:07:20.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.0029616006279825128 Training loss: 9.473995208740234
2025-12-09 13:07:20.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.002961390263659467 Training loss: 9.088619232177734
2025-12-09 13:07:21.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0029611793321993912 Training loss: 9.340551376342773
2025-12-09 13:07:21.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.0029609678336841444 Training loss: 9.157210350036621
2025-12-09 13:07:22.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0029607557681958037 Training loss: 9.35989761352539
2025-12-09 13:07:22.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0029605431358166686 Training loss: 9.123152732849121
2025-12-09 13:07:22.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.002960329936629257 Training loss: 9.312801361083984
2025-12-09 13:07:23.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.002960116170716308 Training loss: 9.462320327758789
2025-12-09 13:07:23.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0029599018381607787 Training loss: 9.244288444519043
2025-12-09 13:07:23.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0029596869390458485 Training loss: 9.123926162719727
2025-12-09 13:07:24.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.002959471473454915 Training loss: 9.4410982131958
2025-12-09 13:07:24.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0029592554414715967 Training loss: 9.23134994506836
2025-12-09 13:07:25.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.002959038843179731 Training loss: 9.245882034301758
2025-12-09 13:07:25.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0029588216786633763 Training loss: 9.791057586669922
2025-12-09 13:07:25.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0029586039480068087 Training loss: 9.15340805053711
2025-12-09 13:07:26.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0029583856512945257 Training loss: 9.361910820007324
2025-12-09 13:07:26.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0029581667886112435 Training loss: 9.182165145874023
2025-12-09 13:07:26.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0029579473600418998 Training loss: 9.573832511901855
2025-12-09 13:07:27.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0029577273656716495 Training loss: 9.241185188293457
2025-12-09 13:07:27.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0029575068055858675 Training loss: 9.285372734069824
2025-12-09 13:07:28.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.0029572856798701507 Training loss: 9.122091293334961
2025-12-09 13:07:28.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0029570639886103123 Training loss: 9.57144832611084
2025-12-09 13:07:28.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0029568417318923865 Training loss: 9.34541130065918
2025-12-09 13:07:29.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.002956618909802627 Training loss: 9.185590744018555
2025-12-09 13:07:29.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0029563955224275068 Training loss: 9.254534721374512
2025-12-09 13:07:29.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0029561715698537185 Training loss: 9.510101318359375
2025-12-09 13:07:30.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0029559470521681726 Training loss: 9.185604095458984
2025-12-09 13:07:30.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.002955721969458001 Training loss: 9.285122871398926
2025-12-09 13:07:31.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0029554963218105536 Training loss: 9.33714485168457
2025-12-09 13:07:31.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0029552701093133998 Training loss: 9.34394359588623
2025-12-09 13:07:31.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0029550433320543286 Training loss: 9.332919120788574
2025-12-09 13:07:32.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0029548159901213473 Training loss: 9.053221702575684
2025-12-09 13:07:32.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.0029545880836026835 Training loss: 9.51785945892334
2025-12-09 13:07:32.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0029543596125867827 Training loss: 9.336798667907715
2025-12-09 13:07:33.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00295413057716231 Training loss: 9.311530113220215
2025-12-09 13:07:33.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.00295390097741815 Training loss: 9.563076972961426
2025-12-09 13:07:33.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0029536708134434058 Training loss: 9.298240661621094
2025-12-09 13:07:34.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.002953440085327399 Training loss: 9.148079872131348
2025-12-09 13:07:34.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0029532087931596718 Training loss: 9.429924011230469
2025-12-09 13:07:35.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0029529769370299826 Training loss: 9.419052124023438
2025-12-09 13:07:35.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0029527445170283114 Training loss: 9.238582611083984
2025-12-09 13:07:35.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.002952511533244856 Training loss: 9.117767333984375
2025-12-09 13:07:36.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0029522779857700326 Training loss: 9.227344512939453
2025-12-09 13:07:36.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0029520438746944754 Training loss: 9.519506454467773
2025-12-09 13:07:36.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00295180920010904 Training loss: 9.356871604919434
2025-12-09 13:07:37.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.0029515739621047976 Training loss: 9.040719985961914
2025-12-09 13:07:37.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0029513381607730402 Training loss: 9.481520652770996
2025-12-09 13:07:38.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.002951101796205278 Training loss: 9.292869567871094
2025-12-09 13:07:38.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0029508648684932392 Training loss: 9.54655647277832
2025-12-09 13:07:38.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0029506273777288703 Training loss: 9.179763793945312
2025-12-09 13:07:39.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.002950389324004337 Training loss: 9.317381858825684
2025-12-09 13:07:39.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.002950150707412024 Training loss: 9.600811958312988
2025-12-09 13:07:39.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.002949911528044533 Training loss: 9.243979454040527
2025-12-09 13:07:40.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0029496717859946848 Training loss: 9.447141647338867
2025-12-09 13:07:40.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0029494314813555194 Training loss: 9.36429214477539
2025-12-09 13:07:41.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.002949190614220294 Training loss: 9.334230422973633
2025-12-09 13:07:41.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.002948949184682484 Training loss: 9.782920837402344
2025-12-09 13:07:41.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0029487071928357834 Training loss: 9.937284469604492
2025-12-09 13:07:42.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0029484646387741057 Training loss: 10.142605781555176
2025-12-09 13:07:42.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0029482215225915803 Training loss: 9.229917526245117
2025-12-09 13:07:42.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0029479778443825553 Training loss: 9.299327850341797
2025-12-09 13:07:43.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.002947733604241599 Training loss: 9.15746784210205
2025-12-09 13:07:43.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.002947488802263496 Training loss: 9.237639427185059
2025-12-09 13:07:44.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0029472434385432477 Training loss: 9.264547348022461
2025-12-09 13:07:44.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0029469975131760765 Training loss: 9.474387168884277
2025-12-09 13:07:44.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0029467510262574203 Training loss: 9.444198608398438
2025-12-09 13:07:45.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0029465039778829366 Training loss: 9.350851058959961
2025-12-09 13:07:45.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0029462563681484995 Training loss: 9.390048027038574
2025-12-09 13:07:45.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.002946008197150202 Training loss: 9.322746276855469
2025-12-09 13:07:46.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0029457594649843536 Training loss: 9.316773414611816
2025-12-09 13:07:46.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0029455101717474836 Training loss: 9.406571388244629
2025-12-09 13:07:46.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0029452603175363365 Training loss: 9.11057186126709
2025-12-09 13:07:47.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0029450099024478766 Training loss: 9.487923622131348
2025-12-09 13:07:47.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.002944758926579285 Training loss: 9.301076889038086
2025-12-09 13:07:48.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.002944507390027961 Training loss: 9.255971908569336
2025-12-09 13:07:48.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0029442552928915203 Training loss: 9.395228385925293
2025-12-09 13:07:48.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.002944002635267797 Training loss: 9.335047721862793
2025-12-09 13:07:49.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.002943749417254843 Training loss: 9.255499839782715
2025-12-09 13:07:49.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0029434956389509264 Training loss: 9.278979301452637
2025-12-09 13:07:49.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0029432413004545346 Training loss: 9.703943252563477
2025-12-09 13:07:50.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.002942986401864371 Training loss: 9.26502799987793
2025-12-09 13:07:50.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.002942730943279357 Training loss: 9.299877166748047
2025-12-09 13:07:51.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0029424749247986305 Training loss: 9.656344413757324
2025-12-09 13:07:51.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.002942218346521548 Training loss: 9.308416366577148
2025-12-09 13:07:51.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.0029419612085476816 Training loss: 9.53298568725586
2025-12-09 13:07:52.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0029417035109768224 Training loss: 9.225993156433105
2025-12-09 13:07:52.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.002941445253908978 Training loss: 9.366342544555664
2025-12-09 13:07:52.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.0029411864374443717 Training loss: 9.260749816894531
2025-12-09 13:07:53.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.002940927061683446 Training loss: 9.390158653259277
2025-12-09 13:07:53.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.002940667126726859 Training loss: 9.858668327331543
2025-12-09 13:07:54.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0029404066326754875 Training loss: 9.802103996276855
2025-12-09 13:07:54.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0029401455796304234 Training loss: 9.341259956359863
2025-12-09 13:07:54.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0029398839676929756 Training loss: 9.547218322753906
2025-12-09 13:07:55.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.002939621796964672 Training loss: 9.322829246520996
2025-12-09 13:07:55.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.002939359067547255 Training loss: 9.328566551208496
2025-12-09 13:07:55.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.002939095779542685 Training loss: 9.349004745483398
2025-12-09 13:07:56.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0029388319330531385 Training loss: 9.26180648803711
2025-12-09 13:07:56.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0029385675281810106 Training loss: 9.384979248046875
2025-12-09 13:07:57.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.00293830256502891 Training loss: 9.144707679748535
2025-12-09 13:07:57.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0029380370436996642 Training loss: 9.281563758850098
2025-12-09 13:07:57.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0029377709642963174 Training loss: 9.265436172485352
2025-12-09 13:07:58.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0029375043269221296 Training loss: 9.350130081176758
2025-12-09 13:07:58.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.002937237131680577 Training loss: 9.123127937316895
2025-12-09 13:07:58.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.002936969378675354 Training loss: 9.231266021728516
2025-12-09 13:07:59.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0029367010680103685 Training loss: 9.52138900756836
2025-12-09 13:07:59.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0029364321997897482 Training loss: 9.335123062133789
2025-12-09 13:08:00.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0029361627741178358 Training loss: 9.22933292388916
2025-12-09 13:08:00.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.002935892791099189 Training loss: 9.283804893493652
2025-12-09 13:08:00.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.002935622250838583 Training loss: 9.314970970153809
2025-12-09 13:08:01.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0029353511534410104 Training loss: 9.506027221679688
2025-12-09 13:08:01.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.002935079499011677 Training loss: 9.371347427368164
2025-12-09 13:08:01.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0029348072876560086 Training loss: 9.184720993041992
2025-12-09 13:08:02.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0029345345194796437 Training loss: 9.238716125488281
2025-12-09 13:08:02.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0029342611945884388 Training loss: 9.262458801269531
2025-12-09 13:08:02.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0029339873130884656 Training loss: 9.542356491088867
2025-12-09 13:08:03.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0029337128750860125 Training loss: 9.216835975646973
2025-12-09 13:08:03.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0029334378806875837 Training loss: 9.312047004699707
2025-12-09 13:08:04.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.002933162329999899 Training loss: 9.468718528747559
2025-12-09 13:08:04.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.002932886223129894 Training loss: 9.182085037231445
2025-12-09 13:08:04.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0029326095601847203 Training loss: 9.274104118347168
2025-12-09 13:08:05.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0029323323412717463 Training loss: 9.471519470214844
2025-12-09 13:08:05.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0029320545664985537 Training loss: 9.277782440185547
2025-12-09 13:08:05.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0029317762359729427 Training loss: 9.32413101196289
2025-12-09 13:08:06.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.002931497349802928 Training loss: 9.462691307067871
2025-12-09 13:08:06.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.002931217908096739 Training loss: 9.188005447387695
2025-12-09 13:08:07.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0029309379109628223 Training loss: 9.223575592041016
2025-12-09 13:08:07.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0029306573585098387 Training loss: 9.282769203186035
2025-12-09 13:08:07.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0029303762508466656 Training loss: 9.279468536376953
2025-12-09 13:08:08.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0029300945880823956 Training loss: 9.300045013427734
2025-12-09 13:08:08.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0029298123703263364 Training loss: 9.643881797790527
2025-12-09 13:08:08.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.002929529597688011 Training loss: 9.093912124633789
2025-12-09 13:08:09.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0029292462702771574 Training loss: 9.190889358520508
2025-12-09 13:08:09.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0029289623882037302 Training loss: 9.242181777954102
2025-12-09 13:08:10.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0029286779515778983 Training loss: 9.164554595947266
2025-12-09 13:08:10.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0029283929605100458 Training loss: 9.333365440368652
2025-12-09 13:08:10.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0029281074151107727 Training loss: 9.340275764465332
2025-12-09 13:08:11.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.002927821315490893 Training loss: 9.223601341247559
2025-12-09 13:08:11.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0029275346617614363 Training loss: 9.461601257324219
2025-12-09 13:08:11.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.002927247454033648 Training loss: 9.145842552185059
2025-12-09 13:08:12.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.002926959692418988 Training loss: 9.1737060546875
2025-12-09 13:08:12.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0029266713770291293 Training loss: 9.092184066772461
2025-12-09 13:08:13.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0029263825079759638 Training loss: 9.300651550292969
2025-12-09 13:08:13.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0029260930853715937 Training loss: 9.175888061523438
2025-12-09 13:08:13.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0029258031093283396 Training loss: 9.552647590637207
2025-12-09 13:08:14.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0029255125799587355 Training loss: 9.325433731079102
2025-12-09 13:08:14.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.002925221497375529 Training loss: 9.222614288330078
2025-12-09 13:08:14.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0029249298616916856 Training loss: 9.306270599365234
2025-12-09 13:08:15.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.002924637673020382 Training loss: 9.281845092773438
2025-12-09 13:08:15.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.002924344931475011 Training loss: 9.487692832946777
2025-12-09 13:08:16.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0029240516371691803 Training loss: 9.2279052734375
2025-12-09 13:08:16.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.002923757790216711 Training loss: 9.474321365356445
2025-12-09 13:08:16.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0029234633907316405 Training loss: 9.34521770477295
2025-12-09 13:08:17.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0029231684388282184 Training loss: 9.287054061889648
2025-12-09 13:08:17.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00292287293462091 Training loss: 9.269393920898438
2025-12-09 13:08:17.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0029225768782243956 Training loss: 8.919231414794922
2025-12-09 13:08:18.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0029222802697535678 Training loss: 9.363842964172363
2025-12-09 13:08:18.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.002921983109323535 Training loss: 9.038269996643066
2025-12-09 13:08:18.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0029216853970496196 Training loss: 9.674710273742676
2025-12-09 13:08:19.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0029213871330473575 Training loss: 9.287984848022461
2025-12-09 13:08:19.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.002921088317432499 Training loss: 9.18457317352295
2025-12-09 13:08:20.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0029207889503210095 Training loss: 9.523695945739746
2025-12-09 13:08:20.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.002920489031829067 Training loss: 9.332012176513672
2025-12-09 13:08:20.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.002920188562073063 Training loss: 9.250235557556152
2025-12-09 13:08:21.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0029198875411696056 Training loss: 9.315071105957031
2025-12-09 13:08:21.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0029195859692355145 Training loss: 9.073363304138184
2025-12-09 13:08:21.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.002919283846387824 Training loss: 9.250810623168945
2025-12-09 13:08:22.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0029189811727437813 Training loss: 9.110254287719727
2025-12-09 13:08:22.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.002918677948420849 Training loss: 9.624336242675781
2025-12-09 13:08:23.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0029183741735367024 Training loss: 9.228384017944336
2025-12-09 13:08:23.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0029180698482092304 Training loss: 9.107142448425293
2025-12-09 13:08:23.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0029177649725565355 Training loss: 9.220852851867676
2025-12-09 13:08:24.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0029174595466969345 Training loss: 8.969204902648926
2025-12-09 13:08:24.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0029171535707489572 Training loss: 9.373642921447754
2025-12-09 13:08:24.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0029168470448313463 Training loss: 9.135238647460938
2025-12-09 13:08:25.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.002916539969063059 Training loss: 9.117030143737793
2025-12-09 13:08:25.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0029162323435632654 Training loss: 9.176196098327637
2025-12-09 13:08:26.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.002915924168451349 Training loss: 9.809307098388672
2025-12-09 13:08:26.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.002915615443846906 Training loss: 9.319313049316406
2025-12-09 13:08:26.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0029153061698697475 Training loss: 9.095032691955566
2025-12-09 13:08:27.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0029149963466398956 Training loss: 9.202287673950195
2025-12-09 13:08:27.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.002914685974277587 Training loss: 9.283754348754883
2025-12-09 13:08:27.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.002914375052903271 Training loss: 9.107033729553223
2025-12-09 13:08:28.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.002914063582637611 Training loss: 9.167243957519531
2025-12-09 13:08:28.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.002913751563601481 Training loss: 9.20693302154541
2025-12-09 13:08:29.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0029134389959159708 Training loss: 9.177382469177246
2025-12-09 13:08:29.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0029131258797023816 Training loss: 9.419050216674805
2025-12-09 13:08:29.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0029128122150822266 Training loss: 9.311614990234375
2025-12-09 13:08:30.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0029124980021772344 Training loss: 8.98601245880127
2025-12-09 13:08:30.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0029121832411093443 Training loss: 8.641982078552246
2025-12-09 13:08:30.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0029118679320007087 Training loss: 9.410351753234863
2025-12-09 13:08:31.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0029115520749736935 Training loss: 9.004199981689453
2025-12-09 13:08:31.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0029112356701508756 Training loss: 9.362251281738281
2025-12-09 13:08:32.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0029109187176550463 Training loss: 9.10265064239502
2025-12-09 13:08:32.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0029106012176092085 Training loss: 9.758932113647461
2025-12-09 13:08:32.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0029102831701365785 Training loss: 9.256242752075195
2025-12-09 13:08:33.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0029099645753605827 Training loss: 8.891318321228027
2025-12-09 13:08:33.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.002909645433404863 Training loss: 9.28231430053711
2025-12-09 13:08:33.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0029093257443932713 Training loss: 9.26772689819336
2025-12-09 13:08:34.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0029090055084498734 Training loss: 8.99370288848877
2025-12-09 13:08:34.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0029086847256989457 Training loss: 8.965267181396484
2025-12-09 13:08:34.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0029083633962649785 Training loss: 9.048850059509277
2025-12-09 13:08:35.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0029080415202726727 Training loss: 9.026113510131836
2025-12-09 13:08:35.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0029077190978469432 Training loss: 9.78535270690918
2025-12-09 13:08:36.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0029073961291129153 Training loss: 8.918665885925293
2025-12-09 13:08:36.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0029070726141959265 Training loss: 9.205900192260742
2025-12-09 13:08:36.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.002906748553221527 Training loss: 9.092613220214844
2025-12-09 13:08:37.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0029064239463154782 Training loss: 9.622781753540039
2025-12-09 13:08:37.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.002906098793603754 Training loss: 9.207123756408691
2025-12-09 13:08:37.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.00290577309521254 Training loss: 9.32422161102295
2025-12-09 13:08:38.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.002905446851268233 Training loss: 9.367684364318848
2025-12-09 13:08:38.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.002905120061897442 Training loss: 8.993806838989258
2025-12-09 13:08:39.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.002904792727226987 Training loss: 8.896286964416504
2025-12-09 13:08:39.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.002904464847383902 Training loss: 9.8137788772583
2025-12-09 13:08:39.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.002904136422495429 Training loss: 9.072197914123535
2025-12-09 13:08:40.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0029038074526890243 Training loss: 8.761316299438477
2025-12-09 13:08:40.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.002903477938092354 Training loss: 8.845333099365234
2025-12-09 13:08:40.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.002903147878833296 Training loss: 9.285897254943848
2025-12-09 13:08:41.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.002902817275039941 Training loss: 9.096366882324219
2025-12-09 13:08:41.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0029024861268405894 Training loss: 8.862618446350098
2025-12-09 13:08:42.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0029021544343637525 Training loss: 9.097819328308105
2025-12-09 13:08:42.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0029018221977381554 Training loss: 8.891399383544922
2025-12-09 13:08:42.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0029014894170927307 Training loss: 8.945721626281738
2025-12-09 13:08:43.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0029011560925566253 Training loss: 9.075552940368652
2025-12-09 13:08:43.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.002900822224259196 Training loss: 8.920930862426758
2025-12-09 13:08:43.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0029004878123300095 Training loss: 9.224239349365234
2025-12-09 13:08:44.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0029001528568988457 Training loss: 9.021904945373535
2025-12-09 13:08:44.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.0028998173580956936 Training loss: 8.934834480285645
2025-12-09 13:08:45.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.002899481316050754 Training loss: 8.893678665161133
2025-12-09 13:08:45.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0028991447308944385 Training loss: 8.94655704498291
2025-12-09 13:08:45.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.002898807602757369 Training loss: 8.925084114074707
2025-12-09 13:08:46.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0028984699317703777 Training loss: 8.663744926452637
2025-12-09 13:08:46.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0028981317180645093 Training loss: 8.796300888061523
2025-12-09 13:08:46.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.002897792961771017 Training loss: 8.807611465454102
2025-12-09 13:08:47.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.002897453663021366 Training loss: 8.7274751663208
2025-12-09 13:08:47.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.002897113821947231 Training loss: 8.996378898620605
2025-12-09 13:08:48.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.0028967734386804982 Training loss: 8.735523223876953
2025-12-09 13:08:48.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.002896432513353264 Training loss: 8.657157897949219
2025-12-09 13:08:48.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.002896091046097834 Training loss: 8.836763381958008
2025-12-09 13:08:49.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0028957490370467255 Training loss: 8.694015502929688
2025-12-09 13:08:49.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0028954064863326652 Training loss: 8.624835968017578
2025-12-09 13:08:49.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.002895063394088591 Training loss: 8.85757064819336
2025-12-09 13:08:50.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0028947197604476493 Training loss: 9.354945182800293
2025-12-09 13:08:50.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.002894375585543199 Training loss: 8.961494445800781
2025-12-09 13:08:51.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0028940308695088062 Training loss: 8.846656799316406
2025-12-09 13:08:51.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.002893685612478249 Training loss: 8.852751731872559
2025-12-09 13:08:51.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0028933398145855158 Training loss: 8.335862159729004
2025-12-09 13:08:52.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0028929934759648022 Training loss: 8.739389419555664
2025-12-09 13:08:52.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0028926465967505175 Training loss: 8.662257194519043
2025-12-09 13:08:52.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.002892299177077277 Training loss: 8.875760078430176
2025-12-09 13:08:53.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0028919512170799085 Training loss: 8.533164978027344
2025-12-09 13:08:53.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0028916027168934483 Training loss: 8.694448471069336
2025-12-09 13:08:54.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0028912536766531423 Training loss: 8.569092750549316
2025-12-09 13:08:54.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.0028909040964944462 Training loss: 8.755568504333496
2025-12-09 13:08:54.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.002890553976553025 Training loss: 8.501291275024414
2025-12-09 13:08:55.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.002890203316964755 Training loss: 9.033110618591309
2025-12-09 13:08:55.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.002889852117865718 Training loss: 8.790454864501953
2025-12-09 13:08:55.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.002889500379392209 Training loss: 8.745816230773926
2025-12-09 13:08:56.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.0028891481016807305 Training loss: 8.79178524017334
2025-12-09 13:08:56.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.0028887952848679946 Training loss: 8.749866485595703
2025-12-09 13:08:56.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.002888441929090922 Training loss: 8.841022491455078
2025-12-09 13:08:57.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.002888088034486645 Training loss: 8.711356163024902
2025-12-09 13:08:57.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0028877336011925007 Training loss: 8.613302230834961
2025-12-09 13:08:58.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.00288737862934604 Training loss: 8.679868698120117
2025-12-09 13:08:58.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.002887023119085019 Training loss: 8.656620979309082
2025-12-09 13:08:58.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.002886667070547405 Training loss: 8.613757133483887
2025-12-09 13:08:59.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.002886310483871373 Training loss: 8.9315824508667
2025-12-09 13:08:59.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.0028859533591953077 Training loss: 8.53674030303955
2025-12-09 13:08:59.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.0028855956966578025 Training loss: 8.691472053527832
2025-12-09 13:09:00.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.002885237496397659 Training loss: 8.659598350524902
2025-12-09 13:09:00.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.0028848787585538872 Training loss: 8.629545211791992
2025-12-09 13:09:01.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.0028845194832657064 Training loss: 8.336341857910156
2025-12-09 13:09:01.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.002884159670672545 Training loss: 8.53593921661377
2025-12-09 13:09:01.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.002883799320914039 Training loss: 9.414990425109863
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 3.30 GiB is free. Including non-PyTorch memory, this process has 90.80 GiB memory in use. Of the allocated memory 89.27 GiB is allocated by PyTorch, and 792.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 50/10000 [00:00<00:19, 499.67it/s]Tokenizing texts:   1%|▏         | 136/10000 [00:00<00:15, 632.66it/s]Tokenizing texts:   2%|▏         | 204/10000 [00:00<00:15, 652.03it/s]Tokenizing texts:   3%|▎         | 269/10000 [00:00<00:14, 649.29it/s]Tokenizing texts:   3%|▎         | 334/10000 [00:00<00:15, 621.96it/s]Tokenizing texts:   4%|▍         | 397/10000 [00:00<00:15, 610.92it/s]Tokenizing texts:   5%|▍         | 459/10000 [00:00<00:16, 589.12it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:00<00:16, 579.81it/s]Tokenizing texts:   6%|▌         | 597/10000 [00:00<00:15, 603.94it/s]Tokenizing texts:   7%|▋         | 658/10000 [00:01<00:15, 596.79it/s]Tokenizing texts:   7%|▋         | 718/10000 [00:01<00:15, 580.91it/s]Tokenizing texts:   8%|▊         | 777/10000 [00:01<00:15, 581.56it/s]Tokenizing texts:   8%|▊         | 836/10000 [00:01<00:15, 577.60it/s]Tokenizing texts:   9%|▉         | 899/10000 [00:01<00:15, 590.93it/s]Tokenizing texts:  10%|▉         | 980/10000 [00:01<00:13, 654.84it/s]Tokenizing texts:  11%|█         | 1051/10000 [00:01<00:14, 636.79it/s]Tokenizing texts:  11%|█         | 1116/10000 [00:01<00:14, 634.36it/s]Tokenizing texts:  12%|█▏        | 1180/10000 [00:01<00:14, 600.31it/s]Tokenizing texts:  12%|█▏        | 1241/10000 [00:02<00:14, 593.11it/s]Tokenizing texts:  13%|█▎        | 1308/10000 [00:02<00:14, 613.18it/s]Tokenizing texts:  14%|█▎        | 1370/10000 [00:02<00:14, 585.48it/s]Tokenizing texts:  14%|█▍        | 1440/10000 [00:02<00:13, 615.68it/s]Tokenizing texts:  15%|█▌        | 1518/10000 [00:02<00:12, 652.46it/s]Tokenizing texts:  16%|█▌        | 1594/10000 [00:02<00:12, 682.37it/s]Tokenizing texts:  17%|█▋        | 1663/10000 [00:02<00:13, 615.21it/s]Tokenizing texts:  17%|█▋        | 1726/10000 [00:02<00:13, 613.51it/s]Tokenizing texts:  18%|█▊        | 1793/10000 [00:02<00:13, 628.57it/s]Tokenizing texts:  19%|█▊        | 1857/10000 [00:03<00:13, 610.22it/s]Tokenizing texts:  19%|█▉        | 1919/10000 [00:03<00:13, 609.09it/s]Tokenizing texts:  20%|█▉        | 1981/10000 [00:03<00:13, 577.19it/s]Tokenizing texts:  21%|██        | 2052/10000 [00:03<00:13, 610.97it/s]Tokenizing texts:  21%|██        | 2114/10000 [00:03<00:14, 559.11it/s]Tokenizing texts:  22%|██▏       | 2172/10000 [00:03<00:13, 559.60it/s]Tokenizing texts:  22%|██▏       | 2249/10000 [00:03<00:12, 616.13it/s]Tokenizing texts:  23%|██▎       | 2312/10000 [00:03<00:12, 598.77it/s]Tokenizing texts:  24%|██▍       | 2381/10000 [00:03<00:12, 623.57it/s]Tokenizing texts:  24%|██▍       | 2445/10000 [00:04<00:12, 627.76it/s]Tokenizing texts:  25%|██▌       | 2509/10000 [00:04<00:12, 624.15it/s]Tokenizing texts:  26%|██▌       | 2572/10000 [00:04<00:12, 611.51it/s]Tokenizing texts:  26%|██▋       | 2634/10000 [00:04<00:12, 573.76it/s]Tokenizing texts:  27%|██▋       | 2694/10000 [00:04<00:13, 553.11it/s]Tokenizing texts:  28%|██▊       | 2768/10000 [00:04<00:12, 601.89it/s]Tokenizing texts:  28%|██▊       | 2829/10000 [00:04<00:12, 556.55it/s]Tokenizing texts:  29%|██▉       | 2888/10000 [00:04<00:12, 562.66it/s]Tokenizing texts:  29%|██▉       | 2949/10000 [00:04<00:12, 575.63it/s]Tokenizing texts:  30%|███       | 3022/10000 [00:05<00:11, 616.76it/s]Tokenizing texts:  31%|███       | 3085/10000 [00:05<00:11, 600.06it/s]Tokenizing texts:  31%|███▏      | 3149/10000 [00:05<00:11, 611.21it/s]Tokenizing texts:  32%|███▏      | 3220/10000 [00:05<00:10, 638.82it/s]Tokenizing texts:  33%|███▎      | 3287/10000 [00:05<00:10, 643.23it/s]Tokenizing texts:  34%|███▎      | 3352/10000 [00:05<00:10, 639.71it/s]Tokenizing texts:  34%|███▍      | 3418/10000 [00:05<00:10, 644.60it/s]Tokenizing texts:  35%|███▍      | 3493/10000 [00:05<00:09, 669.88it/s]Tokenizing texts:  36%|███▌      | 3561/10000 [00:05<00:09, 651.20it/s]Tokenizing texts:  36%|███▋      | 3627/10000 [00:05<00:09, 647.51it/s]Tokenizing texts:  37%|███▋      | 3692/10000 [00:06<00:09, 639.27it/s]Tokenizing texts:  38%|███▊      | 3757/10000 [00:06<00:10, 611.22it/s]Tokenizing texts:  38%|███▊      | 3819/10000 [00:06<00:10, 591.23it/s]Tokenizing texts:  39%|███▉      | 3887/10000 [00:06<00:09, 613.02it/s]Tokenizing texts:  39%|███▉      | 3949/10000 [00:06<00:10, 592.37it/s]Tokenizing texts:  40%|████      | 4009/10000 [00:06<00:10, 588.84it/s]Tokenizing texts:  41%|████      | 4086/10000 [00:06<00:09, 640.07it/s]Tokenizing texts:  42%|████▏     | 4154/10000 [00:06<00:08, 649.84it/s]Tokenizing texts:  42%|████▏     | 4224/10000 [00:06<00:08, 655.89it/s]Tokenizing texts:  43%|████▎     | 4290/10000 [00:07<00:08, 636.42it/s]Tokenizing texts:  44%|████▎     | 4366/10000 [00:07<00:08, 669.49it/s]Tokenizing texts:  44%|████▍     | 4437/10000 [00:07<00:08, 679.32it/s]Tokenizing texts:  45%|████▌     | 4506/10000 [00:07<00:08, 641.04it/s]Tokenizing texts:  46%|████▌     | 4576/10000 [00:07<00:08, 652.06it/s]Tokenizing texts:  46%|████▋     | 4642/10000 [00:07<00:08, 648.92it/s]Tokenizing texts:  47%|████▋     | 4729/10000 [00:07<00:07, 708.82it/s]Tokenizing texts:  48%|████▊     | 4801/10000 [00:07<00:08, 644.83it/s]Tokenizing texts:  49%|████▉     | 4875/10000 [00:07<00:07, 669.10it/s]Tokenizing texts:  49%|████▉     | 4946/10000 [00:07<00:07, 677.25it/s]Tokenizing texts:  50%|█████     | 5024/10000 [00:08<00:07, 706.39it/s]Tokenizing texts:  51%|█████     | 5096/10000 [00:08<00:07, 648.56it/s]Tokenizing texts:  52%|█████▏    | 5163/10000 [00:08<00:07, 641.94it/s]Tokenizing texts:  52%|█████▏    | 5229/10000 [00:08<00:07, 630.10it/s]Tokenizing texts:  53%|█████▎    | 5295/10000 [00:08<00:07, 638.15it/s]Tokenizing texts:  54%|█████▍    | 5381/10000 [00:08<00:06, 697.94it/s]Tokenizing texts:  55%|█████▍    | 5452/10000 [00:08<00:06, 683.92it/s]Tokenizing texts:  55%|█████▌    | 5521/10000 [00:08<00:07, 622.23it/s]Tokenizing texts:  56%|█████▌    | 5591/10000 [00:08<00:06, 638.29it/s]Tokenizing texts:  57%|█████▋    | 5676/10000 [00:09<00:06, 694.72it/s]Tokenizing texts:  57%|█████▋    | 5747/10000 [00:09<00:06, 668.58it/s]Tokenizing texts:  58%|█████▊    | 5815/10000 [00:09<00:06, 605.24it/s]Tokenizing texts:  59%|█████▉    | 5885/10000 [00:09<00:06, 628.68it/s]Tokenizing texts:  60%|█████▉    | 5950/10000 [00:09<00:06, 614.28it/s]Tokenizing texts:  60%|██████    | 6013/10000 [00:09<00:06, 601.97it/s]Tokenizing texts:  61%|██████    | 6090/10000 [00:09<00:06, 642.79it/s]Tokenizing texts:  62%|██████▏   | 6160/10000 [00:09<00:05, 658.62it/s]Tokenizing texts:  62%|██████▏   | 6227/10000 [00:09<00:05, 640.22it/s]Tokenizing texts:  63%|██████▎   | 6292/10000 [00:10<00:05, 628.07it/s]Tokenizing texts:  64%|██████▎   | 6356/10000 [00:10<00:05, 613.24it/s]Tokenizing texts:  64%|██████▍   | 6423/10000 [00:10<00:05, 626.78it/s]Tokenizing texts:  65%|██████▍   | 6486/10000 [00:10<00:05, 623.71it/s]Tokenizing texts:  65%|██████▌   | 6549/10000 [00:10<00:05, 622.03it/s]Tokenizing texts:  66%|██████▌   | 6622/10000 [00:10<00:05, 653.31it/s]Tokenizing texts:  67%|██████▋   | 6688/10000 [00:10<00:05, 649.60it/s]Tokenizing texts:  68%|██████▊   | 6754/10000 [00:10<00:05, 625.31it/s]Tokenizing texts:  68%|██████▊   | 6839/10000 [00:10<00:04, 687.18it/s]Tokenizing texts:  69%|██████▉   | 6909/10000 [00:11<00:05, 607.07it/s]Tokenizing texts:  70%|██████▉   | 6977/10000 [00:11<00:04, 625.84it/s]Tokenizing texts:  70%|███████   | 7042/10000 [00:11<00:04, 630.82it/s]Tokenizing texts:  71%|███████   | 7107/10000 [00:11<00:05, 572.23it/s]Tokenizing texts:  72%|███████▏  | 7170/10000 [00:11<00:04, 583.10it/s]Tokenizing texts:  72%|███████▏  | 7240/10000 [00:11<00:04, 612.78it/s]Tokenizing texts:  73%|███████▎  | 7324/10000 [00:11<00:03, 676.46it/s]Tokenizing texts:  74%|███████▍  | 7393/10000 [00:11<00:04, 647.04it/s]Tokenizing texts:  75%|███████▍  | 7459/10000 [00:11<00:04, 578.54it/s]Tokenizing texts:  75%|███████▌  | 7537/10000 [00:12<00:03, 631.32it/s]Tokenizing texts:  76%|███████▌  | 7611/10000 [00:12<00:03, 656.05it/s]Tokenizing texts:  77%|███████▋  | 7679/10000 [00:12<00:03, 662.23it/s]Tokenizing texts:  77%|███████▋  | 7747/10000 [00:12<00:03, 644.58it/s]Tokenizing texts:  78%|███████▊  | 7813/10000 [00:12<00:03, 611.42it/s]Tokenizing texts:  79%|███████▉  | 7878/10000 [00:12<00:03, 620.98it/s]Tokenizing texts:  80%|███████▉  | 7955/10000 [00:12<00:03, 657.66it/s]Tokenizing texts:  80%|████████  | 8027/10000 [00:12<00:02, 671.49it/s]Tokenizing texts:  81%|████████  | 8095/10000 [00:12<00:03, 571.98it/s]Tokenizing texts:  82%|████████▏ | 8160/10000 [00:13<00:03, 587.74it/s]Tokenizing texts:  82%|████████▏ | 8224/10000 [00:13<00:02, 599.23it/s]Tokenizing texts:  83%|████████▎ | 8286/10000 [00:13<00:02, 581.27it/s]Tokenizing texts:  84%|████████▎ | 8353/10000 [00:13<00:02, 604.90it/s]Tokenizing texts:  84%|████████▍ | 8415/10000 [00:13<00:02, 586.67it/s]Tokenizing texts:  85%|████████▍ | 8487/10000 [00:13<00:02, 623.43it/s]Tokenizing texts:  86%|████████▌ | 8559/10000 [00:13<00:02, 648.70it/s]Tokenizing texts:  86%|████████▋ | 8626/10000 [00:13<00:02, 654.44it/s]Tokenizing texts:  87%|████████▋ | 8697/10000 [00:13<00:01, 669.98it/s]Tokenizing texts:  88%|████████▊ | 8773/10000 [00:14<00:01, 695.15it/s]Tokenizing texts:  88%|████████▊ | 8843/10000 [00:14<00:01, 596.73it/s]Tokenizing texts:  89%|████████▉ | 8922/10000 [00:14<00:01, 647.32it/s]Tokenizing texts:  90%|████████▉ | 8994/10000 [00:14<00:01, 666.00it/s]Tokenizing texts:  91%|█████████ | 9063/10000 [00:14<00:01, 622.07it/s]Tokenizing texts:  91%|█████████▏| 9132/10000 [00:14<00:01, 640.39it/s]Tokenizing texts:  92%|█████████▏| 9207/10000 [00:14<00:01, 670.65it/s]Tokenizing texts:  93%|█████████▎| 9276/10000 [00:14<00:01, 633.54it/s]Tokenizing texts:  93%|█████████▎| 9341/10000 [00:14<00:01, 622.01it/s]Tokenizing texts:  94%|█████████▍| 9405/10000 [00:15<00:00, 596.21it/s]Tokenizing texts:  95%|█████████▍| 9469/10000 [00:15<00:00, 608.01it/s]Tokenizing texts:  95%|█████████▌| 9531/10000 [00:15<00:00, 562.28it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:15<00:00, 577.37it/s]Tokenizing texts:  97%|█████████▋| 9655/10000 [00:15<00:00, 579.93it/s]Tokenizing texts:  97%|█████████▋| 9714/10000 [00:15<00:00, 572.71it/s]Tokenizing texts:  98%|█████████▊| 9772/10000 [00:15<00:00, 550.95it/s]Tokenizing texts:  98%|█████████▊| 9849/10000 [00:15<00:00, 609.01it/s]Tokenizing texts:  99%|█████████▉| 9922/10000 [00:15<00:00, 641.78it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:16<00:00, 680.81it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:16<00:00, 624.51it/s]
2025-12-09 13:09:50.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 12.228447914123535
2025-12-09 13:09:51.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 12.176467895507812
2025-12-09 13:09:51.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 12.1774263381958
2025-12-09 13:09:52.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 12.156624794006348
2025-12-09 13:09:52.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 12.167801856994629
2025-12-09 13:09:52.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 12.142793655395508
2025-12-09 13:09:53.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 12.175086975097656
2025-12-09 13:09:53.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 12.11697769165039
2025-12-09 13:09:53.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 12.092219352722168
2025-12-09 13:09:54.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 12.046205520629883
2025-12-09 13:09:54.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 12.010110855102539
2025-12-09 13:09:55.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 12.0075044631958
2025-12-09 13:09:55.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 11.949197769165039
2025-12-09 13:09:55.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 11.920598030090332
2025-12-09 13:09:56.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 11.821887016296387
2025-12-09 13:09:56.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 11.699545860290527
2025-12-09 13:09:56.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 11.586967468261719
2025-12-09 13:09:57.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 11.711678504943848
2025-12-09 13:09:57.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 11.222427368164062
2025-12-09 13:09:58.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 10.995390892028809
2025-12-09 13:09:58.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 10.998819351196289
2025-12-09 13:09:58.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 10.984935760498047
2025-12-09 13:09:59.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 10.695796012878418
2025-12-09 13:09:59.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 10.347335815429688
2025-12-09 13:09:59.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 10.543725967407227
2025-12-09 13:10:00.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 9.985557556152344
2025-12-09 13:10:00.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 10.173218727111816
2025-12-09 13:10:01.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 10.036910057067871
2025-12-09 13:10:01.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 10.219732284545898
2025-12-09 13:10:01.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 10.167370796203613
2025-12-09 13:10:02.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 10.089715003967285
2025-12-09 13:10:02.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 9.791729927062988
2025-12-09 13:10:02.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 9.889993667602539
2025-12-09 13:10:03.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 9.972036361694336
2025-12-09 13:10:03.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 9.931248664855957
2025-12-09 13:10:03.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 9.694828987121582
2025-12-09 13:10:04.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 10.17264461517334
2025-12-09 13:10:04.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 9.58043098449707
2025-12-09 13:10:05.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 9.535674095153809
2025-12-09 13:10:05.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 9.885869979858398
2025-12-09 13:10:05.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 9.510890007019043
2025-12-09 13:10:06.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 9.468329429626465
2025-12-09 13:10:06.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 9.5087890625
2025-12-09 13:10:06.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 9.756381034851074
2025-12-09 13:10:07.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 9.55583381652832
2025-12-09 13:10:07.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 9.342890739440918
2025-12-09 13:10:08.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 9.609959602355957
2025-12-09 13:10:08.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 9.512639999389648
2025-12-09 13:10:08.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 9.370044708251953
2025-12-09 13:10:09.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 9.261642456054688
2025-12-09 13:10:09.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 9.29152774810791
2025-12-09 13:10:09.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 9.238359451293945
2025-12-09 13:10:10.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 9.603260040283203
2025-12-09 13:10:10.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 9.123373031616211
2025-12-09 13:10:11.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 9.110001564025879
2025-12-09 13:10:11.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 9.065703392028809
2025-12-09 13:10:11.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 9.183055877685547
2025-12-09 13:10:12.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 8.948360443115234
2025-12-09 13:10:12.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 9.093331336975098
2025-12-09 13:10:12.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 9.348267555236816
2025-12-09 13:10:13.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 9.079964637756348
2025-12-09 13:10:13.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 8.850421905517578
2025-12-09 13:10:13.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 9.34522533416748
2025-12-09 13:10:14.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 9.421954154968262
2025-12-09 13:10:14.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 8.978281021118164
2025-12-09 13:10:15.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 9.184998512268066
2025-12-09 13:10:15.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 8.713006019592285
2025-12-09 13:10:15.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 8.984136581420898
2025-12-09 13:10:16.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 9.038206100463867
2025-12-09 13:10:16.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 8.82160472869873
2025-12-09 13:10:16.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 8.968396186828613
2025-12-09 13:10:17.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 8.887715339660645
2025-12-09 13:10:17.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 9.000792503356934
2025-12-09 13:10:18.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 9.109031677246094
2025-12-09 13:10:18.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 8.827005386352539
2025-12-09 13:10:18.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 9.024970054626465
2025-12-09 13:10:19.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 8.949843406677246
2025-12-09 13:10:19.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 8.879640579223633
2025-12-09 13:10:19.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 9.201669692993164
2025-12-09 13:10:20.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 8.896008491516113
2025-12-09 13:10:20.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 9.037449836730957
2025-12-09 13:10:21.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 8.99584674835205
2025-12-09 13:10:21.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 8.939580917358398
2025-12-09 13:10:21.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 8.752754211425781
2025-12-09 13:10:22.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 8.998023986816406
2025-12-09 13:10:22.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 8.947823524475098
2025-12-09 13:10:22.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 8.872363090515137
2025-12-09 13:10:23.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 8.757092475891113
2025-12-09 13:10:23.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 8.743398666381836
2025-12-09 13:10:24.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 8.983194351196289
2025-12-09 13:10:24.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 9.12671184539795
2025-12-09 13:10:24.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 8.842978477478027
2025-12-09 13:10:25.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 8.967425346374512
2025-12-09 13:10:25.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 8.840327262878418
2025-12-09 13:10:25.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 9.592591285705566
2025-12-09 13:10:26.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 8.747812271118164
2025-12-09 13:10:26.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 8.805548667907715
2025-12-09 13:10:26.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 8.785333633422852
2025-12-09 13:10:27.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 9.090217590332031
2025-12-09 13:10:27.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 8.78970718383789
2025-12-09 13:10:28.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999999029798808 Training loss: 8.864959716796875
2025-12-09 13:10:28.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00999999611919561 Training loss: 8.933235168457031
2025-12-09 13:10:28.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009999991268191535 Training loss: 9.165722846984863
2025-12-09 13:10:29.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009999984476788465 Training loss: 8.946503639221191
2025-12-09 13:10:29.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009999975744989035 Training loss: 8.744743347167969
2025-12-09 13:10:29.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009999965072796636 Training loss: 8.883211135864258
2025-12-09 13:10:30.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009999952460215409 Training loss: 9.31610107421875
2025-12-09 13:10:30.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.009999937907250246 Training loss: 8.952108383178711
2025-12-09 13:10:31.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.009999921413906798 Training loss: 9.366732597351074
2025-12-09 13:10:31.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.009999902980191464 Training loss: 8.982998847961426
2025-12-09 13:10:31.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009999882606111399 Training loss: 9.07873821258545
2025-12-09 13:10:32.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.009999860291674507 Training loss: 8.897778511047363
2025-12-09 13:10:32.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009999836036889453 Training loss: 8.767288208007812
2025-12-09 13:10:32.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009999809841765645 Training loss: 9.239409446716309
2025-12-09 13:10:33.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00999978170631325 Training loss: 8.942790031433105
2025-12-09 13:10:33.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009999751630543188 Training loss: 8.874521255493164
2025-12-09 13:10:34.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00999971961446713 Training loss: 9.67358684539795
2025-12-09 13:10:34.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.009999685658097501 Training loss: 8.95768928527832
2025-12-09 13:10:34.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.009999649761447477 Training loss: 9.15085506439209
2025-12-09 13:10:35.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009999611924530994 Training loss: 8.839241981506348
2025-12-09 13:10:35.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.00999957214736273 Training loss: 9.052416801452637
2025-12-09 13:10:35.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.009999530429958124 Training loss: 9.013389587402344
2025-12-09 13:10:36.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.009999486772333366 Training loss: 9.041997909545898
2025-12-09 13:10:36.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0099994411745054 Training loss: 8.89342212677002
2025-12-09 13:10:36.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.009999393636491919 Training loss: 8.801093101501465
2025-12-09 13:10:37.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00999934415831137 Training loss: 8.89303970336914
2025-12-09 13:10:37.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009999292739982958 Training loss: 9.284602165222168
2025-12-09 13:10:38.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009999239381526638 Training loss: 8.7337646484375
2025-12-09 13:10:38.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009999184082963117 Training loss: 9.143253326416016
2025-12-09 13:10:38.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009999126844313852 Training loss: 8.841092109680176
2025-12-09 13:10:39.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00999906766560106 Training loss: 9.010751724243164
2025-12-09 13:10:39.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009999006546847707 Training loss: 9.280981063842773
2025-12-09 13:10:39.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.009998943488077507 Training loss: 8.960319519042969
2025-12-09 13:10:40.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009998878489314937 Training loss: 8.864572525024414
2025-12-09 13:10:40.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009998811550585221 Training loss: 9.114290237426758
2025-12-09 13:10:41.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009998742671914335 Training loss: 8.988884925842285
2025-12-09 13:10:41.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.00999867185332901 Training loss: 9.259868621826172
2025-12-09 13:10:41.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00999859909485673 Training loss: 8.811807632446289
2025-12-09 13:10:42.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00999852439652573 Training loss: 9.15906810760498
2025-12-09 13:10:42.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009998447758365002 Training loss: 8.962761878967285
2025-12-09 13:10:42.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009998369180404282 Training loss: 9.06021785736084
2025-12-09 13:10:43.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00999828866267407 Training loss: 9.00865650177002
2025-12-09 13:10:43.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.00999820620520561 Training loss: 9.5128755569458
2025-12-09 13:10:44.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009998121808030905 Training loss: 9.110533714294434
2025-12-09 13:10:44.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.009998035471182706 Training loss: 9.063050270080566
2025-12-09 13:10:44.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00999794719469452 Training loss: 9.205594062805176
2025-12-09 13:10:45.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.009997856978600603 Training loss: 9.057448387145996
2025-12-09 13:10:45.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009997764822935967 Training loss: 8.992290496826172
2025-12-09 13:10:45.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.009997670727736378 Training loss: 9.141718864440918
2025-12-09 13:10:46.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009997574693038351 Training loss: 8.743653297424316
2025-12-09 13:10:46.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009997476718879152 Training loss: 9.168517112731934
2025-12-09 13:10:47.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009997376805296809 Training loss: 9.171615600585938
2025-12-09 13:10:47.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009997274952330094 Training loss: 9.00473403930664
2025-12-09 13:10:47.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00999717116001853 Training loss: 9.16842269897461
2025-12-09 13:10:48.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009997065428402403 Training loss: 9.255667686462402
2025-12-09 13:10:48.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009996957757522741 Training loss: 8.989107131958008
2025-12-09 13:10:48.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009996848147421333 Training loss: 9.139580726623535
2025-12-09 13:10:49.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009996736598140715 Training loss: 9.340619087219238
2025-12-09 13:10:49.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.009996623109724174 Training loss: 9.258831977844238
2025-12-09 13:10:49.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.009996507682215754 Training loss: 9.044055938720703
2025-12-09 13:10:50.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.009996390315660254 Training loss: 9.445371627807617
2025-12-09 13:10:50.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.009996271010103216 Training loss: 9.774422645568848
2025-12-09 13:10:51.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.009996149765590946 Training loss: 9.391633033752441
2025-12-09 13:10:51.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00999602658217049 Training loss: 9.312009811401367
2025-12-09 13:10:51.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.009995901459889657 Training loss: 9.049168586730957
2025-12-09 13:10:52.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.009995774398797007 Training loss: 9.215801239013672
2025-12-09 13:10:52.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.009995645398941846 Training loss: 9.19323444366455
2025-12-09 13:10:52.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.009995514460374237 Training loss: 9.162668228149414
2025-12-09 13:10:53.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.009995381583144995 Training loss: 9.451207160949707
2025-12-09 13:10:53.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.009995246767305689 Training loss: 9.276504516601562
2025-12-09 13:10:54.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.009995110012908634 Training loss: 9.505694389343262
2025-12-09 13:10:54.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.009994971320006905 Training loss: 9.154692649841309
2025-12-09 13:10:54.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.009994830688654326 Training loss: 9.506285667419434
2025-12-09 13:10:55.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.009994688118905472 Training loss: 9.5762357711792
2025-12-09 13:10:55.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.009994543610815672 Training loss: 9.271133422851562
2025-12-09 13:10:55.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.009994397164441006 Training loss: 9.10254955291748
2025-12-09 13:10:56.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.009994248779838311 Training loss: 9.129833221435547
2025-12-09 13:10:56.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.009994098457065167 Training loss: 9.20254135131836
2025-12-09 13:10:57.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.009993946196179913 Training loss: 9.147496223449707
2025-12-09 13:10:57.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.009993791997241638 Training loss: 9.381064414978027
2025-12-09 13:10:57.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.009993635860310187 Training loss: 9.416991233825684
2025-12-09 13:10:58.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00999347778544615 Training loss: 9.257408142089844
2025-12-09 13:10:58.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.009993317772710874 Training loss: 9.388641357421875
2025-12-09 13:10:58.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.009993155822166457 Training loss: 9.28558349609375
2025-12-09 13:10:59.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.009992991933875747 Training loss: 9.064892768859863
2025-12-09 13:10:59.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.009992826107902348 Training loss: 9.66897201538086
2025-12-09 13:10:59.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.009992658344310614 Training loss: 9.231343269348145
2025-12-09 13:11:00.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00999248864316565 Training loss: 9.425575256347656
2025-12-09 13:11:00.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.009992317004533314 Training loss: 9.230413436889648
2025-12-09 13:11:01.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.009992143428480213 Training loss: 9.229246139526367
2025-12-09 13:11:01.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.009991967915073714 Training loss: 9.465941429138184
2025-12-09 13:11:01.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.009991790464381926 Training loss: 9.250617980957031
2025-12-09 13:11:02.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.009991611076473714 Training loss: 9.207152366638184
2025-12-09 13:11:02.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.009991429751418698 Training loss: 9.343293190002441
2025-12-09 13:11:02.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.009991246489287245 Training loss: 9.202630043029785
2025-12-09 13:11:03.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.009991061290150474 Training loss: 9.248703956604004
2025-12-09 13:11:03.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.009990874154080258 Training loss: 9.102326393127441
2025-12-09 13:11:04.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.009990685081149222 Training loss: 9.319246292114258
2025-12-09 13:11:04.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.009990494071430742 Training loss: 9.206050872802734
2025-12-09 13:11:04.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.009990301124998944 Training loss: 9.317939758300781
2025-12-09 13:11:05.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.009990106241928705 Training loss: 9.32448673248291
2025-12-09 13:11:05.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.009989909422295658 Training loss: 9.136221885681152
2025-12-09 13:11:05.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.009989710666176184 Training loss: 9.349414825439453
2025-12-09 13:11:06.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.009989509973647417 Training loss: 9.48200798034668
2025-12-09 13:11:06.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.009989307344787242 Training loss: 9.279471397399902
2025-12-09 13:11:07.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.009989102779674294 Training loss: 9.203923225402832
2025-12-09 13:11:07.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00998889627838796 Training loss: 9.187878608703613
2025-12-09 13:11:07.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00998868784100838 Training loss: 9.710264205932617
2025-12-09 13:11:08.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.009988477467616446 Training loss: 8.979211807250977
2025-12-09 13:11:08.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0099882651582938 Training loss: 9.248551368713379
2025-12-09 13:11:08.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.009988050913122831 Training loss: 9.227609634399414
2025-12-09 13:11:09.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.009987834732186687 Training loss: 9.125322341918945
2025-12-09 13:11:09.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.009987616615569263 Training loss: 9.361985206604004
2025-12-09 13:11:10.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.009987396563355204 Training loss: 9.011301040649414
2025-12-09 13:11:10.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.009987174575629911 Training loss: 9.136917114257812
2025-12-09 13:11:10.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.009986950652479532 Training loss: 9.456768035888672
2025-12-09 13:11:11.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.009986724793990967 Training loss: 9.12902545928955
2025-12-09 13:11:11.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.009986497000251867 Training loss: 8.99203872680664
2025-12-09 13:11:11.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.009986267271350633 Training loss: 9.383419036865234
2025-12-09 13:11:12.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.00998603560737642 Training loss: 9.149850845336914
2025-12-09 13:11:12.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.009985802008419132 Training loss: 9.124327659606934
2025-12-09 13:11:13.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.009985566474569425 Training loss: 8.824821472167969
2025-12-09 13:11:13.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.009985329005918702 Training loss: 8.778244018554688
2025-12-09 13:11:13.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.009985089602559125 Training loss: 8.954407691955566
2025-12-09 13:11:14.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.009984848264583597 Training loss: 9.049784660339355
2025-12-09 13:11:14.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00998460499208578 Training loss: 9.6630220413208
2025-12-09 13:11:14.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00998435978516008 Training loss: 9.067483901977539
2025-12-09 13:11:15.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00998411264390166 Training loss: 8.973881721496582
2025-12-09 13:11:15.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.009983863568406429 Training loss: 8.93305492401123
2025-12-09 13:11:15.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.009983612558771048 Training loss: 8.682831764221191
2025-12-09 13:11:16.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00998335961509293 Training loss: 9.116209983825684
2025-12-09 13:11:16.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.009983104737470239 Training loss: 9.134395599365234
2025-12-09 13:11:17.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.009982847926001886 Training loss: 8.898098945617676
2025-12-09 13:11:17.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.009982589180787534 Training loss: 8.746655464172363
2025-12-09 13:11:17.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.009982328501927597 Training loss: 8.704415321350098
2025-12-09 13:11:18.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.009982065889523242 Training loss: 8.440322875976562
2025-12-09 13:11:18.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00998180134367638 Training loss: 8.803951263427734
2025-12-09 13:11:18.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.009981534864489678 Training loss: 8.493977546691895
2025-12-09 13:11:19.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.009981266452066553 Training loss: 8.945446014404297
2025-12-09 13:11:19.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.009980996106511169 Training loss: 8.737678527832031
2025-12-09 13:11:20.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.009980723827928441 Training loss: 8.885041236877441
2025-12-09 13:11:20.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.009980449616424037 Training loss: 9.442167282104492
2025-12-09 13:11:20.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00998017347210437 Training loss: 8.369009971618652
2025-12-09 13:11:21.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.009979895395076608 Training loss: 8.663230895996094
2025-12-09 13:11:21.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.009979615385448668 Training loss: 8.584721565246582
2025-12-09 13:11:21.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.009979333443329217 Training loss: 8.721433639526367
2025-12-09 13:11:22.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00997904956882767 Training loss: 8.675638198852539
2025-12-09 13:11:22.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.009978763762054192 Training loss: 8.830130577087402
2025-12-09 13:11:23.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0099784760231197 Training loss: 8.295258522033691
2025-12-09 13:11:23.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.00997818635213586 Training loss: 8.78605842590332
2025-12-09 13:11:23.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.009977894749215089 Training loss: 8.452340126037598
2025-12-09 13:11:24.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00997760121447055 Training loss: 8.705872535705566
2025-12-09 13:11:24.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.009977305748016158 Training loss: 8.484517097473145
2025-12-09 13:11:24.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00997700834996658 Training loss: 8.429856300354004
2025-12-09 13:11:25.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.009976709020437229 Training loss: 8.325092315673828
2025-12-09 13:11:25.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.00997640775954427 Training loss: 8.702852249145508
2025-12-09 13:11:26.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.009976104567404616 Training loss: 8.586064338684082
2025-12-09 13:11:26.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.009975799444135928 Training loss: 8.497361183166504
2025-12-09 13:11:26.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.009975492389856622 Training loss: 8.758792877197266
2025-12-09 13:11:27.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.009975183404685856 Training loss: 8.998326301574707
2025-12-09 13:11:27.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.009974872488743543 Training loss: 8.773110389709473
2025-12-09 13:11:27.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.009974559642150344 Training loss: 8.616877555847168
2025-12-09 13:11:28.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.009974244865027668 Training loss: 8.608402252197266
2025-12-09 13:11:28.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.009973928157497673 Training loss: 8.34932804107666
2025-12-09 13:11:29.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.009973609519683268 Training loss: 8.427875518798828
2025-12-09 13:11:29.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.009973288951708112 Training loss: 8.481014251708984
2025-12-09 13:11:29.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.009972966453696608 Training loss: 8.842296600341797
2025-12-09 13:11:30.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.009972642025773911 Training loss: 8.459600448608398
2025-12-09 13:11:30.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.009972315668065928 Training loss: 8.44632625579834
2025-12-09 13:11:30.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00997198738069931 Training loss: 8.587074279785156
2025-12-09 13:11:31.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.009971657163801459 Training loss: 8.660039901733398
2025-12-09 13:11:31.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.009971325017500525 Training loss: 8.451679229736328
2025-12-09 13:11:32.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00997099094192541 Training loss: 8.848788261413574
2025-12-09 13:11:32.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.009970654937205762 Training loss: 8.409294128417969
2025-12-09 13:11:32.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.009970317003471976 Training loss: 9.017101287841797
2025-12-09 13:11:33.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.009969977140855197 Training loss: 9.010394096374512
2025-12-09 13:11:33.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.009969635349487322 Training loss: 9.133064270019531
2025-12-09 13:11:33.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.009969291629500991 Training loss: 8.728806495666504
2025-12-09 13:11:34.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.009968945981029596 Training loss: 8.698768615722656
2025-12-09 13:11:34.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.009968598404207276 Training loss: 8.345735549926758
2025-12-09 13:11:34.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.009968248899168919 Training loss: 8.208115577697754
2025-12-09 13:11:35.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.00996789746605016 Training loss: 8.694747924804688
2025-12-09 13:11:35.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.009967544104987387 Training loss: 9.647924423217773
2025-12-09 13:11:36.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.009967188816117727 Training loss: 8.48486614227295
2025-12-09 13:11:36.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.009966831599579066 Training loss: 8.853681564331055
2025-12-09 13:11:36.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00996647245551003 Training loss: 8.407713890075684
2025-12-09 13:11:37.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.009966111384049996 Training loss: 8.982681274414062
2025-12-09 13:11:37.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.009965748385339089 Training loss: 8.454930305480957
2025-12-09 13:11:37.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.00996538345951818 Training loss: 8.541191101074219
2025-12-09 13:11:38.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.009965016606728895 Training loss: 8.238876342773438
2025-12-09 13:11:38.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.009964647827113595 Training loss: 9.06275463104248
2025-12-09 13:11:39.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.009964277120815402 Training loss: 8.447680473327637
2025-12-09 13:11:39.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.009963904487978178 Training loss: 8.189151763916016
2025-12-09 13:11:39.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.009963529928746533 Training loss: 8.35023021697998
2025-12-09 13:11:40.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.009963153443265827 Training loss: 8.363040924072266
2025-12-09 13:11:40.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.00996277503168217 Training loss: 8.640978813171387
2025-12-09 13:11:40.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00996239469414241 Training loss: 8.97992992401123
2025-12-09 13:11:41.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.009962012430794153 Training loss: 8.370939254760742
2025-12-09 13:11:41.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.009961628241785746 Training loss: 8.306798934936523
2025-12-09 13:11:42.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.009961242127266288 Training loss: 8.52563190460205
2025-12-09 13:11:42.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.009960854087385618 Training loss: 8.516481399536133
2025-12-09 13:11:42.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.00996046412229433 Training loss: 8.028878211975098
2025-12-09 13:11:43.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.009960072232143761 Training loss: 8.248695373535156
2025-12-09 13:11:43.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.009959678417085997 Training loss: 8.775650024414062
2025-12-09 13:11:43.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.009959282677273869 Training loss: 8.54509162902832
2025-12-09 13:11:44.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.009958885012860954 Training loss: 8.414705276489258
2025-12-09 13:11:44.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.009958485424001582 Training loss: 8.313929557800293
2025-12-09 13:11:45.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.009958083910850821 Training loss: 8.201604843139648
2025-12-09 13:11:45.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.009957680473564495 Training loss: 8.642755508422852
2025-12-09 13:11:45.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.009957275112299165 Training loss: 8.306745529174805
2025-12-09 13:11:46.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.009956867827212149 Training loss: 8.59276008605957
2025-12-09 13:11:46.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.009956458618461502 Training loss: 8.618143081665039
2025-12-09 13:11:46.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.009956047486206033 Training loss: 8.472160339355469
2025-12-09 13:11:47.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.00995563443060529 Training loss: 8.40075969696045
2025-12-09 13:11:47.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00995521945181958 Training loss: 8.270065307617188
2025-12-09 13:11:48.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00995480255000994 Training loss: 8.5399169921875
2025-12-09 13:11:48.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.009954383725338167 Training loss: 8.221206665039062
2025-12-09 13:11:48.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.009953962977966795 Training loss: 8.298680305480957
2025-12-09 13:11:49.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00995354030805911 Training loss: 8.584012985229492
2025-12-09 13:11:49.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.009953115715779141 Training loss: 7.982574462890625
2025-12-09 13:11:49.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.009952689201291663 Training loss: 8.419824600219727
2025-12-09 13:11:50.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0099522607647622 Training loss: 8.394527435302734
2025-12-09 13:11:50.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00995183040635702 Training loss: 8.649023056030273
2025-12-09 13:11:51.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.009951398126243134 Training loss: 8.00965690612793
2025-12-09 13:11:51.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.009950963924588304 Training loss: 8.119402885437012
2025-12-09 13:11:51.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.009950527801561034 Training loss: 8.327729225158691
2025-12-09 13:11:52.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.009950089757330574 Training loss: 8.099409103393555
2025-12-09 13:11:52.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.009949649792066922 Training loss: 8.256771087646484
2025-12-09 13:11:52.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00994920790594082 Training loss: 8.579849243164062
2025-12-09 13:11:53.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.009948764099123755 Training loss: 8.185922622680664
2025-12-09 13:11:53.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.00994831837178796 Training loss: 8.218714714050293
2025-12-09 13:11:53.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.009947870724106411 Training loss: 8.206968307495117
2025-12-09 13:11:54.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.009947421156252837 Training loss: 8.361903190612793
2025-12-09 13:11:54.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.009946969668401697 Training loss: 8.283296585083008
2025-12-09 13:11:55.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.009946516260728214 Training loss: 8.406719207763672
2025-12-09 13:11:55.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.009946060933408342 Training loss: 8.516447067260742
2025-12-09 13:11:55.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.009945603686618785 Training loss: 8.441115379333496
2025-12-09 13:11:56.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.009945144520536991 Training loss: 8.477124214172363
2025-12-09 13:11:56.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.009944683435341155 Training loss: 8.526995658874512
2025-12-09 13:11:56.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.009944220431210215 Training loss: 7.94723653793335
2025-12-09 13:11:57.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.009943755508323854 Training loss: 8.161864280700684
2025-12-09 13:11:57.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.009943288666862497 Training loss: 8.378137588500977
2025-12-09 13:11:58.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.00994281990700732 Training loss: 8.186347007751465
2025-12-09 13:11:58.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.009942349228940238 Training loss: 8.155170440673828
2025-12-09 13:11:58.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00994187663284391 Training loss: 8.494901657104492
2025-12-09 13:11:59.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.009941402118901743 Training loss: 8.175789833068848
2025-12-09 13:11:59.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.009940925687297887 Training loss: 8.283684730529785
2025-12-09 13:11:59.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.009940447338217234 Training loss: 8.555160522460938
2025-12-09 13:12:00.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.009939967071845425 Training loss: 8.6202974319458
2025-12-09 13:12:00.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.009939484888368837 Training loss: 8.716303825378418
2025-12-09 13:12:01.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.009939000787974602 Training loss: 8.574373245239258
2025-12-09 13:12:01.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.009938514770850585 Training loss: 8.555651664733887
2025-12-09 13:12:01.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.009938026837185403 Training loss: 8.262391090393066
2025-12-09 13:12:02.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.009937536987168413 Training loss: 8.800002098083496
2025-12-09 13:12:02.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.009937045220989716 Training loss: 8.476533889770508
2025-12-09 13:12:02.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.009936551538840153 Training loss: 9.132368087768555
2025-12-09 13:12:03.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.009936055940911319 Training loss: 8.66287899017334
2025-12-09 13:12:03.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.009935558427395541 Training loss: 9.182419776916504
2025-12-09 13:12:04.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.009935058998485898 Training loss: 9.118325233459473
2025-12-09 13:12:04.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.009934557654376204 Training loss: 9.24512767791748
2025-12-09 13:12:04.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.009934054395261025 Training loss: 9.281805992126465
2025-12-09 13:12:05.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.009933549221335665 Training loss: 8.727169036865234
2025-12-09 13:12:05.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.009933042132796171 Training loss: 8.75688362121582
2025-12-09 13:12:05.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.009932533129839334 Training loss: 8.12627124786377
2025-12-09 13:12:06.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00993202221266269 Training loss: 10.00216293334961
2025-12-09 13:12:06.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.009931509381464514 Training loss: 9.192999839782715
2025-12-09 13:12:07.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.009930994636443828 Training loss: 9.576756477355957
2025-12-09 13:12:07.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.009930477977800391 Training loss: 11.195716857910156
2025-12-09 13:12:07.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.009929959405734712 Training loss: 10.858930587768555
2025-12-09 13:12:08.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.009929438920448038 Training loss: 10.692230224609375
2025-12-09 13:12:08.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.009928916522142357 Training loss: 11.614816665649414
2025-12-09 13:12:08.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0099283922110204 Training loss: 11.399797439575195
2025-12-09 13:12:09.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.009927865987285648 Training loss: 10.950042724609375
2025-12-09 13:12:09.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.009927337851142314 Training loss: 10.989275932312012
2025-12-09 13:12:10.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.009926807802795359 Training loss: 10.476880073547363
2025-12-09 13:12:10.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.009926275842450481 Training loss: 11.036757469177246
2025-12-09 13:12:10.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.009925741970314128 Training loss: 10.912335395812988
2025-12-09 13:12:11.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.009925206186593483 Training loss: 10.780916213989258
2025-12-09 13:12:11.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.009924668491496473 Training loss: 10.757296562194824
2025-12-09 13:12:11.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.009924128885231769 Training loss: 10.55020523071289
2025-12-09 13:12:12.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.009923587368008779 Training loss: 10.776464462280273
2025-12-09 13:12:12.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.009923043940037657 Training loss: 10.594496726989746
2025-12-09 13:12:12.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.009922498601529295 Training loss: 10.425519943237305
2025-12-09 13:12:13.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.00992195135269533 Training loss: 10.498388290405273
2025-12-09 13:12:13.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.009921402193748138 Training loss: 9.945602416992188
2025-12-09 13:12:14.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.009920851124900838 Training loss: 10.18868350982666
2025-12-09 13:12:14.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.009920298146367286 Training loss: 10.269847869873047
2025-12-09 13:12:14.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.009919743258362085 Training loss: 10.082464218139648
2025-12-09 13:12:15.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.009919186461100576 Training loss: 9.994592666625977
2025-12-09 13:12:15.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.009918627754798839 Training loss: 10.0178804397583
2025-12-09 13:12:15.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0099180671396737 Training loss: 9.868131637573242
2025-12-09 13:12:16.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.009917504615942721 Training loss: 9.955007553100586
2025-12-09 13:12:16.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.009916940183824205 Training loss: 9.844023704528809
2025-12-09 13:12:17.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.009916373843537201 Training loss: 10.276814460754395
2025-12-09 13:12:17.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.009915805595301492 Training loss: 9.689563751220703
2025-12-09 13:12:17.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.009915235439337602 Training loss: 9.812110900878906
2025-12-09 13:12:18.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.009914663375866804 Training loss: 9.785568237304688
2025-12-09 13:12:18.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.009914089405111097 Training loss: 9.596806526184082
2025-12-09 13:12:18.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.009913513527293234 Training loss: 9.283040046691895
2025-12-09 13:12:19.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.009912935742636698 Training loss: 9.50669002532959
2025-12-09 13:12:19.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.009912356051365718 Training loss: 9.451751708984375
2025-12-09 13:12:20.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.009911774453705257 Training loss: 9.268180847167969
2025-12-09 13:12:20.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00991119094988103 Training loss: 9.408090591430664
2025-12-09 13:12:20.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.009910605540119475 Training loss: 9.336223602294922
2025-12-09 13:12:21.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.009910018224647781 Training loss: 9.123523712158203
2025-12-09 13:12:21.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.009909429003693876 Training loss: 8.598420143127441
2025-12-09 13:12:21.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.009908837877486422 Training loss: 9.36140251159668
2025-12-09 13:12:22.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.009908244846254825 Training loss: 8.832862854003906
2025-12-09 13:12:22.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.009907649910229228 Training loss: 9.283405303955078
2025-12-09 13:12:22.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.009907053069640516 Training loss: 9.40083122253418
2025-12-09 13:12:23.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.009906454324720308 Training loss: 8.802804946899414
2025-12-09 13:12:23.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.009905853675700968 Training loss: 9.267644882202148
2025-12-09 13:12:24.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.009905251122815597 Training loss: 9.101616859436035
2025-12-09 13:12:24.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00990464666629803 Training loss: 8.975057601928711
2025-12-09 13:12:24.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.009904040306382847 Training loss: 11.228934288024902
2025-12-09 13:12:25.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.009903432043305365 Training loss: 9.983576774597168
2025-12-09 13:12:25.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.009902821877301638 Training loss: 10.314884185791016
2025-12-09 13:12:25.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00990220980860846 Training loss: 10.551383972167969
2025-12-09 13:12:26.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.009901595837463363 Training loss: 10.889707565307617
2025-12-09 13:12:26.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.009900979964104618 Training loss: 10.912153244018555
2025-12-09 13:12:27.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00990036218877123 Training loss: 10.87401294708252
2025-12-09 13:12:27.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.00989974251170295 Training loss: 10.780677795410156
2025-12-09 13:12:27.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00989912093314026 Training loss: 10.697880744934082
2025-12-09 13:12:28.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.009898497453324384 Training loss: 10.30872631072998
2025-12-09 13:12:28.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.009897872072497281 Training loss: 10.212142944335938
2025-12-09 13:12:28.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.00989724479090165 Training loss: 9.743542671203613
2025-12-09 13:12:29.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.009896615608780924 Training loss: 9.550501823425293
2025-12-09 13:12:29.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.00989598452637928 Training loss: 9.993632316589355
2025-12-09 13:12:30.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.009895351543941628 Training loss: 9.342411041259766
2025-12-09 13:12:30.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.009894716661713616 Training loss: 9.236868858337402
2025-12-09 13:12:30.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.009894079879941628 Training loss: 9.467195510864258
2025-12-09 13:12:31.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.009893441198872787 Training loss: 9.679627418518066
2025-12-09 13:12:31.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.009892800618754954 Training loss: 9.244816780090332
2025-12-09 13:12:31.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.009892158139836724 Training loss: 9.490766525268555
2025-12-09 13:12:32.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.00989151376236743 Training loss: 9.398930549621582
2025-12-09 13:12:32.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.009890867486597146 Training loss: 9.399369239807129
2025-12-09 13:12:32.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.009890219312776677 Training loss: 9.16834831237793
2025-12-09 13:12:33.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.009889569241157564 Training loss: 9.232344627380371
2025-12-09 13:12:33.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.00988891727199209 Training loss: 8.998865127563477
2025-12-09 13:12:34.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.009888263405533271 Training loss: 9.365270614624023
2025-12-09 13:12:34.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.009887607642034859 Training loss: 8.908317565917969
2025-12-09 13:12:34.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.009886949981751346 Training loss: 9.354024887084961
2025-12-09 13:12:35.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.009886290424937952 Training loss: 8.967437744140625
2025-12-09 13:12:35.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.009885628971850642 Training loss: 8.285024642944336
2025-12-09 13:12:35.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.009884965622746112 Training loss: 9.672759056091309
2025-12-09 13:12:36.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.009884300377881794 Training loss: 9.889303207397461
2025-12-09 13:12:36.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.009883633237515857 Training loss: 9.03799819946289
2025-12-09 13:12:37.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.009882964201907207 Training loss: 8.991358757019043
2025-12-09 13:12:37.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.00988229327131548 Training loss: 9.509421348571777
2025-12-09 13:12:37.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.009881620446001056 Training loss: 9.693193435668945
2025-12-09 13:12:38.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.00988094572622504 Training loss: 9.631263732910156
2025-12-09 13:12:38.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.00988026911224928 Training loss: 9.652572631835938
2025-12-09 13:12:38.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.00987959060433636 Training loss: 9.507336616516113
2025-12-09 13:12:39.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.009878910202749589 Training loss: 9.509023666381836
2025-12-09 13:12:39.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.009878227907753022 Training loss: 9.564977645874023
2025-12-09 13:12:40.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.009877543719611444 Training loss: 9.412973403930664
2025-12-09 13:12:40.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.009876857638590373 Training loss: 9.130678176879883
2025-12-09 13:12:40.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.009876169664956067 Training loss: 9.737207412719727
2025-12-09 13:12:41.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.009875479798975512 Training loss: 9.621660232543945
2025-12-09 13:12:41.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.009874788040916432 Training loss: 9.181953430175781
2025-12-09 13:12:41.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.009874094391047288 Training loss: 9.128803253173828
2025-12-09 13:12:42.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.009873398849637267 Training loss: 8.855826377868652
2025-12-09 13:12:42.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.009872701416956299 Training loss: 9.447500228881836
2025-12-09 13:12:43.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.009872002093275042 Training loss: 8.828648567199707
2025-12-09 13:12:43.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.00987130087886489 Training loss: 9.031810760498047
2025-12-09 13:12:43.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.009870597773997972 Training loss: 8.869590759277344
2025-12-09 13:12:44.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.009869892778947148 Training loss: 9.020797729492188
2025-12-09 13:12:44.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.009869185893986013 Training loss: 8.832709312438965
2025-12-09 13:12:44.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.009868477119388895 Training loss: 8.898728370666504
2025-12-09 13:12:45.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.009867766455430856 Training loss: 8.907358169555664
2025-12-09 13:12:45.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.009867053902387693 Training loss: 8.947744369506836
2025-12-09 13:12:45.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.009866339460535929 Training loss: 8.623112678527832
2025-12-09 13:12:46.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.009865623130152828 Training loss: 8.9430570602417
2025-12-09 13:12:46.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.009864904911516384 Training loss: 8.807302474975586
2025-12-09 13:12:47.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.009864184804905323 Training loss: 8.637237548828125
2025-12-09 13:12:47.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.009863462810599103 Training loss: 8.855793952941895
2025-12-09 13:12:47.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.009862738928877922 Training loss: 8.75036334991455
2025-12-09 13:12:48.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.009862013160022696 Training loss: 8.907853126525879
2025-12-09 13:12:48.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.009861285504315085 Training loss: 8.478752136230469
2025-12-09 13:12:48.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00986055596203748 Training loss: 8.840555191040039
2025-12-09 13:12:49.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.009859824533472998 Training loss: 8.79007625579834
2025-12-09 13:12:49.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.009859091218905498 Training loss: 8.997928619384766
2025-12-09 13:12:50.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.00985835601861956 Training loss: 8.578655242919922
2025-12-09 13:12:50.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.009857618932900504 Training loss: 8.647725105285645
2025-12-09 13:12:50.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.009856879962034375 Training loss: 8.811426162719727
2025-12-09 13:12:51.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.009856139106307955 Training loss: 8.84260082244873
2025-12-09 13:12:51.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.009855396366008757 Training loss: 8.546354293823242
2025-12-09 13:12:51.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.009854651741425023 Training loss: 8.650622367858887
2025-12-09 13:12:52.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.009853905232845728 Training loss: 8.44255542755127
2025-12-09 13:12:52.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.009853156840560576 Training loss: 8.5891752243042
2025-12-09 13:12:53.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.009852406564860004 Training loss: 9.106389045715332
2025-12-09 13:12:53.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.009851654406035179 Training loss: 8.60027027130127
2025-12-09 13:12:53.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.009850900364378 Training loss: 8.739046096801758
2025-12-09 13:12:54.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.009850144440181096 Training loss: 8.79409122467041
2025-12-09 13:12:54.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.009849386633737824 Training loss: 8.706872940063477
2025-12-09 13:12:54.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.009848626945342278 Training loss: 8.616816520690918
2025-12-09 13:12:55.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.009847865375289276 Training loss: 8.915752410888672
2025-12-09 13:12:55.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.009847101923874366 Training loss: 9.689776420593262
2025-12-09 13:12:55.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.009846336591393832 Training loss: 8.642672538757324
2025-12-09 13:12:56.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.009845569378144686 Training loss: 8.382413864135742
2025-12-09 13:12:56.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.009844800284424663 Training loss: 8.594144821166992
2025-12-09 13:12:57.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.00984402931053224 Training loss: 8.912498474121094
2025-12-09 13:12:57.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.009843256456766609 Training loss: 9.035158157348633
2025-12-09 13:12:57.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.009842481723427705 Training loss: 8.373581886291504
2025-12-09 13:12:58.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.009841705110816185 Training loss: 8.484698295593262
2025-12-09 13:12:58.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00984092661923344 Training loss: 8.265260696411133
2025-12-09 13:12:58.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.009840146248981585 Training loss: 8.380967140197754
2025-12-09 13:12:59.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.009839364000363466 Training loss: 8.261665344238281
2025-12-09 13:12:59.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00983857987368266 Training loss: 8.309547424316406
2025-12-09 13:13:00.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.009837793869243468 Training loss: 8.784241676330566
2025-12-09 13:13:00.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.009837005987350926 Training loss: 8.65254020690918
2025-12-09 13:13:00.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.009836216228310797 Training loss: 8.490094184875488
2025-12-09 13:13:01.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.009835424592429568 Training loss: 8.446672439575195
2025-12-09 13:13:01.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.009834631080014457 Training loss: 8.399996757507324
2025-12-09 13:13:01.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.009833835691373412 Training loss: 8.289497375488281
2025-12-09 13:13:02.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.00983303842681511 Training loss: 8.276774406433105
2025-12-09 13:13:02.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.009832239286648949 Training loss: 8.429899215698242
2025-12-09 13:13:03.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.009831438271185065 Training loss: 8.970438957214355
2025-12-09 13:13:03.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.009830635380734313 Training loss: 8.503022193908691
2025-12-09 13:13:03.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.009829830615608279 Training loss: 8.485600471496582
2025-12-09 13:13:04.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.009829023976119278 Training loss: 8.739173889160156
2025-12-09 13:13:04.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.009828215462580352 Training loss: 8.26645278930664
2025-12-09 13:13:04.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.009827405075305266 Training loss: 8.179166793823242
2025-12-09 13:13:05.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.009826592814608518 Training loss: 8.674980163574219
2025-12-09 13:13:05.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.00982577868080533 Training loss: 8.457673072814941
2025-12-09 13:13:06.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.009824962674211653 Training loss: 8.31863784790039
2025-12-09 13:13:06.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.009824144795144159 Training loss: 8.571975708007812
2025-12-09 13:13:06.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.009823325043920255 Training loss: 8.27828311920166
2025-12-09 13:13:07.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.009822503420858067 Training loss: 8.373564720153809
2025-12-09 13:13:07.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.009821679926276456 Training loss: 8.389744758605957
2025-12-09 13:13:07.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.009820854560494998 Training loss: 8.313326835632324
2025-12-09 13:13:08.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.009820027323834007 Training loss: 8.258461952209473
2025-12-09 13:13:08.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.009819198216614512 Training loss: 8.569479942321777
2025-12-09 13:13:09.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.009818367239158278 Training loss: 8.24058723449707
2025-12-09 13:13:09.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.009817534391787787 Training loss: 8.935025215148926
2025-12-09 13:13:09.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.009816699674826256 Training loss: 8.44576358795166
2025-12-09 13:13:10.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.009815863088597618 Training loss: 8.018842697143555
2025-12-09 13:13:10.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.009815024633426537 Training loss: 8.517646789550781
2025-12-09 13:13:10.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0098141843096384 Training loss: 8.480903625488281
2025-12-09 13:13:11.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.009813342117559323 Training loss: 8.189178466796875
2025-12-09 13:13:11.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.009812498057516142 Training loss: 8.570281982421875
2025-12-09 13:13:12.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.009811652129836422 Training loss: 8.39298152923584
2025-12-09 13:13:12.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.009810804334848449 Training loss: 8.311617851257324
2025-12-09 13:13:12.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.009809954672881238 Training loss: 8.253241539001465
2025-12-09 13:13:13.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.009809103144264524 Training loss: 8.370970726013184
2025-12-09 13:13:13.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.009808249749328769 Training loss: 8.185665130615234
2025-12-09 13:13:13.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.009807394488405159 Training loss: 8.664266586303711
2025-12-09 13:13:14.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.009806537361825607 Training loss: 8.232905387878418
2025-12-09 13:13:14.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.009805678369922742 Training loss: 8.133962631225586
2025-12-09 13:13:14.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.009804817513029926 Training loss: 8.71971321105957
2025-12-09 13:13:15.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.009803954791481238 Training loss: 8.615044593811035
2025-12-09 13:13:15.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.009803090205611487 Training loss: 8.437287330627441
2025-12-09 13:13:16.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.009802223755756198 Training loss: 8.532258987426758
2025-12-09 13:13:16.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.009801355442251625 Training loss: 8.611583709716797
2025-12-09 13:13:16.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.009800485265434745 Training loss: 8.335166931152344
2025-12-09 13:13:17.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.009799613225643253 Training loss: 8.095807075500488
2025-12-09 13:13:17.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.009798739323215573 Training loss: 8.46304702758789
2025-12-09 13:13:17.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00979786355849085 Training loss: 8.91032886505127
2025-12-09 13:13:18.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.009796985931808949 Training loss: 8.512331008911133
2025-12-09 13:13:18.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.009796106443510462 Training loss: 8.331439018249512
2025-12-09 13:13:19.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.009795225093936702 Training loss: 8.08790111541748
2025-12-09 13:13:19.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.009794341883429699 Training loss: 8.892387390136719
2025-12-09 13:13:19.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.009793456812332214 Training loss: 8.35748291015625
2025-12-09 13:13:20.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.009792569880987725 Training loss: 9.566664695739746
2025-12-09 13:13:20.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.009791681089740432 Training loss: 8.459173202514648
2025-12-09 13:13:20.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.009790790438935257 Training loss: 8.368313789367676
2025-12-09 13:13:21.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.009789897928917846 Training loss: 8.102931022644043
2025-12-09 13:13:21.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.009789003560034561 Training loss: 8.804061889648438
2025-12-09 13:13:22.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.009788107332632493 Training loss: 8.671090126037598
2025-12-09 13:13:22.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.009787209247059453 Training loss: 8.699127197265625
2025-12-09 13:13:22.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.009786309303663962 Training loss: 8.703975677490234
2025-12-09 13:13:23.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.009785407502795277 Training loss: 8.796463012695312
2025-12-09 13:13:23.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.009784503844803368 Training loss: 9.036885261535645
2025-12-09 13:13:23.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.009783598330038924 Training loss: 9.253085136413574
2025-12-09 13:13:24.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.009782690958853361 Training loss: 8.857900619506836
2025-12-09 13:13:24.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.009781781731598813 Training loss: 8.981711387634277
2025-12-09 13:13:25.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.00978087064862813 Training loss: 9.062100410461426
2025-12-09 13:13:25.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.009779957710294886 Training loss: 8.562967300415039
2025-12-09 13:13:25.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.009779042916953376 Training loss: 8.822884559631348
2025-12-09 13:13:26.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.009778126268958612 Training loss: 8.801102638244629
2025-12-09 13:13:26.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.009777207766666329 Training loss: 8.132596015930176
2025-12-09 13:13:26.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.00977628741043298 Training loss: 8.514755249023438
2025-12-09 13:13:27.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.009775365200615735 Training loss: 8.195518493652344
2025-12-09 13:13:27.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.009774441137572488 Training loss: 8.347167015075684
2025-12-09 13:13:28.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.009773515221661847 Training loss: 8.142433166503906
2025-12-09 13:13:28.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.009772587453243142 Training loss: 8.586560249328613
2025-12-09 13:13:28.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.009771657832676426 Training loss: 8.619710922241211
2025-12-09 13:13:29.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.009770726360322463 Training loss: 8.274099349975586
2025-12-09 13:13:29.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.009769793036542742 Training loss: 8.768365859985352
2025-12-09 13:13:29.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.009768857861699462 Training loss: 8.473889350891113
2025-12-09 13:13:30.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.009767920836155552 Training loss: 8.229280471801758
2025-12-09 13:13:30.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.009766981960274652 Training loss: 8.446586608886719
2025-12-09 13:13:31.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.009766041234421121 Training loss: 8.168609619140625
2025-12-09 13:13:31.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.009765098658960036 Training loss: 8.30630874633789
2025-12-09 13:13:31.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.00976415423425719 Training loss: 8.156416893005371
2025-12-09 13:13:32.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0097632079606791 Training loss: 8.460888862609863
2025-12-09 13:13:32.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.009762259838592994 Training loss: 8.3751220703125
2025-12-09 13:13:32.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.009761309868366819 Training loss: 7.894808292388916
2025-12-09 13:13:33.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.009760358050369242 Training loss: 8.250213623046875
2025-12-09 13:13:33.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.009759404384969644 Training loss: 8.199503898620605
2025-12-09 13:13:33.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.009758448872538121 Training loss: 7.951406955718994
2025-12-09 13:13:34.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.009757491513445493 Training loss: 8.355525016784668
2025-12-09 13:13:34.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.009756532308063294 Training loss: 8.147793769836426
2025-12-09 13:13:35.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.009755571256763764 Training loss: 8.477519989013672
2025-12-09 13:13:35.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.009754608359919878 Training loss: 8.666414260864258
2025-12-09 13:13:35.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.009753643617905313 Training loss: 8.208948135375977
2025-12-09 13:13:36.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.009752677031094465 Training loss: 8.201213836669922
2025-12-09 13:13:36.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.009751708599862451 Training loss: 8.280441284179688
2025-12-09 13:13:36.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.009750738324585098 Training loss: 8.130416870117188
2025-12-09 13:13:37.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.009749766205638952 Training loss: 8.370251655578613
2025-12-09 13:13:37.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.009748792243401274 Training loss: 8.369912147521973
2025-12-09 13:13:38.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.009747816438250036 Training loss: 8.468066215515137
2025-12-09 13:13:38.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.009746838790563934 Training loss: 8.321732521057129
2025-12-09 13:13:38.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.009745859300722371 Training loss: 8.385282516479492
2025-12-09 13:13:39.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.009744877969105468 Training loss: 8.348361015319824
2025-12-09 13:13:39.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.009743894796094062 Training loss: 8.29699993133545
2025-12-09 13:13:39.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.009742909782069702 Training loss: 8.157179832458496
2025-12-09 13:13:40.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.009741922927414652 Training loss: 8.269774436950684
2025-12-09 13:13:40.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.009740934232511893 Training loss: 8.362128257751465
2025-12-09 13:13:41.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.009739943697745118 Training loss: 8.235089302062988
2025-12-09 13:13:41.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.009738951323498732 Training loss: 8.362451553344727
2025-12-09 13:13:41.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.009737957110157859 Training loss: 8.190342903137207
2025-12-09 13:13:42.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.009736961058108331 Training loss: 7.807857036590576
2025-12-09 13:13:42.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.009735963167736698 Training loss: 8.555281639099121
2025-12-09 13:13:42.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.009734963439430222 Training loss: 8.511462211608887
2025-12-09 13:13:43.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.009733961873576877 Training loss: 8.245444297790527
2025-12-09 13:13:43.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.009732958470565352 Training loss: 8.717205047607422
2025-12-09 13:13:44.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.009731953230785049 Training loss: 8.18218994140625
2025-12-09 13:13:44.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.009730946154626078 Training loss: 8.231352806091309
2025-12-09 13:13:44.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.00972993724247927 Training loss: 8.371774673461914
2025-12-09 13:13:45.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.009728926494736164 Training loss: 8.31093692779541
2025-12-09 13:13:45.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.009727913911789008 Training loss: 8.195709228515625
2025-12-09 13:13:45.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.009726899494030768 Training loss: 7.742800712585449
2025-12-09 13:13:46.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.009725883241855119 Training loss: 8.18171501159668
2025-12-09 13:13:46.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.009724865155656449 Training loss: 8.703834533691406
2025-12-09 13:13:47.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.009723845235829857 Training loss: 8.049294471740723
2025-12-09 13:13:47.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.009722823482771155 Training loss: 8.379953384399414
2025-12-09 13:13:47.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.009721799896876864 Training loss: 7.997731685638428
2025-12-09 13:13:48.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.009720774478544218 Training loss: 8.220396041870117
2025-12-09 13:13:48.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.009719747228171163 Training loss: 8.175371170043945
2025-12-09 13:13:48.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.009718718146156354 Training loss: 8.664199829101562
2025-12-09 13:13:49.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.00971768723289916 Training loss: 8.190731048583984
2025-12-09 13:13:49.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.009716654488799652 Training loss: 8.066977500915527
2025-12-09 13:13:50.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.009715619914258624 Training loss: 8.355950355529785
2025-12-09 13:13:50.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00971458350967757 Training loss: 8.226592063903809
2025-12-09 13:13:50.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.009713545275458703 Training loss: 8.264175415039062
2025-12-09 13:13:51.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.009712505212004938 Training loss: 8.057500839233398
2025-12-09 13:13:51.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.009711463319719903 Training loss: 7.9794816970825195
2025-12-09 13:13:51.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.009710419599007938 Training loss: 8.307188987731934
2025-12-09 13:13:52.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.009709374050274088 Training loss: 8.401161193847656
2025-12-09 13:13:52.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.009708326673924114 Training loss: 8.159701347351074
2025-12-09 13:13:53.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.009707277470364482 Training loss: 8.287335395812988
2025-12-09 13:13:53.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.009706226440002363 Training loss: 8.45374584197998
2025-12-09 13:13:53.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.009705173583245644 Training loss: 7.7779459953308105
2025-12-09 13:13:54.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.009704118900502918 Training loss: 8.233938217163086
2025-12-09 13:13:54.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.009703062392183489 Training loss: 8.03526782989502
2025-12-09 13:13:54.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.009702004058697363 Training loss: 8.232162475585938
2025-12-09 13:13:55.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.00970094390045526 Training loss: 8.40182113647461
2025-12-09 13:13:55.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00969988191786861 Training loss: 8.12759017944336
2025-12-09 13:13:55.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.009698818111349544 Training loss: 8.366401672363281
2025-12-09 13:13:56.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.009697752481310905 Training loss: 8.040838241577148
2025-12-09 13:13:56.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.009696685028166244 Training loss: 7.96970796585083
2025-12-09 13:13:57.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.00969561575232982 Training loss: 8.621082305908203
2025-12-09 13:13:57.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.009694544654216595 Training loss: 7.97683572769165
2025-12-09 13:13:57.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.009693471734242244 Training loss: 8.89610767364502
2025-12-09 13:13:58.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.009692396992823146 Training loss: 7.834351062774658
2025-12-09 13:13:58.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.009691320430376385 Training loss: 8.012181282043457
2025-12-09 13:13:58.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.009690242047319756 Training loss: 8.207509994506836
2025-12-09 13:13:59.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.009689161844071757 Training loss: 8.254684448242188
2025-12-09 13:13:59.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.009688079821051594 Training loss: 8.720407485961914
2025-12-09 13:14:00.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.009686995978679181 Training loss: 8.188003540039062
2025-12-09 13:14:00.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.009685910317375132 Training loss: 8.297990798950195
2025-12-09 13:14:00.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.009684822837560777 Training loss: 8.424456596374512
2025-12-09 13:14:01.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.00968373353965814 Training loss: 7.855988025665283
2025-12-09 13:14:01.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.009682642424089958 Training loss: 8.125341415405273
2025-12-09 13:14:01.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.009681549491279673 Training loss: 8.711525917053223
2025-12-09 13:14:02.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.00968045474165143 Training loss: 8.318947792053223
2025-12-09 13:14:02.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.00967935817563008 Training loss: 8.502359390258789
2025-12-09 13:14:03.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00967825979364118 Training loss: 8.19237232208252
2025-12-09 13:14:03.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.009677159596110986 Training loss: 8.260255813598633
2025-12-09 13:14:03.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.009676057583466471 Training loss: 8.56001091003418
2025-12-09 13:14:04.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.009674953756135297 Training loss: 8.619430541992188
2025-12-09 13:14:04.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.009673848114545842 Training loss: 8.401790618896484
2025-12-09 13:14:04.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.009672740659127184 Training loss: 8.23123550415039
2025-12-09 13:14:05.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.009671631390309103 Training loss: 7.939908981323242
2025-12-09 13:14:05.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.009670520308522083 Training loss: 8.415566444396973
2025-12-09 13:14:06.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.009669407414197318 Training loss: 8.27199935913086
2025-12-09 13:14:06.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.009668292707766698 Training loss: 8.562663078308105
2025-12-09 13:14:06.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.009667176189662818 Training loss: 8.313098907470703
2025-12-09 13:14:07.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.009666057860318978 Training loss: 8.138205528259277
2025-12-09 13:14:07.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00966493772016918 Training loss: 8.348596572875977
2025-12-09 13:14:07.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.009663815769648127 Training loss: 8.374784469604492
2025-12-09 13:14:08.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00966269200919123 Training loss: 8.60035228729248
2025-12-09 13:14:08.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.009661566439234593 Training loss: 8.816232681274414
2025-12-09 13:14:09.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.00966043906021503 Training loss: 8.767436027526855
2025-12-09 13:14:09.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.009659309872570057 Training loss: 8.198219299316406
2025-12-09 13:14:09.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.009658178876737887 Training loss: 8.359108924865723
2025-12-09 13:14:10.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.009657046073157436 Training loss: 8.452247619628906
2025-12-09 13:14:10.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.009655911462268327 Training loss: 8.706830978393555
2025-12-09 13:14:10.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.00965477504451088 Training loss: 9.064619064331055
2025-12-09 13:14:11.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.009653636820326113 Training loss: 8.912735939025879
2025-12-09 13:14:11.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.009652496790155752 Training loss: 10.392815589904785
2025-12-09 13:14:12.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.009651354954442217 Training loss: 10.87518310546875
2025-12-09 13:14:12.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.009650211313628636 Training loss: 9.797324180603027
2025-12-09 13:14:12.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.009649065868158831 Training loss: 9.711053848266602
2025-12-09 13:14:13.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.009647918618477328 Training loss: 9.110719680786133
2025-12-09 13:14:13.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.009646769565029354 Training loss: 9.509180068969727
2025-12-09 13:14:13.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00964561870826083 Training loss: 9.504183769226074
2025-12-09 13:14:14.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.009644466048618386 Training loss: 8.898429870605469
2025-12-09 13:14:14.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.009643311586549342 Training loss: 9.334185600280762
2025-12-09 13:14:14.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.009642155322501724 Training loss: 8.994682312011719
2025-12-09 13:14:15.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.009640997256924256 Training loss: 9.475881576538086
2025-12-09 13:14:15.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.009639837390266361 Training loss: 8.914874076843262
2025-12-09 13:14:16.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.009638675722978161 Training loss: 9.091996192932129
2025-12-09 13:14:16.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.009637512255510475 Training loss: 8.86036205291748
2025-12-09 13:14:16.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.009636346988314821 Training loss: 9.392401695251465
2025-12-09 13:14:17.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.009635179921843418 Training loss: 9.016768455505371
2025-12-09 13:14:17.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.009634011056549182 Training loss: 8.512210845947266
2025-12-09 13:14:17.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.009632840392885726 Training loss: 8.857441902160645
2025-12-09 13:14:18.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.009631667931307365 Training loss: 8.707261085510254
2025-12-09 13:14:18.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.009630493672269102 Training loss: 9.190818786621094
2025-12-09 13:14:19.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.009629317616226648 Training loss: 8.829736709594727
2025-12-09 13:14:19.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.009628139763636408 Training loss: 8.70205307006836
2025-12-09 13:14:19.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.009626960114955483 Training loss: 8.571283340454102
2025-12-09 13:14:20.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.009625778670641669 Training loss: 9.374959945678711
2025-12-09 13:14:20.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.009624595431153467 Training loss: 8.753549575805664
2025-12-09 13:14:20.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.009623410396950064 Training loss: 8.556015968322754
2025-12-09 13:14:21.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.009622223568491349 Training loss: 8.372405052185059
2025-12-09 13:14:21.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.00962103494623791 Training loss: 8.695683479309082
2025-12-09 13:14:22.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.009619844530651026 Training loss: 8.601668357849121
2025-12-09 13:14:22.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.009618652322192675 Training loss: 8.36759090423584
2025-12-09 13:14:22.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.00961745832132553 Training loss: 8.100297927856445
2025-12-09 13:14:23.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.009616262528512956 Training loss: 8.307289123535156
2025-12-09 13:14:23.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.009615064944219022 Training loss: 8.479369163513184
2025-12-09 13:14:23.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.009613865568908484 Training loss: 8.004989624023438
2025-12-09 13:14:24.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.009612664403046797 Training loss: 8.566488265991211
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 3.33 GiB is free. Including non-PyTorch memory, this process has 90.80 GiB memory in use. Of the allocated memory 89.27 GiB is allocated by PyTorch, and 792.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
