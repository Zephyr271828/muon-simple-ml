Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 15/10000 [00:00<01:06, 149.99it/s]Tokenizing texts:   0%|          | 34/10000 [00:00<00:58, 170.40it/s]Tokenizing texts:   1%|          | 52/10000 [00:00<01:24, 117.49it/s]Tokenizing texts:   1%|          | 72/10000 [00:00<01:10, 140.96it/s]Tokenizing texts:   1%|          | 97/10000 [00:00<00:57, 171.54it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 213.48it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:53, 185.71it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 184.11it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 196.62it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 201.76it/s]Tokenizing texts:   2%|▏         | 240/10000 [00:01<00:47, 203.83it/s]Tokenizing texts:   3%|▎         | 261/10000 [00:01<00:47, 203.06it/s]Tokenizing texts:   3%|▎         | 291/10000 [00:01<00:42, 228.10it/s]Tokenizing texts:   3%|▎         | 315/10000 [00:01<00:42, 226.81it/s]Tokenizing texts:   3%|▎         | 338/10000 [00:01<00:52, 182.35it/s]Tokenizing texts:   4%|▎         | 363/10000 [00:01<00:48, 198.43it/s]Tokenizing texts:   4%|▍         | 385/10000 [00:02<00:52, 184.50it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 185.90it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:51, 185.49it/s]Tokenizing texts:   4%|▍         | 446/10000 [00:02<00:51, 187.08it/s]Tokenizing texts:   5%|▍         | 468/10000 [00:02<00:48, 195.63it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 217.94it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:40, 236.12it/s]Tokenizing texts:   6%|▌         | 550/10000 [00:02<00:50, 185.58it/s]Tokenizing texts:   6%|▌         | 579/10000 [00:02<00:44, 210.81it/s]Tokenizing texts:   6%|▌         | 603/10000 [00:03<00:44, 210.06it/s]Tokenizing texts:   6%|▋         | 626/10000 [00:03<00:46, 199.81it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 201.98it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 199.87it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 191.24it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 200.57it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 200.05it/s]Tokenizing texts:   8%|▊         | 766/10000 [00:03<00:43, 210.01it/s]Tokenizing texts:   8%|▊         | 788/10000 [00:04<00:45, 201.98it/s]Tokenizing texts:   8%|▊         | 812/10000 [00:04<00:43, 212.10it/s]Tokenizing texts:   8%|▊         | 834/10000 [00:04<00:43, 209.90it/s]Tokenizing texts:   9%|▊         | 859/10000 [00:04<00:41, 220.69it/s]Tokenizing texts:   9%|▉         | 882/10000 [00:04<00:42, 214.24it/s]Tokenizing texts:   9%|▉         | 910/10000 [00:04<00:39, 229.53it/s]Tokenizing texts:   9%|▉         | 940/10000 [00:04<00:36, 248.83it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:04<00:33, 267.93it/s]Tokenizing texts:  10%|█         | 1001/10000 [00:04<00:33, 271.59it/s]Tokenizing texts:  10%|█         | 1032/10000 [00:04<00:31, 281.52it/s]Tokenizing texts:  11%|█         | 1061/10000 [00:05<00:39, 224.13it/s]Tokenizing texts:  11%|█         | 1086/10000 [00:05<00:39, 224.85it/s]Tokenizing texts:  11%|█         | 1110/10000 [00:05<00:39, 226.82it/s]Tokenizing texts:  11%|█▏        | 1134/10000 [00:05<00:45, 193.15it/s]Tokenizing texts:  12%|█▏        | 1159/10000 [00:05<00:43, 204.34it/s]Tokenizing texts:  12%|█▏        | 1184/10000 [00:05<00:40, 215.27it/s]Tokenizing texts:  12%|█▏        | 1207/10000 [00:05<00:43, 202.92it/s]Tokenizing texts:  12%|█▏        | 1235/10000 [00:06<00:41, 211.46it/s]Tokenizing texts:  13%|█▎        | 1257/10000 [00:06<00:42, 205.60it/s]Tokenizing texts:  13%|█▎        | 1291/10000 [00:06<00:36, 239.73it/s]Tokenizing texts:  13%|█▎        | 1316/10000 [00:06<00:36, 234.79it/s]Tokenizing texts:  13%|█▎        | 1340/10000 [00:06<00:42, 202.67it/s]Tokenizing texts:  14%|█▎        | 1368/10000 [00:06<00:39, 219.84it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:40, 214.97it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 232.30it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:34, 249.44it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:07<00:31, 272.35it/s]Tokenizing texts:  15%|█▌        | 1516/10000 [00:07<00:31, 265.55it/s]Tokenizing texts:  15%|█▌        | 1544/10000 [00:07<00:33, 248.83it/s]Tokenizing texts:  16%|█▌        | 1576/10000 [00:07<00:31, 267.70it/s]Tokenizing texts:  16%|█▌        | 1604/10000 [00:07<00:33, 250.16it/s]Tokenizing texts:  16%|█▋        | 1630/10000 [00:07<00:40, 206.29it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 216.19it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 217.11it/s]Tokenizing texts:  17%|█▋        | 1703/10000 [00:08<00:37, 219.91it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:33, 243.97it/s]Tokenizing texts:  18%|█▊        | 1761/10000 [00:08<00:33, 244.50it/s]Tokenizing texts:  18%|█▊        | 1786/10000 [00:08<00:33, 245.55it/s]Tokenizing texts:  18%|█▊        | 1820/10000 [00:08<00:30, 271.95it/s]Tokenizing texts:  18%|█▊        | 1848/10000 [00:08<00:37, 217.32it/s]Tokenizing texts:  19%|█▊        | 1872/10000 [00:08<00:36, 219.78it/s]Tokenizing texts:  19%|█▉        | 1896/10000 [00:08<00:36, 224.38it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:08<00:35, 225.08it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 195.09it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:38, 210.31it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:35, 227.12it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 232.30it/s]Tokenizing texts:  21%|██        | 2055/10000 [00:09<00:32, 246.20it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:38, 206.93it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:38, 207.29it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:37, 210.95it/s]Tokenizing texts:  22%|██▏       | 2150/10000 [00:10<00:39, 199.66it/s]Tokenizing texts:  22%|██▏       | 2179/10000 [00:10<00:35, 221.73it/s]Tokenizing texts:  22%|██▏       | 2209/10000 [00:10<00:32, 242.34it/s]Tokenizing texts:  22%|██▏       | 2240/10000 [00:10<00:29, 259.87it/s]Tokenizing texts:  23%|██▎       | 2269/10000 [00:10<00:28, 267.21it/s]Tokenizing texts:  23%|██▎       | 2297/10000 [00:10<00:30, 255.02it/s]Tokenizing texts:  23%|██▎       | 2323/10000 [00:10<00:35, 218.26it/s]Tokenizing texts:  23%|██▎       | 2349/10000 [00:10<00:33, 228.16it/s]Tokenizing texts:  24%|██▍       | 2382/10000 [00:10<00:30, 253.90it/s]Tokenizing texts:  24%|██▍       | 2412/10000 [00:11<00:28, 265.73it/s]Tokenizing texts:  24%|██▍       | 2440/10000 [00:11<00:31, 240.52it/s]Tokenizing texts:  25%|██▍       | 2473/10000 [00:11<00:29, 259.42it/s]Tokenizing texts:  25%|██▌       | 2500/10000 [00:11<00:31, 240.69it/s]Tokenizing texts:  25%|██▌       | 2530/10000 [00:11<00:29, 256.10it/s]Tokenizing texts:  26%|██▌       | 2557/10000 [00:11<00:29, 252.36it/s]Tokenizing texts:  26%|██▌       | 2583/10000 [00:11<00:35, 211.27it/s]Tokenizing texts:  26%|██▌       | 2606/10000 [00:11<00:35, 209.40it/s]Tokenizing texts:  26%|██▋       | 2628/10000 [00:12<00:34, 211.83it/s]Tokenizing texts:  27%|██▋       | 2654/10000 [00:12<00:33, 222.22it/s]Tokenizing texts:  27%|██▋       | 2677/10000 [00:12<00:32, 224.17it/s]Tokenizing texts:  27%|██▋       | 2700/10000 [00:12<00:38, 189.97it/s]Tokenizing texts:  27%|██▋       | 2730/10000 [00:12<00:33, 217.33it/s]Tokenizing texts:  28%|██▊       | 2762/10000 [00:12<00:29, 244.15it/s]Tokenizing texts:  28%|██▊       | 2788/10000 [00:12<00:39, 183.10it/s]Tokenizing texts:  28%|██▊       | 2818/10000 [00:12<00:34, 207.49it/s]Tokenizing texts:  28%|██▊       | 2846/10000 [00:13<00:31, 224.08it/s]Tokenizing texts:  29%|██▊       | 2871/10000 [00:13<00:35, 203.56it/s]Tokenizing texts:  29%|██▉       | 2902/10000 [00:13<00:31, 228.10it/s]Tokenizing texts:  29%|██▉       | 2931/10000 [00:13<00:29, 243.05it/s]Tokenizing texts:  30%|██▉       | 2957/10000 [00:13<00:30, 227.30it/s]Tokenizing texts:  30%|██▉       | 2981/10000 [00:13<00:31, 225.96it/s]Tokenizing texts:  30%|███       | 3018/10000 [00:13<00:26, 261.73it/s]Tokenizing texts:  30%|███       | 3046/10000 [00:13<00:30, 229.78it/s]Tokenizing texts:  31%|███       | 3080/10000 [00:13<00:27, 252.33it/s]Tokenizing texts:  31%|███       | 3107/10000 [00:14<00:28, 246.14it/s]Tokenizing texts:  31%|███▏      | 3139/10000 [00:14<00:26, 259.00it/s]Tokenizing texts:  32%|███▏      | 3166/10000 [00:14<00:27, 251.79it/s]Tokenizing texts:  32%|███▏      | 3196/10000 [00:14<00:25, 264.42it/s]Tokenizing texts:  32%|███▏      | 3223/10000 [00:14<00:25, 265.14it/s]Tokenizing texts:  32%|███▎      | 3250/10000 [00:14<00:26, 250.84it/s]Tokenizing texts:  33%|███▎      | 3280/10000 [00:14<00:25, 263.97it/s]Tokenizing texts:  33%|███▎      | 3307/10000 [00:14<00:26, 256.79it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:14<00:27, 242.19it/s]Tokenizing texts:  34%|███▎      | 3362/10000 [00:15<00:26, 254.31it/s]Tokenizing texts:  34%|███▍      | 3388/10000 [00:15<00:26, 247.25it/s]Tokenizing texts:  34%|███▍      | 3414/10000 [00:15<00:26, 246.29it/s]Tokenizing texts:  35%|███▍      | 3455/10000 [00:15<00:22, 288.48it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:15<00:22, 286.30it/s]Tokenizing texts:  35%|███▌      | 3516/10000 [00:15<00:26, 244.24it/s]Tokenizing texts:  35%|███▌      | 3542/10000 [00:15<00:27, 230.89it/s]Tokenizing texts:  36%|███▌      | 3569/10000 [00:15<00:26, 239.05it/s]Tokenizing texts:  36%|███▌      | 3600/10000 [00:16<00:25, 252.62it/s]Tokenizing texts:  36%|███▋      | 3626/10000 [00:16<00:25, 251.94it/s]Tokenizing texts:  37%|███▋      | 3652/10000 [00:16<00:25, 250.01it/s]Tokenizing texts:  37%|███▋      | 3678/10000 [00:16<00:25, 246.58it/s]Tokenizing texts:  37%|███▋      | 3705/10000 [00:16<00:25, 251.09it/s]Tokenizing texts:  37%|███▋      | 3732/10000 [00:16<00:24, 255.37it/s]Tokenizing texts:  38%|███▊      | 3758/10000 [00:16<00:27, 224.68it/s]Tokenizing texts:  38%|███▊      | 3784/10000 [00:16<00:26, 233.78it/s]Tokenizing texts:  38%|███▊      | 3808/10000 [00:16<00:30, 205.21it/s]Tokenizing texts:  38%|███▊      | 3839/10000 [00:17<00:26, 231.82it/s]Tokenizing texts:  39%|███▊      | 3867/10000 [00:17<00:25, 241.91it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:17<00:24, 249.17it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:26, 232.49it/s]Tokenizing texts:  39%|███▉      | 3945/10000 [00:17<00:26, 227.50it/s]Tokenizing texts:  40%|███▉      | 3976/10000 [00:17<00:24, 247.65it/s]Tokenizing texts:  40%|████      | 4002/10000 [00:17<00:26, 222.15it/s]Tokenizing texts:  40%|████      | 4029/10000 [00:17<00:25, 234.19it/s]Tokenizing texts:  41%|████      | 4056/10000 [00:17<00:24, 242.03it/s]Tokenizing texts:  41%|████      | 4089/10000 [00:18<00:22, 264.34it/s]Tokenizing texts:  41%|████      | 4121/10000 [00:18<00:21, 278.55it/s]Tokenizing texts:  42%|████▏     | 4150/10000 [00:18<00:20, 280.54it/s]Tokenizing texts:  42%|████▏     | 4179/10000 [00:18<00:21, 273.61it/s]Tokenizing texts:  42%|████▏     | 4214/10000 [00:18<00:19, 295.34it/s]Tokenizing texts:  42%|████▏     | 4244/10000 [00:18<00:24, 234.23it/s]Tokenizing texts:  43%|████▎     | 4270/10000 [00:18<00:24, 234.79it/s]Tokenizing texts:  43%|████▎     | 4297/10000 [00:18<00:23, 240.63it/s]Tokenizing texts:  43%|████▎     | 4333/10000 [00:18<00:20, 272.02it/s]Tokenizing texts:  44%|████▎     | 4362/10000 [00:19<00:20, 269.46it/s]Tokenizing texts:  44%|████▍     | 4393/10000 [00:19<00:20, 276.41it/s]Tokenizing texts:  44%|████▍     | 4423/10000 [00:19<00:19, 282.24it/s]Tokenizing texts:  45%|████▍     | 4452/10000 [00:19<00:21, 255.79it/s]Tokenizing texts:  45%|████▍     | 4479/10000 [00:19<00:21, 256.79it/s]Tokenizing texts:  45%|████▌     | 4506/10000 [00:19<00:23, 238.24it/s]Tokenizing texts:  45%|████▌     | 4531/10000 [00:19<00:22, 240.96it/s]Tokenizing texts:  46%|████▌     | 4561/10000 [00:19<00:21, 256.40it/s]Tokenizing texts:  46%|████▌     | 4588/10000 [00:19<00:22, 245.82it/s]Tokenizing texts:  46%|████▌     | 4613/10000 [00:20<00:22, 234.92it/s]Tokenizing texts:  47%|████▋     | 4653/10000 [00:20<00:19, 278.64it/s]Tokenizing texts:  47%|████▋     | 4685/10000 [00:20<00:18, 289.74it/s]Tokenizing texts:  47%|████▋     | 4720/10000 [00:20<00:17, 303.93it/s]Tokenizing texts:  48%|████▊     | 4751/10000 [00:20<00:19, 271.04it/s]Tokenizing texts:  48%|████▊     | 4779/10000 [00:20<00:20, 250.30it/s]Tokenizing texts:  48%|████▊     | 4807/10000 [00:20<00:20, 256.52it/s]Tokenizing texts:  48%|████▊     | 4837/10000 [00:20<00:19, 265.84it/s]Tokenizing texts:  49%|████▊     | 4867/10000 [00:21<00:18, 273.48it/s]Tokenizing texts:  49%|████▉     | 4899/10000 [00:21<00:17, 284.14it/s]Tokenizing texts:  49%|████▉     | 4928/10000 [00:21<00:19, 264.83it/s]Tokenizing texts:  50%|████▉     | 4956/10000 [00:21<00:19, 265.32it/s]Tokenizing texts:  50%|████▉     | 4989/10000 [00:21<00:17, 281.84it/s]Tokenizing texts:  50%|█████     | 5018/10000 [00:21<00:17, 279.78it/s]Tokenizing texts:  50%|█████     | 5047/10000 [00:21<00:19, 256.83it/s]Tokenizing texts:  51%|█████     | 5074/10000 [00:21<00:20, 242.28it/s]Tokenizing texts:  51%|█████     | 5099/10000 [00:21<00:20, 244.22it/s]Tokenizing texts:  51%|█████     | 5124/10000 [00:22<00:20, 233.05it/s]Tokenizing texts:  52%|█████▏    | 5151/10000 [00:22<00:20, 240.90it/s]Tokenizing texts:  52%|█████▏    | 5180/10000 [00:22<00:19, 252.81it/s]Tokenizing texts:  52%|█████▏    | 5206/10000 [00:22<00:20, 230.97it/s]Tokenizing texts:  52%|█████▏    | 5233/10000 [00:22<00:19, 241.29it/s]Tokenizing texts:  53%|█████▎    | 5267/10000 [00:22<00:17, 266.27it/s]Tokenizing texts:  53%|█████▎    | 5295/10000 [00:22<00:18, 255.34it/s]Tokenizing texts:  53%|█████▎    | 5330/10000 [00:22<00:16, 279.76it/s]Tokenizing texts:  54%|█████▎    | 5363/10000 [00:22<00:15, 291.90it/s]Tokenizing texts:  54%|█████▍    | 5393/10000 [00:23<00:15, 290.93it/s]Tokenizing texts:  54%|█████▍    | 5427/10000 [00:23<00:15, 302.64it/s]Tokenizing texts:  55%|█████▍    | 5458/10000 [00:23<00:16, 271.37it/s]Tokenizing texts:  55%|█████▍    | 5486/10000 [00:23<00:17, 254.39it/s]Tokenizing texts:  55%|█████▌    | 5513/10000 [00:23<00:20, 214.61it/s]Tokenizing texts:  55%|█████▌    | 5544/10000 [00:23<00:19, 226.46it/s]Tokenizing texts:  56%|█████▌    | 5572/10000 [00:23<00:18, 235.91it/s]Tokenizing texts:  56%|█████▌    | 5606/10000 [00:23<00:16, 260.87it/s]Tokenizing texts:  56%|█████▋    | 5641/10000 [00:23<00:15, 283.12it/s]Tokenizing texts:  57%|█████▋    | 5673/10000 [00:24<00:15, 287.83it/s]Tokenizing texts:  57%|█████▋    | 5706/10000 [00:24<00:14, 297.62it/s]Tokenizing texts:  57%|█████▋    | 5737/10000 [00:24<00:15, 271.95it/s]Tokenizing texts:  58%|█████▊    | 5765/10000 [00:24<00:16, 252.31it/s]Tokenizing texts:  58%|█████▊    | 5791/10000 [00:24<00:19, 220.16it/s]Tokenizing texts:  58%|█████▊    | 5815/10000 [00:24<00:19, 216.80it/s]Tokenizing texts:  58%|█████▊    | 5838/10000 [00:24<00:19, 213.60it/s]Tokenizing texts:  59%|█████▊    | 5873/10000 [00:24<00:16, 247.99it/s]Tokenizing texts:  59%|█████▉    | 5899/10000 [00:25<00:17, 233.80it/s]Tokenizing texts:  59%|█████▉    | 5924/10000 [00:25<00:17, 230.76it/s]Tokenizing texts:  60%|█████▉    | 5951/10000 [00:25<00:16, 240.08it/s]Tokenizing texts:  60%|█████▉    | 5978/10000 [00:25<00:16, 246.57it/s]Tokenizing texts:  60%|██████    | 6007/10000 [00:25<00:15, 253.95it/s]Tokenizing texts:  60%|██████    | 6033/10000 [00:25<00:16, 243.41it/s]Tokenizing texts:  61%|██████    | 6066/10000 [00:25<00:15, 257.12it/s]Tokenizing texts:  61%|██████    | 6092/10000 [00:25<00:15, 256.28it/s]Tokenizing texts:  61%|██████    | 6118/10000 [00:25<00:15, 255.74it/s]Tokenizing texts:  62%|██████▏   | 6150/10000 [00:26<00:14, 273.18it/s]Tokenizing texts:  62%|██████▏   | 6178/10000 [00:26<00:14, 265.93it/s]Tokenizing texts:  62%|██████▏   | 6205/10000 [00:26<00:14, 263.24it/s]Tokenizing texts:  62%|██████▏   | 6232/10000 [00:26<00:15, 248.14it/s]Tokenizing texts:  63%|██████▎   | 6261/10000 [00:26<00:15, 244.39it/s]Tokenizing texts:  63%|██████▎   | 6289/10000 [00:26<00:15, 246.88it/s]Tokenizing texts:  63%|██████▎   | 6314/10000 [00:26<00:16, 230.36it/s]Tokenizing texts:  63%|██████▎   | 6338/10000 [00:26<00:16, 228.09it/s]Tokenizing texts:  64%|██████▎   | 6365/10000 [00:26<00:15, 237.52it/s]Tokenizing texts:  64%|██████▍   | 6390/10000 [00:27<00:15, 239.61it/s]Tokenizing texts:  64%|██████▍   | 6420/10000 [00:27<00:14, 252.48it/s]Tokenizing texts:  64%|██████▍   | 6446/10000 [00:27<00:13, 254.32it/s]Tokenizing texts:  65%|██████▍   | 6472/10000 [00:27<00:14, 244.85it/s]Tokenizing texts:  65%|██████▍   | 6498/10000 [00:27<00:14, 246.52it/s]Tokenizing texts:  65%|██████▌   | 6523/10000 [00:27<00:15, 218.33it/s]Tokenizing texts:  66%|██████▌   | 6561/10000 [00:27<00:13, 258.59it/s]Tokenizing texts:  66%|██████▌   | 6595/10000 [00:27<00:12, 272.37it/s]Tokenizing texts:  66%|██████▌   | 6623/10000 [00:27<00:12, 263.43it/s]Tokenizing texts:  66%|██████▋   | 6650/10000 [00:28<00:12, 258.56it/s]Tokenizing texts:  67%|██████▋   | 6677/10000 [00:28<00:12, 260.07it/s]Tokenizing texts:  67%|██████▋   | 6704/10000 [00:28<00:13, 236.82it/s]Tokenizing texts:  67%|██████▋   | 6732/10000 [00:28<00:13, 241.42it/s]Tokenizing texts:  68%|██████▊   | 6760/10000 [00:28<00:12, 249.65it/s]Tokenizing texts:  68%|██████▊   | 6798/10000 [00:28<00:11, 285.59it/s]Tokenizing texts:  68%|██████▊   | 6830/10000 [00:28<00:10, 293.64it/s]Tokenizing texts:  69%|██████▊   | 6860/10000 [00:28<00:10, 291.89it/s]Tokenizing texts:  69%|██████▉   | 6890/10000 [00:28<00:11, 275.88it/s]Tokenizing texts:  69%|██████▉   | 6918/10000 [00:29<00:14, 215.15it/s]Tokenizing texts:  69%|██████▉   | 6944/10000 [00:29<00:13, 223.15it/s]Tokenizing texts:  70%|██████▉   | 6972/10000 [00:29<00:12, 237.00it/s]Tokenizing texts:  70%|██████▉   | 6998/10000 [00:29<00:12, 240.98it/s]Tokenizing texts:  70%|███████   | 7024/10000 [00:29<00:12, 240.73it/s]Tokenizing texts:  70%|███████   | 7049/10000 [00:29<00:12, 233.15it/s]Tokenizing texts:  71%|███████   | 7073/10000 [00:29<00:13, 209.93it/s]Tokenizing texts:  71%|███████   | 7097/10000 [00:29<00:13, 217.21it/s]Tokenizing texts:  71%|███████   | 7121/10000 [00:30<00:12, 222.70it/s]Tokenizing texts:  71%|███████▏  | 7148/10000 [00:30<00:12, 231.73it/s]Tokenizing texts:  72%|███████▏  | 7175/10000 [00:30<00:11, 240.00it/s]Tokenizing texts:  72%|███████▏  | 7205/10000 [00:30<00:10, 255.36it/s]Tokenizing texts:  72%|███████▏  | 7231/10000 [00:30<00:11, 245.73it/s]Tokenizing texts:  73%|███████▎  | 7261/10000 [00:30<00:10, 260.15it/s]Tokenizing texts:  73%|███████▎  | 7301/10000 [00:30<00:09, 299.76it/s]Tokenizing texts:  73%|███████▎  | 7334/10000 [00:30<00:08, 305.41it/s]Tokenizing texts:  74%|███████▎  | 7365/10000 [00:30<00:10, 258.89it/s]Tokenizing texts:  74%|███████▍  | 7394/10000 [00:31<00:09, 265.21it/s]Tokenizing texts:  74%|███████▍  | 7422/10000 [00:31<00:11, 221.13it/s]Tokenizing texts:  74%|███████▍  | 7446/10000 [00:31<00:12, 201.47it/s]Tokenizing texts:  75%|███████▍  | 7478/10000 [00:31<00:11, 226.09it/s]Tokenizing texts:  75%|███████▌  | 7514/10000 [00:31<00:09, 259.25it/s]Tokenizing texts:  75%|███████▌  | 7542/10000 [00:31<00:09, 261.93it/s]Tokenizing texts:  76%|███████▌  | 7570/10000 [00:31<00:09, 262.23it/s]Tokenizing texts:  76%|███████▌  | 7607/10000 [00:31<00:08, 291.18it/s]Tokenizing texts:  76%|███████▋  | 7637/10000 [00:32<00:08, 266.66it/s]Tokenizing texts:  77%|███████▋  | 7668/10000 [00:32<00:08, 276.21it/s]Tokenizing texts:  77%|███████▋  | 7701/10000 [00:32<00:08, 284.62it/s]Tokenizing texts:  77%|███████▋  | 7731/10000 [00:32<00:08, 264.58it/s]Tokenizing texts:  78%|███████▊  | 7759/10000 [00:32<00:09, 235.67it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 252.47it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 234.55it/s]Tokenizing texts:  78%|███████▊  | 7845/10000 [00:32<00:08, 243.47it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 255.75it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:33<00:08, 253.28it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:33<00:07, 281.02it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:33<00:07, 279.63it/s]Tokenizing texts:  80%|████████  | 8000/10000 [00:33<00:06, 287.95it/s]Tokenizing texts:  80%|████████  | 8030/10000 [00:33<00:06, 289.81it/s]Tokenizing texts:  81%|████████  | 8060/10000 [00:33<00:11, 173.05it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 199.94it/s]Tokenizing texts:  81%|████████  | 8120/10000 [00:34<00:09, 208.53it/s]Tokenizing texts:  81%|████████▏ | 8148/10000 [00:34<00:08, 224.84it/s]Tokenizing texts:  82%|████████▏ | 8176/10000 [00:34<00:07, 238.21it/s]Tokenizing texts:  82%|████████▏ | 8210/10000 [00:34<00:06, 264.37it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 206.43it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 218.13it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 252.16it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 248.99it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:35<00:06, 252.24it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:35<00:06, 252.10it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:35<00:06, 229.78it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:35<00:06, 256.09it/s]Tokenizing texts:  85%|████████▍ | 8472/10000 [00:35<00:05, 262.30it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:35<00:05, 277.32it/s]Tokenizing texts:  85%|████████▌ | 8533/10000 [00:35<00:05, 260.11it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:35<00:05, 265.75it/s]Tokenizing texts:  86%|████████▌ | 8590/10000 [00:35<00:05, 260.29it/s]Tokenizing texts:  86%|████████▌ | 8624/10000 [00:36<00:04, 282.25it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:36<00:04, 284.32it/s]Tokenizing texts:  87%|████████▋ | 8688/10000 [00:36<00:04, 283.07it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:36<00:04, 284.35it/s]Tokenizing texts:  88%|████████▊ | 8757/10000 [00:36<00:04, 290.86it/s]Tokenizing texts:  88%|████████▊ | 8787/10000 [00:36<00:04, 292.47it/s]Tokenizing texts:  88%|████████▊ | 8817/10000 [00:36<00:05, 218.70it/s]Tokenizing texts:  88%|████████▊ | 8842/10000 [00:36<00:05, 220.57it/s]Tokenizing texts:  89%|████████▉ | 8876/10000 [00:36<00:04, 247.90it/s]Tokenizing texts:  89%|████████▉ | 8914/10000 [00:37<00:03, 272.87it/s]Tokenizing texts:  89%|████████▉ | 8946/10000 [00:37<00:03, 283.91it/s]Tokenizing texts:  90%|████████▉ | 8976/10000 [00:37<00:03, 281.34it/s]Tokenizing texts:  90%|█████████ | 9005/10000 [00:37<00:04, 243.60it/s]Tokenizing texts:  90%|█████████ | 9036/10000 [00:37<00:03, 260.13it/s]Tokenizing texts:  91%|█████████ | 9064/10000 [00:37<00:03, 248.07it/s]Tokenizing texts:  91%|█████████ | 9096/10000 [00:37<00:03, 265.05it/s]Tokenizing texts:  91%|█████████ | 9124/10000 [00:37<00:03, 256.15it/s]Tokenizing texts:  92%|█████████▏| 9157/10000 [00:38<00:03, 273.86it/s]Tokenizing texts:  92%|█████████▏| 9189/10000 [00:38<00:02, 284.57it/s]Tokenizing texts:  92%|█████████▏| 9220/10000 [00:38<00:02, 290.93it/s]Tokenizing texts:  92%|█████████▎| 9250/10000 [00:38<00:02, 270.76it/s]Tokenizing texts:  93%|█████████▎| 9278/10000 [00:38<00:02, 242.01it/s]Tokenizing texts:  93%|█████████▎| 9307/10000 [00:38<00:02, 253.47it/s]Tokenizing texts:  93%|█████████▎| 9334/10000 [00:38<00:02, 236.10it/s]Tokenizing texts:  94%|█████████▎| 9359/10000 [00:38<00:03, 209.59it/s]Tokenizing texts:  94%|█████████▍| 9390/10000 [00:39<00:02, 234.03it/s]Tokenizing texts:  94%|█████████▍| 9416/10000 [00:39<00:02, 239.37it/s]Tokenizing texts:  94%|█████████▍| 9441/10000 [00:39<00:02, 217.63it/s]Tokenizing texts:  95%|█████████▍| 9474/10000 [00:39<00:02, 240.73it/s]Tokenizing texts:  95%|█████████▍| 9499/10000 [00:39<00:02, 180.02it/s]Tokenizing texts:  95%|█████████▌| 9535/10000 [00:39<00:02, 218.82it/s]Tokenizing texts:  96%|█████████▌| 9563/10000 [00:39<00:01, 232.58it/s]Tokenizing texts:  96%|█████████▌| 9592/10000 [00:39<00:01, 246.44it/s]Tokenizing texts:  96%|█████████▌| 9619/10000 [00:40<00:01, 225.32it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:40<00:01, 229.82it/s]Tokenizing texts:  97%|█████████▋| 9673/10000 [00:40<00:01, 234.77it/s]Tokenizing texts:  97%|█████████▋| 9698/10000 [00:40<00:01, 231.94it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:40<00:01, 236.35it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 205.78it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 213.90it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 258.11it/s]Tokenizing texts:  98%|█████████▊| 9840/10000 [00:40<00:00, 248.89it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:41<00:00, 255.32it/s]Tokenizing texts:  99%|█████████▉| 9906/10000 [00:41<00:00, 280.21it/s]Tokenizing texts:  99%|█████████▉| 9935/10000 [00:41<00:00, 279.13it/s]Tokenizing texts: 100%|█████████▉| 9966/10000 [00:41<00:00, 287.09it/s]Tokenizing texts: 100%|█████████▉| 9999/10000 [00:41<00:00, 298.89it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 241.03it/s]
2025-12-09 11:47:21.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 12.058746337890625
2025-12-09 11:47:21.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 12.047463417053223
2025-12-09 11:47:21.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 12.040019989013672
2025-12-09 11:47:21.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 12.021944046020508
2025-12-09 11:47:21.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 12.031246185302734
2025-12-09 11:47:21.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 12.065295219421387
2025-12-09 11:47:22.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 12.00881576538086
2025-12-09 11:47:22.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 12.023128509521484
2025-12-09 11:47:22.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 12.00207233428955
2025-12-09 11:47:22.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 12.027674674987793
2025-12-09 11:47:22.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 11.999529838562012
2025-12-09 11:47:22.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 11.977723121643066
2025-12-09 11:47:22.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 11.976625442504883
2025-12-09 11:47:22.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 11.928468704223633
2025-12-09 11:47:22.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 11.920300483703613
2025-12-09 11:47:22.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 11.904302597045898
2025-12-09 11:47:22.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 11.901408195495605
2025-12-09 11:47:22.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 11.890891075134277
2025-12-09 11:47:22.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 11.88991928100586
2025-12-09 11:47:23.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 11.795872688293457
2025-12-09 11:47:23.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 11.813419342041016
2025-12-09 11:47:23.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 11.757567405700684
2025-12-09 11:47:23.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 11.680413246154785
2025-12-09 11:47:23.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 11.652056694030762
2025-12-09 11:47:23.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 11.578988075256348
2025-12-09 11:47:23.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 11.663060188293457
2025-12-09 11:47:23.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 11.465970039367676
2025-12-09 11:47:23.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 11.491703987121582
2025-12-09 11:47:23.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 11.419609069824219
2025-12-09 11:47:23.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 11.319672584533691
2025-12-09 11:47:23.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 11.361021995544434
2025-12-09 11:47:23.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 11.285365104675293
2025-12-09 11:47:24.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 11.262641906738281
2025-12-09 11:47:24.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 11.219244956970215
2025-12-09 11:47:24.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 11.242781639099121
2025-12-09 11:47:24.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 11.137950897216797
2025-12-09 11:47:24.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 11.181000709533691
2025-12-09 11:47:24.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 11.154083251953125
2025-12-09 11:47:24.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 11.084664344787598
2025-12-09 11:47:24.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 11.035626411437988
2025-12-09 11:47:24.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 11.105875015258789
2025-12-09 11:47:24.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 11.026712417602539
2025-12-09 11:47:24.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 11.002145767211914
2025-12-09 11:47:24.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 10.960858345031738
2025-12-09 11:47:25.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 10.909406661987305
2025-12-09 11:47:25.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 10.91308307647705
2025-12-09 11:47:25.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 10.853530883789062
2025-12-09 11:47:25.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 10.888113021850586
2025-12-09 11:47:25.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 10.8202486038208
2025-12-09 11:47:25.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 10.823427200317383
2025-12-09 11:47:25.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 10.817632675170898
2025-12-09 11:47:25.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 10.772550582885742
2025-12-09 11:47:25.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 10.883918762207031
2025-12-09 11:47:25.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 10.823238372802734
2025-12-09 11:47:25.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 10.739999771118164
2025-12-09 11:47:25.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 10.612144470214844
2025-12-09 11:47:26.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 10.666644096374512
2025-12-09 11:47:26.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 10.70080280303955
2025-12-09 11:47:26.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 10.557497024536133
2025-12-09 11:47:26.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 10.502692222595215
2025-12-09 11:47:26.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 10.566631317138672
2025-12-09 11:47:26.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 10.496646881103516
2025-12-09 11:47:26.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 10.380380630493164
2025-12-09 11:47:26.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 10.398892402648926
2025-12-09 11:47:26.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 10.314779281616211
2025-12-09 11:47:26.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 10.539515495300293
2025-12-09 11:47:26.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 10.394402503967285
2025-12-09 11:47:26.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 10.577799797058105
2025-12-09 11:47:26.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 10.333544731140137
2025-12-09 11:47:27.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 10.443134307861328
2025-12-09 11:47:27.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 10.184123039245605
2025-12-09 11:47:27.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 10.223897933959961
2025-12-09 11:47:27.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 10.23540210723877
2025-12-09 11:47:27.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 10.014187812805176
2025-12-09 11:47:27.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 9.85098648071289
2025-12-09 11:47:27.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 9.939727783203125
2025-12-09 11:47:27.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 9.839680671691895
2025-12-09 11:47:27.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 9.80948543548584
2025-12-09 11:47:27.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 9.836109161376953
2025-12-09 11:47:27.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 9.646668434143066
2025-12-09 11:47:27.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 9.714461326599121
2025-12-09 11:47:27.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 9.672624588012695
2025-12-09 11:47:28.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 9.733128547668457
2025-12-09 11:47:28.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 9.54235553741455
2025-12-09 11:47:28.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 9.392038345336914
2025-12-09 11:47:28.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 9.572073936462402
2025-12-09 11:47:28.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 9.536100387573242
2025-12-09 11:47:28.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 9.174680709838867
2025-12-09 11:47:28.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 9.480826377868652
2025-12-09 11:47:28.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 9.246331214904785
2025-12-09 11:47:28.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 9.303801536560059
2025-12-09 11:47:28.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 9.311644554138184
2025-12-09 11:47:28.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 9.229165077209473
2025-12-09 11:47:28.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 9.218422889709473
2025-12-09 11:47:28.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 9.539928436279297
2025-12-09 11:47:29.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 9.035710334777832
2025-12-09 11:47:29.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 8.963011741638184
2025-12-09 11:47:29.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 9.026657104492188
2025-12-09 11:47:29.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 8.738920211791992
2025-12-09 11:47:29.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 8.914459228515625
2025-12-09 11:47:29.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.999999072578703e-05 Training loss: 8.772466659545898
2025-12-09 11:47:29.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 9.999996290315153e-05 Training loss: 8.583992958068848
2025-12-09 11:47:29.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 9.999991653210385e-05 Training loss: 8.858194351196289
2025-12-09 11:47:29.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 9.999985161266117e-05 Training loss: 8.779383659362793
2025-12-09 11:47:29.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 9.999976814484758e-05 Training loss: 9.296645164489746
2025-12-09 11:47:29.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 9.999966612869405e-05 Training loss: 8.78957748413086
2025-12-09 11:47:29.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 9.999954556423843e-05 Training loss: 8.651520729064941
2025-12-09 11:47:30.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.999940645152541e-05 Training loss: 9.349114418029785
2025-12-09 11:47:30.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 9.999924879060665e-05 Training loss: 8.576260566711426
2025-12-09 11:47:30.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 9.999907258154059e-05 Training loss: 8.531791687011719
2025-12-09 11:47:30.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 9.999887782439263e-05 Training loss: 8.679030418395996
2025-12-09 11:47:30.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 9.999866451923501e-05 Training loss: 8.676392555236816
2025-12-09 11:47:30.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 9.999843266614685e-05 Training loss: 8.471203804016113
2025-12-09 11:47:30.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 9.999818226521415e-05 Training loss: 8.759708404541016
2025-12-09 11:47:30.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 9.999791331652984e-05 Training loss: 8.160985946655273
2025-12-09 11:47:30.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 9.999762582019365e-05 Training loss: 8.373709678649902
2025-12-09 11:47:30.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 9.999731977631227e-05 Training loss: 8.332498550415039
2025-12-09 11:47:30.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 9.999699518499921e-05 Training loss: 8.683293342590332
2025-12-09 11:47:30.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 9.999665204637487e-05 Training loss: 8.186853408813477
2025-12-09 11:47:30.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 9.999629036056657e-05 Training loss: 8.22629451751709
2025-12-09 11:47:31.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 9.999591012770848e-05 Training loss: 8.368176460266113
2025-12-09 11:47:31.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 9.999551134794164e-05 Training loss: 8.082226753234863
2025-12-09 11:47:31.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 9.999509402141401e-05 Training loss: 8.09953498840332
2025-12-09 11:47:31.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 9.999465814828036e-05 Training loss: 8.319204330444336
2025-12-09 11:47:31.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 9.999420372870242e-05 Training loss: 8.269523620605469
2025-12-09 11:47:31.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 9.999373076284877e-05 Training loss: 8.027741432189941
2025-12-09 11:47:31.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 9.999323925089486e-05 Training loss: 8.21882152557373
2025-12-09 11:47:31.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 9.999272919302301e-05 Training loss: 8.01248550415039
2025-12-09 11:47:31.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 9.999220058942245e-05 Training loss: 8.270660400390625
2025-12-09 11:47:31.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 9.999165344028926e-05 Training loss: 7.498575687408447
2025-12-09 11:47:31.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 9.999108774582645e-05 Training loss: 8.05610466003418
2025-12-09 11:47:31.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 9.999050350624382e-05 Training loss: 7.899975299835205
2025-12-09 11:47:31.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 9.998990072175813e-05 Training loss: 8.86954402923584
2025-12-09 11:47:32.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 9.998927939259303e-05 Training loss: 7.979518890380859
2025-12-09 11:47:32.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 9.998863951897897e-05 Training loss: 7.935895919799805
2025-12-09 11:47:32.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 9.998798110115333e-05 Training loss: 8.103260040283203
2025-12-09 11:47:32.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 9.998730413936037e-05 Training loss: 8.180353164672852
2025-12-09 11:47:32.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 9.998660863385123e-05 Training loss: 8.024861335754395
2025-12-09 11:47:32.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 9.99858945848839e-05 Training loss: 8.005507469177246
2025-12-09 11:47:32.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 9.998516199272327e-05 Training loss: 7.742966651916504
2025-12-09 11:47:32.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 9.998441085764113e-05 Training loss: 8.05791187286377
2025-12-09 11:47:32.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 9.998364117991612e-05 Training loss: 7.645146369934082
2025-12-09 11:47:32.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 9.998285295983376e-05 Training loss: 7.88720178604126
2025-12-09 11:47:32.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 9.998204619768646e-05 Training loss: 7.845494747161865
2025-12-09 11:47:32.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 9.998122089377349e-05 Training loss: 7.736382961273193
2025-12-09 11:47:32.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 9.998037704840102e-05 Training loss: 7.898648738861084
2025-12-09 11:47:33.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 9.99795146618821e-05 Training loss: 7.7926859855651855
2025-12-09 11:47:33.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 9.997863373453663e-05 Training loss: 7.9229559898376465
2025-12-09 11:47:33.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 9.997773426669142e-05 Training loss: 7.756710052490234
2025-12-09 11:47:33.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 9.997681625868013e-05 Training loss: 7.707780361175537
2025-12-09 11:47:33.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 9.997587971084335e-05 Training loss: 7.649811744689941
2025-12-09 11:47:33.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 9.997492462352846e-05 Training loss: 7.6501288414001465
2025-12-09 11:47:33.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 9.997395099708982e-05 Training loss: 7.871026992797852
2025-12-09 11:47:33.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 9.997295883188856e-05 Training loss: 8.069583892822266
2025-12-09 11:47:33.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 9.997194812829276e-05 Training loss: 7.6219868659973145
2025-12-09 11:47:33.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 9.997091888667738e-05 Training loss: 7.783152103424072
2025-12-09 11:47:33.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 9.996987110742422e-05 Training loss: 7.605862617492676
2025-12-09 11:47:33.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 9.996880479092198e-05 Training loss: 7.753307819366455
2025-12-09 11:47:34.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 9.996771993756621e-05 Training loss: 7.683359622955322
2025-12-09 11:47:34.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 9.996661654775938e-05 Training loss: 7.814484596252441
2025-12-09 11:47:34.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 9.996549462191082e-05 Training loss: 8.049883842468262
2025-12-09 11:47:34.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 9.99643541604367e-05 Training loss: 7.695612907409668
2025-12-09 11:47:34.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 9.99631951637601e-05 Training loss: 7.40531063079834
2025-12-09 11:47:34.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 9.996201763231099e-05 Training loss: 7.4806952476501465
2025-12-09 11:47:34.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 9.996082156652618e-05 Training loss: 7.85573673248291
2025-12-09 11:47:34.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 9.995960696684939e-05 Training loss: 7.7657999992370605
2025-12-09 11:47:34.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 9.995837383373119e-05 Training loss: 7.216968059539795
2025-12-09 11:47:34.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 9.995712216762902e-05 Training loss: 7.372346878051758
2025-12-09 11:47:34.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 9.995585196900723e-05 Training loss: 7.580717086791992
2025-12-09 11:47:34.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 9.9954563238337e-05 Training loss: 7.686054706573486
2025-12-09 11:47:34.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 9.995325597609645e-05 Training loss: 8.39360523223877
2025-12-09 11:47:35.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 9.99519301827705e-05 Training loss: 7.529862880706787
2025-12-09 11:47:35.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 9.995058585885095e-05 Training loss: 7.752241134643555
2025-12-09 11:47:35.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 9.994922300483656e-05 Training loss: 7.554677963256836
2025-12-09 11:47:35.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 9.99478416212329e-05 Training loss: 7.25088357925415
2025-12-09 11:47:35.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 9.994644170855237e-05 Training loss: 7.45564079284668
2025-12-09 11:47:35.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 9.994502326731434e-05 Training loss: 8.467617988586426
2025-12-09 11:47:35.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 9.994358629804499e-05 Training loss: 7.768613815307617
2025-12-09 11:47:35.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 9.994213080127739e-05 Training loss: 7.329117774963379
2025-12-09 11:47:35.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 9.994065677755147e-05 Training loss: 7.530876636505127
2025-12-09 11:47:35.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 9.993916422741409e-05 Training loss: 7.753755569458008
2025-12-09 11:47:35.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 9.99376531514189e-05 Training loss: 7.614683151245117
2025-12-09 11:47:35.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 9.993612355012647e-05 Training loss: 7.653003692626953
2025-12-09 11:47:35.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 9.993457542410424e-05 Training loss: 7.476591110229492
2025-12-09 11:47:36.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 9.993300877392651e-05 Training loss: 7.5974812507629395
2025-12-09 11:47:36.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 9.993142360017446e-05 Training loss: 7.507686138153076
2025-12-09 11:47:36.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 9.992981990343614e-05 Training loss: 8.664939880371094
2025-12-09 11:47:36.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 9.992819768430647e-05 Training loss: 7.61230993270874
2025-12-09 11:47:36.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 9.992655694338725e-05 Training loss: 7.416517734527588
2025-12-09 11:47:36.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 9.992489768128713e-05 Training loss: 7.71442985534668
2025-12-09 11:47:36.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 9.992321989862166e-05 Training loss: 7.4928154945373535
2025-12-09 11:47:36.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 9.992152359601322e-05 Training loss: 7.319448947906494
2025-12-09 11:47:36.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 9.99198087740911e-05 Training loss: 7.874454975128174
2025-12-09 11:47:36.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 9.991807543349146e-05 Training loss: 7.694149017333984
2025-12-09 11:47:36.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 9.99163235748573e-05 Training loss: 7.245898246765137
2025-12-09 11:47:36.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 9.99145531988385e-05 Training loss: 7.1605706214904785
2025-12-09 11:47:36.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 9.99127643060918e-05 Training loss: 7.5001540184021
2025-12-09 11:47:37.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 9.991095689728087e-05 Training loss: 7.663257598876953
2025-12-09 11:47:37.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 9.990913097307614e-05 Training loss: 7.6622490882873535
2025-12-09 11:47:37.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 9.990728653415504e-05 Training loss: 7.675044536590576
2025-12-09 11:47:37.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 9.990542358120174e-05 Training loss: 7.6656951904296875
2025-12-09 11:47:37.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 9.990354211490735e-05 Training loss: 7.688006401062012
2025-12-09 11:47:37.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 9.990164213596986e-05 Training loss: 7.698512554168701
2025-12-09 11:47:37.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 9.989972364509408e-05 Training loss: 7.3362345695495605
2025-12-09 11:47:37.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 9.989778664299172e-05 Training loss: 7.295418739318848
2025-12-09 11:47:37.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 9.989583113038135e-05 Training loss: 7.531406402587891
2025-12-09 11:47:37.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 9.989385710798837e-05 Training loss: 7.101434707641602
2025-12-09 11:47:37.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 9.989186457654513e-05 Training loss: 7.554769039154053
2025-12-09 11:47:37.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 9.988985353679077e-05 Training loss: 7.53627347946167
2025-12-09 11:47:38.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 9.988782398947131e-05 Training loss: 7.668855667114258
2025-12-09 11:47:38.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 9.988577593533967e-05 Training loss: 7.125316619873047
2025-12-09 11:47:38.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 9.988370937515561e-05 Training loss: 7.807003498077393
2025-12-09 11:47:38.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 9.988162430968575e-05 Training loss: 7.413041591644287
2025-12-09 11:47:38.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 9.987952073970359e-05 Training loss: 7.819079399108887
2025-12-09 11:47:38.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 9.98773986659895e-05 Training loss: 7.294627666473389
2025-12-09 11:47:38.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 9.987525808933068e-05 Training loss: 7.038696765899658
2025-12-09 11:47:38.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 9.987309901052121e-05 Training loss: 7.80073356628418
2025-12-09 11:47:38.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 9.98709214303621e-05 Training loss: 7.401167392730713
2025-12-09 11:47:38.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 9.986872534966109e-05 Training loss: 7.352801322937012
2025-12-09 11:47:38.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 9.986651076923288e-05 Training loss: 7.673228740692139
2025-12-09 11:47:38.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 9.986427768989903e-05 Training loss: 7.5898847579956055
2025-12-09 11:47:38.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 9.986202611248793e-05 Training loss: 8.516287803649902
2025-12-09 11:47:39.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 9.985975603783484e-05 Training loss: 7.915626525878906
2025-12-09 11:47:39.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 9.98574674667819e-05 Training loss: 7.355075836181641
2025-12-09 11:47:39.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 9.985516040017807e-05 Training loss: 7.3626790046691895
2025-12-09 11:47:39.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 9.985283483887923e-05 Training loss: 7.475336074829102
2025-12-09 11:47:39.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 9.985049078374806e-05 Training loss: 7.818435192108154
2025-12-09 11:47:39.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 9.984812823565417e-05 Training loss: 7.797003746032715
2025-12-09 11:47:39.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 9.984574719547395e-05 Training loss: 7.4940080642700195
2025-12-09 11:47:39.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 9.984334766409071e-05 Training loss: 7.483902931213379
2025-12-09 11:47:39.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 9.98409296423946e-05 Training loss: 8.055243492126465
2025-12-09 11:47:39.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 9.983849313128264e-05 Training loss: 7.861867904663086
2025-12-09 11:47:39.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 9.983603813165869e-05 Training loss: 8.347960472106934
2025-12-09 11:47:39.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 9.983356464443347e-05 Training loss: 7.122816562652588
2025-12-09 11:47:39.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 9.983107267052457e-05 Training loss: 7.475600719451904
2025-12-09 11:47:40.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 9.982856221085644e-05 Training loss: 7.402277946472168
2025-12-09 11:47:40.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 9.982603326636037e-05 Training loss: 7.245658874511719
2025-12-09 11:47:40.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 9.982348583797454e-05 Training loss: 7.50010347366333
2025-12-09 11:47:40.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 9.982091992664392e-05 Training loss: 7.123426914215088
2025-12-09 11:47:40.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 9.981833553332045e-05 Training loss: 7.66259765625
2025-12-09 11:47:40.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 9.981573265896281e-05 Training loss: 8.309528350830078
2025-12-09 11:47:40.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 9.981311130453659e-05 Training loss: 7.3809003829956055
2025-12-09 11:47:40.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 9.981047147101426e-05 Training loss: 7.339447021484375
2025-12-09 11:47:40.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 9.980781315937507e-05 Training loss: 7.277618408203125
2025-12-09 11:47:40.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 9.98051363706052e-05 Training loss: 7.210360050201416
2025-12-09 11:47:40.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 9.980244110569765e-05 Training loss: 7.894155502319336
2025-12-09 11:47:40.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 9.979972736565226e-05 Training loss: 7.565732479095459
2025-12-09 11:47:41.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 9.979699515147578e-05 Training loss: 7.45833158493042
2025-12-09 11:47:41.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 9.979424446418173e-05 Training loss: 7.444190502166748
2025-12-09 11:47:41.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 9.979147530479056e-05 Training loss: 7.551541805267334
2025-12-09 11:47:41.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 9.978868767432954e-05 Training loss: 8.538854598999023
2025-12-09 11:47:41.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 9.978588157383277e-05 Training loss: 7.516109466552734
2025-12-09 11:47:41.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 9.978305700434125e-05 Training loss: 6.76131534576416
2025-12-09 11:47:41.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 9.97802139669028e-05 Training loss: 7.31398868560791
2025-12-09 11:47:41.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 9.97773524625721e-05 Training loss: 7.472067356109619
2025-12-09 11:47:41.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 9.977447249241066e-05 Training loss: 7.223167896270752
2025-12-09 11:47:41.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 9.977157405748687e-05 Training loss: 7.323052406311035
2025-12-09 11:47:41.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 9.976865715887595e-05 Training loss: 7.339308738708496
2025-12-09 11:47:41.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 9.976572179765999e-05 Training loss: 7.591770648956299
2025-12-09 11:47:41.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 9.976276797492793e-05 Training loss: 7.5346879959106445
2025-12-09 11:47:42.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 9.975979569177552e-05 Training loss: 7.210093975067139
2025-12-09 11:47:42.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 9.975680494930538e-05 Training loss: 7.438710689544678
2025-12-09 11:47:42.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 9.9753795748627e-05 Training loss: 7.2711029052734375
2025-12-09 11:47:42.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 9.975076809085669e-05 Training loss: 7.361930847167969
2025-12-09 11:47:42.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 9.974772197711761e-05 Training loss: 7.6838507652282715
2025-12-09 11:47:42.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 9.97446574085398e-05 Training loss: 7.157968997955322
2025-12-09 11:47:42.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 9.974157438626008e-05 Training loss: 7.221490383148193
2025-12-09 11:47:42.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 9.973847291142218e-05 Training loss: 7.182781219482422
2025-12-09 11:47:42.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 9.973535298517663e-05 Training loss: 7.274876117706299
2025-12-09 11:47:42.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 9.973221460868086e-05 Training loss: 7.5398945808410645
2025-12-09 11:47:42.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 9.972905778309906e-05 Training loss: 7.342170715332031
2025-12-09 11:47:42.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 9.972588250960234e-05 Training loss: 7.345520496368408
2025-12-09 11:47:42.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 9.972268878936863e-05 Training loss: 6.974417686462402
2025-12-09 11:47:43.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 9.97194766235827e-05 Training loss: 7.368894100189209
2025-12-09 11:47:43.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 9.971624601343615e-05 Training loss: 7.155433654785156
2025-12-09 11:47:43.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 9.971299696012743e-05 Training loss: 7.546354293823242
2025-12-09 11:47:43.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 9.970972946486185e-05 Training loss: 7.100396633148193
2025-12-09 11:47:43.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 9.970644352885157e-05 Training loss: 7.599368572235107
2025-12-09 11:47:43.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 9.970313915331553e-05 Training loss: 7.47255802154541
2025-12-09 11:47:43.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 9.969981633947956e-05 Training loss: 6.565474510192871
2025-12-09 11:47:43.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 9.969647508857631e-05 Training loss: 7.284725666046143
2025-12-09 11:47:43.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 9.969311540184532e-05 Training loss: 8.071304321289062
2025-12-09 11:47:43.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 9.968973728053288e-05 Training loss: 7.617247104644775
2025-12-09 11:47:43.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 9.968634072589218e-05 Training loss: 7.301468849182129
2025-12-09 11:47:43.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 9.968292573918325e-05 Training loss: 7.3002610206604
2025-12-09 11:47:43.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 9.967949232167294e-05 Training loss: 7.065669059753418
2025-12-09 11:47:44.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 9.967604047463493e-05 Training loss: 7.2260637283325195
2025-12-09 11:47:44.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 9.967257019934975e-05 Training loss: 7.721137046813965
2025-12-09 11:47:44.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 9.966908149710476e-05 Training loss: 7.137831687927246
2025-12-09 11:47:44.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 9.966557436919416e-05 Training loss: 7.210350036621094
2025-12-09 11:47:44.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 9.966204881691898e-05 Training loss: 7.28524923324585
2025-12-09 11:47:44.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 9.965850484158711e-05 Training loss: 7.557133674621582
2025-12-09 11:47:44.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 9.965494244451324e-05 Training loss: 7.142302513122559
2025-12-09 11:47:44.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 9.96513616270189e-05 Training loss: 7.2918524742126465
2025-12-09 11:47:44.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 9.964776239043246e-05 Training loss: 7.421982288360596
2025-12-09 11:47:44.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 9.964414473608912e-05 Training loss: 7.633086681365967
2025-12-09 11:47:44.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 9.964050866533094e-05 Training loss: 7.253627777099609
2025-12-09 11:47:44.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 9.963685417950677e-05 Training loss: 7.492575168609619
2025-12-09 11:47:45.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 9.96331812799723e-05 Training loss: 7.42072057723999
2025-12-09 11:47:45.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 9.962948996809008e-05 Training loss: 7.200303077697754
2025-12-09 11:47:45.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 9.962578024522948e-05 Training loss: 7.497224807739258
2025-12-09 11:47:45.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 9.962205211276665e-05 Training loss: 7.124241828918457
2025-12-09 11:47:45.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 9.961830557208464e-05 Training loss: 6.914914608001709
2025-12-09 11:47:45.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 9.961454062457329e-05 Training loss: 6.4792633056640625
2025-12-09 11:47:45.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 9.961075727162928e-05 Training loss: 7.118865013122559
2025-12-09 11:47:45.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 9.960695551465611e-05 Training loss: 7.10830020904541
2025-12-09 11:47:45.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 9.960313535506411e-05 Training loss: 7.653045177459717
2025-12-09 11:47:45.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 9.959929679427047e-05 Training loss: 7.521003246307373
2025-12-09 11:47:45.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 9.959543983369912e-05 Training loss: 7.318120956420898
2025-12-09 11:47:45.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 9.959156447478091e-05 Training loss: 7.060929298400879
2025-12-09 11:47:45.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 9.958767071895347e-05 Training loss: 7.1591877937316895
2025-12-09 11:47:46.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 9.958375856766127e-05 Training loss: 6.511119842529297
2025-12-09 11:47:46.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 9.957982802235556e-05 Training loss: 7.2660136222839355
2025-12-09 11:47:46.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 9.957587908449448e-05 Training loss: 6.824777126312256
2025-12-09 11:47:46.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 9.957191175554294e-05 Training loss: 8.41396427154541
2025-12-09 11:47:46.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 9.956792603697273e-05 Training loss: 7.069240570068359
2025-12-09 11:47:46.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 9.956392193026239e-05 Training loss: 7.455111980438232
2025-12-09 11:47:46.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 9.955989943689734e-05 Training loss: 7.155764102935791
2025-12-09 11:47:46.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 9.955585855836978e-05 Training loss: 7.226047992706299
2025-12-09 11:47:46.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 9.955179929617875e-05 Training loss: 7.078392505645752
2025-12-09 11:47:46.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 9.954772165183013e-05 Training loss: 7.140961170196533
2025-12-09 11:47:46.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 9.954362562683658e-05 Training loss: 7.301523685455322
2025-12-09 11:47:46.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 9.95395112227176e-05 Training loss: 7.440243244171143
2025-12-09 11:47:46.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 9.95353784409995e-05 Training loss: 6.91231632232666
2025-12-09 11:47:47.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 9.953122728321542e-05 Training loss: 7.041860103607178
2025-12-09 11:47:47.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 9.952705775090529e-05 Training loss: 7.334483623504639
2025-12-09 11:47:47.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 9.952286984561592e-05 Training loss: 7.1511335372924805
2025-12-09 11:47:47.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 9.951866356890084e-05 Training loss: 7.364853382110596
2025-12-09 11:47:47.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 9.951443892232047e-05 Training loss: 6.634129047393799
2025-12-09 11:47:47.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 9.951019590744203e-05 Training loss: 7.140320301055908
2025-12-09 11:47:47.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 9.950593452583952e-05 Training loss: 8.04305648803711
2025-12-09 11:47:47.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.95016547790938e-05 Training loss: 6.988540172576904
2025-12-09 11:47:47.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.949735666879252e-05 Training loss: 7.13421630859375
2025-12-09 11:47:47.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.949304019653012e-05 Training loss: 7.747879505157471
2025-12-09 11:47:47.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 9.94887053639079e-05 Training loss: 6.904531955718994
2025-12-09 11:47:47.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 9.948435217253393e-05 Training loss: 7.350003719329834
2025-12-09 11:47:48.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 9.947998062402313e-05 Training loss: 7.444461345672607
2025-12-09 11:47:48.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 9.947559071999719e-05 Training loss: 8.144964218139648
2025-12-09 11:47:48.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 9.947118246208462e-05 Training loss: 7.033306121826172
2025-12-09 11:47:48.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 9.946675585192075e-05 Training loss: 7.582684516906738
2025-12-09 11:47:48.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 9.946231089114774e-05 Training loss: 7.216197490692139
2025-12-09 11:47:48.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 9.945784758141448e-05 Training loss: 7.382091522216797
2025-12-09 11:47:48.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 9.945336592437678e-05 Training loss: 6.979730606079102
2025-12-09 11:47:48.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 9.944886592169713e-05 Training loss: 7.203716278076172
2025-12-09 11:47:48.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 9.944434757504492e-05 Training loss: 7.28632116317749
2025-12-09 11:47:48.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 9.94398108860963e-05 Training loss: 7.364989757537842
2025-12-09 11:47:48.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 9.943525585653428e-05 Training loss: 7.190920829772949
2025-12-09 11:47:48.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 9.94306824880486e-05 Training loss: 7.047240734100342
2025-12-09 11:47:48.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 9.942609078233581e-05 Training loss: 7.1847920417785645
2025-12-09 11:47:49.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 9.942148074109934e-05 Training loss: 7.153236389160156
2025-12-09 11:47:49.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 9.941685236604934e-05 Training loss: 7.402868270874023
2025-12-09 11:47:49.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 9.941220565890279e-05 Training loss: 7.332263469696045
2025-12-09 11:47:49.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 9.94075406213835e-05 Training loss: 7.9756059646606445
2025-12-09 11:47:49.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 9.940285725522203e-05 Training loss: 6.631132125854492
2025-12-09 11:47:49.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 9.939815556215575e-05 Training loss: 7.024997234344482
2025-12-09 11:47:49.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 9.939343554392886e-05 Training loss: 7.258488178253174
2025-12-09 11:47:49.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.938869720229234e-05 Training loss: 7.004754066467285
2025-12-09 11:47:49.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.938394053900395e-05 Training loss: 7.0125346183776855
2025-12-09 11:47:49.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 9.937916555582828e-05 Training loss: 6.923409461975098
2025-12-09 11:47:49.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 9.937437225453669e-05 Training loss: 7.559650421142578
2025-12-09 11:47:49.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 9.936956063690733e-05 Training loss: 7.283326148986816
2025-12-09 11:47:49.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 9.936473070472518e-05 Training loss: 7.2547712326049805
2025-12-09 11:47:50.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 9.935988245978199e-05 Training loss: 7.488759517669678
2025-12-09 11:47:50.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 9.935501590387628e-05 Training loss: 7.107236385345459
2025-12-09 11:47:50.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 9.935013103881343e-05 Training loss: 7.290452480316162
2025-12-09 11:47:50.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 9.934522786640555e-05 Training loss: 6.711036205291748
2025-12-09 11:47:50.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 9.934030638847155e-05 Training loss: 7.053341865539551
2025-12-09 11:47:50.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 9.933536660683717e-05 Training loss: 7.0018486976623535
2025-12-09 11:47:50.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 9.933040852333488e-05 Training loss: 6.935805797576904
2025-12-09 11:47:50.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 9.9325432139804e-05 Training loss: 6.281933784484863
2025-12-09 11:47:50.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 9.932043745809063e-05 Training loss: 7.315720081329346
2025-12-09 11:47:50.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 9.93154244800476e-05 Training loss: 7.2709641456604
2025-12-09 11:47:50.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.931039320753456e-05 Training loss: 7.3827619552612305
2025-12-09 11:47:50.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 9.9305343642418e-05 Training loss: 7.046770095825195
2025-12-09 11:47:51.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 9.930027578657113e-05 Training loss: 7.071276664733887
2025-12-09 11:47:51.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 9.929518964187395e-05 Training loss: 6.7774529457092285
2025-12-09 11:47:51.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 9.929008521021325e-05 Training loss: 6.998430252075195
2025-12-09 11:47:51.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 9.928496249348265e-05 Training loss: 7.254282474517822
2025-12-09 11:47:51.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 9.92798214935825e-05 Training loss: 7.382518768310547
2025-12-09 11:47:51.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 9.927466221241996e-05 Training loss: 7.444723129272461
2025-12-09 11:47:51.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 9.926948465190892e-05 Training loss: 5.9781880378723145
2025-12-09 11:47:51.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 9.926428881397015e-05 Training loss: 6.7414445877075195
2025-12-09 11:47:51.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 9.925907470053111e-05 Training loss: 7.185693264007568
2025-12-09 11:47:51.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 9.925384231352606e-05 Training loss: 7.639219284057617
2025-12-09 11:47:51.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 9.924859165489608e-05 Training loss: 6.999787330627441
2025-12-09 11:47:51.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 9.924332272658898e-05 Training loss: 6.844648361206055
2025-12-09 11:47:51.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 9.923803553055937e-05 Training loss: 6.967176914215088
2025-12-09 11:47:52.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 9.923273006876865e-05 Training loss: 6.918915748596191
2025-12-09 11:47:52.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 9.922740634318495e-05 Training loss: 6.811710357666016
2025-12-09 11:47:52.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 9.922206435578323e-05 Training loss: 7.108652591705322
2025-12-09 11:47:52.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 9.921670410854518e-05 Training loss: 7.286106109619141
2025-12-09 11:47:52.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 9.92113256034593e-05 Training loss: 6.945909023284912
2025-12-09 11:47:52.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 9.920592884252082e-05 Training loss: 7.706091403961182
2025-12-09 11:47:52.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 9.920051382773179e-05 Training loss: 7.048513889312744
2025-12-09 11:47:52.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 9.919508056110102e-05 Training loss: 6.9651384353637695
2025-12-09 11:47:52.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 9.918962904464407e-05 Training loss: 7.112910270690918
2025-12-09 11:47:52.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 9.918415928038325e-05 Training loss: 7.271515369415283
2025-12-09 11:47:52.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 9.917867127034772e-05 Training loss: 7.106359004974365
2025-12-09 11:47:52.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 9.917316501657334e-05 Training loss: 7.411102294921875
2025-12-09 11:47:52.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 9.916764052110274e-05 Training loss: 6.941203594207764
2025-12-09 11:47:53.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 9.916209778598535e-05 Training loss: 7.27545166015625
2025-12-09 11:47:53.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 9.915653681327737e-05 Training loss: 6.9497971534729
2025-12-09 11:47:53.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 9.915095760504169e-05 Training loss: 7.065344333648682
2025-12-09 11:47:53.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 9.914536016334808e-05 Training loss: 6.96903657913208
2025-12-09 11:47:53.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 9.913974449027298e-05 Training loss: 6.78751802444458
2025-12-09 11:47:53.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 9.913411058789963e-05 Training loss: 6.97876501083374
2025-12-09 11:47:53.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 9.912845845831805e-05 Training loss: 6.922370910644531
2025-12-09 11:47:53.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 9.912278810362498e-05 Training loss: 6.133237361907959
2025-12-09 11:47:53.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 9.911709952592397e-05 Training loss: 7.136641979217529
2025-12-09 11:47:53.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 9.911139272732526e-05 Training loss: 7.211658000946045
2025-12-09 11:47:53.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 9.910566770994594e-05 Training loss: 6.9560346603393555
2025-12-09 11:47:53.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 9.909992447590979e-05 Training loss: 7.075508117675781
2025-12-09 11:47:54.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 9.909416302734736e-05 Training loss: 7.219806671142578
2025-12-09 11:47:54.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 9.908838336639597e-05 Training loss: 6.598976135253906
2025-12-09 11:47:54.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 9.908258549519971e-05 Training loss: 7.1279377937316895
2025-12-09 11:47:54.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 9.907676941590939e-05 Training loss: 6.986969947814941
2025-12-09 11:47:54.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 9.907093513068259e-05 Training loss: 7.042565822601318
2025-12-09 11:47:54.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 9.906508264168366e-05 Training loss: 7.332411289215088
2025-12-09 11:47:54.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 9.905921195108368e-05 Training loss: 7.22831392288208
2025-12-09 11:47:54.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 9.90533230610605e-05 Training loss: 7.349850654602051
2025-12-09 11:47:54.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 9.90474159737987e-05 Training loss: 6.871868133544922
2025-12-09 11:47:54.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 9.904149069148963e-05 Training loss: 7.091795444488525
2025-12-09 11:47:54.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 9.903554721633139e-05 Training loss: 6.988157749176025
2025-12-09 11:47:54.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 9.902958555052882e-05 Training loss: 7.090649604797363
2025-12-09 11:47:54.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 9.902360569629349e-05 Training loss: 7.23708963394165
2025-12-09 11:47:55.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 9.901760765584375e-05 Training loss: 6.815766334533691
2025-12-09 11:47:55.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 9.901159143140471e-05 Training loss: 7.029305458068848
2025-12-09 11:47:55.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 9.900555702520816e-05 Training loss: 6.828954219818115
2025-12-09 11:47:55.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 9.89995044394927e-05 Training loss: 7.250179290771484
2025-12-09 11:47:55.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 9.899343367650365e-05 Training loss: 6.8299360275268555
2025-12-09 11:47:55.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 9.898734473849305e-05 Training loss: 7.130030632019043
2025-12-09 11:47:55.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 9.898123762771971e-05 Training loss: 7.0294976234436035
2025-12-09 11:47:55.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 9.89751123464492e-05 Training loss: 6.906290054321289
2025-12-09 11:47:55.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 9.896896889695378e-05 Training loss: 7.176604270935059
2025-12-09 11:47:55.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 9.896280728151248e-05 Training loss: 6.625710487365723
2025-12-09 11:47:55.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 9.895662750241108e-05 Training loss: 6.812738418579102
2025-12-09 11:47:55.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 9.89504295619421e-05 Training loss: 6.926389217376709
2025-12-09 11:47:55.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 9.894421346240473e-05 Training loss: 7.084692478179932
2025-12-09 11:47:56.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 9.893797920610496e-05 Training loss: 6.995283603668213
2025-12-09 11:47:56.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 9.893172679535553e-05 Training loss: 6.67564058303833
2025-12-09 11:47:56.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 9.892545623247586e-05 Training loss: 6.703566551208496
2025-12-09 11:47:56.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 9.891916751979218e-05 Training loss: 7.450158596038818
2025-12-09 11:47:56.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 9.891286065963734e-05 Training loss: 6.959616184234619
2025-12-09 11:47:56.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 9.890653565435101e-05 Training loss: 7.116901874542236
2025-12-09 11:47:56.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 9.89001925062796e-05 Training loss: 6.668600082397461
2025-12-09 11:47:56.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 9.889383121777617e-05 Training loss: 7.158792018890381
2025-12-09 11:47:56.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 9.88874517912006e-05 Training loss: 6.846454620361328
2025-12-09 11:47:56.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 9.888105422891943e-05 Training loss: 7.047648906707764
2025-12-09 11:47:56.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 9.887463853330594e-05 Training loss: 7.059458255767822
2025-12-09 11:47:56.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 9.886820470674018e-05 Training loss: 6.9202189445495605
2025-12-09 11:47:57.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 9.88617527516089e-05 Training loss: 6.837771892547607
2025-12-09 11:47:57.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 9.885528267030557e-05 Training loss: 7.271174907684326
2025-12-09 11:47:57.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 9.884879446523035e-05 Training loss: 6.958810806274414
2025-12-09 11:47:57.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 9.88422881387902e-05 Training loss: 6.8136372566223145
2025-12-09 11:47:57.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 9.883576369339875e-05 Training loss: 7.38771915435791
2025-12-09 11:47:57.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 9.882922113147637e-05 Training loss: 6.98392915725708
2025-12-09 11:47:57.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 9.882266045545012e-05 Training loss: 7.026437282562256
2025-12-09 11:47:57.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 9.881608166775383e-05 Training loss: 7.342523097991943
2025-12-09 11:47:57.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 9.880948477082804e-05 Training loss: 6.986699104309082
2025-12-09 11:47:57.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 9.880286976711992e-05 Training loss: 6.848290920257568
2025-12-09 11:47:57.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 9.87962366590835e-05 Training loss: 6.713563442230225
2025-12-09 11:47:57.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 9.878958544917942e-05 Training loss: 7.099308013916016
2025-12-09 11:47:57.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 9.87829161398751e-05 Training loss: 6.8011298179626465
2025-12-09 11:47:58.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 9.87762287336446e-05 Training loss: 6.965012073516846
2025-12-09 11:47:58.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 9.876952323296877e-05 Training loss: 7.082759380340576
2025-12-09 11:47:58.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 9.876279964033512e-05 Training loss: 6.919312953948975
2025-12-09 11:47:58.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 9.87560579582379e-05 Training loss: 6.867776393890381
2025-12-09 11:47:58.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 9.874929818917806e-05 Training loss: 7.074415683746338
2025-12-09 11:47:58.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 9.874252033566327e-05 Training loss: 7.062204837799072
2025-12-09 11:47:58.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 9.87357244002079e-05 Training loss: 6.6144561767578125
2025-12-09 11:47:58.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 9.8728910385333e-05 Training loss: 6.979772090911865
2025-12-09 11:47:58.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 9.872207829356641e-05 Training loss: 7.291295528411865
2025-12-09 11:47:58.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 9.871522812744256e-05 Training loss: 6.647942543029785
2025-12-09 11:47:58.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 9.870835988950268e-05 Training loss: 7.301982402801514
2025-12-09 11:47:58.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 9.870147358229467e-05 Training loss: 6.9203572273254395
2025-12-09 11:47:58.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 9.869456920837312e-05 Training loss: 7.212258338928223
2025-12-09 11:47:59.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 9.868764677029934e-05 Training loss: 7.433319568634033
2025-12-09 11:47:59.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 9.868070627064135e-05 Training loss: 6.765559673309326
2025-12-09 11:47:59.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 9.867374771197383e-05 Training loss: 7.278492450714111
2025-12-09 11:47:59.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 9.866677109687822e-05 Training loss: 7.1148858070373535
2025-12-09 11:47:59.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 9.86597764279426e-05 Training loss: 7.40189790725708
2025-12-09 11:47:59.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 9.865276370776177e-05 Training loss: 6.918796062469482
2025-12-09 11:47:59.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 9.864573293893725e-05 Training loss: 7.018254280090332
2025-12-09 11:47:59.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 9.863868412407721e-05 Training loss: 6.859600067138672
2025-12-09 11:47:59.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 9.863161726579655e-05 Training loss: 6.941996097564697
2025-12-09 11:47:59.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 9.862453236671685e-05 Training loss: 6.918183326721191
2025-12-09 11:47:59.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 9.861742942946639e-05 Training loss: 6.952181339263916
2025-12-09 11:47:59.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 9.861030845668014e-05 Training loss: 7.014543056488037
2025-12-09 11:48:00.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 9.860316945099973e-05 Training loss: 6.915017604827881
2025-12-09 11:48:00.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 9.859601241507353e-05 Training loss: 7.059138774871826
2025-12-09 11:48:00.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 9.858883735155657e-05 Training loss: 7.071444511413574
2025-12-09 11:48:00.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 9.858164426311059e-05 Training loss: 6.642602443695068
2025-12-09 11:48:00.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 9.857443315240397e-05 Training loss: 7.166582107543945
2025-12-09 11:48:00.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 9.856720402211182e-05 Training loss: 7.083230495452881
2025-12-09 11:48:00.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 9.855995687491591e-05 Training loss: 6.656909942626953
2025-12-09 11:48:00.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 9.855269171350471e-05 Training loss: 7.157357215881348
2025-12-09 11:48:00.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 9.854540854057337e-05 Training loss: 7.008232593536377
2025-12-09 11:48:00.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 9.85381073588237e-05 Training loss: 6.768768310546875
2025-12-09 11:48:00.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 9.853078817096424e-05 Training loss: 6.990975379943848
2025-12-09 11:48:00.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 9.852345097971016e-05 Training loss: 6.709018230438232
2025-12-09 11:48:00.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 9.851609578778332e-05 Training loss: 7.060492038726807
2025-12-09 11:48:01.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 9.850872259791228e-05 Training loss: 6.632411479949951
2025-12-09 11:48:01.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 9.850133141283226e-05 Training loss: 6.868953704833984
2025-12-09 11:48:01.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 9.849392223528514e-05 Training loss: 6.882602691650391
2025-12-09 11:48:01.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 9.84864950680195e-05 Training loss: 7.13527774810791
2025-12-09 11:48:01.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 9.84790499137906e-05 Training loss: 6.652709484100342
2025-12-09 11:48:01.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 9.847158677536034e-05 Training loss: 6.799656867980957
2025-12-09 11:48:01.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 9.84641056554973e-05 Training loss: 6.617558002471924
2025-12-09 11:48:01.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 9.845660655697679e-05 Training loss: 6.8556694984436035
2025-12-09 11:48:01.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 9.844908948258067e-05 Training loss: 6.946117401123047
2025-12-09 11:48:01.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 9.844155443509759e-05 Training loss: 6.813621997833252
2025-12-09 11:48:01.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 9.84340014173228e-05 Training loss: 6.742480754852295
2025-12-09 11:48:01.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 9.842643043205822e-05 Training loss: 7.269994258880615
2025-12-09 11:48:01.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 9.841884148211247e-05 Training loss: 6.8249711990356445
2025-12-09 11:48:02.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 9.84112345703008e-05 Training loss: 6.8056960105896
2025-12-09 11:48:02.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 9.84036096994451e-05 Training loss: 6.8502302169799805
2025-12-09 11:48:02.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 9.839596687237403e-05 Training loss: 7.1498703956604
2025-12-09 11:48:02.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 9.838830609192277e-05 Training loss: 6.573131561279297
2025-12-09 11:48:02.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 9.838062736093328e-05 Training loss: 7.27178955078125
2025-12-09 11:48:02.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 9.837293068225408e-05 Training loss: 6.837069511413574
2025-12-09 11:48:02.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 9.836521605874044e-05 Training loss: 7.032338619232178
2025-12-09 11:48:02.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 9.835748349325422e-05 Training loss: 6.900774002075195
2025-12-09 11:48:02.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 9.834973298866395e-05 Training loss: 6.932324409484863
2025-12-09 11:48:02.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 9.834196454784485e-05 Training loss: 7.108122825622559
2025-12-09 11:48:02.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 9.833417817367874e-05 Training loss: 7.030912399291992
2025-12-09 11:48:02.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 9.832637386905412e-05 Training loss: 6.872769832611084
2025-12-09 11:48:03.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 9.831855163686618e-05 Training loss: 6.858492851257324
2025-12-09 11:48:03.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 9.831071148001667e-05 Training loss: 7.080787181854248
2025-12-09 11:48:03.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 9.830285340141408e-05 Training loss: 6.7824296951293945
2025-12-09 11:48:03.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 9.829497740397349e-05 Training loss: 6.974208831787109
2025-12-09 11:48:03.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 9.828708349061664e-05 Training loss: 7.125760078430176
2025-12-09 11:48:03.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 9.827917166427195e-05 Training loss: 6.97523832321167
2025-12-09 11:48:03.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 9.827124192787444e-05 Training loss: 6.6570868492126465
2025-12-09 11:48:03.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 9.82632942843658e-05 Training loss: 6.9689106941223145
2025-12-09 11:48:03.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 9.825532873669435e-05 Training loss: 6.369747638702393
2025-12-09 11:48:03.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 9.824734528781505e-05 Training loss: 6.344858646392822
2025-12-09 11:48:03.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 9.823934394068952e-05 Training loss: 6.825485706329346
2025-12-09 11:48:03.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 9.823132469828601e-05 Training loss: 7.087259292602539
2025-12-09 11:48:03.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 9.822328756357942e-05 Training loss: 7.038547039031982
2025-12-09 11:48:04.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 9.821523253955122e-05 Training loss: 7.201675891876221
2025-12-09 11:48:04.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 9.820715962918964e-05 Training loss: 7.019094467163086
2025-12-09 11:48:04.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 9.819906883548943e-05 Training loss: 6.812409400939941
2025-12-09 11:48:04.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 9.819096016145203e-05 Training loss: 6.94157600402832
2025-12-09 11:48:04.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 9.81828336100855e-05 Training loss: 7.081506729125977
2025-12-09 11:48:04.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 9.817468918440454e-05 Training loss: 6.741630554199219
2025-12-09 11:48:04.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 9.816652688743049e-05 Training loss: 6.801320552825928
2025-12-09 11:48:04.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 9.815834672219127e-05 Training loss: 6.769839286804199
2025-12-09 11:48:04.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 9.815014869172149e-05 Training loss: 7.431736469268799
2025-12-09 11:48:04.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 9.814193279906237e-05 Training loss: 6.769658088684082
2025-12-09 11:48:04.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 9.81336990472617e-05 Training loss: 6.677889347076416
2025-12-09 11:48:04.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 9.8125447439374e-05 Training loss: 6.7448248863220215
2025-12-09 11:48:04.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 9.811717797846033e-05 Training loss: 6.938527584075928
2025-12-09 11:48:05.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 9.81088906675884e-05 Training loss: 6.797345161437988
2025-12-09 11:48:05.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 9.810058550983254e-05 Training loss: 6.682545185089111
2025-12-09 11:48:05.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 9.809226250827371e-05 Training loss: 7.036134719848633
2025-12-09 11:48:05.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 9.808392166599948e-05 Training loss: 6.953001499176025
2025-12-09 11:48:05.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 9.807556298610404e-05 Training loss: 7.0224504470825195
2025-12-09 11:48:05.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 9.806718647168818e-05 Training loss: 6.902774810791016
2025-12-09 11:48:05.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 9.805879212585933e-05 Training loss: 6.633796215057373
2025-12-09 11:48:05.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 9.805037995173155e-05 Training loss: 6.6283063888549805
2025-12-09 11:48:05.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 9.804194995242548e-05 Training loss: 6.573456287384033
2025-12-09 11:48:05.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 9.803350213106836e-05 Training loss: 6.478825092315674
2025-12-09 11:48:05.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 9.802503649079411e-05 Training loss: 6.529083251953125
2025-12-09 11:48:05.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 9.801655303474318e-05 Training loss: 6.625452518463135
2025-12-09 11:48:06.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 9.80080517660627e-05 Training loss: 7.100996017456055
2025-12-09 11:48:06.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 9.799953268790633e-05 Training loss: 6.822604179382324
2025-12-09 11:48:06.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 9.799099580343441e-05 Training loss: 7.569572448730469
2025-12-09 11:48:06.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 9.798244111581382e-05 Training loss: 7.148040771484375
2025-12-09 11:48:06.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 9.797386862821813e-05 Training loss: 6.871345520019531
2025-12-09 11:48:06.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 9.796527834382745e-05 Training loss: 7.117297649383545
2025-12-09 11:48:06.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 9.795667026582847e-05 Training loss: 6.767588138580322
2025-12-09 11:48:06.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 9.794804439741456e-05 Training loss: 6.4756059646606445
2025-12-09 11:48:06.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 9.79394007417856e-05 Training loss: 6.838159561157227
2025-12-09 11:48:06.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 9.793073930214817e-05 Training loss: 6.977770805358887
2025-12-09 11:48:06.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 9.792206008171533e-05 Training loss: 6.541825294494629
2025-12-09 11:48:06.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 9.791336308370687e-05 Training loss: 6.846035003662109
2025-12-09 11:48:06.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 9.790464831134903e-05 Training loss: 6.890985488891602
2025-12-09 11:48:07.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 9.789591576787476e-05 Training loss: 6.992082118988037
2025-12-09 11:48:07.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 9.788716545652353e-05 Training loss: 6.72558069229126
2025-12-09 11:48:07.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 9.787839738054146e-05 Training loss: 6.933849334716797
2025-12-09 11:48:07.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 9.786961154318121e-05 Training loss: 6.834643840789795
2025-12-09 11:48:07.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 9.786080794770207e-05 Training loss: 6.7137837409973145
2025-12-09 11:48:07.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 9.785198659736988e-05 Training loss: 7.065999507904053
2025-12-09 11:48:07.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 9.784314749545707e-05 Training loss: 6.7295637130737305
2025-12-09 11:48:07.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 9.78342906452427e-05 Training loss: 7.057391166687012
2025-12-09 11:48:07.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 9.782541605001235e-05 Training loss: 7.51981782913208
2025-12-09 11:48:07.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 9.781652371305824e-05 Training loss: 6.4841156005859375
2025-12-09 11:48:07.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 9.780761363767914e-05 Training loss: 6.184696674346924
2025-12-09 11:48:07.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 9.779868582718041e-05 Training loss: 6.745713710784912
2025-12-09 11:48:08.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 9.778974028487398e-05 Training loss: 7.116688251495361
2025-12-09 11:48:08.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 9.778077701407837e-05 Training loss: 6.217001438140869
2025-12-09 11:48:08.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 9.777179601811867e-05 Training loss: 6.896542072296143
2025-12-09 11:48:08.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 9.776279730032654e-05 Training loss: 7.337210178375244
2025-12-09 11:48:08.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 9.775378086404023e-05 Training loss: 7.015468120574951
2025-12-09 11:48:08.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 9.774474671260457e-05 Training loss: 6.999605655670166
2025-12-09 11:48:08.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 9.77356948493709e-05 Training loss: 6.696375370025635
2025-12-09 11:48:08.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 9.77266252776972e-05 Training loss: 7.096564292907715
2025-12-09 11:48:08.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 9.771753800094803e-05 Training loss: 6.44432258605957
2025-12-09 11:48:08.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 9.770843302249443e-05 Training loss: 6.943272113800049
2025-12-09 11:48:08.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 9.769931034571408e-05 Training loss: 6.726090908050537
2025-12-09 11:48:08.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 9.76901699739912e-05 Training loss: 6.6840362548828125
2025-12-09 11:48:08.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 9.768101191071661e-05 Training loss: 6.788958549499512
2025-12-09 11:48:09.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 9.767183615928765e-05 Training loss: 6.787280559539795
2025-12-09 11:48:09.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 9.766264272310822e-05 Training loss: 6.580959320068359
2025-12-09 11:48:09.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 9.765343160558879e-05 Training loss: 6.622275352478027
2025-12-09 11:48:09.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 9.764420281014642e-05 Training loss: 6.67003059387207
2025-12-09 11:48:09.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 9.763495634020467e-05 Training loss: 7.083355903625488
2025-12-09 11:48:09.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 9.762569219919372e-05 Training loss: 6.8015899658203125
2025-12-09 11:48:09.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 9.761641039055026e-05 Training loss: 6.986777305603027
2025-12-09 11:48:09.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 9.760711091771755e-05 Training loss: 7.184927463531494
2025-12-09 11:48:09.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 9.759779378414542e-05 Training loss: 6.543187618255615
2025-12-09 11:48:09.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 9.758845899329021e-05 Training loss: 6.801649570465088
2025-12-09 11:48:09.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 9.757910654861483e-05 Training loss: 6.911181926727295
2025-12-09 11:48:09.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 9.756973645358876e-05 Training loss: 6.973934650421143
2025-12-09 11:48:09.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 9.7560348711688e-05 Training loss: 6.9251790046691895
2025-12-09 11:48:10.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 9.755094332639512e-05 Training loss: 6.7273077964782715
2025-12-09 11:48:10.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 9.754152030119921e-05 Training loss: 6.676401138305664
2025-12-09 11:48:10.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 9.75320796395959e-05 Training loss: 6.7032928466796875
2025-12-09 11:48:10.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 9.752262134508742e-05 Training loss: 7.097793102264404
2025-12-09 11:48:10.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 9.751314542118246e-05 Training loss: 6.682460308074951
2025-12-09 11:48:10.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 9.750365187139632e-05 Training loss: 6.257428169250488
2025-12-09 11:48:10.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 9.749414069925078e-05 Training loss: 6.825041770935059
2025-12-09 11:48:10.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 9.74846119082742e-05 Training loss: 6.686817646026611
2025-12-09 11:48:10.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 9.747506550200146e-05 Training loss: 6.826496124267578
2025-12-09 11:48:10.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 9.746550148397398e-05 Training loss: 6.93682336807251
2025-12-09 11:48:10.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 9.745591985773971e-05 Training loss: 6.641208171844482
2025-12-09 11:48:10.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 9.744632062685311e-05 Training loss: 6.712526798248291
2025-12-09 11:48:11.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 9.743670379487522e-05 Training loss: 7.025758266448975
2025-12-09 11:48:11.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 9.742706936537358e-05 Training loss: 7.193492889404297
2025-12-09 11:48:11.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 9.741741734192224e-05 Training loss: 6.782867431640625
2025-12-09 11:48:11.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 9.740774772810182e-05 Training loss: 6.727471351623535
2025-12-09 11:48:11.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 9.739806052749943e-05 Training loss: 6.863407611846924
2025-12-09 11:48:11.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 9.738835574370871e-05 Training loss: 6.993798732757568
2025-12-09 11:48:11.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 9.737863338032985e-05 Training loss: 6.92694091796875
2025-12-09 11:48:11.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 9.736889344096952e-05 Training loss: 6.054592132568359
2025-12-09 11:48:11.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 9.735913592924093e-05 Training loss: 7.045905590057373
2025-12-09 11:48:11.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 9.734936084876383e-05 Training loss: 5.812610626220703
2025-12-09 11:48:11.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 9.733956820316444e-05 Training loss: 5.693755149841309
2025-12-09 11:48:11.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 9.732975799607555e-05 Training loss: 7.487269401550293
2025-12-09 11:48:11.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 9.731993023113642e-05 Training loss: 6.768971920013428
2025-12-09 11:48:12.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 9.731008491199284e-05 Training loss: 6.965523719787598
2025-12-09 11:48:12.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 9.730022204229714e-05 Training loss: 6.624269008636475
2025-12-09 11:48:12.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 9.729034162570811e-05 Training loss: 6.696186542510986
2025-12-09 11:48:12.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 9.728044366589108e-05 Training loss: 6.434374809265137
2025-12-09 11:48:12.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 9.727052816651788e-05 Training loss: 6.931676387786865
2025-12-09 11:48:12.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 9.726059513126685e-05 Training loss: 6.362941265106201
2025-12-09 11:48:12.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 9.725064456382283e-05 Training loss: 7.280350208282471
2025-12-09 11:48:12.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 9.724067646787717e-05 Training loss: 7.056229114532471
2025-12-09 11:48:12.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 9.723069084712772e-05 Training loss: 6.6999969482421875
2025-12-09 11:48:12.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 9.722068770527883e-05 Training loss: 6.561904430389404
2025-12-09 11:48:12.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 9.721066704604134e-05 Training loss: 6.645705223083496
2025-12-09 11:48:12.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 9.720062887313261e-05 Training loss: 7.062802314758301
2025-12-09 11:48:12.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 9.71905731902765e-05 Training loss: 7.07933235168457
2025-12-09 11:48:13.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 9.718050000120334e-05 Training loss: 6.760692596435547
2025-12-09 11:48:13.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 9.717040930964995e-05 Training loss: 6.936244010925293
2025-12-09 11:48:13.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 9.716030111935967e-05 Training loss: 6.4838175773620605
2025-12-09 11:48:13.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 9.715017543408233e-05 Training loss: 6.843307971954346
2025-12-09 11:48:13.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 9.714003225757424e-05 Training loss: 7.0926642417907715
2025-12-09 11:48:13.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 9.712987159359818e-05 Training loss: 5.8847270011901855
2025-12-09 11:48:13.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 9.711969344592346e-05 Training loss: 6.491323471069336
2025-12-09 11:48:13.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 9.710949781832585e-05 Training loss: 7.102264404296875
2025-12-09 11:48:13.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 9.709928471458759e-05 Training loss: 6.822762489318848
2025-12-09 11:48:13.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 9.708905413849743e-05 Training loss: 6.727867126464844
2025-12-09 11:48:13.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 9.707880609385059e-05 Training loss: 6.905406475067139
2025-12-09 11:48:13.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 9.706854058444876e-05 Training loss: 6.955163955688477
2025-12-09 11:48:14.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 9.705825761410014e-05 Training loss: 6.639646530151367
2025-12-09 11:48:14.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 9.704795718661939e-05 Training loss: 6.937645435333252
2025-12-09 11:48:14.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 9.703763930582761e-05 Training loss: 7.126399993896484
2025-12-09 11:48:14.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 9.702730397555247e-05 Training loss: 7.125583648681641
2025-12-09 11:48:14.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 9.7016951199628e-05 Training loss: 6.749417304992676
2025-12-09 11:48:14.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 9.700658098189475e-05 Training loss: 7.290666580200195
2025-12-09 11:48:14.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 9.69961933261998e-05 Training loss: 6.841743469238281
2025-12-09 11:48:14.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 9.698578823639659e-05 Training loss: 6.517068386077881
2025-12-09 11:48:14.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 9.697536571634509e-05 Training loss: 6.978731155395508
2025-12-09 11:48:14.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 9.696492576991174e-05 Training loss: 6.827451705932617
2025-12-09 11:48:14.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 9.695446840096944e-05 Training loss: 6.742269039154053
2025-12-09 11:48:14.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 9.694399361339752e-05 Training loss: 6.644808292388916
2025-12-09 11:48:14.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 9.693350141108182e-05 Training loss: 6.8003249168396
2025-12-09 11:48:15.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 9.692299179791459e-05 Training loss: 6.748841762542725
2025-12-09 11:48:15.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 9.69124647777946e-05 Training loss: 6.700552463531494
2025-12-09 11:48:15.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 9.690192035462702e-05 Training loss: 6.975743770599365
2025-12-09 11:48:15.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 9.689135853232349e-05 Training loss: 6.0929670333862305
2025-12-09 11:48:15.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 9.688077931480212e-05 Training loss: 6.548263072967529
2025-12-09 11:48:15.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 9.687018270598749e-05 Training loss: 7.4603986740112305
2025-12-09 11:48:15.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 9.685956870981058e-05 Training loss: 7.10015869140625
2025-12-09 11:48:15.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 9.684893733020888e-05 Training loss: 6.779463291168213
2025-12-09 11:48:15.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 9.683828857112627e-05 Training loss: 6.846255302429199
2025-12-09 11:48:15.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 9.682762243651308e-05 Training loss: 6.7962236404418945
2025-12-09 11:48:15.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 9.681693893032618e-05 Training loss: 6.758493900299072
2025-12-09 11:48:15.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 9.680623805652876e-05 Training loss: 6.733339309692383
2025-12-09 11:48:15.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 9.679551981909053e-05 Training loss: 6.618514537811279
2025-12-09 11:48:16.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 9.67847842219876e-05 Training loss: 6.436631202697754
2025-12-09 11:48:16.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 9.677403126920256e-05 Training loss: 6.336502552032471
2025-12-09 11:48:16.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 9.676326096472441e-05 Training loss: 7.577909469604492
2025-12-09 11:48:16.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 9.675247331254858e-05 Training loss: 7.03058385848999
2025-12-09 11:48:16.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 9.674166831667697e-05 Training loss: 7.383903503417969
2025-12-09 11:48:16.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 9.673084598111789e-05 Training loss: 6.710518836975098
2025-12-09 11:48:16.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 9.672000630988605e-05 Training loss: 6.777950286865234
2025-12-09 11:48:16.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 9.670914930700267e-05 Training loss: 6.8835344314575195
2025-12-09 11:48:16.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 9.669827497649536e-05 Training loss: 6.897540092468262
2025-12-09 11:48:16.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 9.668738332239813e-05 Training loss: 6.755372524261475
2025-12-09 11:48:16.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 9.667647434875145e-05 Training loss: 6.617744445800781
2025-12-09 11:48:16.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 9.66655480596022e-05 Training loss: 7.122166156768799
2025-12-09 11:48:17.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 9.665460445900368e-05 Training loss: 6.803271770477295
2025-12-09 11:48:17.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 9.664364355101565e-05 Training loss: 6.458399772644043
2025-12-09 11:48:17.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 9.663266533970424e-05 Training loss: 6.880390644073486
2025-12-09 11:48:17.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 9.662166982914203e-05 Training loss: 6.7874579429626465
2025-12-09 11:48:17.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 9.661065702340801e-05 Training loss: 6.5898284912109375
2025-12-09 11:48:17.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 9.659962692658758e-05 Training loss: 6.614902973175049
2025-12-09 11:48:17.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 9.658857954277254e-05 Training loss: 6.669793128967285
2025-12-09 11:48:17.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 9.657751487606115e-05 Training loss: 6.516036033630371
2025-12-09 11:48:17.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 9.656643293055804e-05 Training loss: 6.395203113555908
2025-12-09 11:48:17.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 9.655533371037426e-05 Training loss: 6.404778480529785
2025-12-09 11:48:18.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 9.65442172196273e-05 Training loss: 7.014291286468506
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.49 GiB is free. Including non-PyTorch memory, this process has 90.89 GiB memory in use. Of the allocated memory 89.10 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 15/10000 [00:00<01:06, 149.77it/s]Tokenizing texts:   0%|          | 34/10000 [00:00<00:58, 170.87it/s]Tokenizing texts:   1%|          | 52/10000 [00:00<01:24, 117.81it/s]Tokenizing texts:   1%|          | 72/10000 [00:00<01:10, 141.23it/s]Tokenizing texts:   1%|          | 97/10000 [00:00<00:57, 171.82it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 213.86it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:52, 186.01it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 184.96it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 197.57it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 202.41it/s]Tokenizing texts:   2%|▏         | 241/10000 [00:01<00:47, 205.22it/s]Tokenizing texts:   3%|▎         | 262/10000 [00:01<00:47, 205.00it/s]Tokenizing texts:   3%|▎         | 291/10000 [00:01<00:42, 227.75it/s]Tokenizing texts:   3%|▎         | 315/10000 [00:01<00:42, 226.62it/s]Tokenizing texts:   3%|▎         | 338/10000 [00:01<00:53, 182.16it/s]Tokenizing texts:   4%|▎         | 363/10000 [00:01<00:48, 198.58it/s]Tokenizing texts:   4%|▍         | 385/10000 [00:02<00:51, 184.95it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 186.53it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:51, 186.26it/s]Tokenizing texts:   4%|▍         | 446/10000 [00:02<00:50, 187.49it/s]Tokenizing texts:   5%|▍         | 468/10000 [00:02<00:48, 195.83it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 217.90it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:40, 236.47it/s]Tokenizing texts:   6%|▌         | 550/10000 [00:02<00:50, 186.15it/s]Tokenizing texts:   6%|▌         | 580/10000 [00:02<00:44, 211.73it/s]Tokenizing texts:   6%|▌         | 603/10000 [00:03<00:44, 210.54it/s]Tokenizing texts:   6%|▋         | 626/10000 [00:03<00:46, 200.43it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 202.27it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 200.30it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 191.51it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 200.80it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 200.45it/s]Tokenizing texts:   8%|▊         | 766/10000 [00:03<00:43, 211.22it/s]Tokenizing texts:   8%|▊         | 788/10000 [00:04<00:45, 203.21it/s]Tokenizing texts:   8%|▊         | 813/10000 [00:04<00:43, 212.94it/s]Tokenizing texts:   8%|▊         | 835/10000 [00:04<00:42, 213.36it/s]Tokenizing texts:   9%|▊         | 861/10000 [00:04<00:40, 226.40it/s]Tokenizing texts:   9%|▉         | 884/10000 [00:04<00:41, 218.58it/s]Tokenizing texts:   9%|▉         | 910/10000 [00:04<00:39, 228.87it/s]Tokenizing texts:   9%|▉         | 940/10000 [00:04<00:36, 248.47it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:04<00:33, 268.30it/s]Tokenizing texts:  10%|█         | 1001/10000 [00:04<00:33, 272.52it/s]Tokenizing texts:  10%|█         | 1033/10000 [00:04<00:31, 285.42it/s]Tokenizing texts:  11%|█         | 1062/10000 [00:05<00:39, 223.61it/s]Tokenizing texts:  11%|█         | 1087/10000 [00:05<00:39, 228.36it/s]Tokenizing texts:  11%|█         | 1112/10000 [00:05<00:38, 231.71it/s]Tokenizing texts:  11%|█▏        | 1137/10000 [00:05<00:45, 196.88it/s]Tokenizing texts:  12%|█▏        | 1160/10000 [00:05<00:43, 204.63it/s]Tokenizing texts:  12%|█▏        | 1186/10000 [00:05<00:40, 218.35it/s]Tokenizing texts:  12%|█▏        | 1209/10000 [00:05<00:42, 204.71it/s]Tokenizing texts:  12%|█▏        | 1235/10000 [00:05<00:41, 209.58it/s]Tokenizing texts:  13%|█▎        | 1257/10000 [00:06<00:42, 204.29it/s]Tokenizing texts:  13%|█▎        | 1292/10000 [00:06<00:36, 241.28it/s]Tokenizing texts:  13%|█▎        | 1317/10000 [00:06<00:37, 230.13it/s]Tokenizing texts:  13%|█▎        | 1341/10000 [00:06<00:41, 206.39it/s]Tokenizing texts:  14%|█▎        | 1368/10000 [00:06<00:39, 220.91it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:39, 215.52it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 233.24it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:34, 250.78it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:07<00:31, 273.90it/s]Tokenizing texts:  15%|█▌        | 1516/10000 [00:07<00:31, 267.75it/s]Tokenizing texts:  15%|█▌        | 1544/10000 [00:07<00:33, 251.14it/s]Tokenizing texts:  16%|█▌        | 1577/10000 [00:07<00:31, 271.68it/s]Tokenizing texts:  16%|█▌        | 1605/10000 [00:07<00:33, 252.90it/s]Tokenizing texts:  16%|█▋        | 1631/10000 [00:07<00:40, 208.73it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 215.86it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 216.92it/s]Tokenizing texts:  17%|█▋        | 1704/10000 [00:07<00:37, 222.46it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:33, 244.22it/s]Tokenizing texts:  18%|█▊        | 1761/10000 [00:08<00:33, 244.72it/s]Tokenizing texts:  18%|█▊        | 1786/10000 [00:08<00:33, 245.70it/s]Tokenizing texts:  18%|█▊        | 1820/10000 [00:08<00:30, 271.86it/s]Tokenizing texts:  18%|█▊        | 1848/10000 [00:08<00:37, 217.98it/s]Tokenizing texts:  19%|█▊        | 1872/10000 [00:08<00:36, 221.14it/s]Tokenizing texts:  19%|█▉        | 1896/10000 [00:08<00:35, 225.62it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:08<00:35, 226.35it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 195.55it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:38, 210.76it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:35, 228.18it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 233.66it/s]Tokenizing texts:  21%|██        | 2055/10000 [00:09<00:32, 247.87it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:37, 208.69it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:37, 209.01it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:36, 214.12it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:09<00:38, 204.77it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:35, 222.18it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:31, 249.93it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:29, 259.98it/s]Tokenizing texts:  23%|██▎       | 2274/10000 [00:10<00:28, 270.75it/s]Tokenizing texts:  23%|██▎       | 2302/10000 [00:10<00:29, 261.19it/s]Tokenizing texts:  23%|██▎       | 2329/10000 [00:10<00:36, 211.16it/s]Tokenizing texts:  24%|██▎       | 2365/10000 [00:10<00:30, 246.83it/s]Tokenizing texts:  24%|██▍       | 2399/10000 [00:10<00:28, 267.94it/s]Tokenizing texts:  24%|██▍       | 2428/10000 [00:11<00:30, 248.10it/s]Tokenizing texts:  25%|██▍       | 2455/10000 [00:11<00:29, 251.64it/s]Tokenizing texts:  25%|██▍       | 2482/10000 [00:11<00:30, 246.53it/s]Tokenizing texts:  25%|██▌       | 2508/10000 [00:11<00:30, 245.52it/s]Tokenizing texts:  25%|██▌       | 2538/10000 [00:11<00:28, 259.13it/s]Tokenizing texts:  26%|██▌       | 2565/10000 [00:11<00:33, 223.01it/s]Tokenizing texts:  26%|██▌       | 2589/10000 [00:11<00:33, 222.11it/s]Tokenizing texts:  26%|██▌       | 2612/10000 [00:11<00:36, 204.04it/s]Tokenizing texts:  26%|██▋       | 2641/10000 [00:11<00:33, 222.66it/s]Tokenizing texts:  27%|██▋       | 2667/10000 [00:12<00:31, 231.12it/s]Tokenizing texts:  27%|██▋       | 2691/10000 [00:12<00:32, 223.38it/s]Tokenizing texts:  27%|██▋       | 2714/10000 [00:12<00:35, 202.79it/s]Tokenizing texts:  27%|██▋       | 2749/10000 [00:12<00:30, 239.57it/s]Tokenizing texts:  28%|██▊       | 2776/10000 [00:12<00:29, 242.82it/s]Tokenizing texts:  28%|██▊       | 2801/10000 [00:12<00:40, 179.03it/s]Tokenizing texts:  28%|██▊       | 2836/10000 [00:12<00:33, 216.81it/s]Tokenizing texts:  29%|██▊       | 2861/10000 [00:13<00:35, 198.83it/s]Tokenizing texts:  29%|██▉       | 2890/10000 [00:13<00:32, 218.89it/s]Tokenizing texts:  29%|██▉       | 2923/10000 [00:13<00:28, 245.47it/s]Tokenizing texts:  30%|██▉       | 2950/10000 [00:13<00:31, 224.59it/s]Tokenizing texts:  30%|██▉       | 2975/10000 [00:13<00:30, 226.95it/s]Tokenizing texts:  30%|███       | 3013/10000 [00:13<00:26, 263.52it/s]Tokenizing texts:  30%|███       | 3041/10000 [00:13<00:30, 228.36it/s]Tokenizing texts:  31%|███       | 3074/10000 [00:13<00:27, 253.21it/s]Tokenizing texts:  31%|███       | 3101/10000 [00:13<00:28, 242.26it/s]Tokenizing texts:  31%|███▏      | 3135/10000 [00:14<00:25, 266.89it/s]Tokenizing texts:  32%|███▏      | 3163/10000 [00:14<00:26, 253.54it/s]Tokenizing texts:  32%|███▏      | 3190/10000 [00:14<00:26, 256.57it/s]Tokenizing texts:  32%|███▏      | 3220/10000 [00:14<00:25, 265.72it/s]Tokenizing texts:  32%|███▏      | 3248/10000 [00:14<00:26, 254.81it/s]Tokenizing texts:  33%|███▎      | 3280/10000 [00:14<00:24, 270.30it/s]Tokenizing texts:  33%|███▎      | 3308/10000 [00:14<00:25, 262.87it/s]Tokenizing texts:  33%|███▎      | 3335/10000 [00:14<00:26, 248.63it/s]Tokenizing texts:  34%|███▎      | 3365/10000 [00:14<00:25, 258.12it/s]Tokenizing texts:  34%|███▍      | 3392/10000 [00:15<00:26, 253.17it/s]Tokenizing texts:  34%|███▍      | 3420/10000 [00:15<00:25, 259.10it/s]Tokenizing texts:  35%|███▍      | 3460/10000 [00:15<00:22, 294.88it/s]Tokenizing texts:  35%|███▍      | 3490/10000 [00:15<00:23, 277.42it/s]Tokenizing texts:  35%|███▌      | 3519/10000 [00:15<00:25, 249.41it/s]Tokenizing texts:  35%|███▌      | 3545/10000 [00:15<00:27, 232.34it/s]Tokenizing texts:  36%|███▌      | 3574/10000 [00:15<00:26, 245.26it/s]Tokenizing texts:  36%|███▌      | 3601/10000 [00:15<00:25, 251.78it/s]Tokenizing texts:  36%|███▋      | 3627/10000 [00:16<00:25, 253.80it/s]Tokenizing texts:  37%|███▋      | 3653/10000 [00:16<00:25, 249.82it/s]Tokenizing texts:  37%|███▋      | 3679/10000 [00:16<00:25, 247.93it/s]Tokenizing texts:  37%|███▋      | 3705/10000 [00:16<00:25, 251.29it/s]Tokenizing texts:  37%|███▋      | 3732/10000 [00:16<00:24, 256.22it/s]Tokenizing texts:  38%|███▊      | 3758/10000 [00:16<00:27, 225.67it/s]Tokenizing texts:  38%|███▊      | 3784/10000 [00:16<00:26, 234.01it/s]Tokenizing texts:  38%|███▊      | 3808/10000 [00:16<00:30, 205.81it/s]Tokenizing texts:  38%|███▊      | 3840/10000 [00:16<00:26, 234.03it/s]Tokenizing texts:  39%|███▊      | 3867/10000 [00:17<00:25, 241.38it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:17<00:24, 249.38it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:25, 234.17it/s]Tokenizing texts:  39%|███▉      | 3946/10000 [00:17<00:26, 228.96it/s]Tokenizing texts:  40%|███▉      | 3979/10000 [00:17<00:23, 256.00it/s]Tokenizing texts:  40%|████      | 4006/10000 [00:17<00:27, 219.37it/s]Tokenizing texts:  40%|████      | 4037/10000 [00:17<00:25, 237.20it/s]Tokenizing texts:  41%|████      | 4067/10000 [00:17<00:23, 253.12it/s]Tokenizing texts:  41%|████      | 4099/10000 [00:17<00:21, 271.16it/s]Tokenizing texts:  41%|████▏     | 4132/10000 [00:18<00:20, 286.93it/s]Tokenizing texts:  42%|████▏     | 4162/10000 [00:18<00:21, 272.55it/s]Tokenizing texts:  42%|████▏     | 4195/10000 [00:18<00:20, 286.15it/s]Tokenizing texts:  42%|████▏     | 4225/10000 [00:18<00:22, 261.60it/s]Tokenizing texts:  43%|████▎     | 4252/10000 [00:18<00:23, 246.95it/s]Tokenizing texts:  43%|████▎     | 4278/10000 [00:18<00:24, 236.55it/s]Tokenizing texts:  43%|████▎     | 4310/10000 [00:18<00:22, 257.44it/s]Tokenizing texts:  43%|████▎     | 4345/10000 [00:18<00:20, 280.01it/s]Tokenizing texts:  44%|████▎     | 4374/10000 [00:19<00:21, 261.99it/s]Tokenizing texts:  44%|████▍     | 4411/10000 [00:19<00:19, 290.98it/s]Tokenizing texts:  44%|████▍     | 4441/10000 [00:19<00:20, 275.57it/s]Tokenizing texts:  45%|████▍     | 4470/10000 [00:19<00:21, 251.91it/s]Tokenizing texts:  45%|████▍     | 4496/10000 [00:19<00:23, 237.21it/s]Tokenizing texts:  45%|████▌     | 4521/10000 [00:19<00:22, 240.13it/s]Tokenizing texts:  46%|████▌     | 4552/10000 [00:19<00:21, 250.98it/s]Tokenizing texts:  46%|████▌     | 4578/10000 [00:19<00:21, 248.03it/s]Tokenizing texts:  46%|████▌     | 4604/10000 [00:19<00:22, 236.55it/s]Tokenizing texts:  46%|████▋     | 4637/10000 [00:20<00:20, 259.20it/s]Tokenizing texts:  47%|████▋     | 4671/10000 [00:20<00:19, 279.53it/s]Tokenizing texts:  47%|████▋     | 4708/10000 [00:20<00:17, 302.33it/s]Tokenizing texts:  47%|████▋     | 4739/10000 [00:20<00:18, 290.68it/s]Tokenizing texts:  48%|████▊     | 4769/10000 [00:20<00:18, 280.65it/s]Tokenizing texts:  48%|████▊     | 4798/10000 [00:20<00:21, 244.09it/s]Tokenizing texts:  48%|████▊     | 4830/10000 [00:20<00:19, 259.52it/s]Tokenizing texts:  49%|████▊     | 4859/10000 [00:20<00:19, 264.50it/s]Tokenizing texts:  49%|████▉     | 4887/10000 [00:20<00:19, 265.75it/s]Tokenizing texts:  49%|████▉     | 4915/10000 [00:21<00:18, 269.67it/s]Tokenizing texts:  49%|████▉     | 4943/10000 [00:21<00:18, 268.30it/s]Tokenizing texts:  50%|████▉     | 4971/10000 [00:21<00:19, 259.00it/s]Tokenizing texts:  50%|█████     | 5009/10000 [00:21<00:17, 292.70it/s]Tokenizing texts:  50%|█████     | 5039/10000 [00:21<00:18, 275.13it/s]Tokenizing texts:  51%|█████     | 5067/10000 [00:21<00:20, 245.93it/s]Tokenizing texts:  51%|█████     | 5093/10000 [00:21<00:20, 239.91it/s]Tokenizing texts:  51%|█████     | 5122/10000 [00:21<00:20, 234.99it/s]Tokenizing texts:  52%|█████▏    | 5150/10000 [00:22<00:19, 244.59it/s]Tokenizing texts:  52%|█████▏    | 5178/10000 [00:22<00:19, 251.17it/s]Tokenizing texts:  52%|█████▏    | 5204/10000 [00:22<00:20, 232.73it/s]Tokenizing texts:  52%|█████▏    | 5230/10000 [00:22<00:20, 236.00it/s]Tokenizing texts:  53%|█████▎    | 5262/10000 [00:22<00:18, 254.04it/s]Tokenizing texts:  53%|█████▎    | 5288/10000 [00:22<00:19, 238.80it/s]Tokenizing texts:  53%|█████▎    | 5323/10000 [00:22<00:17, 267.84it/s]Tokenizing texts:  54%|█████▎    | 5356/10000 [00:22<00:16, 283.94it/s]Tokenizing texts:  54%|█████▍    | 5387/10000 [00:22<00:15, 291.19it/s]Tokenizing texts:  54%|█████▍    | 5418/10000 [00:22<00:15, 295.74it/s]Tokenizing texts:  54%|█████▍    | 5448/10000 [00:23<00:17, 266.77it/s]Tokenizing texts:  55%|█████▍    | 5476/10000 [00:23<00:18, 248.56it/s]Tokenizing texts:  55%|█████▌    | 5502/10000 [00:23<00:21, 208.30it/s]Tokenizing texts:  55%|█████▌    | 5537/10000 [00:23<00:18, 240.06it/s]Tokenizing texts:  56%|█████▌    | 5564/10000 [00:23<00:17, 247.21it/s]Tokenizing texts:  56%|█████▌    | 5591/10000 [00:23<00:17, 247.78it/s]Tokenizing texts:  56%|█████▋    | 5625/10000 [00:23<00:16, 272.59it/s]Tokenizing texts:  57%|█████▋    | 5662/10000 [00:23<00:14, 298.27it/s]Tokenizing texts:  57%|█████▋    | 5693/10000 [00:24<00:14, 299.74it/s]Tokenizing texts:  57%|█████▋    | 5724/10000 [00:24<00:14, 297.14it/s]Tokenizing texts:  58%|█████▊    | 5755/10000 [00:24<00:16, 258.94it/s]Tokenizing texts:  58%|█████▊    | 5783/10000 [00:24<00:17, 241.49it/s]Tokenizing texts:  58%|█████▊    | 5809/10000 [00:24<00:20, 206.30it/s]Tokenizing texts:  58%|█████▊    | 5833/10000 [00:24<00:19, 212.18it/s]Tokenizing texts:  59%|█████▊    | 5865/10000 [00:24<00:17, 237.01it/s]Tokenizing texts:  59%|█████▉    | 5891/10000 [00:24<00:16, 242.58it/s]Tokenizing texts:  59%|█████▉    | 5917/10000 [00:25<00:16, 243.26it/s]Tokenizing texts:  59%|█████▉    | 5942/10000 [00:25<00:16, 242.17it/s]Tokenizing texts:  60%|█████▉    | 5968/10000 [00:25<00:16, 243.77it/s]Tokenizing texts:  60%|█████▉    | 5993/10000 [00:25<00:16, 244.02it/s]Tokenizing texts:  60%|██████    | 6018/10000 [00:25<00:16, 235.70it/s]Tokenizing texts:  61%|██████    | 6051/10000 [00:25<00:15, 261.79it/s]Tokenizing texts:  61%|██████    | 6078/10000 [00:25<00:14, 262.32it/s]Tokenizing texts:  61%|██████    | 6105/10000 [00:25<00:15, 256.18it/s]Tokenizing texts:  61%|██████▏   | 6138/10000 [00:25<00:14, 275.82it/s]Tokenizing texts:  62%|██████▏   | 6166/10000 [00:25<00:13, 275.72it/s]Tokenizing texts:  62%|██████▏   | 6194/10000 [00:26<00:13, 275.22it/s]Tokenizing texts:  62%|██████▏   | 6222/10000 [00:26<00:15, 243.36it/s]Tokenizing texts:  63%|██████▎   | 6253/10000 [00:26<00:14, 259.72it/s]Tokenizing texts:  63%|██████▎   | 6280/10000 [00:26<00:14, 250.64it/s]Tokenizing texts:  63%|██████▎   | 6306/10000 [00:26<00:15, 246.11it/s]Tokenizing texts:  63%|██████▎   | 6331/10000 [00:26<00:16, 222.77it/s]Tokenizing texts:  64%|██████▎   | 6359/10000 [00:26<00:15, 236.31it/s]Tokenizing texts:  64%|██████▍   | 6386/10000 [00:26<00:14, 244.41it/s]Tokenizing texts:  64%|██████▍   | 6414/10000 [00:27<00:14, 253.57it/s]Tokenizing texts:  64%|██████▍   | 6442/10000 [00:27<00:13, 254.75it/s]Tokenizing texts:  65%|██████▍   | 6468/10000 [00:27<00:14, 248.59it/s]Tokenizing texts:  65%|██████▍   | 6494/10000 [00:27<00:14, 248.14it/s]Tokenizing texts:  65%|██████▌   | 6519/10000 [00:27<00:15, 226.80it/s]Tokenizing texts:  66%|██████▌   | 6553/10000 [00:27<00:13, 256.41it/s]Tokenizing texts:  66%|██████▌   | 6592/10000 [00:27<00:11, 292.54it/s]Tokenizing texts:  66%|██████▌   | 6622/10000 [00:27<00:12, 264.11it/s]Tokenizing texts:  66%|██████▋   | 6650/10000 [00:27<00:12, 261.08it/s]Tokenizing texts:  67%|██████▋   | 6677/10000 [00:28<00:12, 262.77it/s]Tokenizing texts:  67%|██████▋   | 6704/10000 [00:28<00:13, 240.26it/s]Tokenizing texts:  67%|██████▋   | 6732/10000 [00:28<00:13, 245.39it/s]Tokenizing texts:  68%|██████▊   | 6760/10000 [00:28<00:12, 253.30it/s]Tokenizing texts:  68%|██████▊   | 6799/10000 [00:28<00:11, 288.50it/s]Tokenizing texts:  68%|██████▊   | 6831/10000 [00:28<00:10, 291.53it/s]Tokenizing texts:  69%|██████▊   | 6865/10000 [00:28<00:10, 299.76it/s]Tokenizing texts:  69%|██████▉   | 6896/10000 [00:28<00:13, 237.10it/s]Tokenizing texts:  69%|██████▉   | 6922/10000 [00:29<00:13, 231.21it/s]Tokenizing texts:  69%|██████▉   | 6948/10000 [00:29<00:13, 234.10it/s]Tokenizing texts:  70%|██████▉   | 6977/10000 [00:29<00:12, 247.17it/s]Tokenizing texts:  70%|███████   | 7005/10000 [00:29<00:11, 254.68it/s]Tokenizing texts:  70%|███████   | 7032/10000 [00:29<00:11, 249.11it/s]Tokenizing texts:  71%|███████   | 7058/10000 [00:29<00:12, 236.95it/s]Tokenizing texts:  71%|███████   | 7083/10000 [00:29<00:13, 213.49it/s]Tokenizing texts:  71%|███████   | 7105/10000 [00:29<00:13, 214.55it/s]Tokenizing texts:  71%|███████▏  | 7135/10000 [00:29<00:12, 236.91it/s]Tokenizing texts:  72%|███████▏  | 7160/10000 [00:30<00:11, 237.05it/s]Tokenizing texts:  72%|███████▏  | 7190/10000 [00:30<00:11, 252.76it/s]Tokenizing texts:  72%|███████▏  | 7216/10000 [00:30<00:11, 245.74it/s]Tokenizing texts:  72%|███████▏  | 7244/10000 [00:30<00:10, 253.30it/s]Tokenizing texts:  73%|███████▎  | 7278/10000 [00:30<00:09, 277.45it/s]Tokenizing texts:  73%|███████▎  | 7311/10000 [00:30<00:09, 292.34it/s]Tokenizing texts:  73%|███████▎  | 7341/10000 [00:30<00:09, 280.93it/s]Tokenizing texts:  74%|███████▎  | 7370/10000 [00:30<00:09, 271.16it/s]Tokenizing texts:  74%|███████▍  | 7398/10000 [00:30<00:10, 251.71it/s]Tokenizing texts:  74%|███████▍  | 7424/10000 [00:31<00:11, 225.84it/s]Tokenizing texts:  74%|███████▍  | 7448/10000 [00:31<00:12, 204.10it/s]Tokenizing texts:  75%|███████▍  | 7480/10000 [00:31<00:10, 232.37it/s]Tokenizing texts:  75%|███████▌  | 7516/10000 [00:31<00:09, 264.52it/s]Tokenizing texts:  75%|███████▌  | 7544/10000 [00:31<00:09, 262.30it/s]Tokenizing texts:  76%|███████▌  | 7572/10000 [00:31<00:09, 263.89it/s]Tokenizing texts:  76%|███████▌  | 7611/10000 [00:31<00:08, 290.31it/s]Tokenizing texts:  76%|███████▋  | 7641/10000 [00:31<00:08, 272.15it/s]Tokenizing texts:  77%|███████▋  | 7674/10000 [00:31<00:08, 286.26it/s]Tokenizing texts:  77%|███████▋  | 7704/10000 [00:32<00:07, 288.58it/s]Tokenizing texts:  77%|███████▋  | 7734/10000 [00:32<00:09, 248.39it/s]Tokenizing texts:  78%|███████▊  | 7761/10000 [00:32<00:09, 244.20it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 257.67it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 237.27it/s]Tokenizing texts:  78%|███████▊  | 7845/10000 [00:32<00:08, 245.75it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 257.20it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 254.95it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:32<00:07, 283.21it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:33<00:07, 279.85it/s]Tokenizing texts:  80%|███████▉  | 7999/10000 [00:33<00:06, 290.92it/s]Tokenizing texts:  80%|████████  | 8029/10000 [00:33<00:06, 287.36it/s]Tokenizing texts:  81%|████████  | 8058/10000 [00:33<00:11, 170.37it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 200.70it/s]Tokenizing texts:  81%|████████  | 8120/10000 [00:33<00:08, 209.46it/s]Tokenizing texts:  81%|████████▏ | 8149/10000 [00:33<00:08, 226.55it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:34<00:07, 238.20it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:34<00:07, 249.22it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 213.79it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 224.25it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 258.06it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 253.70it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:34<00:06, 257.39it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:34<00:06, 256.66it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:35<00:06, 232.25it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:35<00:06, 258.27it/s]Tokenizing texts:  85%|████████▍ | 8472/10000 [00:35<00:05, 264.05it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:35<00:05, 279.20it/s]Tokenizing texts:  85%|████████▌ | 8533/10000 [00:35<00:05, 261.52it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:35<00:05, 268.31it/s]Tokenizing texts:  86%|████████▌ | 8590/10000 [00:35<00:05, 263.25it/s]Tokenizing texts:  86%|████████▋ | 8625/10000 [00:35<00:04, 286.66it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:35<00:04, 285.77it/s]Tokenizing texts:  87%|████████▋ | 8688/10000 [00:35<00:04, 283.58it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:36<00:04, 284.97it/s]Tokenizing texts:  88%|████████▊ | 8757/10000 [00:36<00:04, 291.70it/s]Tokenizing texts:  88%|████████▊ | 8787/10000 [00:36<00:04, 292.67it/s]Tokenizing texts:  88%|████████▊ | 8817/10000 [00:36<00:05, 220.85it/s]Tokenizing texts:  88%|████████▊ | 8842/10000 [00:36<00:05, 222.65it/s]Tokenizing texts:  89%|████████▉ | 8876/10000 [00:36<00:04, 250.14it/s]Tokenizing texts:  89%|████████▉ | 8914/10000 [00:36<00:03, 274.98it/s]Tokenizing texts:  89%|████████▉ | 8946/10000 [00:36<00:03, 285.48it/s]Tokenizing texts:  90%|████████▉ | 8976/10000 [00:37<00:03, 282.71it/s]Tokenizing texts:  90%|█████████ | 9006/10000 [00:37<00:04, 246.25it/s]Tokenizing texts:  90%|█████████ | 9038/10000 [00:37<00:03, 262.86it/s]Tokenizing texts:  91%|█████████ | 9066/10000 [00:37<00:03, 252.69it/s]Tokenizing texts:  91%|█████████ | 9097/10000 [00:37<00:03, 267.57it/s]Tokenizing texts:  91%|█████████▏| 9125/10000 [00:37<00:03, 257.34it/s]Tokenizing texts:  92%|█████████▏| 9158/10000 [00:37<00:03, 271.20it/s]Tokenizing texts:  92%|█████████▏| 9191/10000 [00:37<00:02, 286.16it/s]Tokenizing texts:  92%|█████████▏| 9221/10000 [00:38<00:02, 284.38it/s]Tokenizing texts:  92%|█████████▎| 9250/10000 [00:38<00:02, 277.28it/s]Tokenizing texts:  93%|█████████▎| 9278/10000 [00:38<00:02, 248.69it/s]Tokenizing texts:  93%|█████████▎| 9309/10000 [00:38<00:02, 264.11it/s]Tokenizing texts:  93%|█████████▎| 9337/10000 [00:38<00:02, 246.68it/s]Tokenizing texts:  94%|█████████▎| 9363/10000 [00:38<00:03, 211.73it/s]Tokenizing texts:  94%|█████████▍| 9400/10000 [00:38<00:02, 249.20it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:38<00:02, 227.97it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:39<00:02, 232.62it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:39<00:02, 241.29it/s]Tokenizing texts:  95%|█████████▌| 9504/10000 [00:39<00:02, 183.49it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 221.97it/s]Tokenizing texts:  96%|█████████▌| 9566/10000 [00:39<00:01, 229.30it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 233.79it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 232.30it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 235.09it/s]Tokenizing texts:  97%|█████████▋| 9674/10000 [00:39<00:01, 241.34it/s]Tokenizing texts:  97%|█████████▋| 9699/10000 [00:40<00:01, 237.14it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:40<00:01, 239.52it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 207.91it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 216.44it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 261.91it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 254.83it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 260.54it/s]Tokenizing texts:  99%|█████████▉| 9907/10000 [00:40<00:00, 287.15it/s]Tokenizing texts:  99%|█████████▉| 9937/10000 [00:40<00:00, 284.01it/s]Tokenizing texts: 100%|█████████▉| 9969/10000 [00:41<00:00, 292.79it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 242.78it/s]
2025-12-09 11:49:25.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 12.022917747497559
2025-12-09 11:49:25.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 12.03929328918457
2025-12-09 11:49:26.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 12.031697273254395
2025-12-09 11:49:26.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 11.9603853225708
2025-12-09 11:49:26.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 12.022083282470703
2025-12-09 11:49:26.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 11.983308792114258
2025-12-09 11:49:26.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 11.94221305847168
2025-12-09 11:49:26.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 11.926393508911133
2025-12-09 11:49:26.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 11.912534713745117
2025-12-09 11:49:26.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 11.789511680603027
2025-12-09 11:49:26.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 11.833733558654785
2025-12-09 11:49:26.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 11.715140342712402
2025-12-09 11:49:26.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 11.670446395874023
2025-12-09 11:49:26.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 11.641057968139648
2025-12-09 11:49:26.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 11.525322914123535
2025-12-09 11:49:27.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 11.559477806091309
2025-12-09 11:49:27.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 11.37295150756836
2025-12-09 11:49:27.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 11.32907772064209
2025-12-09 11:49:27.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 11.142629623413086
2025-12-09 11:49:27.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 11.212306022644043
2025-12-09 11:49:27.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 11.152433395385742
2025-12-09 11:49:27.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 11.029457092285156
2025-12-09 11:49:27.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 11.05952262878418
2025-12-09 11:49:27.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 11.004185676574707
2025-12-09 11:49:27.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 11.053193092346191
2025-12-09 11:49:27.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 11.303510665893555
2025-12-09 11:49:27.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 10.967705726623535
2025-12-09 11:49:28.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 10.916887283325195
2025-12-09 11:49:28.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 10.690774917602539
2025-12-09 11:49:28.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 10.790489196777344
2025-12-09 11:49:28.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 10.883894920349121
2025-12-09 11:49:28.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 10.636125564575195
2025-12-09 11:49:28.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 10.789085388183594
2025-12-09 11:49:28.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 10.508096694946289
2025-12-09 11:49:28.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 10.67058277130127
2025-12-09 11:49:28.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 10.597228050231934
2025-12-09 11:49:28.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 10.308250427246094
2025-12-09 11:49:28.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 10.39227294921875
2025-12-09 11:49:28.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 10.377647399902344
2025-12-09 11:49:28.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 10.2415189743042
2025-12-09 11:49:29.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 10.13681697845459
2025-12-09 11:49:29.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 10.184930801391602
2025-12-09 11:49:29.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 10.11464786529541
2025-12-09 11:49:29.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 10.063512802124023
2025-12-09 11:49:29.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 9.947866439819336
2025-12-09 11:49:29.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 9.973384857177734
2025-12-09 11:49:29.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 9.760729789733887
2025-12-09 11:49:29.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 9.80138111114502
2025-12-09 11:49:29.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 9.841633796691895
2025-12-09 11:49:29.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 9.597935676574707
2025-12-09 11:49:29.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 9.641388893127441
2025-12-09 11:49:29.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 9.442887306213379
2025-12-09 11:49:30.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 9.244016647338867
2025-12-09 11:49:30.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 9.449249267578125
2025-12-09 11:49:30.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 9.2712984085083
2025-12-09 11:49:30.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 9.295279502868652
2025-12-09 11:49:30.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 9.213801383972168
2025-12-09 11:49:30.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 9.375447273254395
2025-12-09 11:49:30.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 8.85957145690918
2025-12-09 11:49:30.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 8.92824649810791
2025-12-09 11:49:30.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 8.835716247558594
2025-12-09 11:49:30.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 8.8471040725708
2025-12-09 11:49:30.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 8.641042709350586
2025-12-09 11:49:30.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 8.723389625549316
2025-12-09 11:49:30.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 8.779032707214355
2025-12-09 11:49:31.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 8.61290168762207
2025-12-09 11:49:31.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 8.319899559020996
2025-12-09 11:49:31.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 8.687915802001953
2025-12-09 11:49:31.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 8.70229434967041
2025-12-09 11:49:31.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 8.46846866607666
2025-12-09 11:49:31.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 8.25837230682373
2025-12-09 11:49:31.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 8.289531707763672
2025-12-09 11:49:31.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 8.456315994262695
2025-12-09 11:49:31.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 8.081035614013672
2025-12-09 11:49:31.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 8.089778900146484
2025-12-09 11:49:31.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 8.366904258728027
2025-12-09 11:49:31.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 8.236995697021484
2025-12-09 11:49:31.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 8.283830642700195
2025-12-09 11:49:32.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 8.00112533569336
2025-12-09 11:49:32.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 8.206353187561035
2025-12-09 11:49:32.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 8.083961486816406
2025-12-09 11:49:32.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 8.037602424621582
2025-12-09 11:49:32.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 7.384226322174072
2025-12-09 11:49:32.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 7.777473449707031
2025-12-09 11:49:32.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 8.31849479675293
2025-12-09 11:49:32.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 7.641660690307617
2025-12-09 11:49:32.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 7.975449085235596
2025-12-09 11:49:32.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 7.174251556396484
2025-12-09 11:49:32.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 7.996501922607422
2025-12-09 11:49:32.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 8.278509140014648
2025-12-09 11:49:32.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 8.074238777160645
2025-12-09 11:49:33.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 7.717060089111328
2025-12-09 11:49:33.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 8.062973976135254
2025-12-09 11:49:33.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 8.042778015136719
2025-12-09 11:49:33.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 8.040776252746582
2025-12-09 11:49:33.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 7.623143672943115
2025-12-09 11:49:33.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 7.953939437866211
2025-12-09 11:49:33.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 8.164855003356934
2025-12-09 11:49:33.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 7.768545150756836
2025-12-09 11:49:33.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 7.9188008308410645
2025-12-09 11:49:33.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999997217736103 Training loss: 8.20323657989502
2025-12-09 11:49:33.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029999988870945456 Training loss: 8.005054473876953
2025-12-09 11:49:33.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0002999997495963115 Training loss: 7.83929967880249
2025-12-09 11:49:33.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00029999955483798346 Training loss: 8.53234577178955
2025-12-09 11:49:34.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0002999993044345427 Training loss: 7.690252780914307
2025-12-09 11:49:34.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002999989983860821 Training loss: 7.840057373046875
2025-12-09 11:49:34.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.00029999863669271526 Training loss: 7.150750637054443
2025-12-09 11:49:34.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0002999982193545762 Training loss: 8.490509986877441
2025-12-09 11:49:34.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0002999977463718199 Training loss: 7.715825080871582
2025-12-09 11:49:34.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.00029999721774462174 Training loss: 7.819842338562012
2025-12-09 11:49:34.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029999663347317785 Training loss: 7.634561538696289
2025-12-09 11:49:34.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.00029999599355770497 Training loss: 7.623813152313232
2025-12-09 11:49:34.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0002999952979984405 Training loss: 7.507184028625488
2025-12-09 11:49:34.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.00029999454679564244 Training loss: 7.826843738555908
2025-12-09 11:49:34.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0002999937399495895 Training loss: 7.669567108154297
2025-12-09 11:49:34.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00029999287746058093 Training loss: 7.615144729614258
2025-12-09 11:49:35.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029999195932893676 Training loss: 7.569164276123047
2025-12-09 11:49:35.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.00029999098555499756 Training loss: 7.653378486633301
2025-12-09 11:49:35.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002999899561391246 Training loss: 7.626823902130127
2025-12-09 11:49:35.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.00029998887108169967 Training loss: 7.546725749969482
2025-12-09 11:49:35.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0002999877303831254 Training loss: 7.783524513244629
2025-12-09 11:49:35.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00029998653404382487 Training loss: 7.596975803375244
2025-12-09 11:49:35.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.000299985282064242 Training loss: 7.596611499786377
2025-12-09 11:49:35.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00029998397444484104 Training loss: 7.735917568206787
2025-12-09 11:49:35.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002999826111861073 Training loss: 7.653336524963379
2025-12-09 11:49:35.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00029998119228854625 Training loss: 7.614952087402344
2025-12-09 11:49:35.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0002999797177526845 Training loss: 7.855758190155029
2025-12-09 11:49:35.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.000299978187579069 Training loss: 7.687498569488525
2025-12-09 11:49:35.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0002999766017682673 Training loss: 7.477484226226807
2025-12-09 11:49:36.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029997496032086775 Training loss: 7.758425235748291
2025-12-09 11:49:36.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00029997326323747927 Training loss: 7.753204345703125
2025-12-09 11:49:36.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0002999715105187314 Training loss: 7.59666633605957
2025-12-09 11:49:36.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.00029996970216527436 Training loss: 7.760939121246338
2025-12-09 11:49:36.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.000299967838177779 Training loss: 7.668829441070557
2025-12-09 11:49:36.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.00029996591855693686 Training loss: 7.57655668258667
2025-12-09 11:49:36.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.00029996394330345996 Training loss: 7.664094924926758
2025-12-09 11:49:36.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002999619124180811 Training loss: 7.112219333648682
2025-12-09 11:49:36.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00029995982590155367 Training loss: 7.913653373718262
2025-12-09 11:49:36.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00029995768375465164 Training loss: 7.447478294372559
2025-12-09 11:49:36.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0002999554859781698 Training loss: 7.90937614440918
2025-12-09 11:49:36.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.00029995323257292337 Training loss: 7.654564380645752
2025-12-09 11:49:36.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0002999509235397483 Training loss: 7.392548561096191
2025-12-09 11:49:37.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.00029994855887950124 Training loss: 8.060029983520508
2025-12-09 11:49:37.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.00029994613859305933 Training loss: 7.40842866897583
2025-12-09 11:49:37.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0002999436626813204 Training loss: 7.742898464202881
2025-12-09 11:49:37.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.000299941131145203 Training loss: 7.9634904861450195
2025-12-09 11:49:37.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0002999385439856462 Training loss: 7.627131938934326
2025-12-09 11:49:37.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0002999359012036099 Training loss: 7.68714714050293
2025-12-09 11:49:37.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0002999332028000742 Training loss: 7.557322978973389
2025-12-09 11:49:37.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002999304487760404 Training loss: 7.566822052001953
2025-12-09 11:49:37.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.00029992763913253 Training loss: 7.989405632019043
2025-12-09 11:49:37.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00029992477387058537 Training loss: 7.92080020904541
2025-12-09 11:49:37.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0002999218529912694 Training loss: 7.893219470977783
2025-12-09 11:49:37.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00029991887649566564 Training loss: 8.26335334777832
2025-12-09 11:49:37.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.00029991584438487825 Training loss: 7.482118606567383
2025-12-09 11:49:38.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0002999127566600321 Training loss: 7.431668281555176
2025-12-09 11:49:38.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.00029990961332227264 Training loss: 7.479715347290039
2025-12-09 11:49:38.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002999064143727659 Training loss: 8.091959953308105
2025-12-09 11:49:38.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.00029990315981269863 Training loss: 7.562514781951904
2025-12-09 11:49:38.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0002998998496432781 Training loss: 7.498954772949219
2025-12-09 11:49:38.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0002998964838657324 Training loss: 7.412390232086182
2025-12-09 11:49:38.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0002998930624813101 Training loss: 7.695816516876221
2025-12-09 11:49:38.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.00029988958549128026 Training loss: 7.486027240753174
2025-12-09 11:49:38.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00029988605289693295 Training loss: 7.64178991317749
2025-12-09 11:49:38.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0002998824646995785 Training loss: 7.528872966766357
2025-12-09 11:49:38.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.00029987882090054817 Training loss: 7.4131693840026855
2025-12-09 11:49:38.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0002998751215011935 Training loss: 7.4649434089660645
2025-12-09 11:49:39.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.000299871366502887 Training loss: 7.277200698852539
2025-12-09 11:49:39.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00029986755590702164 Training loss: 7.118022918701172
2025-12-09 11:49:39.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.000299863689715011 Training loss: 7.506657600402832
2025-12-09 11:49:39.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0002998597679282893 Training loss: 6.9591898918151855
2025-12-09 11:49:39.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00029985579054831146 Training loss: 7.888956546783447
2025-12-09 11:49:39.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0002998517575765528 Training loss: 7.224789142608643
2025-12-09 11:49:39.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00029984766901450965 Training loss: 7.393390655517578
2025-12-09 11:49:39.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.00029984352486369867 Training loss: 7.5859761238098145
2025-12-09 11:49:39.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00029983932512565707 Training loss: 7.636523246765137
2025-12-09 11:49:39.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.00029983506980194296 Training loss: 7.344714164733887
2025-12-09 11:49:39.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00029983075889413493 Training loss: 7.284005165100098
2025-12-09 11:49:39.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00029982639240383214 Training loss: 7.697219371795654
2025-12-09 11:49:39.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.00029982197033265437 Training loss: 7.361255645751953
2025-12-09 11:49:40.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00029981749268224225 Training loss: 7.5401506423950195
2025-12-09 11:49:40.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00029981295945425665 Training loss: 7.259552955627441
2025-12-09 11:49:40.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.00029980837065037935 Training loss: 6.888531684875488
2025-12-09 11:49:40.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.00029980372627231265 Training loss: 7.325112342834473
2025-12-09 11:49:40.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00029979902632177945 Training loss: 7.1315131187438965
2025-12-09 11:49:40.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0002997942708005233 Training loss: 7.490108489990234
2025-12-09 11:49:40.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.00029978945971030835 Training loss: 7.44459342956543
2025-12-09 11:49:40.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0002997845930529194 Training loss: 8.354593276977539
2025-12-09 11:49:40.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.00029977967083016173 Training loss: 7.371376991271973
2025-12-09 11:49:40.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00029977469304386133 Training loss: 7.486782073974609
2025-12-09 11:49:40.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0002997696596958649 Training loss: 7.233062267303467
2025-12-09 11:49:40.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0002997645707880396 Training loss: 7.0738091468811035
2025-12-09 11:49:40.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0002997594263222733 Training loss: 7.612941265106201
2025-12-09 11:49:41.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.00029975422630047435 Training loss: 7.537703514099121
2025-12-09 11:49:41.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.00029974897072457187 Training loss: 7.292914867401123
2025-12-09 11:49:41.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0002997436595965154 Training loss: 7.52146053314209
2025-12-09 11:49:41.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0002997382929182754 Training loss: 7.222888469696045
2025-12-09 11:49:41.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00029973287069184255 Training loss: 7.2586493492126465
2025-12-09 11:49:41.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0002997273929192284 Training loss: 7.581826686859131
2025-12-09 11:49:41.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0002997218596024651 Training loss: 7.263091564178467
2025-12-09 11:49:41.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.00029971627074360516 Training loss: 7.02011251449585
2025-12-09 11:49:41.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00029971062634472203 Training loss: 7.642578125
2025-12-09 11:49:41.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00029970492640790956 Training loss: 7.052178859710693
2025-12-09 11:49:41.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0002996991709352822 Training loss: 7.293113708496094
2025-12-09 11:49:41.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0002996933599289751 Training loss: 7.299419403076172
2025-12-09 11:49:42.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.000299687493391144 Training loss: 7.398262023925781
2025-12-09 11:49:42.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00029968157132396507 Training loss: 7.6060895919799805
2025-12-09 11:49:42.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00029967559372963534 Training loss: 7.498603343963623
2025-12-09 11:49:42.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00029966956061037227 Training loss: 7.180474281311035
2025-12-09 11:49:42.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.00029966347196841393 Training loss: 7.59361457824707
2025-12-09 11:49:42.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.000299657327806019 Training loss: 7.484757423400879
2025-12-09 11:49:42.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0002996511281254668 Training loss: 7.336860179901123
2025-12-09 11:49:42.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0002996448729290572 Training loss: 7.502363681793213
2025-12-09 11:49:42.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.00029963856221911075 Training loss: 7.71257209777832
2025-12-09 11:49:42.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00029963219599796843 Training loss: 7.242525577545166
2025-12-09 11:49:42.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.000299625774267992 Training loss: 7.2464399337768555
2025-12-09 11:49:42.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0002996192970315636 Training loss: 7.1378655433654785
2025-12-09 11:49:42.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00029961276429108625 Training loss: 6.31089448928833
2025-12-09 11:49:43.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00029960617604898323 Training loss: 7.283717632293701
2025-12-09 11:49:43.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0002995995323076986 Training loss: 7.2384796142578125
2025-12-09 11:49:43.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.00029959283306969705 Training loss: 6.701837539672852
2025-12-09 11:49:43.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.00029958607833746375 Training loss: 7.37832498550415
2025-12-09 11:49:43.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0002995792681135045 Training loss: 7.7987895011901855
2025-12-09 11:49:43.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.00029957240240034564 Training loss: 7.271308898925781
2025-12-09 11:49:43.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0002995654812005342 Training loss: 7.36983585357666
2025-12-09 11:49:43.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0002995585045166376 Training loss: 7.008050441741943
2025-12-09 11:49:43.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00029955147235124417 Training loss: 7.186526298522949
2025-12-09 11:49:43.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00029954438470696247 Training loss: 7.276304721832275
2025-12-09 11:49:43.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0002995372415864218 Training loss: 7.2980170249938965
2025-12-09 11:49:43.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0002995300429922721 Training loss: 7.459540843963623
2025-12-09 11:49:43.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00029952278892718376 Training loss: 7.219966411590576
2025-12-09 11:49:44.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0002995154793938479 Training loss: 7.008509635925293
2025-12-09 11:49:44.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.00029950811439497606 Training loss: 7.380312442779541
2025-12-09 11:49:44.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0002995006939333004 Training loss: 7.123464107513428
2025-12-09 11:49:44.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.00029949321801157365 Training loss: 7.138940811157227
2025-12-09 11:49:44.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.00029948568663256927 Training loss: 7.8550190925598145
2025-12-09 11:49:44.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0002994780997990811 Training loss: 7.453029155731201
2025-12-09 11:49:44.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0002994704575139236 Training loss: 6.979371547698975
2025-12-09 11:49:44.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00029946275977993175 Training loss: 7.439810276031494
2025-12-09 11:49:44.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0002994550065999613 Training loss: 7.310916423797607
2025-12-09 11:49:44.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0002994471979768884 Training loss: 7.368133068084717
2025-12-09 11:49:44.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00029943933391360974 Training loss: 7.197159290313721
2025-12-09 11:49:44.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00029943141441304274 Training loss: 7.395552158355713
2025-12-09 11:49:44.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.00029942343947812517 Training loss: 6.909994125366211
2025-12-09 11:49:45.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0002994154091118156 Training loss: 7.093952655792236
2025-12-09 11:49:45.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0002994073233170929 Training loss: 7.159701347351074
2025-12-09 11:49:45.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00029939918209695676 Training loss: 6.964846134185791
2025-12-09 11:49:45.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0002993909854544273 Training loss: 7.227149963378906
2025-12-09 11:49:45.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00029938273339254515 Training loss: 7.2635416984558105
2025-12-09 11:49:45.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0002993744259143716 Training loss: 7.118897438049316
2025-12-09 11:49:45.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0002993660630229886 Training loss: 7.068315029144287
2025-12-09 11:49:45.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0002993576447214983 Training loss: 7.1756720542907715
2025-12-09 11:49:45.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0002993491710130237 Training loss: 7.512078285217285
2025-12-09 11:49:45.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00029934064190070836 Training loss: 7.155608177185059
2025-12-09 11:49:45.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00029933205738771624 Training loss: 7.242990493774414
2025-12-09 11:49:45.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0002993234174772319 Training loss: 7.165905952453613
2025-12-09 11:49:46.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00029931472217246057 Training loss: 6.726588726043701
2025-12-09 11:49:46.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0002993059714766278 Training loss: 7.088388919830322
2025-12-09 11:49:46.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00029929716539297993 Training loss: 6.9714674949646
2025-12-09 11:49:46.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00029928830392478376 Training loss: 6.74341344833374
2025-12-09 11:49:46.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0002992793870753265 Training loss: 7.154893398284912
2025-12-09 11:49:46.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0002992704148479161 Training loss: 6.987329959869385
2025-12-09 11:49:46.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00029926138724588097 Training loss: 7.281476020812988
2025-12-09 11:49:46.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.00029925230427257004 Training loss: 6.912214756011963
2025-12-09 11:49:46.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0002992431659313528 Training loss: 7.243417263031006
2025-12-09 11:49:46.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.00029923397222561933 Training loss: 7.840315818786621
2025-12-09 11:49:46.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0002992247231587802 Training loss: 7.213523864746094
2025-12-09 11:49:46.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00029921541873426647 Training loss: 7.886925220489502
2025-12-09 11:49:46.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00029920605895552985 Training loss: 7.107638835906982
2025-12-09 11:49:47.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0002991966438260425 Training loss: 7.42542839050293
2025-12-09 11:49:47.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0002991871733492971 Training loss: 7.299022197723389
2025-12-09 11:49:47.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00029917764752880697 Training loss: 7.3768768310546875
2025-12-09 11:49:47.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0002991680663681059 Training loss: 7.52036714553833
2025-12-09 11:49:47.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00029915842987074804 Training loss: 7.092085361480713
2025-12-09 11:49:47.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0002991487380403084 Training loss: 7.474972248077393
2025-12-09 11:49:47.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00029913899088038226 Training loss: 7.294279098510742
2025-12-09 11:49:47.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.00029912918839458555 Training loss: 7.094139099121094
2025-12-09 11:49:47.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00029911933058655464 Training loss: 7.124512195587158
2025-12-09 11:49:47.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.00029910941745994653 Training loss: 7.50665283203125
2025-12-09 11:49:47.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.00029909944901843863 Training loss: 7.116771697998047
2025-12-09 11:49:47.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0002990894252657289 Training loss: 6.9041900634765625
2025-12-09 11:49:47.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0002990793462055359 Training loss: 6.95803165435791
2025-12-09 11:49:48.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0002990692118415986 Training loss: 6.889223098754883
2025-12-09 11:49:48.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002990590221776765 Training loss: 7.020948886871338
2025-12-09 11:49:48.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0002990487772175497 Training loss: 7.208645343780518
2025-12-09 11:49:48.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00029903847696501876 Training loss: 6.96891975402832
2025-12-09 11:49:48.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.00029902812142390474 Training loss: 6.951238632202148
2025-12-09 11:49:48.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0002990177105980492 Training loss: 7.1861748695373535
2025-12-09 11:49:48.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.00029900724449131424 Training loss: 6.882845401763916
2025-12-09 11:49:48.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.00029899672310758243 Training loss: 6.836357116699219
2025-12-09 11:49:48.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0002989861464507569 Training loss: 6.654816627502441
2025-12-09 11:49:48.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0002989755145247613 Training loss: 7.014607906341553
2025-12-09 11:49:48.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.00029896482733353965 Training loss: 7.401691436767578
2025-12-09 11:49:48.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00029895408488105665 Training loss: 7.27256965637207
2025-12-09 11:49:49.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0002989432871712973 Training loss: 6.901540756225586
2025-12-09 11:49:49.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0002989324342082673 Training loss: 7.48282527923584
2025-12-09 11:49:49.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00029892152599599275 Training loss: 7.106666564941406
2025-12-09 11:49:49.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.00029891056253852026 Training loss: 7.835544109344482
2025-12-09 11:49:49.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0002988995438399169 Training loss: 7.236545562744141
2025-12-09 11:49:49.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0002988884699042702 Training loss: 7.30889368057251
2025-12-09 11:49:49.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002988773407356884 Training loss: 7.317384719848633
2025-12-09 11:49:49.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0002988661563382999 Training loss: 7.837449550628662
2025-12-09 11:49:49.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0002988549167162539 Training loss: 7.297603130340576
2025-12-09 11:49:49.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.00029884362187371986 Training loss: 6.668960094451904
2025-12-09 11:49:49.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0002988322718148878 Training loss: 7.190618515014648
2025-12-09 11:49:49.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0002988208665439683 Training loss: 7.102064609527588
2025-12-09 11:49:49.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0002988094060651923 Training loss: 6.860217571258545
2025-12-09 11:49:50.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0002987978903828114 Training loss: 6.934686183929443
2025-12-09 11:49:50.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.00029878631950109734 Training loss: 7.077159881591797
2025-12-09 11:49:50.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0002987746934243427 Training loss: 6.853156089782715
2025-12-09 11:49:50.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0002987630121568604 Training loss: 7.040361404418945
2025-12-09 11:49:50.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.00029875127570298376 Training loss: 7.695837497711182
2025-12-09 11:49:50.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0002987394840670666 Training loss: 6.96958589553833
2025-12-09 11:49:50.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0002987276372534834 Training loss: 6.9690327644348145
2025-12-09 11:49:50.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0002987157352666288 Training loss: 7.0918121337890625
2025-12-09 11:49:50.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0002987037781109182 Training loss: 6.972352504730225
2025-12-09 11:49:50.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00029869176579078714 Training loss: 7.3644609451293945
2025-12-09 11:49:50.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.000298679698310692 Training loss: 6.977267265319824
2025-12-09 11:49:50.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00029866757567510927 Training loss: 6.9114089012146
2025-12-09 11:49:50.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0002986553978885362 Training loss: 6.998930931091309
2025-12-09 11:49:51.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00029864316495549037 Training loss: 6.9258294105529785
2025-12-09 11:49:51.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0002986308768805097 Training loss: 7.066584587097168
2025-12-09 11:49:51.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00029861853366815275 Training loss: 7.1897101402282715
2025-12-09 11:49:51.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00029860613532299845 Training loss: 6.6911773681640625
2025-12-09 11:49:51.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00029859368184964624 Training loss: 7.052047252655029
2025-12-09 11:49:51.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00029858117325271585 Training loss: 7.251319885253906
2025-12-09 11:49:51.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.00029856860953684773 Training loss: 6.826042652130127
2025-12-09 11:49:51.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0002985559907067025 Training loss: 7.129032135009766
2025-12-09 11:49:51.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00029854331676696137 Training loss: 6.866272926330566
2025-12-09 11:49:51.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.000298530587722326 Training loss: 7.072092056274414
2025-12-09 11:49:51.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.00029851780357751853 Training loss: 6.937743186950684
2025-12-09 11:49:51.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00029850496433728136 Training loss: 7.320376396179199
2025-12-09 11:49:52.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0002984920700063775 Training loss: 7.310477256774902
2025-12-09 11:49:52.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.00029847912058959033 Training loss: 7.033052444458008
2025-12-09 11:49:52.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00029846611609172363 Training loss: 6.689600467681885
2025-12-09 11:49:52.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.00029845305651760175 Training loss: 6.80612325668335
2025-12-09 11:49:52.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00029843994187206933 Training loss: 7.201594829559326
2025-12-09 11:49:52.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0002984267721599915 Training loss: 7.283949375152588
2025-12-09 11:49:52.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0002984135473862538 Training loss: 7.17133092880249
2025-12-09 11:49:52.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0002984002675557622 Training loss: 6.637888431549072
2025-12-09 11:49:52.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0002983869326734432 Training loss: 7.073939800262451
2025-12-09 11:49:52.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0002983735427442434 Training loss: 6.9979119300842285
2025-12-09 11:49:52.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.00029836009777313026 Training loss: 7.105987071990967
2025-12-09 11:49:52.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.00029834659776509134 Training loss: 6.672767162322998
2025-12-09 11:49:52.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0002983330427251347 Training loss: 6.90500545501709
2025-12-09 11:49:53.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0002983194326582889 Training loss: 7.249108791351318
2025-12-09 11:49:53.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0002983057675696028 Training loss: 7.024021148681641
2025-12-09 11:49:53.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0002982920474641457 Training loss: 6.874941349029541
2025-12-09 11:49:53.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0002982782723470074 Training loss: 6.864948749542236
2025-12-09 11:49:53.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.000298264442223298 Training loss: 7.800896167755127
2025-12-09 11:49:53.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00029825055709814795 Training loss: 6.908851623535156
2025-12-09 11:49:53.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00029823661697670834 Training loss: 6.639284610748291
2025-12-09 11:49:53.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00029822262186415046 Training loss: 6.547501564025879
2025-12-09 11:49:53.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00029820857176566606 Training loss: 6.634566783905029
2025-12-09 11:49:53.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0002981944666864672 Training loss: 7.455486297607422
2025-12-09 11:49:53.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0002981803066317865 Training loss: 7.01628303527832
2025-12-09 11:49:53.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00029816609160687697 Training loss: 7.07353401184082
2025-12-09 11:49:53.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0002981518216170118 Training loss: 7.848316192626953
2025-12-09 11:49:54.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002981374966674848 Training loss: 6.669867038726807
2025-12-09 11:49:54.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.00029812311676361003 Training loss: 7.197988033294678
2025-12-09 11:49:54.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00029810868191072195 Training loss: 7.114689826965332
2025-12-09 11:49:54.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.00029809419211417553 Training loss: 6.891899108886719
2025-12-09 11:49:54.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0002980796473793459 Training loss: 7.423203468322754
2025-12-09 11:49:54.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0002980650477116288 Training loss: 7.0113348960876465
2025-12-09 11:49:54.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00029805039311644023 Training loss: 6.705874919891357
2025-12-09 11:49:54.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0002980356835992166 Training loss: 6.885561943054199
2025-12-09 11:49:54.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0002980209191654146 Training loss: 6.778318881988525
2025-12-09 11:49:54.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.00029800609982051147 Training loss: 7.150258541107178
2025-12-09 11:49:54.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0002979912255700046 Training loss: 6.888184547424316
2025-12-09 11:49:54.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.000297976296419412 Training loss: 6.918683052062988
2025-12-09 11:49:55.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00029796131237427186 Training loss: 6.73431921005249
2025-12-09 11:49:55.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00029794627344014276 Training loss: 7.07568359375
2025-12-09 11:49:55.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.00029793117962260366 Training loss: 6.878641605377197
2025-12-09 11:49:55.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.000297916030927254 Training loss: 6.944648742675781
2025-12-09 11:49:55.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0002979008273597133 Training loss: 6.976009368896484
2025-12-09 11:49:55.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0002978855689256218 Training loss: 6.81984806060791
2025-12-09 11:49:55.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.00029787025563063975 Training loss: 6.596701145172119
2025-12-09 11:49:55.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.0002978548874804479 Training loss: 7.0378098487854
2025-12-09 11:49:55.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.0002978394644807475 Training loss: 6.311950206756592
2025-12-09 11:49:55.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0002978239866372598 Training loss: 6.731938362121582
2025-12-09 11:49:55.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.00029780845395572673 Training loss: 7.114897727966309
2025-12-09 11:49:55.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0002977928664419104 Training loss: 7.123117446899414
2025-12-09 11:49:55.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0002977772241015933 Training loss: 6.946656227111816
2025-12-09 11:49:56.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.00029776152694057815 Training loss: 7.067821979522705
2025-12-09 11:49:56.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0002977457749646882 Training loss: 6.910171985626221
2025-12-09 11:49:56.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.00029772996817976693 Training loss: 6.5286688804626465
2025-12-09 11:49:56.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.00029771410659167806 Training loss: 7.132164478302002
2025-12-09 11:49:56.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.00029769819020630594 Training loss: 6.806880474090576
2025-12-09 11:49:56.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0002976822190295548 Training loss: 6.871652603149414
2025-12-09 11:49:56.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.00029766619306734963 Training loss: 7.210798263549805
2025-12-09 11:49:56.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0002976501123256355 Training loss: 7.180484771728516
2025-12-09 11:49:56.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.00029763397681037787 Training loss: 6.804178714752197
2025-12-09 11:49:56.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.00029761778652756245 Training loss: 7.081483364105225
2025-12-09 11:49:56.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.00029760154148319534 Training loss: 6.968852519989014
2025-12-09 11:49:56.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.000297585241683303 Training loss: 7.386040210723877
2025-12-09 11:49:56.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.00029756888713393213 Training loss: 6.483817100524902
2025-12-09 11:49:57.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.00029755247784114976 Training loss: 6.551835060119629
2025-12-09 11:49:57.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0002975360138110431 Training loss: 6.2974114418029785
2025-12-09 11:49:57.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.00029751949504972 Training loss: 6.825104713439941
2025-12-09 11:49:57.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0002975029215633082 Training loss: 6.7991414070129395
2025-12-09 11:49:57.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.000297486293357956 Training loss: 6.379952430725098
2025-12-09 11:49:57.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.00029746961043983206 Training loss: 6.70382833480835
2025-12-09 11:49:57.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00029745287281512505 Training loss: 6.691915035247803
2025-12-09 11:49:57.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0002974360804900442 Training loss: 7.042909145355225
2025-12-09 11:49:57.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0002974192334708189 Training loss: 6.677278518676758
2025-12-09 11:49:57.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.00029740233176369887 Training loss: 6.87652587890625
2025-12-09 11:49:57.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0002973853753749541 Training loss: 7.229581832885742
2025-12-09 11:49:57.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00029736836431087493 Training loss: 6.565544605255127
2025-12-09 11:49:58.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.00029735129857777183 Training loss: 6.83445405960083
2025-12-09 11:49:58.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.00029733417818197575 Training loss: 6.96191930770874
2025-12-09 11:49:58.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.00029731700312983776 Training loss: 6.621639728546143
2025-12-09 11:49:58.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0002972997734277293 Training loss: 6.774930953979492
2025-12-09 11:49:58.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.000297282489082042 Training loss: 6.855233669281006
2025-12-09 11:49:58.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00029726515009918786 Training loss: 6.657163619995117
2025-12-09 11:49:58.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0002972477564855991 Training loss: 7.0287652015686035
2025-12-09 11:49:58.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0002972303082477281 Training loss: 6.955630302429199
2025-12-09 11:49:58.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.00029721280539204774 Training loss: 6.642544746398926
2025-12-09 11:49:58.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.0002971952479250509 Training loss: 6.596696853637695
2025-12-09 11:49:58.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.000297177635853251 Training loss: 6.670385837554932
2025-12-09 11:49:58.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0002971599691831815 Training loss: 6.996053218841553
2025-12-09 11:49:58.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00029714224792139605 Training loss: 6.119178295135498
2025-12-09 11:49:59.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0002971244720744688 Training loss: 7.06197452545166
2025-12-09 11:49:59.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00029710664164899413 Training loss: 6.934283256530762
2025-12-09 11:49:59.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0002970887566515864 Training loss: 6.941514015197754
2025-12-09 11:49:59.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0002970708170888804 Training loss: 7.117506504058838
2025-12-09 11:49:59.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0002970528229675312 Training loss: 7.174221038818359
2025-12-09 11:49:59.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0002970347742942141 Training loss: 6.634354591369629
2025-12-09 11:49:59.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0002970166710756244 Training loss: 6.900686740875244
2025-12-09 11:49:59.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0002969985133184781 Training loss: 6.797189712524414
2025-12-09 11:49:59.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0002969803010295109 Training loss: 8.17842960357666
2025-12-09 11:49:59.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0002969620342154791 Training loss: 6.94358491897583
2025-12-09 11:49:59.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0002969437128831591 Training loss: 6.982273578643799
2025-12-09 11:49:59.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00029692533703934757 Training loss: 6.847466945648193
2025-12-09 11:49:59.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.00029690690669086127 Training loss: 6.719287395477295
2025-12-09 11:50:00.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0002968884218445374 Training loss: 6.8390326499938965
2025-12-09 11:50:00.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.0002968698825072332 Training loss: 6.677685737609863
2025-12-09 11:50:00.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0002968512886858262 Training loss: 7.026154518127441
2025-12-09 11:50:00.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.00029683264038721414 Training loss: 6.809884071350098
2025-12-09 11:50:00.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.00029681393761831485 Training loss: 7.0807952880859375
2025-12-09 11:50:00.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0002967951803860665 Training loss: 6.743804454803467
2025-12-09 11:50:00.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0002967763686974276 Training loss: 6.711183071136475
2025-12-09 11:50:00.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.00029675750255937647 Training loss: 6.981916904449463
2025-12-09 11:50:00.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.000296738581978912 Training loss: 6.4919328689575195
2025-12-09 11:50:00.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.00029671960696305304 Training loss: 6.96497917175293
2025-12-09 11:50:00.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.00029670057751883874 Training loss: 6.705583572387695
2025-12-09 11:50:00.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0002966814936533285 Training loss: 7.028931617736816
2025-12-09 11:50:00.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.00029666235537360175 Training loss: 6.883530139923096
2025-12-09 11:50:01.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.00029664316268675824 Training loss: 6.623773574829102
2025-12-09 11:50:01.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0002966239155999178 Training loss: 6.9324049949646
2025-12-09 11:50:01.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0002966046141202205 Training loss: 6.657166004180908
2025-12-09 11:50:01.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0002965852582548267 Training loss: 6.505704879760742
2025-12-09 11:50:01.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.00029656584801091663 Training loss: 6.881917953491211
2025-12-09 11:50:01.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.000296546383395691 Training loss: 6.634422779083252
2025-12-09 11:50:01.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00029652686441637054 Training loss: 6.759552001953125
2025-12-09 11:50:01.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.00029650729108019624 Training loss: 7.0146002769470215
2025-12-09 11:50:01.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0002964876633944291 Training loss: 6.7103705406188965
2025-12-09 11:50:01.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.00029646798136635034 Training loss: 7.206474781036377
2025-12-09 11:50:01.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.0002964482450032615 Training loss: 6.956394672393799
2025-12-09 11:50:01.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.00029642845431248406 Training loss: 6.673056602478027
2025-12-09 11:50:02.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0002964086093013597 Training loss: 6.8764967918396
2025-12-09 11:50:02.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.00029638870997725046 Training loss: 7.1491875648498535
2025-12-09 11:50:02.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00029636875634753824 Training loss: 6.782650470733643
2025-12-09 11:50:02.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00029634874841962525 Training loss: 6.443638324737549
2025-12-09 11:50:02.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00029632868620093375 Training loss: 7.563922882080078
2025-12-09 11:50:02.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0002963085696989063 Training loss: 6.857316970825195
2025-12-09 11:50:02.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.00029628839892100535 Training loss: 7.049998760223389
2025-12-09 11:50:02.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00029626817387471365 Training loss: 7.14515495300293
2025-12-09 11:50:02.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.00029624789456753417 Training loss: 6.77110481262207
2025-12-09 11:50:02.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.00029622756100698976 Training loss: 6.86090087890625
2025-12-09 11:50:02.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.0002962071732006237 Training loss: 6.503394603729248
2025-12-09 11:50:02.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00029618673115599896 Training loss: 6.8166279792785645
2025-12-09 11:50:02.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0002961662348806992 Training loss: 6.635363578796387
2025-12-09 11:50:03.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.00029614568438232766 Training loss: 6.664464473724365
2025-12-09 11:50:03.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.000296125079668508 Training loss: 6.486570358276367
2025-12-09 11:50:03.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.00029610442074688394 Training loss: 6.901461601257324
2025-12-09 11:50:03.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.00029608370762511935 Training loss: 6.554469108581543
2025-12-09 11:50:03.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.000296062940310898 Training loss: 6.614957809448242
2025-12-09 11:50:03.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.000296042118811924 Training loss: 6.725991725921631
2025-12-09 11:50:03.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0002960212431359215 Training loss: 6.900108814239502
2025-12-09 11:50:03.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00029600031329063463 Training loss: 7.153680324554443
2025-12-09 11:50:03.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0002959793292838277 Training loss: 6.660323619842529
2025-12-09 11:50:03.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0002959582911232853 Training loss: 6.590625286102295
2025-12-09 11:50:03.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.00029593719881681167 Training loss: 7.173040866851807
2025-12-09 11:50:03.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00029591605237223157 Training loss: 6.939190864562988
2025-12-09 11:50:03.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0002958948517973896 Training loss: 7.402918815612793
2025-12-09 11:50:04.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0002958735971001505 Training loss: 6.724474906921387
2025-12-09 11:50:04.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0002958522882883991 Training loss: 6.594705104827881
2025-12-09 11:50:04.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0002958309253700404 Training loss: 6.425965785980225
2025-12-09 11:50:04.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.00029580950835299914 Training loss: 6.970651149749756
2025-12-09 11:50:04.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0002957880372452206 Training loss: 7.637017250061035
2025-12-09 11:50:04.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0002957665120546697 Training loss: 7.455521106719971
2025-12-09 11:50:04.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0002957449327893317 Training loss: 6.461907863616943
2025-12-09 11:50:04.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.00029572329945721186 Training loss: 6.714443683624268
2025-12-09 11:50:04.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0002957016120663354 Training loss: 6.24020528793335
2025-12-09 11:50:04.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.00029567987062474767 Training loss: 6.6962151527404785
2025-12-09 11:50:04.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00029565807514051406 Training loss: 6.379500865936279
2025-12-09 11:50:04.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0002956362256217201 Training loss: 6.8755879402160645
2025-12-09 11:50:05.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0002956143220764711 Training loss: 7.04494047164917
2025-12-09 11:50:05.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.0002955923645128927 Training loss: 6.600071907043457
2025-12-09 11:50:05.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.00029557035293913044 Training loss: 7.010098457336426
2025-12-09 11:50:05.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.00029554828736334994 Training loss: 6.692595958709717
2025-12-09 11:50:05.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0002955261677937368 Training loss: 6.863259792327881
2025-12-09 11:50:05.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.00029550399423849673 Training loss: 6.352699279785156
2025-12-09 11:50:05.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0002954817667058554 Training loss: 6.4724321365356445
2025-12-09 11:50:05.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00029545948520405844 Training loss: 6.652831554412842
2025-12-09 11:50:05.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00029543714974137177 Training loss: 6.425192832946777
2025-12-09 11:50:05.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.000295414760326081 Training loss: 6.739644527435303
2025-12-09 11:50:05.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.0002953923169664919 Training loss: 6.768087387084961
2025-12-09 11:50:05.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00029536981967093033 Training loss: 6.982234001159668
2025-12-09 11:50:05.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.00029534726844774196 Training loss: 5.84339714050293
2025-12-09 11:50:06.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.00029532466330529277 Training loss: 6.096770286560059
2025-12-09 11:50:06.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.00029530200425196835 Training loss: 6.9405598640441895
2025-12-09 11:50:06.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.00029527929129617464 Training loss: 5.648384094238281
2025-12-09 11:50:06.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.00029525652444633736 Training loss: 6.780670166015625
2025-12-09 11:50:06.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0002952337037109023 Training loss: 6.37116813659668
2025-12-09 11:50:06.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.0002952108290983353 Training loss: 6.858189105987549
2025-12-09 11:50:06.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.00029518790061712204 Training loss: 6.550508975982666
2025-12-09 11:50:06.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0002951649182757683 Training loss: 6.638645172119141
2025-12-09 11:50:06.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.00029514188208279977 Training loss: 6.69959020614624
2025-12-09 11:50:06.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0002951187920467622 Training loss: 6.8729166984558105
2025-12-09 11:50:06.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0002950956481762213 Training loss: 6.840127944946289
2025-12-09 11:50:06.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0002950724504797626 Training loss: 6.349193572998047
2025-12-09 11:50:06.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0002950491989659918 Training loss: 6.79841947555542
2025-12-09 11:50:07.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.00029502589364353447 Training loss: 6.701127529144287
2025-12-09 11:50:07.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.00029500253452103615 Training loss: 7.182693004608154
2025-12-09 11:50:07.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.00029497912160716234 Training loss: 6.897195816040039
2025-12-09 11:50:07.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0002949556549105985 Training loss: 6.567178249359131
2025-12-09 11:50:07.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.00029493213444005 Training loss: 7.269718647003174
2025-12-09 11:50:07.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0002949085602042422 Training loss: 7.823538303375244
2025-12-09 11:50:07.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.00029488493221192043 Training loss: 6.525765895843506
2025-12-09 11:50:07.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.00029486125047184985 Training loss: 7.020506381988525
2025-12-09 11:50:07.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0002948375149928158 Training loss: 6.776528835296631
2025-12-09 11:50:07.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0002948137257836233 Training loss: 6.999644756317139
2025-12-09 11:50:07.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0002947898828530974 Training loss: 6.735950469970703
2025-12-09 11:50:07.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.000294765986210083 Training loss: 6.5086588859558105
2025-12-09 11:50:08.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0002947420358634451 Training loss: 7.060620307922363
2025-12-09 11:50:08.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.00029471803182206855 Training loss: 6.552618026733398
2025-12-09 11:50:08.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.000294693974094858 Training loss: 6.454341888427734
2025-12-09 11:50:08.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0002946698626907382 Training loss: 7.0145182609558105
2025-12-09 11:50:08.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.00029464569761865366 Training loss: 6.753801345825195
2025-12-09 11:50:08.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.0002946214788875689 Training loss: 6.971619129180908
2025-12-09 11:50:08.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.00029459720650646824 Training loss: 6.3994526863098145
2025-12-09 11:50:08.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.00029457288048435605 Training loss: 7.024234294891357
2025-12-09 11:50:08.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.00029454850083025644 Training loss: 6.470425128936768
2025-12-09 11:50:08.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0002945240675532136 Training loss: 7.077261924743652
2025-12-09 11:50:08.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0002944995806622914 Training loss: 6.905831336975098
2025-12-09 11:50:08.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0002944750401665738 Training loss: 6.554381847381592
2025-12-09 11:50:08.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00029445044607516447 Training loss: 6.639280796051025
2025-12-09 11:50:09.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.00029442579839718703 Training loss: 6.992579460144043
2025-12-09 11:50:09.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.0002944010971417851 Training loss: 6.784980773925781
2025-12-09 11:50:09.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.000294376342318122 Training loss: 6.531332015991211
2025-12-09 11:50:09.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.000294351533935381 Training loss: 6.609664440155029
2025-12-09 11:50:09.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.00029432667200276515 Training loss: 6.488472938537598
2025-12-09 11:50:09.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0002943017565294976 Training loss: 7.112024784088135
2025-12-09 11:50:09.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0002942767875248211 Training loss: 6.856496810913086
2025-12-09 11:50:09.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0002942517649979984 Training loss: 6.899887561798096
2025-12-09 11:50:09.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.00029422668895831203 Training loss: 6.329341411590576
2025-12-09 11:50:09.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00029420155941506447 Training loss: 6.565956115722656
2025-12-09 11:50:09.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00029417637637757797 Training loss: 6.3351569175720215
2025-12-09 11:50:09.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.00029415113985519463 Training loss: 6.880064010620117
2025-12-09 11:50:09.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0002941258498572764 Training loss: 6.735240459442139
2025-12-09 11:50:10.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.0002941005063932051 Training loss: 6.653906345367432
2025-12-09 11:50:10.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0002940751094723823 Training loss: 6.8442912101745605
2025-12-09 11:50:10.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.00029404965910422953 Training loss: 6.600853443145752
2025-12-09 11:50:10.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.00029402415529818804 Training loss: 7.222557544708252
2025-12-09 11:50:10.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.00029399859806371895 Training loss: 6.922097682952881
2025-12-09 11:50:10.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.0002939729874103032 Training loss: 7.018787860870361
2025-12-09 11:50:10.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.00029394732334744146 Training loss: 6.635349750518799
2025-12-09 11:50:10.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.00029392160588465434 Training loss: 6.588504314422607
2025-12-09 11:50:10.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0002938958350314823 Training loss: 6.642216682434082
2025-12-09 11:50:10.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.00029387001079748536 Training loss: 7.201101303100586
2025-12-09 11:50:10.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0002938441331922436 Training loss: 6.653697490692139
2025-12-09 11:50:10.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0002938182022253568 Training loss: 6.75634765625
2025-12-09 11:50:11.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0002937922179064445 Training loss: 6.732439994812012
2025-12-09 11:50:11.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.000293766180245146 Training loss: 6.878624439239502
2025-12-09 11:50:11.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.00029374008925112056 Training loss: 6.757541656494141
2025-12-09 11:50:11.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.00029371394493404705 Training loss: 6.581099510192871
2025-12-09 11:50:11.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.00029368774730362425 Training loss: 6.819985389709473
2025-12-09 11:50:11.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0002936614963695706 Training loss: 6.368555068969727
2025-12-09 11:50:11.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0002936351921416244 Training loss: 6.36779260635376
2025-12-09 11:50:11.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0002936088346295436 Training loss: 6.76501989364624
2025-12-09 11:50:11.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0002935824238431062 Training loss: 6.898863315582275
2025-12-09 11:50:11.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0002935559597921096 Training loss: 6.657962322235107
2025-12-09 11:50:11.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.00029352944248637117 Training loss: 6.896695137023926
2025-12-09 11:50:11.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00029350287193572806 Training loss: 6.728277683258057
2025-12-09 11:50:11.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.000293476248150037 Training loss: 7.098748683929443
2025-12-09 11:50:12.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.00029344957113917473 Training loss: 6.648132801055908
2025-12-09 11:50:12.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0002934228409130374 Training loss: 6.438867568969727
2025-12-09 11:50:12.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0002933960574815412 Training loss: 7.323878288269043
2025-12-09 11:50:12.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0002933692208546219 Training loss: 6.703121185302734
2025-12-09 11:50:12.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.00029334233104223506 Training loss: 6.689023017883301
2025-12-09 11:50:12.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00029331538805435595 Training loss: 7.045675754547119
2025-12-09 11:50:12.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.00029328839190097955 Training loss: 6.794045448303223
2025-12-09 11:50:12.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.00029326134259212064 Training loss: 6.451050281524658
2025-12-09 11:50:12.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0002932342401378136 Training loss: 6.675292491912842
2025-12-09 11:50:12.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0002932070845481126 Training loss: 6.417590618133545
2025-12-09 11:50:12.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.00029317987583309156 Training loss: 7.3681721687316895
2025-12-09 11:50:12.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.00029315261400284404 Training loss: 6.810523509979248
2025-12-09 11:50:12.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0002931252990674832 Training loss: 6.74442195892334
2025-12-09 11:50:13.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0002930979310371422 Training loss: 6.8791890144348145
2025-12-09 11:50:13.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0002930705099219736 Training loss: 6.514463901519775
2025-12-09 11:50:13.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0002930430357321498 Training loss: 6.531470775604248
2025-12-09 11:50:13.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0002930155084778629 Training loss: 6.357743263244629
2025-12-09 11:50:13.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0002929879281693246 Training loss: 6.264065742492676
2025-12-09 11:50:13.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0002929602948167663 Training loss: 5.8646464347839355
2025-12-09 11:50:13.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0002929326084304392 Training loss: 6.782566547393799
2025-12-09 11:50:13.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.00029290486902061396 Training loss: 6.816980838775635
2025-12-09 11:50:13.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0002928770765975811 Training loss: 6.637170314788818
2025-12-09 11:50:13.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.00029284923117165075 Training loss: 6.7876973152160645
2025-12-09 11:50:13.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0002928213327531526 Training loss: 6.417848110198975
2025-12-09 11:50:13.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.00029279338135243624 Training loss: 7.452795028686523
2025-12-09 11:50:13.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0002927653769798706 Training loss: 6.790030002593994
2025-12-09 11:50:14.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.00029273731964584446 Training loss: 6.493849754333496
2025-12-09 11:50:14.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.00029270920936076624 Training loss: 6.689105987548828
2025-12-09 11:50:14.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.00029268104613506396 Training loss: 6.50053596496582
2025-12-09 11:50:14.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.00029265282997918533 Training loss: 6.975635051727295
2025-12-09 11:50:14.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00029262456090359756 Training loss: 6.36379337310791
2025-12-09 11:50:14.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0002925962389187877 Training loss: 7.249479293823242
2025-12-09 11:50:14.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0002925678640352622 Training loss: 6.629060745239258
2025-12-09 11:50:14.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.00029253943626354734 Training loss: 6.683732032775879
2025-12-09 11:50:14.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0002925109556141889 Training loss: 7.426257133483887
2025-12-09 11:50:14.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0002924824220977523 Training loss: 6.822257995605469
2025-12-09 11:50:14.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0002924538357248226 Training loss: 6.664454460144043
2025-12-09 11:50:14.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.00029242519650600436 Training loss: 7.069363594055176
2025-12-09 11:50:15.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.0002923965044519219 Training loss: 6.289216995239258
2025-12-09 11:50:15.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0002923677595732191 Training loss: 6.475266456604004
2025-12-09 11:50:15.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0002923389618805593 Training loss: 6.549525737762451
2025-12-09 11:50:15.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.00029231011138462564 Training loss: 6.27994441986084
2025-12-09 11:50:15.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0002922812080961207 Training loss: 6.704629898071289
2025-12-09 11:50:15.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0002922522520257667 Training loss: 6.3486127853393555
2025-12-09 11:50:15.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0002922232431843054 Training loss: 6.625386714935303
2025-12-09 11:50:15.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.00029219418158249824 Training loss: 6.681668281555176
2025-12-09 11:50:15.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0002921650672311261 Training loss: 6.6633219718933105
2025-12-09 11:50:15.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0002921359001409895 Training loss: 6.711310863494873
2025-12-09 11:50:15.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0002921066803229085 Training loss: 6.673356533050537
2025-12-09 11:50:15.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.00029207740778772277 Training loss: 6.866294860839844
2025-12-09 11:50:15.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.00029204808254629146 Training loss: 6.758859634399414
2025-12-09 11:50:16.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.00029201870460949326 Training loss: 6.694269180297852
2025-12-09 11:50:16.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.00029198927398822657 Training loss: 6.560081481933594
2025-12-09 11:50:16.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0002919597906934092 Training loss: 6.610280990600586
2025-12-09 11:50:16.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0002919302547359785 Training loss: 7.726731777191162
2025-12-09 11:50:16.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0002919006661268914 Training loss: 6.866664886474609
2025-12-09 11:50:16.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0002918710248771243 Training loss: 6.6115193367004395
2025-12-09 11:50:16.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0002918413309976732 Training loss: 6.626249313354492
2025-12-09 11:50:16.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00029181158449955363 Training loss: 6.644842147827148
2025-12-09 11:50:16.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0002917817853938005 Training loss: 7.202512264251709
2025-12-09 11:50:16.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0002917519336914684 Training loss: 6.417269706726074
2025-12-09 11:50:16.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.00029172202940363145 Training loss: 6.740836143493652
2025-12-09 11:50:16.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0002916920725413831 Training loss: 6.077275276184082
2025-12-09 11:50:16.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.00029166206311583644 Training loss: 6.747816562652588
2025-12-09 11:50:17.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.000291632001138124 Training loss: 6.482982635498047
2025-12-09 11:50:17.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0002916018866193978 Training loss: 5.952305793762207
2025-12-09 11:50:17.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0002915717195708295 Training loss: 6.628898620605469
2025-12-09 11:50:17.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.00029154150000360995 Training loss: 6.3786163330078125
2025-12-09 11:50:17.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.00029151122792894985 Training loss: 6.881253242492676
2025-12-09 11:50:17.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.000291480903358079 Training loss: 6.617435932159424
2025-12-09 11:50:17.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00029145052630224696 Training loss: 6.695003032684326
2025-12-09 11:50:17.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0002914200967727227 Training loss: 7.191332817077637
2025-12-09 11:50:17.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00029138961478079455 Training loss: 6.811758995056152
2025-12-09 11:50:17.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.00029135908033777033 Training loss: 6.8850507736206055
2025-12-09 11:50:17.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.00029132849345497755 Training loss: 6.766876220703125
2025-12-09 11:50:17.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.00029129785414376276 Training loss: 6.493654727935791
2025-12-09 11:50:18.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.00029126716241549224 Training loss: 6.788607597351074
2025-12-09 11:50:18.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0002912364182815517 Training loss: 6.7405571937561035
2025-12-09 11:50:18.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.00029120562175334624 Training loss: 6.699353218078613
2025-12-09 11:50:18.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0002911747728423004 Training loss: 6.798404216766357
2025-12-09 11:50:18.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.00029114387155985814 Training loss: 6.732692241668701
2025-12-09 11:50:18.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0002911129179174828 Training loss: 6.592498302459717
2025-12-09 11:50:18.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.00029108191192665734 Training loss: 6.4325642585754395
2025-12-09 11:50:18.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.00029105085359888396 Training loss: 7.313146114349365
2025-12-09 11:50:18.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.00029101974294568425 Training loss: 6.778499603271484
2025-12-09 11:50:18.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0002909885799785993 Training loss: 5.925474643707275
2025-12-09 11:50:18.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0002909573647091897 Training loss: 7.056021213531494
2025-12-09 11:50:18.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.00029092609714903523 Training loss: 7.646312236785889
2025-12-09 11:50:18.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.00029089477730973517 Training loss: 7.285665512084961
2025-12-09 11:50:19.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.0002908634052029083 Training loss: 6.762640476226807
2025-12-09 11:50:19.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0002908319808401925 Training loss: 6.577089309692383
2025-12-09 11:50:19.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0002908005042332454 Training loss: 6.624037742614746
2025-12-09 11:50:19.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00029076897539374375 Training loss: 6.868714332580566
2025-12-09 11:50:19.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.00029073739433338377 Training loss: 6.671572208404541
2025-12-09 11:50:19.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.000290705761063881 Training loss: 6.546563148498535
2025-12-09 11:50:19.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.00029067407559697046 Training loss: 7.165592670440674
2025-12-09 11:50:19.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0002906423379444063 Training loss: 6.791481971740723
2025-12-09 11:50:19.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.00029061054811796243 Training loss: 6.666398048400879
2025-12-09 11:50:19.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0002905787061294317 Training loss: 6.694204330444336
2025-12-09 11:50:19.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.00029054681199062657 Training loss: 6.774204730987549
2025-12-09 11:50:19.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.00029051486571337877 Training loss: 6.8090596199035645
2025-12-09 11:50:19.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.00029048286730953924 Training loss: 6.506462097167969
2025-12-09 11:50:20.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0002904508167909785 Training loss: 6.654618740081787
2025-12-09 11:50:20.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.00029041871416958623 Training loss: 6.535301208496094
2025-12-09 11:50:20.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00029038655945727153 Training loss: 6.320193290710449
2025-12-09 11:50:20.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0002903543526659628 Training loss: 6.574721336364746
2025-12-09 11:50:20.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00029032209380760765 Training loss: 6.695728778839111
2025-12-09 11:50:20.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0002902897828941732 Training loss: 6.506008148193359
2025-12-09 11:50:20.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0002902574199376457 Training loss: 6.803055286407471
2025-12-09 11:50:20.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.00029022500495003086 Training loss: 6.829071998596191
2025-12-09 11:50:20.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0002901925379433536 Training loss: 6.476319789886475
2025-12-09 11:50:20.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0002901600189296581 Training loss: 6.542169570922852
2025-12-09 11:50:20.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.000290127447921008 Training loss: 6.9863600730896
2025-12-09 11:50:20.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.00029009482492948607 Training loss: 7.095929145812988
2025-12-09 11:50:21.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.00029006214996719437 Training loss: 6.917407512664795
2025-12-09 11:50:21.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0002900294230462543 Training loss: 6.540807723999023
2025-12-09 11:50:21.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.00028999664417880654 Training loss: 7.4326934814453125
2025-12-09 11:50:21.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.000289963813377011 Training loss: 6.614760398864746
2025-12-09 11:50:21.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0002899309306530469 Training loss: 6.843942165374756
2025-12-09 11:50:21.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.0002898979960191127 Training loss: 6.404439449310303
2025-12-09 11:50:21.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.000289865009487426 Training loss: 6.751867771148682
2025-12-09 11:50:21.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00028983197107022396 Training loss: 7.012430667877197
2025-12-09 11:50:21.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0002897988807797627 Training loss: 7.57473087310791
2025-12-09 11:50:21.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.00028976573862831757 Training loss: 6.704747676849365
2025-12-09 11:50:21.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0002897325446281834 Training loss: 6.527198791503906
2025-12-09 11:50:21.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0002896992987916741 Training loss: 6.707852363586426
2025-12-09 11:50:21.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.00028966600113112276 Training loss: 6.727788925170898
2025-12-09 11:50:22.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.00028963265165888187 Training loss: 7.038327217102051
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.35 GiB is free. Including non-PyTorch memory, this process has 90.89 GiB memory in use. Of the allocated memory 89.10 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:09, 144.44it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:57, 173.97it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:22, 120.91it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:10, 140.20it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:57, 171.77it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 211.14it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:53, 183.42it/s]Tokenizing texts:   2%|▏         | 172/10000 [00:01<00:54, 180.69it/s]Tokenizing texts:   2%|▏         | 196/10000 [00:01<00:50, 195.36it/s]Tokenizing texts:   2%|▏         | 218/10000 [00:01<00:48, 201.49it/s]Tokenizing texts:   2%|▏         | 239/10000 [00:01<00:48, 201.39it/s]Tokenizing texts:   3%|▎         | 260/10000 [00:01<00:49, 198.65it/s]Tokenizing texts:   3%|▎         | 291/10000 [00:01<00:43, 225.74it/s]Tokenizing texts:   3%|▎         | 314/10000 [00:01<00:43, 224.84it/s]Tokenizing texts:   3%|▎         | 337/10000 [00:01<00:53, 180.53it/s]Tokenizing texts:   4%|▎         | 362/10000 [00:01<00:49, 196.67it/s]Tokenizing texts:   4%|▍         | 384/10000 [00:02<00:53, 180.32it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:52, 183.88it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:52, 183.49it/s]Tokenizing texts:   4%|▍         | 445/10000 [00:02<00:52, 182.89it/s]Tokenizing texts:   5%|▍         | 467/10000 [00:02<00:49, 192.07it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:44, 215.95it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:40, 234.41it/s]Tokenizing texts:   6%|▌         | 550/10000 [00:02<00:51, 184.30it/s]Tokenizing texts:   6%|▌         | 579/10000 [00:03<00:45, 209.28it/s]Tokenizing texts:   6%|▌         | 603/10000 [00:03<00:45, 208.49it/s]Tokenizing texts:   6%|▋         | 626/10000 [00:03<00:47, 198.40it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 200.28it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:47, 198.20it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:49, 189.47it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 198.52it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 197.53it/s]Tokenizing texts:   8%|▊         | 766/10000 [00:03<00:44, 208.08it/s]Tokenizing texts:   8%|▊         | 788/10000 [00:04<00:46, 200.19it/s]Tokenizing texts:   8%|▊         | 812/10000 [00:04<00:43, 210.81it/s]Tokenizing texts:   8%|▊         | 834/10000 [00:04<00:43, 208.94it/s]Tokenizing texts:   9%|▊         | 859/10000 [00:04<00:41, 219.90it/s]Tokenizing texts:   9%|▉         | 882/10000 [00:04<00:42, 213.37it/s]Tokenizing texts:   9%|▉         | 909/10000 [00:04<00:39, 228.88it/s]Tokenizing texts:   9%|▉         | 939/10000 [00:04<00:36, 245.01it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:04<00:34, 265.26it/s]Tokenizing texts:  10%|█         | 1000/10000 [00:04<00:33, 269.26it/s]Tokenizing texts:  10%|█         | 1031/10000 [00:05<00:32, 278.83it/s]Tokenizing texts:  11%|█         | 1059/10000 [00:05<00:40, 221.32it/s]Tokenizing texts:  11%|█         | 1084/10000 [00:05<00:39, 224.35it/s]Tokenizing texts:  11%|█         | 1108/10000 [00:05<00:40, 220.92it/s]Tokenizing texts:  11%|█▏        | 1132/10000 [00:05<00:40, 220.22it/s]Tokenizing texts:  12%|█▏        | 1155/10000 [00:05<00:45, 193.59it/s]Tokenizing texts:  12%|█▏        | 1179/10000 [00:05<00:43, 204.43it/s]Tokenizing texts:  12%|█▏        | 1201/10000 [00:05<00:45, 191.93it/s]Tokenizing texts:  12%|█▏        | 1233/10000 [00:06<00:39, 224.43it/s]Tokenizing texts:  13%|█▎        | 1257/10000 [00:06<00:43, 198.85it/s]Tokenizing texts:  13%|█▎        | 1291/10000 [00:06<00:37, 233.39it/s]Tokenizing texts:  13%|█▎        | 1316/10000 [00:06<00:37, 229.71it/s]Tokenizing texts:  13%|█▎        | 1340/10000 [00:06<00:43, 199.87it/s]Tokenizing texts:  14%|█▎        | 1368/10000 [00:06<00:39, 216.15it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:40, 212.20it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:37, 229.10it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:34, 246.26it/s]Tokenizing texts:  15%|█▍        | 1487/10000 [00:07<00:31, 267.97it/s]Tokenizing texts:  15%|█▌        | 1515/10000 [00:07<00:32, 262.93it/s]Tokenizing texts:  15%|█▌        | 1542/10000 [00:07<00:34, 248.53it/s]Tokenizing texts:  16%|█▌        | 1574/10000 [00:07<00:31, 265.66it/s]Tokenizing texts:  16%|█▌        | 1603/10000 [00:07<00:34, 246.28it/s]Tokenizing texts:  16%|█▋        | 1629/10000 [00:07<00:40, 206.88it/s]Tokenizing texts:  17%|█▋        | 1656/10000 [00:07<00:37, 221.52it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:39, 211.86it/s]Tokenizing texts:  17%|█▋        | 1703/10000 [00:08<00:38, 215.37it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:34, 239.19it/s]Tokenizing texts:  18%|█▊        | 1760/10000 [00:08<00:34, 238.94it/s]Tokenizing texts:  18%|█▊        | 1786/10000 [00:08<00:34, 240.65it/s]Tokenizing texts:  18%|█▊        | 1820/10000 [00:08<00:30, 266.60it/s]Tokenizing texts:  18%|█▊        | 1848/10000 [00:08<00:38, 213.99it/s]Tokenizing texts:  19%|█▊        | 1872/10000 [00:08<00:37, 216.77it/s]Tokenizing texts:  19%|█▉        | 1896/10000 [00:08<00:36, 221.56it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:09<00:36, 222.26it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 192.72it/s]Tokenizing texts:  20%|█▉        | 1971/10000 [00:09<00:38, 206.47it/s]Tokenizing texts:  20%|█▉        | 1999/10000 [00:09<00:35, 224.86it/s]Tokenizing texts:  20%|██        | 2024/10000 [00:09<00:34, 231.54it/s]Tokenizing texts:  21%|██        | 2054/10000 [00:09<00:32, 244.51it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:38, 206.82it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:38, 207.08it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:37, 211.72it/s]Tokenizing texts:  22%|██▏       | 2150/10000 [00:10<00:39, 200.64it/s]Tokenizing texts:  22%|██▏       | 2179/10000 [00:10<00:35, 222.45it/s]Tokenizing texts:  22%|██▏       | 2209/10000 [00:10<00:32, 242.85it/s]Tokenizing texts:  22%|██▏       | 2239/10000 [00:10<00:30, 257.98it/s]Tokenizing texts:  23%|██▎       | 2269/10000 [00:10<00:29, 265.95it/s]Tokenizing texts:  23%|██▎       | 2297/10000 [00:10<00:30, 253.05it/s]Tokenizing texts:  23%|██▎       | 2323/10000 [00:10<00:35, 216.66it/s]Tokenizing texts:  23%|██▎       | 2349/10000 [00:10<00:33, 226.55it/s]Tokenizing texts:  24%|██▍       | 2382/10000 [00:11<00:30, 252.27it/s]Tokenizing texts:  24%|██▍       | 2413/10000 [00:11<00:28, 266.31it/s]Tokenizing texts:  24%|██▍       | 2441/10000 [00:11<00:31, 241.31it/s]Tokenizing texts:  25%|██▍       | 2473/10000 [00:11<00:29, 258.61it/s]Tokenizing texts:  25%|██▌       | 2500/10000 [00:11<00:31, 239.01it/s]Tokenizing texts:  25%|██▌       | 2531/10000 [00:11<00:29, 255.78it/s]Tokenizing texts:  26%|██▌       | 2558/10000 [00:11<00:29, 250.44it/s]Tokenizing texts:  26%|██▌       | 2584/10000 [00:11<00:35, 209.03it/s]Tokenizing texts:  26%|██▌       | 2607/10000 [00:12<00:35, 207.96it/s]Tokenizing texts:  26%|██▋       | 2629/10000 [00:12<00:36, 204.23it/s]Tokenizing texts:  27%|██▋       | 2660/10000 [00:12<00:32, 228.41it/s]Tokenizing texts:  27%|██▋       | 2684/10000 [00:12<00:35, 207.14it/s]Tokenizing texts:  27%|██▋       | 2706/10000 [00:12<00:37, 195.60it/s]Tokenizing texts:  27%|██▋       | 2737/10000 [00:12<00:32, 223.21it/s]Tokenizing texts:  28%|██▊       | 2768/10000 [00:12<00:29, 243.96it/s]Tokenizing texts:  28%|██▊       | 2794/10000 [00:12<00:39, 181.38it/s]Tokenizing texts:  28%|██▊       | 2827/10000 [00:13<00:34, 210.85it/s]Tokenizing texts:  29%|██▊       | 2851/10000 [00:13<00:34, 208.99it/s]Tokenizing texts:  29%|██▊       | 2874/10000 [00:13<00:34, 206.27it/s]Tokenizing texts:  29%|██▉       | 2903/10000 [00:13<00:31, 223.54it/s]Tokenizing texts:  29%|██▉       | 2935/10000 [00:13<00:28, 245.57it/s]Tokenizing texts:  30%|██▉       | 2961/10000 [00:13<00:30, 228.64it/s]Tokenizing texts:  30%|██▉       | 2985/10000 [00:13<00:30, 227.59it/s]Tokenizing texts:  30%|███       | 3021/10000 [00:13<00:26, 259.20it/s]Tokenizing texts:  30%|███       | 3048/10000 [00:14<00:30, 226.50it/s]Tokenizing texts:  31%|███       | 3081/10000 [00:14<00:27, 247.45it/s]Tokenizing texts:  31%|███       | 3107/10000 [00:14<00:28, 244.46it/s]Tokenizing texts:  31%|███▏      | 3139/10000 [00:14<00:26, 257.89it/s]Tokenizing texts:  32%|███▏      | 3166/10000 [00:14<00:27, 250.73it/s]Tokenizing texts:  32%|███▏      | 3196/10000 [00:14<00:25, 262.96it/s]Tokenizing texts:  32%|███▏      | 3223/10000 [00:14<00:25, 263.97it/s]Tokenizing texts:  32%|███▎      | 3250/10000 [00:14<00:26, 251.09it/s]Tokenizing texts:  33%|███▎      | 3280/10000 [00:14<00:25, 264.14it/s]Tokenizing texts:  33%|███▎      | 3307/10000 [00:14<00:25, 257.50it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:15<00:27, 244.13it/s]Tokenizing texts:  34%|███▎      | 3361/10000 [00:15<00:26, 252.35it/s]Tokenizing texts:  34%|███▍      | 3387/10000 [00:15<00:26, 248.77it/s]Tokenizing texts:  34%|███▍      | 3413/10000 [00:15<00:26, 251.26it/s]Tokenizing texts:  35%|███▍      | 3452/10000 [00:15<00:22, 290.79it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:15<00:22, 285.83it/s]Tokenizing texts:  35%|███▌      | 3516/10000 [00:15<00:26, 243.14it/s]Tokenizing texts:  35%|███▌      | 3542/10000 [00:15<00:28, 229.18it/s]Tokenizing texts:  36%|███▌      | 3569/10000 [00:16<00:27, 237.16it/s]Tokenizing texts:  36%|███▌      | 3600/10000 [00:16<00:25, 250.30it/s]Tokenizing texts:  36%|███▋      | 3626/10000 [00:16<00:25, 249.10it/s]Tokenizing texts:  37%|███▋      | 3652/10000 [00:16<00:25, 247.20it/s]Tokenizing texts:  37%|███▋      | 3678/10000 [00:16<00:25, 244.36it/s]Tokenizing texts:  37%|███▋      | 3705/10000 [00:16<00:25, 248.27it/s]Tokenizing texts:  37%|███▋      | 3731/10000 [00:16<00:24, 251.27it/s]Tokenizing texts:  38%|███▊      | 3757/10000 [00:16<00:28, 222.09it/s]Tokenizing texts:  38%|███▊      | 3781/10000 [00:16<00:27, 226.81it/s]Tokenizing texts:  38%|███▊      | 3805/10000 [00:17<00:30, 200.95it/s]Tokenizing texts:  38%|███▊      | 3837/10000 [00:17<00:26, 230.58it/s]Tokenizing texts:  39%|███▊      | 3864/10000 [00:17<00:25, 239.87it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:17<00:24, 249.34it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:26, 233.12it/s]Tokenizing texts:  39%|███▉      | 3945/10000 [00:17<00:26, 227.13it/s]Tokenizing texts:  40%|███▉      | 3976/10000 [00:17<00:24, 248.01it/s]Tokenizing texts:  40%|████      | 4002/10000 [00:17<00:26, 222.21it/s]Tokenizing texts:  40%|████      | 4030/10000 [00:17<00:25, 235.44it/s]Tokenizing texts:  41%|████      | 4056/10000 [00:18<00:24, 241.12it/s]Tokenizing texts:  41%|████      | 4089/10000 [00:18<00:22, 263.17it/s]Tokenizing texts:  41%|████      | 4121/10000 [00:18<00:21, 277.21it/s]Tokenizing texts:  42%|████▏     | 4150/10000 [00:18<00:21, 278.23it/s]Tokenizing texts:  42%|████▏     | 4179/10000 [00:18<00:21, 272.80it/s]Tokenizing texts:  42%|████▏     | 4214/10000 [00:18<00:19, 293.79it/s]Tokenizing texts:  42%|████▏     | 4244/10000 [00:18<00:24, 232.90it/s]Tokenizing texts:  43%|████▎     | 4270/10000 [00:18<00:24, 233.84it/s]Tokenizing texts:  43%|████▎     | 4297/10000 [00:19<00:23, 239.42it/s]Tokenizing texts:  43%|████▎     | 4333/10000 [00:19<00:20, 270.46it/s]Tokenizing texts:  44%|████▎     | 4362/10000 [00:19<00:20, 270.19it/s]Tokenizing texts:  44%|████▍     | 4393/10000 [00:19<00:20, 277.35it/s]Tokenizing texts:  44%|████▍     | 4424/10000 [00:19<00:19, 285.33it/s]Tokenizing texts:  45%|████▍     | 4454/10000 [00:19<00:21, 256.54it/s]Tokenizing texts:  45%|████▍     | 4481/10000 [00:19<00:21, 258.02it/s]Tokenizing texts:  45%|████▌     | 4508/10000 [00:19<00:23, 238.22it/s]Tokenizing texts:  45%|████▌     | 4533/10000 [00:19<00:22, 238.71it/s]Tokenizing texts:  46%|████▌     | 4563/10000 [00:20<00:21, 253.89it/s]Tokenizing texts:  46%|████▌     | 4589/10000 [00:20<00:22, 242.32it/s]Tokenizing texts:  46%|████▌     | 4614/10000 [00:20<00:23, 233.20it/s]Tokenizing texts:  47%|████▋     | 4655/10000 [00:20<00:19, 277.73it/s]Tokenizing texts:  47%|████▋     | 4688/10000 [00:20<00:18, 287.22it/s]Tokenizing texts:  47%|████▋     | 4724/10000 [00:20<00:17, 306.57it/s]Tokenizing texts:  48%|████▊     | 4756/10000 [00:20<00:19, 274.35it/s]Tokenizing texts:  48%|████▊     | 4785/10000 [00:20<00:21, 243.79it/s]Tokenizing texts:  48%|████▊     | 4813/10000 [00:20<00:20, 252.55it/s]Tokenizing texts:  48%|████▊     | 4846/10000 [00:21<00:19, 268.59it/s]Tokenizing texts:  49%|████▉     | 4875/10000 [00:21<00:19, 263.76it/s]Tokenizing texts:  49%|████▉     | 4909/10000 [00:21<00:18, 277.19it/s]Tokenizing texts:  49%|████▉     | 4938/10000 [00:21<00:18, 274.02it/s]Tokenizing texts:  50%|████▉     | 4966/10000 [00:21<00:19, 254.06it/s]Tokenizing texts:  50%|█████     | 5004/10000 [00:21<00:17, 287.15it/s]Tokenizing texts:  50%|█████     | 5034/10000 [00:21<00:18, 271.99it/s]Tokenizing texts:  51%|█████     | 5062/10000 [00:21<00:19, 255.25it/s]Tokenizing texts:  51%|█████     | 5089/10000 [00:22<00:21, 232.57it/s]Tokenizing texts:  51%|█████     | 5117/10000 [00:22<00:20, 243.99it/s]Tokenizing texts:  51%|█████▏    | 5143/10000 [00:22<00:20, 233.18it/s]Tokenizing texts:  52%|█████▏    | 5170/10000 [00:22<00:20, 241.00it/s]Tokenizing texts:  52%|█████▏    | 5197/10000 [00:22<00:19, 246.87it/s]Tokenizing texts:  52%|█████▏    | 5223/10000 [00:22<00:20, 231.17it/s]Tokenizing texts:  53%|█████▎    | 5257/10000 [00:22<00:18, 260.22it/s]Tokenizing texts:  53%|█████▎    | 5284/10000 [00:22<00:19, 241.72it/s]Tokenizing texts:  53%|█████▎    | 5318/10000 [00:22<00:17, 267.44it/s]Tokenizing texts:  54%|█████▎    | 5351/10000 [00:23<00:16, 279.66it/s]Tokenizing texts:  54%|█████▍    | 5383/10000 [00:23<00:15, 290.41it/s]Tokenizing texts:  54%|█████▍    | 5413/10000 [00:23<00:15, 288.60it/s]Tokenizing texts:  54%|█████▍    | 5443/10000 [00:23<00:17, 262.29it/s]Tokenizing texts:  55%|█████▍    | 5473/10000 [00:23<00:16, 269.45it/s]Tokenizing texts:  55%|█████▌    | 5501/10000 [00:23<00:21, 204.95it/s]Tokenizing texts:  55%|█████▌    | 5537/10000 [00:23<00:18, 236.90it/s]Tokenizing texts:  56%|█████▌    | 5564/10000 [00:23<00:18, 243.73it/s]Tokenizing texts:  56%|█████▌    | 5591/10000 [00:24<00:18, 244.60it/s]Tokenizing texts:  56%|█████▋    | 5625/10000 [00:24<00:16, 268.64it/s]Tokenizing texts:  57%|█████▋    | 5662/10000 [00:24<00:14, 294.52it/s]Tokenizing texts:  57%|█████▋    | 5693/10000 [00:24<00:14, 296.29it/s]Tokenizing texts:  57%|█████▋    | 5724/10000 [00:24<00:14, 294.78it/s]Tokenizing texts:  58%|█████▊    | 5755/10000 [00:24<00:16, 256.76it/s]Tokenizing texts:  58%|█████▊    | 5782/10000 [00:24<00:17, 240.11it/s]Tokenizing texts:  58%|█████▊    | 5807/10000 [00:24<00:19, 215.68it/s]Tokenizing texts:  58%|█████▊    | 5830/10000 [00:25<00:20, 204.26it/s]Tokenizing texts:  59%|█████▊    | 5859/10000 [00:25<00:18, 225.05it/s]Tokenizing texts:  59%|█████▉    | 5889/10000 [00:25<00:16, 242.55it/s]Tokenizing texts:  59%|█████▉    | 5915/10000 [00:25<00:17, 236.82it/s]Tokenizing texts:  59%|█████▉    | 5940/10000 [00:25<00:17, 235.31it/s]Tokenizing texts:  60%|█████▉    | 5966/10000 [00:25<00:16, 241.98it/s]Tokenizing texts:  60%|█████▉    | 5992/10000 [00:25<00:16, 239.99it/s]Tokenizing texts:  60%|██████    | 6017/10000 [00:25<00:17, 231.91it/s]Tokenizing texts:  61%|██████    | 6051/10000 [00:25<00:15, 259.91it/s]Tokenizing texts:  61%|██████    | 6078/10000 [00:25<00:15, 260.28it/s]Tokenizing texts:  61%|██████    | 6105/10000 [00:26<00:15, 254.38it/s]Tokenizing texts:  61%|██████▏   | 6138/10000 [00:26<00:14, 273.20it/s]Tokenizing texts:  62%|██████▏   | 6166/10000 [00:26<00:14, 272.87it/s]Tokenizing texts:  62%|██████▏   | 6194/10000 [00:26<00:14, 271.49it/s]Tokenizing texts:  62%|██████▏   | 6222/10000 [00:26<00:15, 239.77it/s]Tokenizing texts:  63%|██████▎   | 6252/10000 [00:26<00:14, 255.73it/s]Tokenizing texts:  63%|██████▎   | 6279/10000 [00:26<00:15, 246.03it/s]Tokenizing texts:  63%|██████▎   | 6305/10000 [00:26<00:15, 241.67it/s]Tokenizing texts:  63%|██████▎   | 6330/10000 [00:27<00:16, 219.48it/s]Tokenizing texts:  64%|██████▎   | 6357/10000 [00:27<00:15, 230.52it/s]Tokenizing texts:  64%|██████▍   | 6386/10000 [00:27<00:14, 242.36it/s]Tokenizing texts:  64%|██████▍   | 6414/10000 [00:27<00:14, 251.51it/s]Tokenizing texts:  64%|██████▍   | 6442/10000 [00:27<00:14, 252.35it/s]Tokenizing texts:  65%|██████▍   | 6468/10000 [00:27<00:14, 246.17it/s]Tokenizing texts:  65%|██████▍   | 6493/10000 [00:27<00:14, 245.01it/s]Tokenizing texts:  65%|██████▌   | 6518/10000 [00:27<00:15, 227.11it/s]Tokenizing texts:  65%|██████▌   | 6549/10000 [00:27<00:13, 249.44it/s]Tokenizing texts:  66%|██████▌   | 6587/10000 [00:27<00:12, 282.97it/s]Tokenizing texts:  66%|██████▌   | 6616/10000 [00:28<00:12, 262.04it/s]Tokenizing texts:  66%|██████▋   | 6645/10000 [00:28<00:12, 269.20it/s]Tokenizing texts:  67%|██████▋   | 6673/10000 [00:28<00:13, 254.46it/s]Tokenizing texts:  67%|██████▋   | 6699/10000 [00:28<00:14, 231.06it/s]Tokenizing texts:  67%|██████▋   | 6731/10000 [00:28<00:12, 253.65it/s]Tokenizing texts:  68%|██████▊   | 6758/10000 [00:28<00:13, 245.13it/s]Tokenizing texts:  68%|██████▊   | 6797/10000 [00:28<00:11, 281.85it/s]Tokenizing texts:  68%|██████▊   | 6829/10000 [00:28<00:10, 290.80it/s]Tokenizing texts:  69%|██████▊   | 6859/10000 [00:29<00:10, 289.64it/s]Tokenizing texts:  69%|██████▉   | 6889/10000 [00:29<00:11, 275.00it/s]Tokenizing texts:  69%|██████▉   | 6917/10000 [00:29<00:14, 214.35it/s]Tokenizing texts:  69%|██████▉   | 6944/10000 [00:29<00:13, 223.87it/s]Tokenizing texts:  70%|██████▉   | 6972/10000 [00:29<00:12, 237.33it/s]Tokenizing texts:  70%|██████▉   | 6998/10000 [00:29<00:12, 240.74it/s]Tokenizing texts:  70%|███████   | 7024/10000 [00:29<00:12, 240.19it/s]Tokenizing texts:  70%|███████   | 7049/10000 [00:29<00:12, 232.93it/s]Tokenizing texts:  71%|███████   | 7073/10000 [00:30<00:14, 209.07it/s]Tokenizing texts:  71%|███████   | 7096/10000 [00:30<00:13, 214.14it/s]Tokenizing texts:  71%|███████   | 7120/10000 [00:30<00:13, 220.31it/s]Tokenizing texts:  71%|███████▏  | 7148/10000 [00:30<00:12, 230.26it/s]Tokenizing texts:  72%|███████▏  | 7173/10000 [00:30<00:12, 235.38it/s]Tokenizing texts:  72%|███████▏  | 7202/10000 [00:30<00:11, 247.65it/s]Tokenizing texts:  72%|███████▏  | 7228/10000 [00:30<00:11, 248.25it/s]Tokenizing texts:  73%|███████▎  | 7253/10000 [00:30<00:11, 245.95it/s]Tokenizing texts:  73%|███████▎  | 7297/10000 [00:30<00:09, 296.79it/s]Tokenizing texts:  73%|███████▎  | 7331/10000 [00:30<00:08, 304.80it/s]Tokenizing texts:  74%|███████▎  | 7362/10000 [00:31<00:09, 271.64it/s]Tokenizing texts:  74%|███████▍  | 7390/10000 [00:31<00:10, 259.48it/s]Tokenizing texts:  74%|███████▍  | 7417/10000 [00:31<00:11, 218.54it/s]Tokenizing texts:  74%|███████▍  | 7442/10000 [00:31<00:12, 202.58it/s]Tokenizing texts:  75%|███████▍  | 7473/10000 [00:31<00:11, 227.81it/s]Tokenizing texts:  75%|███████▌  | 7507/10000 [00:31<00:09, 255.19it/s]Tokenizing texts:  75%|███████▌  | 7536/10000 [00:31<00:09, 263.23it/s]Tokenizing texts:  76%|███████▌  | 7564/10000 [00:31<00:09, 256.02it/s]Tokenizing texts:  76%|███████▌  | 7602/10000 [00:32<00:08, 288.85it/s]Tokenizing texts:  76%|███████▋  | 7632/10000 [00:32<00:08, 267.42it/s]Tokenizing texts:  77%|███████▋  | 7662/10000 [00:32<00:08, 274.60it/s]Tokenizing texts:  77%|███████▋  | 7695/10000 [00:32<00:07, 288.38it/s]Tokenizing texts:  77%|███████▋  | 7725/10000 [00:32<00:08, 255.08it/s]Tokenizing texts:  78%|███████▊  | 7752/10000 [00:32<00:08, 256.09it/s]Tokenizing texts:  78%|███████▊  | 7779/10000 [00:32<00:09, 238.63it/s]Tokenizing texts:  78%|███████▊  | 7804/10000 [00:32<00:09, 225.48it/s]Tokenizing texts:  78%|███████▊  | 7834/10000 [00:33<00:08, 242.12it/s]Tokenizing texts:  79%|███████▊  | 7865/10000 [00:33<00:08, 255.77it/s]Tokenizing texts:  79%|███████▉  | 7892/10000 [00:33<00:08, 236.75it/s]Tokenizing texts:  79%|███████▉  | 7928/10000 [00:33<00:07, 268.33it/s]Tokenizing texts:  80%|███████▉  | 7958/10000 [00:33<00:07, 274.26it/s]Tokenizing texts:  80%|███████▉  | 7991/10000 [00:33<00:07, 286.07it/s]Tokenizing texts:  80%|████████  | 8021/10000 [00:33<00:07, 281.58it/s]Tokenizing texts:  80%|████████  | 8050/10000 [00:33<00:08, 242.46it/s]Tokenizing texts:  81%|████████  | 8076/10000 [00:34<00:10, 177.52it/s]Tokenizing texts:  81%|████████  | 8104/10000 [00:34<00:09, 197.89it/s]Tokenizing texts:  81%|████████▏ | 8129/10000 [00:34<00:08, 208.70it/s]Tokenizing texts:  82%|████████▏ | 8157/10000 [00:34<00:08, 224.86it/s]Tokenizing texts:  82%|████████▏ | 8184/10000 [00:34<00:07, 230.67it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:34<00:07, 240.65it/s]Tokenizing texts:  82%|████████▏ | 8238/10000 [00:34<00:08, 205.31it/s]Tokenizing texts:  83%|████████▎ | 8261/10000 [00:34<00:08, 210.92it/s]Tokenizing texts:  83%|████████▎ | 8299/10000 [00:35<00:06, 252.92it/s]Tokenizing texts:  83%|████████▎ | 8326/10000 [00:35<00:06, 247.98it/s]Tokenizing texts:  84%|████████▎ | 8353/10000 [00:35<00:06, 252.30it/s]Tokenizing texts:  84%|████████▍ | 8380/10000 [00:35<00:06, 254.83it/s]Tokenizing texts:  84%|████████▍ | 8407/10000 [00:35<00:06, 245.48it/s]Tokenizing texts:  84%|████████▍ | 8432/10000 [00:35<00:06, 237.95it/s]Tokenizing texts:  85%|████████▍ | 8464/10000 [00:35<00:05, 259.15it/s]Tokenizing texts:  85%|████████▍ | 8492/10000 [00:35<00:05, 264.94it/s]Tokenizing texts:  85%|████████▌ | 8519/10000 [00:35<00:05, 262.88it/s]Tokenizing texts:  85%|████████▌ | 8547/10000 [00:35<00:05, 266.40it/s]Tokenizing texts:  86%|████████▌ | 8574/10000 [00:36<00:05, 254.10it/s]Tokenizing texts:  86%|████████▌ | 8602/10000 [00:36<00:05, 261.05it/s]Tokenizing texts:  86%|████████▋ | 8638/10000 [00:36<00:04, 288.87it/s]Tokenizing texts:  87%|████████▋ | 8668/10000 [00:36<00:04, 276.40it/s]Tokenizing texts:  87%|████████▋ | 8701/10000 [00:36<00:04, 290.70it/s]Tokenizing texts:  87%|████████▋ | 8731/10000 [00:36<00:04, 290.55it/s]Tokenizing texts:  88%|████████▊ | 8761/10000 [00:36<00:04, 286.28it/s]Tokenizing texts:  88%|████████▊ | 8790/10000 [00:36<00:04, 286.29it/s]Tokenizing texts:  88%|████████▊ | 8819/10000 [00:37<00:05, 214.11it/s]Tokenizing texts:  88%|████████▊ | 8844/10000 [00:37<00:05, 215.75it/s]Tokenizing texts:  89%|████████▉ | 8878/10000 [00:37<00:04, 245.76it/s]Tokenizing texts:  89%|████████▉ | 8914/10000 [00:37<00:04, 267.82it/s]Tokenizing texts:  89%|████████▉ | 8945/10000 [00:37<00:03, 278.52it/s]Tokenizing texts:  90%|████████▉ | 8975/10000 [00:37<00:03, 275.51it/s]Tokenizing texts:  90%|█████████ | 9004/10000 [00:37<00:04, 239.63it/s]Tokenizing texts:  90%|█████████ | 9035/10000 [00:37<00:03, 257.24it/s]Tokenizing texts:  91%|█████████ | 9062/10000 [00:37<00:03, 247.41it/s]Tokenizing texts:  91%|█████████ | 9091/10000 [00:38<00:03, 257.68it/s]Tokenizing texts:  91%|█████████ | 9119/10000 [00:38<00:03, 262.32it/s]Tokenizing texts:  91%|█████████▏| 9149/10000 [00:38<00:03, 271.68it/s]Tokenizing texts:  92%|█████████▏| 9178/10000 [00:38<00:02, 274.94it/s]Tokenizing texts:  92%|█████████▏| 9206/10000 [00:38<00:02, 276.28it/s]Tokenizing texts:  92%|█████████▏| 9234/10000 [00:38<00:02, 267.11it/s]Tokenizing texts:  93%|█████████▎| 9262/10000 [00:38<00:02, 267.44it/s]Tokenizing texts:  93%|█████████▎| 9289/10000 [00:38<00:02, 245.95it/s]Tokenizing texts:  93%|█████████▎| 9316/10000 [00:38<00:03, 226.86it/s]Tokenizing texts:  93%|█████████▎| 9348/10000 [00:39<00:02, 249.09it/s]Tokenizing texts:  94%|█████████▎| 9374/10000 [00:39<00:02, 216.02it/s]Tokenizing texts:  94%|█████████▍| 9403/10000 [00:39<00:02, 226.16it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:39<00:02, 225.71it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:39<00:02, 230.20it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:39<00:02, 238.72it/s]Tokenizing texts:  95%|█████████▌| 9504/10000 [00:39<00:02, 179.34it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 218.31it/s]Tokenizing texts:  96%|█████████▌| 9565/10000 [00:40<00:01, 224.19it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:40<00:01, 229.43it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:40<00:01, 228.01it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:40<00:01, 230.25it/s]Tokenizing texts:  97%|█████████▋| 9673/10000 [00:40<00:01, 234.27it/s]Tokenizing texts:  97%|█████████▋| 9697/10000 [00:40<00:01, 233.75it/s]Tokenizing texts:  97%|█████████▋| 9721/10000 [00:40<00:01, 232.45it/s]Tokenizing texts:  97%|█████████▋| 9745/10000 [00:40<00:01, 211.90it/s]Tokenizing texts:  98%|█████████▊| 9767/10000 [00:40<00:01, 206.76it/s]Tokenizing texts:  98%|█████████▊| 9803/10000 [00:41<00:00, 246.16it/s]Tokenizing texts:  98%|█████████▊| 9835/10000 [00:41<00:00, 249.72it/s]Tokenizing texts:  99%|█████████▊| 9867/10000 [00:41<00:00, 265.49it/s]Tokenizing texts:  99%|█████████▉| 9897/10000 [00:41<00:00, 274.87it/s]Tokenizing texts:  99%|█████████▉| 9925/10000 [00:41<00:00, 273.72it/s]Tokenizing texts: 100%|█████████▉| 9955/10000 [00:41<00:00, 280.52it/s]Tokenizing texts: 100%|█████████▉| 9992/10000 [00:41<00:00, 305.08it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 239.40it/s]
2025-12-09 11:51:23.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 12.010464668273926
2025-12-09 11:51:23.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 12.023785591125488
2025-12-09 11:51:23.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 11.970541000366211
2025-12-09 11:51:23.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 12.017266273498535
2025-12-09 11:51:23.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 11.973002433776855
2025-12-09 11:51:23.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 11.920866012573242
2025-12-09 11:51:23.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 11.868256568908691
2025-12-09 11:51:23.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 11.734930992126465
2025-12-09 11:51:23.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 11.676116943359375
2025-12-09 11:51:24.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 11.459640502929688
2025-12-09 11:51:24.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 11.305469512939453
2025-12-09 11:51:24.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 11.24527359008789
2025-12-09 11:51:24.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 11.181814193725586
2025-12-09 11:51:24.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 11.163555145263672
2025-12-09 11:51:24.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 11.043532371520996
2025-12-09 11:51:24.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 11.027451515197754
2025-12-09 11:51:24.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 11.001967430114746
2025-12-09 11:51:24.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 11.008116722106934
2025-12-09 11:51:24.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 10.796326637268066
2025-12-09 11:51:24.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 10.741238594055176
2025-12-09 11:51:24.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 10.579575538635254
2025-12-09 11:51:24.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 10.57547378540039
2025-12-09 11:51:25.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 10.497740745544434
2025-12-09 11:51:25.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 10.399245262145996
2025-12-09 11:51:25.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 10.226043701171875
2025-12-09 11:51:25.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 10.220553398132324
2025-12-09 11:51:25.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 9.965328216552734
2025-12-09 11:51:25.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 9.848882675170898
2025-12-09 11:51:25.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 9.82928466796875
2025-12-09 11:51:25.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 9.792016983032227
2025-12-09 11:51:25.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 9.49974250793457
2025-12-09 11:51:25.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 9.393646240234375
2025-12-09 11:51:25.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 9.045978546142578
2025-12-09 11:51:25.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 8.956381797790527
2025-12-09 11:51:25.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 8.807406425476074
2025-12-09 11:51:26.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 8.749655723571777
2025-12-09 11:51:26.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 8.715524673461914
2025-12-09 11:51:26.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 8.596365928649902
2025-12-09 11:51:26.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 8.525925636291504
2025-12-09 11:51:26.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 8.421818733215332
2025-12-09 11:51:26.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 8.231407165527344
2025-12-09 11:51:26.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 8.213775634765625
2025-12-09 11:51:26.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 8.354307174682617
2025-12-09 11:51:26.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 8.734152793884277
2025-12-09 11:51:26.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 7.555598735809326
2025-12-09 11:51:26.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 8.005509376525879
2025-12-09 11:51:26.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 7.980469703674316
2025-12-09 11:51:26.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 8.190622329711914
2025-12-09 11:51:27.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 7.806957244873047
2025-12-09 11:51:27.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 7.917644023895264
2025-12-09 11:51:27.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 8.693229675292969
2025-12-09 11:51:27.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 7.830593109130859
2025-12-09 11:51:27.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 7.834442615509033
2025-12-09 11:51:27.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 8.0945463180542
2025-12-09 11:51:27.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 7.865991115570068
2025-12-09 11:51:27.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 8.571595191955566
2025-12-09 11:51:27.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 7.99268102645874
2025-12-09 11:51:27.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 8.067710876464844
2025-12-09 11:51:27.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 7.628133773803711
2025-12-09 11:51:27.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 7.957350730895996
2025-12-09 11:51:28.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 8.108834266662598
2025-12-09 11:51:28.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 8.11984634399414
2025-12-09 11:51:28.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 7.91924524307251
2025-12-09 11:51:28.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 7.63769006729126
2025-12-09 11:51:28.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 8.03022575378418
2025-12-09 11:51:28.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 7.7929840087890625
2025-12-09 11:51:28.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 7.811437606811523
2025-12-09 11:51:28.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 7.806396007537842
2025-12-09 11:51:28.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 7.968926906585693
2025-12-09 11:51:28.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 8.021539688110352
2025-12-09 11:51:28.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 7.973887920379639
2025-12-09 11:51:28.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 7.991684436798096
2025-12-09 11:51:28.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 7.942219257354736
2025-12-09 11:51:29.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 7.865083694458008
2025-12-09 11:51:29.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 8.393563270568848
2025-12-09 11:51:29.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 7.920166969299316
2025-12-09 11:51:29.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 7.5799102783203125
2025-12-09 11:51:29.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 7.838821887969971
2025-12-09 11:51:29.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 7.850194931030273
2025-12-09 11:51:29.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 7.914801120758057
2025-12-09 11:51:29.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 7.545301914215088
2025-12-09 11:51:29.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 7.550682067871094
2025-12-09 11:51:29.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 7.802933692932129
2025-12-09 11:51:29.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 7.848031520843506
2025-12-09 11:51:29.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 7.954909324645996
2025-12-09 11:51:29.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 7.71421480178833
2025-12-09 11:51:30.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 7.793353080749512
2025-12-09 11:51:30.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 8.126863479614258
2025-12-09 11:51:30.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 7.8177971839904785
2025-12-09 11:51:30.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 7.487784385681152
2025-12-09 11:51:30.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 7.972564697265625
2025-12-09 11:51:30.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 7.604583740234375
2025-12-09 11:51:30.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 7.96268367767334
2025-12-09 11:51:30.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 7.701143264770508
2025-12-09 11:51:30.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 7.454950332641602
2025-12-09 11:51:30.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 7.635272979736328
2025-12-09 11:51:30.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 7.961788177490234
2025-12-09 11:51:30.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 7.711968421936035
2025-12-09 11:51:31.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 7.966843605041504
2025-12-09 11:51:31.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 7.949162006378174
2025-12-09 11:51:31.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009999999072578703 Training loss: 7.7118449211120605
2025-12-09 11:51:31.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0009999996290315154 Training loss: 7.657882213592529
2025-12-09 11:51:31.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0009999991653210384 Training loss: 7.514604568481445
2025-12-09 11:51:31.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0009999985161266117 Training loss: 7.554245471954346
2025-12-09 11:51:31.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0009999976814484759 Training loss: 7.221536159515381
2025-12-09 11:51:31.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0009999966612869405 Training loss: 7.4526214599609375
2025-12-09 11:51:31.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0009999954556423843 Training loss: 8.12584114074707
2025-12-09 11:51:31.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0009999940645152542 Training loss: 7.454758167266846
2025-12-09 11:51:31.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0009999924879060664 Training loss: 7.72800874710083
2025-12-09 11:51:31.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.000999990725815406 Training loss: 8.29466438293457
2025-12-09 11:51:32.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0009999887782439264 Training loss: 7.393306255340576
2025-12-09 11:51:32.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.00099998664519235 Training loss: 7.434881687164307
2025-12-09 11:51:32.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0009999843266614685 Training loss: 7.8974432945251465
2025-12-09 11:51:32.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0009999818226521416 Training loss: 7.635008811950684
2025-12-09 11:51:32.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0009999791331652982 Training loss: 8.055465698242188
2025-12-09 11:51:32.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0009999762582019365 Training loss: 7.24473237991333
2025-12-09 11:51:32.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.0009999731977631226 Training loss: 7.380916118621826
2025-12-09 11:51:32.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0009999699518499921 Training loss: 7.839840888977051
2025-12-09 11:51:32.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0009999665204637486 Training loss: 7.4179887771606445
2025-12-09 11:51:32.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0009999629036056656 Training loss: 8.006031036376953
2025-12-09 11:51:32.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0009999591012770847 Training loss: 7.636643886566162
2025-12-09 11:51:32.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0009999551134794165 Training loss: 7.822427272796631
2025-12-09 11:51:32.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00099995094021414 Training loss: 7.856200218200684
2025-12-09 11:51:33.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0009999465814828036 Training loss: 7.578744411468506
2025-12-09 11:51:33.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0009999420372870244 Training loss: 7.76528787612915
2025-12-09 11:51:33.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0009999373076284876 Training loss: 7.5900750160217285
2025-12-09 11:51:33.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0009999323925089486 Training loss: 7.537301063537598
2025-12-09 11:51:33.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00099992729193023 Training loss: 7.551696300506592
2025-12-09 11:51:33.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0009999220058942244 Training loss: 7.406437873840332
2025-12-09 11:51:33.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0009999165344028926 Training loss: 7.4949140548706055
2025-12-09 11:51:33.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0009999108774582644 Training loss: 7.546969890594482
2025-12-09 11:51:33.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0009999050350624381 Training loss: 7.231152057647705
2025-12-09 11:51:33.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0009998990072175814 Training loss: 7.676365375518799
2025-12-09 11:51:33.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0009998927939259303 Training loss: 7.83461856842041
2025-12-09 11:51:33.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0009998863951897897 Training loss: 7.45462703704834
2025-12-09 11:51:33.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0009998798110115333 Training loss: 7.449928283691406
2025-12-09 11:51:34.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0009998730413936037 Training loss: 7.587592124938965
2025-12-09 11:51:34.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0009998660863385124 Training loss: 7.942193508148193
2025-12-09 11:51:34.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0009998589458488389 Training loss: 7.5614728927612305
2025-12-09 11:51:34.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0009998516199272328 Training loss: 7.629098892211914
2025-12-09 11:51:34.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0009998441085764113 Training loss: 7.191569805145264
2025-12-09 11:51:34.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0009998364117991612 Training loss: 7.454738140106201
2025-12-09 11:51:34.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0009998285295983375 Training loss: 7.530145168304443
2025-12-09 11:51:34.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0009998204619768645 Training loss: 7.3579421043396
2025-12-09 11:51:34.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0009998122089377348 Training loss: 7.568492412567139
2025-12-09 11:51:34.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0009998037704840102 Training loss: 7.078784942626953
2025-12-09 11:51:34.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.000999795146618821 Training loss: 7.527239799499512
2025-12-09 11:51:34.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0009997863373453664 Training loss: 7.4125518798828125
2025-12-09 11:51:35.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.000999777342666914 Training loss: 8.121725082397461
2025-12-09 11:51:35.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0009997681625868013 Training loss: 6.4106035232543945
2025-12-09 11:51:35.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0009997587971084334 Training loss: 7.228321075439453
2025-12-09 11:51:35.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0009997492462352846 Training loss: 7.257291316986084
2025-12-09 11:51:35.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0009997395099708981 Training loss: 7.843899250030518
2025-12-09 11:51:35.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0009997295883188856 Training loss: 7.281136512756348
2025-12-09 11:51:35.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0009997194812829276 Training loss: 7.60674524307251
2025-12-09 11:51:35.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0009997091888667737 Training loss: 7.385125637054443
2025-12-09 11:51:35.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.000999698711074242 Training loss: 7.459008693695068
2025-12-09 11:51:35.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0009996880479092197 Training loss: 7.2276225090026855
2025-12-09 11:51:35.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.000999677199375662 Training loss: 7.41578483581543
2025-12-09 11:51:35.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0009996661654775938 Training loss: 7.486680507659912
2025-12-09 11:51:35.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.000999654946219108 Training loss: 7.280094623565674
2025-12-09 11:51:36.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.000999643541604367 Training loss: 7.440861225128174
2025-12-09 11:51:36.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.000999631951637601 Training loss: 7.489818096160889
2025-12-09 11:51:36.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0009996201763231099 Training loss: 7.283863544464111
2025-12-09 11:51:36.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0009996082156652618 Training loss: 7.087177753448486
2025-12-09 11:51:36.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.000999596069668494 Training loss: 7.390439033508301
2025-12-09 11:51:36.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.000999583738337312 Training loss: 7.402914524078369
2025-12-09 11:51:36.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0009995712216762903 Training loss: 7.2034711837768555
2025-12-09 11:51:36.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0009995585196900722 Training loss: 7.335602760314941
2025-12-09 11:51:36.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.00099954563238337 Training loss: 7.387183666229248
2025-12-09 11:51:36.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0009995325597609644 Training loss: 7.628028869628906
2025-12-09 11:51:36.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.000999519301827705 Training loss: 7.240253448486328
2025-12-09 11:51:36.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0009995058585885095 Training loss: 7.624595642089844
2025-12-09 11:51:36.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0009994922300483656 Training loss: 7.40614128112793
2025-12-09 11:51:37.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.000999478416212329 Training loss: 6.919980525970459
2025-12-09 11:51:37.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.0009994644170855237 Training loss: 7.418173789978027
2025-12-09 11:51:37.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0009994502326731434 Training loss: 6.885125637054443
2025-12-09 11:51:37.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0009994358629804498 Training loss: 7.238650321960449
2025-12-09 11:51:37.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0009994213080127738 Training loss: 7.454879283905029
2025-12-09 11:51:37.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0009994065677755147 Training loss: 7.680546760559082
2025-12-09 11:51:37.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0009993916422741409 Training loss: 7.653872013092041
2025-12-09 11:51:37.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.000999376531514189 Training loss: 7.382719039916992
2025-12-09 11:51:37.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0009993612355012646 Training loss: 7.311357021331787
2025-12-09 11:51:37.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0009993457542410422 Training loss: 7.327200412750244
2025-12-09 11:51:37.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.000999330087739265 Training loss: 7.072585582733154
2025-12-09 11:51:37.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0009993142360017445 Training loss: 7.047201633453369
2025-12-09 11:51:38.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0009992981990343613 Training loss: 7.482344150543213
2025-12-09 11:51:38.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0009992819768430649 Training loss: 7.1165666580200195
2025-12-09 11:51:38.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0009992655694338725 Training loss: 7.362890720367432
2025-12-09 11:51:38.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0009992489768128714 Training loss: 7.334911346435547
2025-12-09 11:51:38.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0009992321989862165 Training loss: 7.232119083404541
2025-12-09 11:51:38.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0009992152359601322 Training loss: 7.688632488250732
2025-12-09 11:51:38.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.000999198087740911 Training loss: 7.45456075668335
2025-12-09 11:51:38.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0009991807543349145 Training loss: 7.181799411773682
2025-12-09 11:51:38.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.000999163235748573 Training loss: 6.350947380065918
2025-12-09 11:51:38.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0009991455319883849 Training loss: 7.700413703918457
2025-12-09 11:51:38.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0009991276430609181 Training loss: 7.11389684677124
2025-12-09 11:51:38.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0009991095689728087 Training loss: 7.233581066131592
2025-12-09 11:51:38.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0009990913097307613 Training loss: 7.2600417137146
2025-12-09 11:51:39.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0009990728653415503 Training loss: 7.160192489624023
2025-12-09 11:51:39.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0009990542358120174 Training loss: 7.145639896392822
2025-12-09 11:51:39.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0009990354211490736 Training loss: 7.343530178070068
2025-12-09 11:51:39.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0009990164213596986 Training loss: 7.627938747406006
2025-12-09 11:51:39.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0009989972364509408 Training loss: 7.511548042297363
2025-12-09 11:51:39.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0009989778664299172 Training loss: 7.482534408569336
2025-12-09 11:51:39.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0009989583113038133 Training loss: 7.164377689361572
2025-12-09 11:51:39.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0009989385710798837 Training loss: 7.134622573852539
2025-12-09 11:51:39.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.0009989186457654514 Training loss: 7.2337141036987305
2025-12-09 11:51:39.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0009988985353679076 Training loss: 7.3521318435668945
2025-12-09 11:51:39.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0009988782398947132 Training loss: 7.426117420196533
2025-12-09 11:51:39.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0009988577593533967 Training loss: 6.904088973999023
2025-12-09 11:51:39.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.000998837093751556 Training loss: 7.401229381561279
2025-12-09 11:51:40.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0009988162430968576 Training loss: 6.890913963317871
2025-12-09 11:51:40.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.000998795207397036 Training loss: 7.136641502380371
2025-12-09 11:51:40.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.000998773986659895 Training loss: 7.210264682769775
2025-12-09 11:51:40.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0009987525808933069 Training loss: 7.895501613616943
2025-12-09 11:51:40.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0009987309901052122 Training loss: 7.0794782638549805
2025-12-09 11:51:40.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.000998709214303621 Training loss: 7.3572211265563965
2025-12-09 11:51:40.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.000998687253496611 Training loss: 7.09048318862915
2025-12-09 11:51:40.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0009986651076923287 Training loss: 7.007257461547852
2025-12-09 11:51:40.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0009986427768989903 Training loss: 7.047978401184082
2025-12-09 11:51:40.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0009986202611248793 Training loss: 7.129632472991943
2025-12-09 11:51:40.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0009985975603783483 Training loss: 7.4686598777771
2025-12-09 11:51:40.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.000998574674667819 Training loss: 7.160953998565674
2025-12-09 11:51:41.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0009985516040017807 Training loss: 7.355093002319336
2025-12-09 11:51:41.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0009985283483887923 Training loss: 6.938874244689941
2025-12-09 11:51:41.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0009985049078374806 Training loss: 6.823058128356934
2025-12-09 11:51:41.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0009984812823565416 Training loss: 7.537553787231445
2025-12-09 11:51:41.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0009984574719547395 Training loss: 6.954775333404541
2025-12-09 11:51:41.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.000998433476640907 Training loss: 7.279569625854492
2025-12-09 11:51:41.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0009984092964239462 Training loss: 7.324709415435791
2025-12-09 11:51:41.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0009983849313128263 Training loss: 6.816145896911621
2025-12-09 11:51:41.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0009983603813165868 Training loss: 7.326923370361328
2025-12-09 11:51:41.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0009983356464443346 Training loss: 7.40024995803833
2025-12-09 11:51:41.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0009983107267052458 Training loss: 7.733367443084717
2025-12-09 11:51:41.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0009982856221085643 Training loss: 7.488463401794434
2025-12-09 11:51:41.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0009982603326636036 Training loss: 7.071522235870361
2025-12-09 11:51:42.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0009982348583797453 Training loss: 7.255638599395752
2025-12-09 11:51:42.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0009982091992664392 Training loss: 6.841793060302734
2025-12-09 11:51:42.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0009981833553332044 Training loss: 7.197299003601074
2025-12-09 11:51:42.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0009981573265896281 Training loss: 7.038275241851807
2025-12-09 11:51:42.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.000998131113045366 Training loss: 7.064846038818359
2025-12-09 11:51:42.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0009981047147101425 Training loss: 7.700885772705078
2025-12-09 11:51:42.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0009980781315937506 Training loss: 7.073884010314941
2025-12-09 11:51:42.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.000998051363706052 Training loss: 7.425408840179443
2025-12-09 11:51:42.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0009980244110569766 Training loss: 7.163933277130127
2025-12-09 11:51:42.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0009979972736565226 Training loss: 7.391304016113281
2025-12-09 11:51:42.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0009979699515147579 Training loss: 7.262235164642334
2025-12-09 11:51:42.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0009979424446418172 Training loss: 7.219392776489258
2025-12-09 11:51:42.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0009979147530479056 Training loss: 7.30437707901001
2025-12-09 11:51:43.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0009978868767432953 Training loss: 7.7664031982421875
2025-12-09 11:51:43.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0009978588157383277 Training loss: 7.360193252563477
2025-12-09 11:51:43.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0009978305700434125 Training loss: 6.925174713134766
2025-12-09 11:51:43.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.000997802139669028 Training loss: 7.155736446380615
2025-12-09 11:51:43.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0009977735246257209 Training loss: 6.662733554840088
2025-12-09 11:51:43.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0009977447249241065 Training loss: 6.991751194000244
2025-12-09 11:51:43.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.0009977157405748687 Training loss: 7.265401840209961
2025-12-09 11:51:43.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0009976865715887596 Training loss: 8.081013679504395
2025-12-09 11:51:43.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0009976572179766 Training loss: 6.704558372497559
2025-12-09 11:51:43.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0009976276797492793 Training loss: 7.59977388381958
2025-12-09 11:51:43.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0009975979569177551 Training loss: 7.2764973640441895
2025-12-09 11:51:43.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0009975680494930539 Training loss: 6.72761344909668
2025-12-09 11:51:44.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00099753795748627 Training loss: 7.1143927574157715
2025-12-09 11:51:44.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0009975076809085669 Training loss: 7.0477447509765625
2025-12-09 11:51:44.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0009974772197711762 Training loss: 7.247335433959961
2025-12-09 11:51:44.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.000997446574085398 Training loss: 7.2592034339904785
2025-12-09 11:51:44.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0009974157438626008 Training loss: 7.217883586883545
2025-12-09 11:51:44.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0009973847291142217 Training loss: 7.004995346069336
2025-12-09 11:51:44.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0009973535298517663 Training loss: 7.26678991317749
2025-12-09 11:51:44.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0009973221460868086 Training loss: 6.7732768058776855
2025-12-09 11:51:44.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0009972905778309906 Training loss: 6.9399333000183105
2025-12-09 11:51:44.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0009972588250960234 Training loss: 7.2792510986328125
2025-12-09 11:51:44.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0009972268878936862 Training loss: 7.042610168457031
2025-12-09 11:51:44.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.000997194766235827 Training loss: 7.079411506652832
2025-12-09 11:51:44.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0009971624601343614 Training loss: 7.24458646774292
2025-12-09 11:51:45.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0009971299696012743 Training loss: 6.88248872756958
2025-12-09 11:51:45.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009970972946486186 Training loss: 7.1275835037231445
2025-12-09 11:51:45.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0009970644352885157 Training loss: 7.23030424118042
2025-12-09 11:51:45.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0009970313915331553 Training loss: 5.9850544929504395
2025-12-09 11:51:45.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009969981633947955 Training loss: 7.189480781555176
2025-12-09 11:51:45.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0009969647508857632 Training loss: 7.276806354522705
2025-12-09 11:51:45.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.000996931154018453 Training loss: 7.428976535797119
2025-12-09 11:51:45.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009968973728053288 Training loss: 7.114479064941406
2025-12-09 11:51:45.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0009968634072589219 Training loss: 7.42476749420166
2025-12-09 11:51:45.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0009968292573918325 Training loss: 6.774590015411377
2025-12-09 11:51:45.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0009967949232167295 Training loss: 7.080799579620361
2025-12-09 11:51:45.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0009967604047463492 Training loss: 6.852575302124023
2025-12-09 11:51:45.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0009967257019934974 Training loss: 7.379705429077148
2025-12-09 11:51:46.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0009966908149710476 Training loss: 6.946845531463623
2025-12-09 11:51:46.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0009966557436919415 Training loss: 6.958642482757568
2025-12-09 11:51:46.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.00099662048816919 Training loss: 7.2441205978393555
2025-12-09 11:51:46.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.000996585048415871 Training loss: 7.296069622039795
2025-12-09 11:51:46.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0009965494244451323 Training loss: 6.900549411773682
2025-12-09 11:51:46.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0009965136162701888 Training loss: 7.328319549560547
2025-12-09 11:51:46.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0009964776239043244 Training loss: 6.636282444000244
2025-12-09 11:51:46.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0009964414473608912 Training loss: 6.6749796867370605
2025-12-09 11:51:46.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0009964050866533092 Training loss: 7.186819553375244
2025-12-09 11:51:46.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0009963685417950677 Training loss: 7.026028156280518
2025-12-09 11:51:46.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.000996331812799723 Training loss: 7.390953540802002
2025-12-09 11:51:46.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0009962948996809007 Training loss: 7.161046028137207
2025-12-09 11:51:46.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0009962578024522947 Training loss: 6.79483699798584
2025-12-09 11:51:47.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0009962205211276665 Training loss: 7.042616367340088
2025-12-09 11:51:47.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0009961830557208464 Training loss: 6.725320816040039
2025-12-09 11:51:47.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.000996145406245733 Training loss: 6.895034313201904
2025-12-09 11:51:47.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0009961075727162928 Training loss: 6.762147426605225
2025-12-09 11:51:47.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0009960695551465611 Training loss: 7.215448379516602
2025-12-09 11:51:47.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.000996031353550641 Training loss: 6.986387252807617
2025-12-09 11:51:47.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0009959929679427047 Training loss: 6.179299831390381
2025-12-09 11:51:47.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0009959543983369913 Training loss: 7.039339065551758
2025-12-09 11:51:47.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.000995915644747809 Training loss: 6.293409824371338
2025-12-09 11:51:47.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0009958767071895347 Training loss: 6.663031578063965
2025-12-09 11:51:47.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0009958375856766127 Training loss: 7.101485729217529
2025-12-09 11:51:47.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0009957982802235555 Training loss: 6.529369354248047
2025-12-09 11:51:48.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0009957587908449449 Training loss: 7.041457653045654
2025-12-09 11:51:48.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0009957191175554295 Training loss: 6.436420917510986
2025-12-09 11:51:48.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0009956792603697273 Training loss: 6.843862533569336
2025-12-09 11:51:48.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.000995639219302624 Training loss: 6.743094444274902
2025-12-09 11:51:48.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0009955989943689733 Training loss: 7.01346492767334
2025-12-09 11:51:48.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0009955585855836978 Training loss: 7.159257888793945
2025-12-09 11:51:48.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0009955179929617875 Training loss: 6.781172752380371
2025-12-09 11:51:48.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0009954772165183012 Training loss: 7.416774272918701
2025-12-09 11:51:48.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0009954362562683658 Training loss: 6.740233421325684
2025-12-09 11:51:48.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.000995395112227176 Training loss: 6.697243690490723
2025-12-09 11:51:48.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.000995353784409995 Training loss: 7.148102760314941
2025-12-09 11:51:48.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0009953122728321542 Training loss: 7.0362443923950195
2025-12-09 11:51:48.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0009952705775090529 Training loss: 7.517597675323486
2025-12-09 11:51:49.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0009952286984561591 Training loss: 7.0032477378845215
2025-12-09 11:51:49.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0009951866356890083 Training loss: 6.99318790435791
2025-12-09 11:51:49.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0009951443892232048 Training loss: 6.955536365509033
2025-12-09 11:51:49.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0009951019590744203 Training loss: 7.104457855224609
2025-12-09 11:51:49.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0009950593452583952 Training loss: 6.901458263397217
2025-12-09 11:51:49.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0009950165477909379 Training loss: 7.048079967498779
2025-12-09 11:51:49.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0009949735666879252 Training loss: 6.805939197540283
2025-12-09 11:51:49.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.000994930401965301 Training loss: 7.167198181152344
2025-12-09 11:51:49.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.000994887053639079 Training loss: 6.8956427574157715
2025-12-09 11:51:49.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0009948435217253394 Training loss: 6.7080206871032715
2025-12-09 11:51:49.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0009947998062402312 Training loss: 7.784996509552002
2025-12-09 11:51:49.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.000994755907199972 Training loss: 7.21038818359375
2025-12-09 11:51:49.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0009947118246208461 Training loss: 6.823456764221191
2025-12-09 11:51:50.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0009946675585192075 Training loss: 7.3321661949157715
2025-12-09 11:51:50.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0009946231089114773 Training loss: 6.85269832611084
2025-12-09 11:51:50.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.000994578475814145 Training loss: 6.910536766052246
2025-12-09 11:51:50.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0009945336592437678 Training loss: 6.985767841339111
2025-12-09 11:51:50.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0009944886592169711 Training loss: 7.095023155212402
2025-12-09 11:51:50.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.000994443475750449 Training loss: 6.90501070022583
2025-12-09 11:51:50.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.000994398108860963 Training loss: 6.746320724487305
2025-12-09 11:51:50.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0009943525585653428 Training loss: 6.5661301612854
2025-12-09 11:51:50.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0009943068248804859 Training loss: 6.880945682525635
2025-12-09 11:51:50.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.000994260907823358 Training loss: 6.765829086303711
2025-12-09 11:51:50.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0009942148074109933 Training loss: 6.82588529586792
2025-12-09 11:51:50.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0009941685236604934 Training loss: 7.0103888511657715
2025-12-09 11:51:51.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.0009941220565890278 Training loss: 7.12716007232666
2025-12-09 11:51:51.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.000994075406213835 Training loss: 7.056017875671387
2025-12-09 11:51:51.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0009940285725522201 Training loss: 7.218402862548828
2025-12-09 11:51:51.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0009939815556215576 Training loss: 6.839756488800049
2025-12-09 11:51:51.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0009939343554392886 Training loss: 6.730384826660156
2025-12-09 11:51:51.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.0009938869720229233 Training loss: 6.677558898925781
2025-12-09 11:51:51.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0009938394053900395 Training loss: 6.572259902954102
2025-12-09 11:51:51.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0009937916555582827 Training loss: 6.650160789489746
2025-12-09 11:51:51.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.000993743722545367 Training loss: 6.959914684295654
2025-12-09 11:51:51.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0009936956063690734 Training loss: 6.507190227508545
2025-12-09 11:51:51.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0009936473070472518 Training loss: 6.928470611572266
2025-12-09 11:51:51.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0009935988245978198 Training loss: 6.948610305786133
2025-12-09 11:51:51.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0009935501590387628 Training loss: 6.894183158874512
2025-12-09 11:51:52.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0009935013103881344 Training loss: 6.989917278289795
2025-12-09 11:51:52.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0009934522786640555 Training loss: 6.2645039558410645
2025-12-09 11:51:52.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0009934030638847156 Training loss: 6.982326507568359
2025-12-09 11:51:52.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0009933536660683717 Training loss: 6.986245632171631
2025-12-09 11:51:52.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0009933040852333488 Training loss: 6.916191101074219
2025-12-09 11:51:52.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00099325432139804 Training loss: 7.006218910217285
2025-12-09 11:51:52.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0009932043745809064 Training loss: 6.7732977867126465
2025-12-09 11:51:52.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.000993154244800476 Training loss: 7.092275142669678
2025-12-09 11:51:52.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.0009931039320753457 Training loss: 7.56545877456665
2025-12-09 11:51:52.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00099305343642418 Training loss: 7.108737468719482
2025-12-09 11:51:52.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0009930027578657114 Training loss: 6.900667667388916
2025-12-09 11:51:52.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0009929518964187393 Training loss: 7.073702812194824
2025-12-09 11:51:53.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0009929008521021325 Training loss: 7.016791820526123
2025-12-09 11:51:53.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.0009928496249348266 Training loss: 6.63998556137085
2025-12-09 11:51:53.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.000992798214935825 Training loss: 6.825158596038818
2025-12-09 11:51:53.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0009927466221241995 Training loss: 7.141360759735107
2025-12-09 11:51:53.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0009926948465190893 Training loss: 6.9679083824157715
2025-12-09 11:51:53.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0009926428881397015 Training loss: 6.575485706329346
2025-12-09 11:51:53.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0009925907470053111 Training loss: 6.757357120513916
2025-12-09 11:51:53.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.0009925384231352606 Training loss: 7.032236099243164
2025-12-09 11:51:53.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0009924859165489608 Training loss: 7.160279273986816
2025-12-09 11:51:53.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0009924332272658897 Training loss: 7.830148696899414
2025-12-09 11:51:53.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.0009923803553055937 Training loss: 6.293642044067383
2025-12-09 11:51:53.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0009923273006876864 Training loss: 6.6729021072387695
2025-12-09 11:51:53.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0009922740634318494 Training loss: 7.0829057693481445
2025-12-09 11:51:54.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.0009922206435578323 Training loss: 6.969257831573486
2025-12-09 11:51:54.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0009921670410854518 Training loss: 7.931784629821777
2025-12-09 11:51:54.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0009921132560345928 Training loss: 6.6545000076293945
2025-12-09 11:51:54.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0009920592884252082 Training loss: 7.865242958068848
2025-12-09 11:51:54.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0009920051382773178 Training loss: 6.95018196105957
2025-12-09 11:51:54.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.00099195080561101 Training loss: 7.871253967285156
2025-12-09 11:51:54.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.0009918962904464407 Training loss: 6.756125450134277
2025-12-09 11:51:54.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0009918415928038325 Training loss: 6.738308906555176
2025-12-09 11:51:54.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.000991786712703477 Training loss: 6.885183334350586
2025-12-09 11:51:54.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0009917316501657334 Training loss: 6.607789993286133
2025-12-09 11:51:54.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0009916764052110274 Training loss: 6.857388019561768
2025-12-09 11:51:54.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0009916209778598536 Training loss: 7.252175807952881
2025-12-09 11:51:54.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0009915653681327736 Training loss: 6.903665542602539
2025-12-09 11:51:55.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.000991509576050417 Training loss: 6.971885681152344
2025-12-09 11:51:55.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0009914536016334807 Training loss: 6.892545700073242
2025-12-09 11:51:55.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0009913974449027297 Training loss: 6.945133686065674
2025-12-09 11:51:55.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0009913411058789963 Training loss: 7.146856784820557
2025-12-09 11:51:55.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0009912845845831805 Training loss: 6.92061185836792
2025-12-09 11:51:55.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00099122788103625 Training loss: 6.670114040374756
2025-12-09 11:51:55.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0009911709952592396 Training loss: 6.7619452476501465
2025-12-09 11:51:55.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0009911139272732526 Training loss: 6.811689376831055
2025-12-09 11:51:55.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.0009910566770994593 Training loss: 7.168585777282715
2025-12-09 11:51:55.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0009909992447590978 Training loss: 6.883388519287109
2025-12-09 11:51:55.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.0009909416302734736 Training loss: 6.835016250610352
2025-12-09 11:51:55.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.0009908838336639598 Training loss: 6.904512405395508
2025-12-09 11:51:56.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.000990825854951997 Training loss: 7.363039493560791
2025-12-09 11:51:56.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0009907676941590937 Training loss: 6.936519145965576
2025-12-09 11:51:56.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0009907093513068259 Training loss: 7.142208099365234
2025-12-09 11:51:56.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.0009906508264168365 Training loss: 6.89478063583374
2025-12-09 11:51:56.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.0009905921195108368 Training loss: 7.276890277862549
2025-12-09 11:51:56.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0009905332306106049 Training loss: 6.550714492797852
2025-12-09 11:51:56.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.000990474159737987 Training loss: 6.712150573730469
2025-12-09 11:51:56.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0009904149069148963 Training loss: 7.3846540451049805
2025-12-09 11:51:56.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.000990355472163314 Training loss: 6.476271629333496
2025-12-09 11:51:56.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0009902958555052881 Training loss: 7.377499103546143
2025-12-09 11:51:56.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0009902360569629348 Training loss: 6.6583075523376465
2025-12-09 11:51:56.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0009901760765584375 Training loss: 6.700573444366455
2025-12-09 11:51:56.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.000990115914314047 Training loss: 5.976311206817627
2025-12-09 11:51:57.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0009900555702520816 Training loss: 7.150786399841309
2025-12-09 11:51:57.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.000989995044394927 Training loss: 6.73225212097168
2025-12-09 11:51:57.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0009899343367650365 Training loss: 6.670076847076416
2025-12-09 11:51:57.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0009898734473849304 Training loss: 7.647069454193115
2025-12-09 11:51:57.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0009898123762771972 Training loss: 5.945245265960693
2025-12-09 11:51:57.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.000989751123464492 Training loss: 6.411357402801514
2025-12-09 11:51:57.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0009896896889695376 Training loss: 6.578622341156006
2025-12-09 11:51:57.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0009896280728151248 Training loss: 6.572342395782471
2025-12-09 11:51:57.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.0009895662750241108 Training loss: 6.766505718231201
2025-12-09 11:51:57.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0009895042956194209 Training loss: 6.690927505493164
2025-12-09 11:51:57.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0009894421346240473 Training loss: 7.558456897735596
2025-12-09 11:51:57.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0009893797920610496 Training loss: 6.825039863586426
2025-12-09 11:51:57.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0009893172679535552 Training loss: 6.766757965087891
2025-12-09 11:51:58.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0009892545623247585 Training loss: 5.7365899085998535
2025-12-09 11:51:58.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0009891916751979218 Training loss: 6.800753116607666
2025-12-09 11:51:58.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0009891286065963733 Training loss: 7.029073238372803
2025-12-09 11:51:58.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0009890653565435101 Training loss: 6.892744541168213
2025-12-09 11:51:58.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0009890019250627959 Training loss: 6.657166004180908
2025-12-09 11:51:58.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0009889383121777617 Training loss: 6.617234230041504
2025-12-09 11:51:58.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.000988874517912006 Training loss: 7.366568088531494
2025-12-09 11:51:58.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0009888105422891941 Training loss: 6.510559558868408
2025-12-09 11:51:58.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0009887463853330593 Training loss: 6.6148810386657715
2025-12-09 11:51:58.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.000988682047067402 Training loss: 6.807354927062988
2025-12-09 11:51:58.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.000988617527516089 Training loss: 7.1765360832214355
2025-12-09 11:51:58.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0009885528267030556 Training loss: 6.882092475891113
2025-12-09 11:51:59.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0009884879446523036 Training loss: 6.561547756195068
2025-12-09 11:51:59.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.000988422881387902 Training loss: 6.449665069580078
2025-12-09 11:51:59.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0009883576369339876 Training loss: 6.825136184692383
2025-12-09 11:51:59.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0009882922113147636 Training loss: 6.908268928527832
2025-12-09 11:51:59.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0009882266045545011 Training loss: 6.483221054077148
2025-12-09 11:51:59.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.0009881608166775384 Training loss: 6.835115909576416
2025-12-09 11:51:59.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0009880948477082802 Training loss: 7.337255954742432
2025-12-09 11:51:59.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0009880286976711992 Training loss: 6.579277515411377
2025-12-09 11:51:59.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.000987962366590835 Training loss: 6.672330379486084
2025-12-09 11:51:59.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.0009878958544917943 Training loss: 6.909595966339111
2025-12-09 11:51:59.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.0009878291613987509 Training loss: 6.81238317489624
2025-12-09 11:51:59.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.000987762287336446 Training loss: 7.144858360290527
2025-12-09 11:51:59.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0009876952323296876 Training loss: 6.846282005310059
2025-12-09 11:52:00.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0009876279964033511 Training loss: 5.952789306640625
2025-12-09 11:52:00.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.000987560579582379 Training loss: 6.963839530944824
2025-12-09 11:52:00.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0009874929818917805 Training loss: 6.421079635620117
2025-12-09 11:52:00.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0009874252033566326 Training loss: 7.02928352355957
2025-12-09 11:52:00.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.000987357244002079 Training loss: 6.621415615081787
2025-12-09 11:52:00.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00098728910385333 Training loss: 6.480269432067871
2025-12-09 11:52:00.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.000987220782935664 Training loss: 6.810269832611084
2025-12-09 11:52:00.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0009871522812744257 Training loss: 6.870401382446289
2025-12-09 11:52:00.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.0009870835988950268 Training loss: 5.766815185546875
2025-12-09 11:52:00.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0009870147358229467 Training loss: 6.900834560394287
2025-12-09 11:52:00.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0009869456920837312 Training loss: 7.588132858276367
2025-12-09 11:52:00.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0009868764677029933 Training loss: 6.581240653991699
2025-12-09 11:52:00.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0009868070627064133 Training loss: 6.702206134796143
2025-12-09 11:52:01.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0009867374771197384 Training loss: 6.957586288452148
2025-12-09 11:52:01.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0009866677109687822 Training loss: 6.612452507019043
2025-12-09 11:52:01.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0009865977642794259 Training loss: 7.321042537689209
2025-12-09 11:52:01.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0009865276370776177 Training loss: 6.948735237121582
2025-12-09 11:52:01.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0009864573293893724 Training loss: 6.7049994468688965
2025-12-09 11:52:01.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.000986386841240772 Training loss: 6.51460075378418
2025-12-09 11:52:01.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0009863161726579655 Training loss: 7.0079474449157715
2025-12-09 11:52:01.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0009862453236671685 Training loss: 6.53166389465332
2025-12-09 11:52:01.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.000986174294294664 Training loss: 6.432826519012451
2025-12-09 11:52:01.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0009861030845668014 Training loss: 6.7371931076049805
2025-12-09 11:52:01.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0009860316945099973 Training loss: 6.665563106536865
2025-12-09 11:52:01.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0009859601241507354 Training loss: 6.6064300537109375
2025-12-09 11:52:02.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0009858883735155658 Training loss: 6.750744342803955
2025-12-09 11:52:02.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0009858164426311058 Training loss: 6.788314342498779
2025-12-09 11:52:02.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0009857443315240395 Training loss: 6.785475730895996
2025-12-09 11:52:02.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.000985672040221118 Training loss: 6.284713268280029
2025-12-09 11:52:02.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.000985599568749159 Training loss: 6.744259357452393
2025-12-09 11:52:02.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.000985526917135047 Training loss: 7.025311470031738
2025-12-09 11:52:02.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0009854540854057337 Training loss: 6.625792503356934
2025-12-09 11:52:02.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.000985381073588237 Training loss: 6.552813529968262
2025-12-09 11:52:02.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.0009853078817096423 Training loss: 5.741100788116455
2025-12-09 11:52:02.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0009852345097971016 Training loss: 6.637726306915283
2025-12-09 11:52:02.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.0009851609578778332 Training loss: 5.639307022094727
2025-12-09 11:52:02.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0009850872259791227 Training loss: 6.932079315185547
2025-12-09 11:52:02.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0009850133141283226 Training loss: 6.503615379333496
2025-12-09 11:52:03.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0009849392223528514 Training loss: 6.5893731117248535
2025-12-09 11:52:03.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.000984864950680195 Training loss: 6.978758811950684
2025-12-09 11:52:03.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.000984790499137906 Training loss: 6.826613426208496
2025-12-09 11:52:03.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0009847158677536033 Training loss: 6.880680084228516
2025-12-09 11:52:03.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.000984641056554973 Training loss: 6.36993408203125
2025-12-09 11:52:03.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.0009845660655697678 Training loss: 6.75480842590332
2025-12-09 11:52:03.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0009844908948258067 Training loss: 6.621576309204102
2025-12-09 11:52:03.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.000984415544350976 Training loss: 6.542632102966309
2025-12-09 11:52:03.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.000984340014173228 Training loss: 6.983783721923828
2025-12-09 11:52:03.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0009842643043205823 Training loss: 6.986233234405518
2025-12-09 11:52:03.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0009841884148211247 Training loss: 6.875463485717773
2025-12-09 11:52:03.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.000984112345703008 Training loss: 6.616977214813232
2025-12-09 11:52:03.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.000984036096994451 Training loss: 6.789614200592041
2025-12-09 11:52:04.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0009839596687237402 Training loss: 6.713520050048828
2025-12-09 11:52:04.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0009838830609192278 Training loss: 6.673980236053467
2025-12-09 11:52:04.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.0009838062736093327 Training loss: 6.586526393890381
2025-12-09 11:52:04.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0009837293068225407 Training loss: 7.0136189460754395
2025-12-09 11:52:04.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0009836521605874043 Training loss: 6.368414878845215
2025-12-09 11:52:04.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0009835748349325422 Training loss: 6.755921363830566
2025-12-09 11:52:04.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0009834973298866393 Training loss: 6.926650047302246
2025-12-09 11:52:04.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0009834196454784484 Training loss: 6.647897720336914
2025-12-09 11:52:04.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0009833417817367873 Training loss: 6.856756210327148
2025-12-09 11:52:04.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.0009832637386905413 Training loss: 5.533633232116699
2025-12-09 11:52:04.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0009831855163686617 Training loss: 6.666781425476074
2025-12-09 11:52:04.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0009831071148001668 Training loss: 6.9459357261657715
2025-12-09 11:52:05.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0009830285340141408 Training loss: 6.434450626373291
2025-12-09 11:52:05.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0009829497740397348 Training loss: 6.810899257659912
2025-12-09 11:52:05.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0009828708349061664 Training loss: 6.651659965515137
2025-12-09 11:52:05.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0009827917166427196 Training loss: 6.572650909423828
2025-12-09 11:52:05.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0009827124192787445 Training loss: 6.673948764801025
2025-12-09 11:52:05.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.000982632942843658 Training loss: 6.724452018737793
2025-12-09 11:52:05.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0009825532873669433 Training loss: 7.096714973449707
2025-12-09 11:52:05.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0009824734528781505 Training loss: 6.515838146209717
2025-12-09 11:52:05.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.0009823934394068952 Training loss: 6.52962589263916
2025-12-09 11:52:05.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.0009823132469828602 Training loss: 6.5333123207092285
2025-12-09 11:52:05.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.000982232875635794 Training loss: 6.782810688018799
2025-12-09 11:52:05.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.0009821523253955122 Training loss: 6.419739246368408
2025-12-09 11:52:05.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.0009820715962918964 Training loss: 6.1019511222839355
2025-12-09 11:52:06.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0009819906883548942 Training loss: 6.6626176834106445
2025-12-09 11:52:06.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0009819096016145203 Training loss: 6.773568153381348
2025-12-09 11:52:06.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.000981828336100855 Training loss: 6.694509506225586
2025-12-09 11:52:06.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0009817468918440454 Training loss: 6.718832969665527
2025-12-09 11:52:06.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0009816652688743048 Training loss: 6.605858325958252
2025-12-09 11:52:06.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0009815834672219127 Training loss: 6.849700450897217
2025-12-09 11:52:06.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.000981501486917215 Training loss: 6.502627849578857
2025-12-09 11:52:06.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0009814193279906237 Training loss: 6.510743618011475
2025-12-09 11:52:06.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.000981336990472617 Training loss: 6.667941093444824
2025-12-09 11:52:06.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00098125447439374 Training loss: 6.51879358291626
2025-12-09 11:52:06.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0009811717797846033 Training loss: 6.515448093414307
2025-12-09 11:52:06.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.000981088906675884 Training loss: 7.43242883682251
2025-12-09 11:52:06.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0009810058550983253 Training loss: 7.302944183349609
2025-12-09 11:52:07.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.000980922625082737 Training loss: 7.089668273925781
2025-12-09 11:52:07.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0009808392166599947 Training loss: 7.011878490447998
2025-12-09 11:52:07.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.0009807556298610403 Training loss: 6.866997718811035
2025-12-09 11:52:07.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.0009806718647168817 Training loss: 7.422089576721191
2025-12-09 11:52:07.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.0009805879212585933 Training loss: 6.9496283531188965
2025-12-09 11:52:07.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0009805037995173154 Training loss: 7.184122562408447
2025-12-09 11:52:07.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0009804194995242548 Training loss: 6.733880996704102
2025-12-09 11:52:07.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.0009803350213106836 Training loss: 6.7290449142456055
2025-12-09 11:52:07.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.000980250364907941 Training loss: 7.206080913543701
2025-12-09 11:52:07.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0009801655303474318 Training loss: 6.761326789855957
2025-12-09 11:52:07.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.000980080517660627 Training loss: 7.296800136566162
2025-12-09 11:52:07.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.0009799953268790633 Training loss: 6.8048601150512695
2025-12-09 11:52:08.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.000979909958034344 Training loss: 6.765054702758789
2025-12-09 11:52:08.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0009798244111581382 Training loss: 5.859954833984375
2025-12-09 11:52:08.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0009797386862821812 Training loss: 6.723474502563477
2025-12-09 11:52:08.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0009796527834382745 Training loss: 6.501086235046387
2025-12-09 11:52:08.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.0009795667026582847 Training loss: 6.523150444030762
2025-12-09 11:52:08.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0009794804439741454 Training loss: 7.358277797698975
2025-12-09 11:52:08.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.000979394007417856 Training loss: 6.227413177490234
2025-12-09 11:52:08.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0009793073930214817 Training loss: 6.898776054382324
2025-12-09 11:52:08.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0009792206008171535 Training loss: 6.729556083679199
2025-12-09 11:52:08.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0009791336308370687 Training loss: 6.8226189613342285
2025-12-09 11:52:08.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0009790464831134903 Training loss: 6.6291375160217285
2025-12-09 11:52:08.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0009789591576787476 Training loss: 6.760313034057617
2025-12-09 11:52:08.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0009788716545652352 Training loss: 6.587923526763916
2025-12-09 11:52:09.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0009787839738054146 Training loss: 7.269969940185547
2025-12-09 11:52:09.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0009786961154318121 Training loss: 6.610448360443115
2025-12-09 11:52:09.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0009786080794770206 Training loss: 6.912668228149414
2025-12-09 11:52:09.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0009785198659736987 Training loss: 6.706114768981934
2025-12-09 11:52:09.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0009784314749545706 Training loss: 7.382993221282959
2025-12-09 11:52:09.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0009783429064524269 Training loss: 7.256167888641357
2025-12-09 11:52:09.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0009782541605001234 Training loss: 6.628358364105225
2025-12-09 11:52:09.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.0009781652371305826 Training loss: 6.502195358276367
2025-12-09 11:52:09.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0009780761363767914 Training loss: 6.813991069793701
2025-12-09 11:52:09.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.000977986858271804 Training loss: 7.088616847991943
2025-12-09 11:52:09.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0009778974028487398 Training loss: 6.6760125160217285
2025-12-09 11:52:09.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0009778077701407836 Training loss: 6.865967273712158
2025-12-09 11:52:10.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0009777179601811866 Training loss: 6.940196990966797
2025-12-09 11:52:10.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0009776279730032654 Training loss: 6.768579483032227
2025-12-09 11:52:10.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.0009775378086404024 Training loss: 6.609304904937744
2025-12-09 11:52:10.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0009774474671260455 Training loss: 6.851918697357178
2025-12-09 11:52:10.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.000977356948493709 Training loss: 6.544038772583008
2025-12-09 11:52:10.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.000977266252776972 Training loss: 6.5261640548706055
2025-12-09 11:52:10.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0009771753800094803 Training loss: 6.965754985809326
2025-12-09 11:52:10.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0009770843302249442 Training loss: 6.5921454429626465
2025-12-09 11:52:10.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0009769931034571409 Training loss: 6.681338310241699
2025-12-09 11:52:10.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0009769016997399121 Training loss: 6.915383815765381
2025-12-09 11:52:10.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.000976810119107166 Training loss: 6.7612690925598145
2025-12-09 11:52:10.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0009767183615928764 Training loss: 7.09173583984375
2025-12-09 11:52:10.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.000976626427231082 Training loss: 6.5589518547058105
2025-12-09 11:52:11.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0009765343160558879 Training loss: 7.1286940574646
2025-12-09 11:52:11.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0009764420281014641 Training loss: 6.634679794311523
2025-12-09 11:52:11.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0009763495634020466 Training loss: 7.143068790435791
2025-12-09 11:52:11.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0009762569219919371 Training loss: 6.486456394195557
2025-12-09 11:52:11.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0009761641039055025 Training loss: 7.033620357513428
2025-12-09 11:52:11.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0009760711091771755 Training loss: 6.618304252624512
2025-12-09 11:52:11.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.0009759779378414542 Training loss: 7.13875675201416
2025-12-09 11:52:11.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0009758845899329021 Training loss: 6.769988059997559
2025-12-09 11:52:11.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0009757910654861482 Training loss: 6.95413875579834
2025-12-09 11:52:11.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0009756973645358876 Training loss: 6.866994857788086
2025-12-09 11:52:11.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0009756034871168799 Training loss: 6.396811008453369
2025-12-09 11:52:11.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0009755094332639511 Training loss: 7.147069454193115
2025-12-09 11:52:11.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.0009754152030119921 Training loss: 6.616905212402344
2025-12-09 11:52:12.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0009753207963959591 Training loss: 6.705846786499023
2025-12-09 11:52:12.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0009752262134508741 Training loss: 6.555948734283447
2025-12-09 11:52:12.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0009751314542118246 Training loss: 6.6627516746521
2025-12-09 11:52:12.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0009750365187139631 Training loss: 6.69158935546875
2025-12-09 11:52:12.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0009749414069925077 Training loss: 6.806446075439453
2025-12-09 11:52:12.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0009748461190827421 Training loss: 6.566777229309082
2025-12-09 11:52:12.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0009747506550200146 Training loss: 6.478417873382568
2025-12-09 11:52:12.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.0009746550148397397 Training loss: 7.002801418304443
2025-12-09 11:52:12.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0009745591985773971 Training loss: 6.7762131690979
2025-12-09 11:52:12.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0009744632062685312 Training loss: 6.729636192321777
2025-12-09 11:52:12.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0009743670379487523 Training loss: 7.427866458892822
2025-12-09 11:52:12.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0009742706936537357 Training loss: 6.64093017578125
2025-12-09 11:52:13.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0009741741734192224 Training loss: 6.510167598724365
2025-12-09 11:52:13.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0009740774772810182 Training loss: 8.304248809814453
2025-12-09 11:52:13.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0009739806052749942 Training loss: 6.930253982543945
2025-12-09 11:52:13.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0009738835574370871 Training loss: 6.767945766448975
2025-12-09 11:52:13.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0009737863338032984 Training loss: 7.151718616485596
2025-12-09 11:52:13.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0009736889344096951 Training loss: 6.902523040771484
2025-12-09 11:52:13.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0009735913592924093 Training loss: 6.4863972663879395
2025-12-09 11:52:13.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0009734936084876383 Training loss: 6.6821980476379395
2025-12-09 11:52:13.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0009733956820316443 Training loss: 7.218986511230469
2025-12-09 11:52:13.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0009732975799607554 Training loss: 6.480556964874268
2025-12-09 11:52:13.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0009731993023113641 Training loss: 6.656052112579346
2025-12-09 11:52:13.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0009731008491199284 Training loss: 6.812138557434082
2025-12-09 11:52:13.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0009730022204229714 Training loss: 6.7952423095703125
2025-12-09 11:52:14.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0009729034162570811 Training loss: 6.973319053649902
2025-12-09 11:52:14.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0009728044366589108 Training loss: 6.47096586227417
2025-12-09 11:52:14.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.0009727052816651788 Training loss: 6.630427360534668
2025-12-09 11:52:14.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0009726059513126685 Training loss: 6.596108436584473
2025-12-09 11:52:14.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0009725064456382282 Training loss: 6.998650550842285
2025-12-09 11:52:14.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0009724067646787717 Training loss: 6.88342809677124
2025-12-09 11:52:14.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0009723069084712771 Training loss: 6.634810447692871
2025-12-09 11:52:14.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0009722068770527882 Training loss: 6.842645168304443
2025-12-09 11:52:14.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0009721066704604133 Training loss: 6.6019673347473145
2025-12-09 11:52:14.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0009720062887313262 Training loss: 7.210868835449219
2025-12-09 11:52:14.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.000971905731902765 Training loss: 6.7473464012146
2025-12-09 11:52:14.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0009718050000120333 Training loss: 6.5270538330078125
2025-12-09 11:52:14.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0009717040930964996 Training loss: 6.493133068084717
2025-12-09 11:52:15.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0009716030111935968 Training loss: 6.478216648101807
2025-12-09 11:52:15.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0009715017543408234 Training loss: 6.97568416595459
2025-12-09 11:52:15.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0009714003225757424 Training loss: 6.112094879150391
2025-12-09 11:52:15.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0009712987159359818 Training loss: 6.8926591873168945
2025-12-09 11:52:15.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.0009711969344592346 Training loss: 6.619769096374512
2025-12-09 11:52:15.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0009710949781832585 Training loss: 6.339425563812256
2025-12-09 11:52:15.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0009709928471458759 Training loss: 7.685328483581543
2025-12-09 11:52:15.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0009708905413849743 Training loss: 6.851472854614258
2025-12-09 11:52:15.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0009707880609385058 Training loss: 7.002781867980957
2025-12-09 11:52:15.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0009706854058444876 Training loss: 6.621540546417236
2025-12-09 11:52:15.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0009705825761410014 Training loss: 5.927811145782471
2025-12-09 11:52:15.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0009704795718661938 Training loss: 7.565091609954834
2025-12-09 11:52:16.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.000970376393058276 Training loss: 6.860670566558838
2025-12-09 11:52:16.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.0009702730397555246 Training loss: 6.82455587387085
2025-12-09 11:52:16.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0009701695119962799 Training loss: 6.646599769592285
2025-12-09 11:52:16.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.0009700658098189476 Training loss: 6.656980991363525
2025-12-09 11:52:16.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0009699619332619979 Training loss: 6.3064494132995605
2025-12-09 11:52:16.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0009698578823639658 Training loss: 6.761999130249023
2025-12-09 11:52:16.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0009697536571634509 Training loss: 6.473255157470703
2025-12-09 11:52:16.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0009696492576991174 Training loss: 7.0078206062316895
2025-12-09 11:52:16.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.0009695446840096944 Training loss: 6.794955730438232
2025-12-09 11:52:16.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0009694399361339751 Training loss: 6.62446928024292
2025-12-09 11:52:16.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0009693350141108182 Training loss: 6.688542366027832
2025-12-09 11:52:16.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.000969229917979146 Training loss: 6.031070232391357
2025-12-09 11:52:16.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.000969124647777946 Training loss: 6.669424533843994
2025-12-09 11:52:17.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0009690192035462701 Training loss: 6.504159450531006
2025-12-09 11:52:17.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0009689135853232349 Training loss: 6.768467426300049
2025-12-09 11:52:17.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0009688077931480212 Training loss: 6.855185508728027
2025-12-09 11:52:17.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0009687018270598749 Training loss: 6.874655246734619
2025-12-09 11:52:17.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0009685956870981059 Training loss: 6.906521797180176
2025-12-09 11:52:17.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0009684893733020888 Training loss: 6.6745195388793945
2025-12-09 11:52:17.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0009683828857112626 Training loss: 7.301733016967773
2025-12-09 11:52:17.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0009682762243651309 Training loss: 6.5985918045043945
2025-12-09 11:52:17.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0009681693893032617 Training loss: 6.90736722946167
2025-12-09 11:52:17.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.0009680623805652876 Training loss: 6.889953136444092
2025-12-09 11:52:17.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.0009679551981909053 Training loss: 6.483595371246338
2025-12-09 11:52:17.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.000967847842219876 Training loss: 6.534161567687988
2025-12-09 11:52:17.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.0009677403126920255 Training loss: 6.636340618133545
2025-12-09 11:52:18.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0009676326096472441 Training loss: 6.538378715515137
2025-12-09 11:52:18.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0009675247331254858 Training loss: 6.704431533813477
2025-12-09 11:52:18.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.0009674166831667697 Training loss: 7.3016886711120605
2025-12-09 11:52:18.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0009673084598111788 Training loss: 6.827404022216797
2025-12-09 11:52:18.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0009672000630988605 Training loss: 6.503829479217529
2025-12-09 11:52:18.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.0009670914930700268 Training loss: 6.953782081604004
2025-12-09 11:52:18.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.0009669827497649536 Training loss: 6.46366024017334
2025-12-09 11:52:18.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.0009668738332239813 Training loss: 6.5514302253723145
2025-12-09 11:52:18.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0009667647434875144 Training loss: 6.568504333496094
2025-12-09 11:52:18.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0009666554805960219 Training loss: 7.155602931976318
2025-12-09 11:52:18.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.0009665460445900368 Training loss: 7.051901340484619
2025-12-09 11:52:18.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0009664364355101565 Training loss: 6.361676216125488
2025-12-09 11:52:19.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.0009663266533970423 Training loss: 6.632541656494141
2025-12-09 11:52:19.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0009662166982914202 Training loss: 6.462021827697754
2025-12-09 11:52:19.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00096610657023408 Training loss: 6.400888442993164
2025-12-09 11:52:19.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0009659962692658757 Training loss: 6.400732040405273
2025-12-09 11:52:19.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0009658857954277254 Training loss: 6.878224849700928
2025-12-09 11:52:19.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0009657751487606115 Training loss: 6.444714069366455
2025-12-09 11:52:19.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0009656643293055805 Training loss: 6.591592788696289
2025-12-09 11:52:19.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0009655533371037426 Training loss: 6.751415729522705
2025-12-09 11:52:19.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0009654421721962729 Training loss: 6.570725917816162
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.35 GiB is free. Including non-PyTorch memory, this process has 90.89 GiB memory in use. Of the allocated memory 89.10 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:08, 146.21it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 175.72it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:21, 121.51it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:10, 140.61it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:57, 172.25it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 212.30it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:53, 184.36it/s]Tokenizing texts:   2%|▏         | 172/10000 [00:00<00:54, 181.49it/s]Tokenizing texts:   2%|▏         | 196/10000 [00:01<00:49, 196.45it/s]Tokenizing texts:   2%|▏         | 218/10000 [00:01<00:48, 202.65it/s]Tokenizing texts:   2%|▏         | 239/10000 [00:01<00:48, 202.51it/s]Tokenizing texts:   3%|▎         | 260/10000 [00:01<00:48, 199.88it/s]Tokenizing texts:   3%|▎         | 291/10000 [00:01<00:42, 227.48it/s]Tokenizing texts:   3%|▎         | 315/10000 [00:01<00:42, 225.89it/s]Tokenizing texts:   3%|▎         | 338/10000 [00:01<00:53, 181.56it/s]Tokenizing texts:   4%|▎         | 363/10000 [00:01<00:48, 197.30it/s]Tokenizing texts:   4%|▍         | 385/10000 [00:02<00:52, 183.29it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 184.71it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:52, 184.01it/s]Tokenizing texts:   4%|▍         | 445/10000 [00:02<00:52, 183.41it/s]Tokenizing texts:   5%|▍         | 467/10000 [00:02<00:49, 192.80it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 216.43it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:40, 234.74it/s]Tokenizing texts:   6%|▌         | 550/10000 [00:02<00:51, 184.49it/s]Tokenizing texts:   6%|▌         | 579/10000 [00:02<00:44, 209.67it/s]Tokenizing texts:   6%|▌         | 603/10000 [00:03<00:45, 208.65it/s]Tokenizing texts:   6%|▋         | 626/10000 [00:03<00:47, 198.00it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 200.10it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:47, 198.27it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:49, 189.41it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 198.40it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 198.14it/s]Tokenizing texts:   8%|▊         | 766/10000 [00:03<00:44, 208.83it/s]Tokenizing texts:   8%|▊         | 788/10000 [00:04<00:45, 200.63it/s]Tokenizing texts:   8%|▊         | 812/10000 [00:04<00:43, 211.06it/s]Tokenizing texts:   8%|▊         | 834/10000 [00:04<00:43, 209.04it/s]Tokenizing texts:   9%|▊         | 859/10000 [00:04<00:41, 220.03it/s]Tokenizing texts:   9%|▉         | 882/10000 [00:04<00:42, 213.54it/s]Tokenizing texts:   9%|▉         | 909/10000 [00:04<00:39, 228.65it/s]Tokenizing texts:   9%|▉         | 939/10000 [00:04<00:37, 244.85it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:04<00:33, 266.22it/s]Tokenizing texts:  10%|█         | 1001/10000 [00:04<00:33, 270.39it/s]Tokenizing texts:  10%|█         | 1032/10000 [00:04<00:31, 280.88it/s]Tokenizing texts:  11%|█         | 1061/10000 [00:05<00:40, 223.04it/s]Tokenizing texts:  11%|█         | 1086/10000 [00:05<00:39, 223.91it/s]Tokenizing texts:  11%|█         | 1110/10000 [00:05<00:39, 226.38it/s]Tokenizing texts:  11%|█▏        | 1134/10000 [00:05<00:46, 192.67it/s]Tokenizing texts:  12%|█▏        | 1159/10000 [00:05<00:43, 203.82it/s]Tokenizing texts:  12%|█▏        | 1184/10000 [00:05<00:41, 214.60it/s]Tokenizing texts:  12%|█▏        | 1207/10000 [00:05<00:43, 201.96it/s]Tokenizing texts:  12%|█▏        | 1235/10000 [00:06<00:41, 209.96it/s]Tokenizing texts:  13%|█▎        | 1257/10000 [00:06<00:42, 203.74it/s]Tokenizing texts:  13%|█▎        | 1291/10000 [00:06<00:36, 239.12it/s]Tokenizing texts:  13%|█▎        | 1316/10000 [00:06<00:37, 233.54it/s]Tokenizing texts:  13%|█▎        | 1340/10000 [00:06<00:43, 201.18it/s]Tokenizing texts:  14%|█▎        | 1368/10000 [00:06<00:39, 218.07it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:40, 213.61it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:37, 230.90it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:34, 248.06it/s]Tokenizing texts:  15%|█▍        | 1487/10000 [00:07<00:31, 269.66it/s]Tokenizing texts:  15%|█▌        | 1515/10000 [00:07<00:32, 264.14it/s]Tokenizing texts:  15%|█▌        | 1542/10000 [00:07<00:33, 248.78it/s]Tokenizing texts:  16%|█▌        | 1574/10000 [00:07<00:31, 266.08it/s]Tokenizing texts:  16%|█▌        | 1602/10000 [00:07<00:31, 269.84it/s]Tokenizing texts:  16%|█▋        | 1630/10000 [00:07<00:41, 200.55it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:39, 210.34it/s]Tokenizing texts:  17%|█▋        | 1681/10000 [00:07<00:38, 213.57it/s]Tokenizing texts:  17%|█▋        | 1704/10000 [00:08<00:38, 217.65it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:34, 239.31it/s]Tokenizing texts:  18%|█▊        | 1760/10000 [00:08<00:34, 240.10it/s]Tokenizing texts:  18%|█▊        | 1786/10000 [00:08<00:33, 241.94it/s]Tokenizing texts:  18%|█▊        | 1820/10000 [00:08<00:30, 268.36it/s]Tokenizing texts:  18%|█▊        | 1848/10000 [00:08<00:37, 215.13it/s]Tokenizing texts:  19%|█▊        | 1872/10000 [00:08<00:37, 217.81it/s]Tokenizing texts:  19%|█▉        | 1896/10000 [00:08<00:36, 222.31it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:08<00:36, 222.75it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 193.10it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:38, 208.56it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:35, 225.80it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 231.24it/s]Tokenizing texts:  21%|██        | 2054/10000 [00:09<00:32, 244.03it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:38, 205.88it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:38, 206.78it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:37, 211.54it/s]Tokenizing texts:  22%|██▏       | 2150/10000 [00:10<00:39, 200.73it/s]Tokenizing texts:  22%|██▏       | 2179/10000 [00:10<00:35, 223.05it/s]Tokenizing texts:  22%|██▏       | 2209/10000 [00:10<00:32, 243.47it/s]Tokenizing texts:  22%|██▏       | 2239/10000 [00:10<00:29, 258.81it/s]Tokenizing texts:  23%|██▎       | 2269/10000 [00:10<00:29, 266.28it/s]Tokenizing texts:  23%|██▎       | 2297/10000 [00:10<00:30, 253.66it/s]Tokenizing texts:  23%|██▎       | 2323/10000 [00:10<00:35, 217.02it/s]Tokenizing texts:  23%|██▎       | 2349/10000 [00:10<00:33, 226.82it/s]Tokenizing texts:  24%|██▍       | 2382/10000 [00:10<00:30, 253.01it/s]Tokenizing texts:  24%|██▍       | 2413/10000 [00:11<00:28, 267.09it/s]Tokenizing texts:  24%|██▍       | 2441/10000 [00:11<00:31, 241.91it/s]Tokenizing texts:  25%|██▍       | 2473/10000 [00:11<00:29, 259.16it/s]Tokenizing texts:  25%|██▌       | 2500/10000 [00:11<00:31, 239.88it/s]Tokenizing texts:  25%|██▌       | 2531/10000 [00:11<00:29, 257.36it/s]Tokenizing texts:  26%|██▌       | 2558/10000 [00:11<00:29, 252.31it/s]Tokenizing texts:  26%|██▌       | 2584/10000 [00:11<00:35, 210.01it/s]Tokenizing texts:  26%|██▌       | 2607/10000 [00:11<00:35, 208.48it/s]Tokenizing texts:  26%|██▋       | 2629/10000 [00:12<00:36, 204.60it/s]Tokenizing texts:  27%|██▋       | 2660/10000 [00:12<00:32, 229.22it/s]Tokenizing texts:  27%|██▋       | 2684/10000 [00:12<00:35, 208.35it/s]Tokenizing texts:  27%|██▋       | 2706/10000 [00:12<00:37, 196.46it/s]Tokenizing texts:  27%|██▋       | 2737/10000 [00:12<00:32, 224.74it/s]Tokenizing texts:  28%|██▊       | 2768/10000 [00:12<00:29, 245.85it/s]Tokenizing texts:  28%|██▊       | 2794/10000 [00:12<00:39, 181.94it/s]Tokenizing texts:  28%|██▊       | 2827/10000 [00:13<00:33, 211.62it/s]Tokenizing texts:  29%|██▊       | 2852/10000 [00:13<00:33, 210.37it/s]Tokenizing texts:  29%|██▉       | 2876/10000 [00:13<00:33, 210.31it/s]Tokenizing texts:  29%|██▉       | 2903/10000 [00:13<00:31, 223.66it/s]Tokenizing texts:  29%|██▉       | 2935/10000 [00:13<00:28, 245.96it/s]Tokenizing texts:  30%|██▉       | 2961/10000 [00:13<00:30, 229.22it/s]Tokenizing texts:  30%|██▉       | 2985/10000 [00:13<00:30, 228.37it/s]Tokenizing texts:  30%|███       | 3021/10000 [00:13<00:26, 259.09it/s]Tokenizing texts:  30%|███       | 3048/10000 [00:13<00:30, 226.28it/s]Tokenizing texts:  31%|███       | 3081/10000 [00:14<00:27, 247.30it/s]Tokenizing texts:  31%|███       | 3107/10000 [00:14<00:28, 244.63it/s]Tokenizing texts:  31%|███▏      | 3139/10000 [00:14<00:26, 257.80it/s]Tokenizing texts:  32%|███▏      | 3166/10000 [00:14<00:27, 250.70it/s]Tokenizing texts:  32%|███▏      | 3196/10000 [00:14<00:25, 262.98it/s]Tokenizing texts:  32%|███▏      | 3223/10000 [00:14<00:25, 264.72it/s]Tokenizing texts:  32%|███▎      | 3250/10000 [00:14<00:26, 252.48it/s]Tokenizing texts:  33%|███▎      | 3281/10000 [00:14<00:25, 263.67it/s]Tokenizing texts:  33%|███▎      | 3308/10000 [00:14<00:25, 259.49it/s]Tokenizing texts:  33%|███▎      | 3335/10000 [00:15<00:27, 245.46it/s]Tokenizing texts:  34%|███▎      | 3365/10000 [00:15<00:25, 255.20it/s]Tokenizing texts:  34%|███▍      | 3391/10000 [00:15<00:26, 248.38it/s]Tokenizing texts:  34%|███▍      | 3419/10000 [00:15<00:25, 256.87it/s]Tokenizing texts:  35%|███▍      | 3458/10000 [00:15<00:22, 292.08it/s]Tokenizing texts:  35%|███▍      | 3488/10000 [00:15<00:22, 287.61it/s]Tokenizing texts:  35%|███▌      | 3517/10000 [00:15<00:26, 242.31it/s]Tokenizing texts:  35%|███▌      | 3543/10000 [00:15<00:28, 229.88it/s]Tokenizing texts:  36%|███▌      | 3569/10000 [00:15<00:27, 237.39it/s]Tokenizing texts:  36%|███▌      | 3600/10000 [00:16<00:25, 251.02it/s]Tokenizing texts:  36%|███▋      | 3626/10000 [00:16<00:25, 249.96it/s]Tokenizing texts:  37%|███▋      | 3652/10000 [00:16<00:25, 248.25it/s]Tokenizing texts:  37%|███▋      | 3678/10000 [00:16<00:25, 245.19it/s]Tokenizing texts:  37%|███▋      | 3705/10000 [00:16<00:25, 249.53it/s]Tokenizing texts:  37%|███▋      | 3732/10000 [00:16<00:24, 253.64it/s]Tokenizing texts:  38%|███▊      | 3758/10000 [00:16<00:28, 222.81it/s]Tokenizing texts:  38%|███▊      | 3784/10000 [00:16<00:26, 231.63it/s]Tokenizing texts:  38%|███▊      | 3808/10000 [00:17<00:30, 204.02it/s]Tokenizing texts:  38%|███▊      | 3839/10000 [00:17<00:26, 230.71it/s]Tokenizing texts:  39%|███▊      | 3867/10000 [00:17<00:25, 241.00it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:17<00:24, 249.03it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:26, 233.57it/s]Tokenizing texts:  39%|███▉      | 3945/10000 [00:17<00:26, 228.30it/s]Tokenizing texts:  40%|███▉      | 3976/10000 [00:17<00:24, 250.00it/s]Tokenizing texts:  40%|████      | 4002/10000 [00:17<00:26, 223.51it/s]Tokenizing texts:  40%|████      | 4030/10000 [00:17<00:25, 237.23it/s]Tokenizing texts:  41%|████      | 4056/10000 [00:18<00:24, 242.90it/s]Tokenizing texts:  41%|████      | 4089/10000 [00:18<00:22, 265.32it/s]Tokenizing texts:  41%|████      | 4121/10000 [00:18<00:21, 279.00it/s]Tokenizing texts:  42%|████▏     | 4150/10000 [00:18<00:20, 280.05it/s]Tokenizing texts:  42%|████▏     | 4179/10000 [00:18<00:21, 274.36it/s]Tokenizing texts:  42%|████▏     | 4214/10000 [00:18<00:19, 295.71it/s]Tokenizing texts:  42%|████▏     | 4244/10000 [00:18<00:24, 234.53it/s]Tokenizing texts:  43%|████▎     | 4270/10000 [00:18<00:24, 235.56it/s]Tokenizing texts:  43%|████▎     | 4297/10000 [00:18<00:23, 241.06it/s]Tokenizing texts:  43%|████▎     | 4333/10000 [00:19<00:20, 272.43it/s]Tokenizing texts:  44%|████▎     | 4362/10000 [00:19<00:20, 272.70it/s]Tokenizing texts:  44%|████▍     | 4393/10000 [00:19<00:20, 280.12it/s]Tokenizing texts:  44%|████▍     | 4424/10000 [00:19<00:19, 288.12it/s]Tokenizing texts:  45%|████▍     | 4454/10000 [00:19<00:21, 258.73it/s]Tokenizing texts:  45%|████▍     | 4481/10000 [00:19<00:21, 260.34it/s]Tokenizing texts:  45%|████▌     | 4508/10000 [00:19<00:22, 240.08it/s]Tokenizing texts:  45%|████▌     | 4533/10000 [00:19<00:22, 241.28it/s]Tokenizing texts:  46%|████▌     | 4563/10000 [00:19<00:21, 256.52it/s]Tokenizing texts:  46%|████▌     | 4590/10000 [00:20<00:21, 246.25it/s]Tokenizing texts:  46%|████▌     | 4616/10000 [00:20<00:23, 232.90it/s]Tokenizing texts:  47%|████▋     | 4657/10000 [00:20<00:19, 277.46it/s]Tokenizing texts:  47%|████▋     | 4690/10000 [00:20<00:18, 290.15it/s]Tokenizing texts:  47%|████▋     | 4727/10000 [00:20<00:16, 310.73it/s]Tokenizing texts:  48%|████▊     | 4759/10000 [00:20<00:18, 276.07it/s]Tokenizing texts:  48%|████▊     | 4788/10000 [00:20<00:21, 247.51it/s]Tokenizing texts:  48%|████▊     | 4816/10000 [00:20<00:20, 254.16it/s]Tokenizing texts:  48%|████▊     | 4849/10000 [00:21<00:18, 273.03it/s]Tokenizing texts:  49%|████▉     | 4878/10000 [00:21<00:19, 263.71it/s]Tokenizing texts:  49%|████▉     | 4910/10000 [00:21<00:18, 269.34it/s]Tokenizing texts:  49%|████▉     | 4940/10000 [00:21<00:18, 275.23it/s]Tokenizing texts:  50%|████▉     | 4968/10000 [00:21<00:19, 259.23it/s]Tokenizing texts:  50%|█████     | 5005/10000 [00:21<00:17, 288.61it/s]Tokenizing texts:  50%|█████     | 5035/10000 [00:21<00:18, 274.26it/s]Tokenizing texts:  51%|█████     | 5063/10000 [00:21<00:19, 254.27it/s]Tokenizing texts:  51%|█████     | 5089/10000 [00:21<00:21, 233.56it/s]Tokenizing texts:  51%|█████     | 5118/10000 [00:22<00:19, 246.35it/s]Tokenizing texts:  51%|█████▏    | 5144/10000 [00:22<00:20, 238.02it/s]Tokenizing texts:  52%|█████▏    | 5171/10000 [00:22<00:19, 245.01it/s]Tokenizing texts:  52%|█████▏    | 5198/10000 [00:22<00:19, 251.68it/s]Tokenizing texts:  52%|█████▏    | 5224/10000 [00:22<00:20, 236.94it/s]Tokenizing texts:  53%|█████▎    | 5258/10000 [00:22<00:17, 265.03it/s]Tokenizing texts:  53%|█████▎    | 5286/10000 [00:22<00:19, 246.60it/s]Tokenizing texts:  53%|█████▎    | 5321/10000 [00:22<00:17, 274.31it/s]Tokenizing texts:  54%|█████▎    | 5354/10000 [00:22<00:16, 289.14it/s]Tokenizing texts:  54%|█████▍    | 5385/10000 [00:23<00:15, 291.97it/s]Tokenizing texts:  54%|█████▍    | 5416/10000 [00:23<00:15, 296.83it/s]Tokenizing texts:  54%|█████▍    | 5447/10000 [00:23<00:17, 267.77it/s]Tokenizing texts:  55%|█████▍    | 5475/10000 [00:23<00:17, 258.32it/s]Tokenizing texts:  55%|█████▌    | 5502/10000 [00:23<00:21, 207.25it/s]Tokenizing texts:  55%|█████▌    | 5537/10000 [00:23<00:18, 238.98it/s]Tokenizing texts:  56%|█████▌    | 5564/10000 [00:23<00:18, 246.32it/s]Tokenizing texts:  56%|█████▌    | 5591/10000 [00:23<00:17, 247.49it/s]Tokenizing texts:  56%|█████▋    | 5625/10000 [00:24<00:16, 271.86it/s]Tokenizing texts:  57%|█████▋    | 5662/10000 [00:24<00:14, 297.84it/s]Tokenizing texts:  57%|█████▋    | 5693/10000 [00:24<00:14, 300.10it/s]Tokenizing texts:  57%|█████▋    | 5724/10000 [00:24<00:14, 298.17it/s]Tokenizing texts:  58%|█████▊    | 5755/10000 [00:24<00:16, 259.53it/s]Tokenizing texts:  58%|█████▊    | 5783/10000 [00:24<00:17, 241.72it/s]Tokenizing texts:  58%|█████▊    | 5809/10000 [00:24<00:20, 205.50it/s]Tokenizing texts:  58%|█████▊    | 5833/10000 [00:24<00:19, 211.09it/s]Tokenizing texts:  59%|█████▊    | 5865/10000 [00:24<00:17, 235.51it/s]Tokenizing texts:  59%|█████▉    | 5891/10000 [00:25<00:17, 241.20it/s]Tokenizing texts:  59%|█████▉    | 5917/10000 [00:25<00:16, 242.22it/s]Tokenizing texts:  59%|█████▉    | 5942/10000 [00:25<00:16, 240.95it/s]Tokenizing texts:  60%|█████▉    | 5968/10000 [00:25<00:16, 242.80it/s]Tokenizing texts:  60%|█████▉    | 5993/10000 [00:25<00:16, 243.50it/s]Tokenizing texts:  60%|██████    | 6018/10000 [00:25<00:16, 235.43it/s]Tokenizing texts:  61%|██████    | 6051/10000 [00:25<00:15, 261.61it/s]Tokenizing texts:  61%|██████    | 6078/10000 [00:25<00:14, 261.77it/s]Tokenizing texts:  61%|██████    | 6105/10000 [00:25<00:15, 255.54it/s]Tokenizing texts:  61%|██████▏   | 6138/10000 [00:26<00:14, 274.81it/s]Tokenizing texts:  62%|██████▏   | 6166/10000 [00:26<00:13, 274.27it/s]Tokenizing texts:  62%|██████▏   | 6194/10000 [00:26<00:13, 273.78it/s]Tokenizing texts:  62%|██████▏   | 6222/10000 [00:26<00:15, 241.26it/s]Tokenizing texts:  63%|██████▎   | 6252/10000 [00:26<00:14, 256.94it/s]Tokenizing texts:  63%|██████▎   | 6279/10000 [00:26<00:15, 246.60it/s]Tokenizing texts:  63%|██████▎   | 6305/10000 [00:26<00:15, 242.37it/s]Tokenizing texts:  63%|██████▎   | 6330/10000 [00:26<00:16, 219.96it/s]Tokenizing texts:  64%|██████▎   | 6357/10000 [00:26<00:15, 231.52it/s]Tokenizing texts:  64%|██████▍   | 6386/10000 [00:27<00:14, 242.96it/s]Tokenizing texts:  64%|██████▍   | 6414/10000 [00:27<00:14, 251.59it/s]Tokenizing texts:  64%|██████▍   | 6442/10000 [00:27<00:14, 251.97it/s]Tokenizing texts:  65%|██████▍   | 6468/10000 [00:27<00:14, 246.43it/s]Tokenizing texts:  65%|██████▍   | 6493/10000 [00:27<00:14, 245.07it/s]Tokenizing texts:  65%|██████▌   | 6518/10000 [00:27<00:15, 227.69it/s]Tokenizing texts:  66%|██████▌   | 6550/10000 [00:27<00:13, 250.60it/s]Tokenizing texts:  66%|██████▌   | 6589/10000 [00:27<00:11, 288.75it/s]Tokenizing texts:  66%|██████▌   | 6619/10000 [00:27<00:12, 266.05it/s]Tokenizing texts:  66%|██████▋   | 6648/10000 [00:28<00:12, 271.32it/s]Tokenizing texts:  67%|██████▋   | 6676/10000 [00:28<00:12, 259.07it/s]Tokenizing texts:  67%|██████▋   | 6703/10000 [00:28<00:13, 237.26it/s]Tokenizing texts:  67%|██████▋   | 6732/10000 [00:28<00:13, 243.87it/s]Tokenizing texts:  68%|██████▊   | 6760/10000 [00:28<00:12, 251.94it/s]Tokenizing texts:  68%|██████▊   | 6799/10000 [00:28<00:11, 287.01it/s]Tokenizing texts:  68%|██████▊   | 6831/10000 [00:28<00:10, 289.56it/s]Tokenizing texts:  69%|██████▊   | 6864/10000 [00:28<00:10, 300.60it/s]Tokenizing texts:  69%|██████▉   | 6895/10000 [00:29<00:13, 234.45it/s]Tokenizing texts:  69%|██████▉   | 6921/10000 [00:29<00:13, 227.44it/s]Tokenizing texts:  69%|██████▉   | 6947/10000 [00:29<00:12, 235.13it/s]Tokenizing texts:  70%|██████▉   | 6976/10000 [00:29<00:12, 249.06it/s]Tokenizing texts:  70%|███████   | 7003/10000 [00:29<00:12, 249.42it/s]Tokenizing texts:  70%|███████   | 7029/10000 [00:29<00:12, 241.56it/s]Tokenizing texts:  71%|███████   | 7054/10000 [00:29<00:12, 236.85it/s]Tokenizing texts:  71%|███████   | 7079/10000 [00:29<00:13, 212.45it/s]Tokenizing texts:  71%|███████   | 7101/10000 [00:29<00:13, 213.02it/s]Tokenizing texts:  71%|███████▏  | 7129/10000 [00:30<00:12, 229.77it/s]Tokenizing texts:  72%|███████▏  | 7153/10000 [00:30<00:12, 230.80it/s]Tokenizing texts:  72%|███████▏  | 7181/10000 [00:30<00:11, 244.58it/s]Tokenizing texts:  72%|███████▏  | 7207/10000 [00:30<00:11, 247.96it/s]Tokenizing texts:  72%|███████▏  | 7233/10000 [00:30<00:11, 245.72it/s]Tokenizing texts:  73%|███████▎  | 7264/10000 [00:30<00:10, 261.33it/s]Tokenizing texts:  73%|███████▎  | 7303/10000 [00:30<00:09, 296.90it/s]Tokenizing texts:  73%|███████▎  | 7335/10000 [00:30<00:08, 301.25it/s]Tokenizing texts:  74%|███████▎  | 7366/10000 [00:30<00:10, 262.78it/s]Tokenizing texts:  74%|███████▍  | 7394/10000 [00:31<00:09, 267.04it/s]Tokenizing texts:  74%|███████▍  | 7422/10000 [00:31<00:11, 222.45it/s]Tokenizing texts:  74%|███████▍  | 7446/10000 [00:31<00:12, 201.88it/s]Tokenizing texts:  75%|███████▍  | 7478/10000 [00:31<00:11, 227.16it/s]Tokenizing texts:  75%|███████▌  | 7514/10000 [00:31<00:09, 260.42it/s]Tokenizing texts:  75%|███████▌  | 7542/10000 [00:31<00:09, 262.69it/s]Tokenizing texts:  76%|███████▌  | 7570/10000 [00:31<00:09, 262.96it/s]Tokenizing texts:  76%|███████▌  | 7608/10000 [00:31<00:08, 291.94it/s]Tokenizing texts:  76%|███████▋  | 7638/10000 [00:32<00:08, 269.62it/s]Tokenizing texts:  77%|███████▋  | 7668/10000 [00:32<00:08, 277.12it/s]Tokenizing texts:  77%|███████▋  | 7701/10000 [00:32<00:08, 286.12it/s]Tokenizing texts:  77%|███████▋  | 7731/10000 [00:32<00:08, 265.12it/s]Tokenizing texts:  78%|███████▊  | 7759/10000 [00:32<00:09, 236.71it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 253.33it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 234.04it/s]Tokenizing texts:  78%|███████▊  | 7845/10000 [00:32<00:08, 241.87it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 253.89it/s]Tokenizing texts:  79%|███████▉  | 7901/10000 [00:33<00:08, 250.01it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:33<00:07, 280.25it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:33<00:07, 278.34it/s]Tokenizing texts:  80%|███████▉  | 7999/10000 [00:33<00:06, 289.34it/s]Tokenizing texts:  80%|████████  | 8029/10000 [00:33<00:06, 286.32it/s]Tokenizing texts:  81%|████████  | 8058/10000 [00:33<00:11, 169.16it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 199.21it/s]Tokenizing texts:  81%|████████  | 8120/10000 [00:34<00:09, 208.09it/s]Tokenizing texts:  81%|████████▏ | 8149/10000 [00:34<00:08, 225.51it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:34<00:07, 237.88it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:34<00:07, 248.30it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 212.47it/s]Tokenizing texts:  83%|████████▎ | 8264/10000 [00:34<00:07, 221.19it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 256.76it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 252.35it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:35<00:06, 256.23it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:35<00:06, 255.66it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:35<00:06, 231.60it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:35<00:06, 257.11it/s]Tokenizing texts:  85%|████████▍ | 8472/10000 [00:35<00:05, 262.62it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:35<00:05, 277.74it/s]Tokenizing texts:  85%|████████▌ | 8533/10000 [00:35<00:05, 259.97it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:35<00:05, 265.63it/s]Tokenizing texts:  86%|████████▌ | 8590/10000 [00:35<00:05, 260.87it/s]Tokenizing texts:  86%|████████▌ | 8624/10000 [00:36<00:04, 281.97it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:36<00:04, 283.61it/s]Tokenizing texts:  87%|████████▋ | 8686/10000 [00:36<00:04, 291.04it/s]Tokenizing texts:  87%|████████▋ | 8722/10000 [00:36<00:04, 310.49it/s]Tokenizing texts:  88%|████████▊ | 8754/10000 [00:36<00:04, 278.41it/s]Tokenizing texts:  88%|████████▊ | 8785/10000 [00:36<00:04, 283.01it/s]Tokenizing texts:  88%|████████▊ | 8814/10000 [00:36<00:05, 216.49it/s]Tokenizing texts:  88%|████████▊ | 8839/10000 [00:36<00:05, 218.21it/s]Tokenizing texts:  89%|████████▊ | 8871/10000 [00:36<00:04, 242.69it/s]Tokenizing texts:  89%|████████▉ | 8910/10000 [00:37<00:03, 280.08it/s]Tokenizing texts:  89%|████████▉ | 8941/10000 [00:37<00:03, 283.24it/s]Tokenizing texts:  90%|████████▉ | 8971/10000 [00:37<00:03, 274.92it/s]Tokenizing texts:  90%|█████████ | 9000/10000 [00:37<00:04, 247.17it/s]Tokenizing texts:  90%|█████████ | 9030/10000 [00:37<00:03, 258.13it/s]Tokenizing texts:  91%|█████████ | 9057/10000 [00:37<00:03, 248.98it/s]Tokenizing texts:  91%|█████████ | 9085/10000 [00:37<00:03, 254.83it/s]Tokenizing texts:  91%|█████████ | 9114/10000 [00:37<00:03, 263.50it/s]Tokenizing texts:  91%|█████████▏| 9144/10000 [00:37<00:03, 273.53it/s]Tokenizing texts:  92%|█████████▏| 9172/10000 [00:38<00:03, 273.93it/s]Tokenizing texts:  92%|█████████▏| 9205/10000 [00:38<00:02, 280.16it/s]Tokenizing texts:  92%|█████████▏| 9234/10000 [00:38<00:02, 271.57it/s]Tokenizing texts:  93%|█████████▎| 9262/10000 [00:38<00:02, 271.58it/s]Tokenizing texts:  93%|█████████▎| 9290/10000 [00:38<00:02, 251.52it/s]Tokenizing texts:  93%|█████████▎| 9316/10000 [00:38<00:02, 228.80it/s]Tokenizing texts:  93%|█████████▎| 9348/10000 [00:38<00:02, 250.83it/s]Tokenizing texts:  94%|█████████▎| 9374/10000 [00:38<00:02, 217.52it/s]Tokenizing texts:  94%|█████████▍| 9403/10000 [00:39<00:02, 227.42it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:39<00:02, 227.41it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:39<00:02, 231.58it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:39<00:02, 239.98it/s]Tokenizing texts:  95%|█████████▌| 9504/10000 [00:39<00:02, 179.56it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 218.87it/s]Tokenizing texts:  96%|█████████▌| 9565/10000 [00:39<00:01, 224.91it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 230.36it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:40<00:01, 228.80it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:40<00:01, 231.56it/s]Tokenizing texts:  97%|█████████▋| 9673/10000 [00:40<00:01, 235.71it/s]Tokenizing texts:  97%|█████████▋| 9698/10000 [00:40<00:01, 232.97it/s]Tokenizing texts:  97%|█████████▋| 9723/10000 [00:40<00:01, 237.61it/s]Tokenizing texts:  97%|█████████▋| 9748/10000 [00:40<00:01, 203.98it/s]Tokenizing texts:  98%|█████████▊| 9773/10000 [00:40<00:01, 214.80it/s]Tokenizing texts:  98%|█████████▊| 9808/10000 [00:40<00:00, 250.18it/s]Tokenizing texts:  98%|█████████▊| 9835/10000 [00:40<00:00, 251.46it/s]Tokenizing texts:  99%|█████████▊| 9867/10000 [00:41<00:00, 267.03it/s]Tokenizing texts:  99%|█████████▉| 9898/10000 [00:41<00:00, 277.06it/s]Tokenizing texts:  99%|█████████▉| 9927/10000 [00:41<00:00, 272.07it/s]Tokenizing texts: 100%|█████████▉| 9960/10000 [00:41<00:00, 287.67it/s]Tokenizing texts: 100%|█████████▉| 9994/10000 [00:41<00:00, 294.54it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 240.94it/s]
2025-12-09 11:53:20.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 12.009098052978516
2025-12-09 11:53:20.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 12.017152786254883
2025-12-09 11:53:20.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 12.02890396118164
2025-12-09 11:53:20.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 11.899643898010254
2025-12-09 11:53:20.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 11.789310455322266
2025-12-09 11:53:21.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 11.64932632446289
2025-12-09 11:53:21.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 11.285983085632324
2025-12-09 11:53:21.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 11.192520141601562
2025-12-09 11:53:21.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 11.120076179504395
2025-12-09 11:53:21.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 11.165185928344727
2025-12-09 11:53:21.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 10.959159851074219
2025-12-09 11:53:21.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 10.831549644470215
2025-12-09 11:53:21.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 10.561138153076172
2025-12-09 11:53:21.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 10.507753372192383
2025-12-09 11:53:21.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 10.31694507598877
2025-12-09 11:53:21.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 10.122363090515137
2025-12-09 11:53:21.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 9.893074035644531
2025-12-09 11:53:21.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 9.617085456848145
2025-12-09 11:53:22.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 9.508902549743652
2025-12-09 11:53:22.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 9.309374809265137
2025-12-09 11:53:22.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 9.269715309143066
2025-12-09 11:53:22.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 9.283414840698242
2025-12-09 11:53:22.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 8.830001831054688
2025-12-09 11:53:22.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 8.615086555480957
2025-12-09 11:53:22.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 8.491084098815918
2025-12-09 11:53:22.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 8.850181579589844
2025-12-09 11:53:22.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 8.423858642578125
2025-12-09 11:53:22.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 8.208181381225586
2025-12-09 11:53:22.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 7.875988960266113
2025-12-09 11:53:22.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 8.150135040283203
2025-12-09 11:53:22.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 8.160780906677246
2025-12-09 11:53:23.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 7.88335657119751
2025-12-09 11:53:23.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 8.132401466369629
2025-12-09 11:53:23.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 7.835785865783691
2025-12-09 11:53:23.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 8.217789649963379
2025-12-09 11:53:23.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 8.464860916137695
2025-12-09 11:53:23.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 8.110757827758789
2025-12-09 11:53:23.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 7.812037467956543
2025-12-09 11:53:23.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 8.245192527770996
2025-12-09 11:53:23.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 7.824046611785889
2025-12-09 11:53:23.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 8.083730697631836
2025-12-09 11:53:23.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 8.547029495239258
2025-12-09 11:53:23.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 8.400196075439453
2025-12-09 11:53:23.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 7.985250473022461
2025-12-09 11:53:24.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 8.210016250610352
2025-12-09 11:53:24.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 7.951878547668457
2025-12-09 11:53:24.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 8.11397933959961
2025-12-09 11:53:24.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 7.9811787605285645
2025-12-09 11:53:24.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 8.835654258728027
2025-12-09 11:53:24.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 7.843908309936523
2025-12-09 11:53:24.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 8.321881294250488
2025-12-09 11:53:24.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 8.168910026550293
2025-12-09 11:53:24.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 7.992438316345215
2025-12-09 11:53:24.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 8.299551963806152
2025-12-09 11:53:24.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 7.468395709991455
2025-12-09 11:53:24.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 8.207155227661133
2025-12-09 11:53:25.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 8.211689949035645
2025-12-09 11:53:25.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 8.211209297180176
2025-12-09 11:53:25.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 7.989394664764404
2025-12-09 11:53:25.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 8.028884887695312
2025-12-09 11:53:25.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 8.304654121398926
2025-12-09 11:53:25.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 8.128897666931152
2025-12-09 11:53:25.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 8.261049270629883
2025-12-09 11:53:25.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 8.015047073364258
2025-12-09 11:53:25.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 8.42009162902832
2025-12-09 11:53:25.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 7.938027381896973
2025-12-09 11:53:25.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 8.51538372039795
2025-12-09 11:53:25.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 8.185559272766113
2025-12-09 11:53:25.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 8.19789981842041
2025-12-09 11:53:26.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 8.125591278076172
2025-12-09 11:53:26.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 7.9817795753479
2025-12-09 11:53:26.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 8.069066047668457
2025-12-09 11:53:26.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 7.981679916381836
2025-12-09 11:53:26.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 8.059549331665039
2025-12-09 11:53:26.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 8.007674217224121
2025-12-09 11:53:26.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 8.281046867370605
2025-12-09 11:53:26.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 8.637401580810547
2025-12-09 11:53:26.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 8.535751342773438
2025-12-09 11:53:26.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 7.803657531738281
2025-12-09 11:53:26.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 8.067160606384277
2025-12-09 11:53:26.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 7.963364601135254
2025-12-09 11:53:26.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 8.189638137817383
2025-12-09 11:53:27.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 8.086050987243652
2025-12-09 11:53:27.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 8.381065368652344
2025-12-09 11:53:27.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 7.651259422302246
2025-12-09 11:53:27.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 8.059021949768066
2025-12-09 11:53:27.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 7.75242805480957
2025-12-09 11:53:27.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 7.982885837554932
2025-12-09 11:53:27.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 7.874251365661621
2025-12-09 11:53:27.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 8.271088600158691
2025-12-09 11:53:27.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 8.147571563720703
2025-12-09 11:53:27.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 8.166125297546387
2025-12-09 11:53:27.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 7.8411712646484375
2025-12-09 11:53:27.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 7.877776622772217
2025-12-09 11:53:27.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 8.384093284606934
2025-12-09 11:53:28.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 7.893198013305664
2025-12-09 11:53:28.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 8.022703170776367
2025-12-09 11:53:28.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 8.025874137878418
2025-12-09 11:53:28.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 8.067768096923828
2025-12-09 11:53:28.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 7.969457149505615
2025-12-09 11:53:28.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999997217736107 Training loss: 7.951020240783691
2025-12-09 11:53:28.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002999998887094546 Training loss: 8.122194290161133
2025-12-09 11:53:28.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029999974959631155 Training loss: 7.956821918487549
2025-12-09 11:53:28.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0029999955483798347 Training loss: 8.117585182189941
2025-12-09 11:53:28.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0029999930443454273 Training loss: 7.584741115570068
2025-12-09 11:53:28.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.002999989983860821 Training loss: 8.389920234680176
2025-12-09 11:53:28.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029999863669271528 Training loss: 7.868276119232178
2025-12-09 11:53:29.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0029999821935457623 Training loss: 8.09997844696045
2025-12-09 11:53:29.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0029999774637181993 Training loss: 7.852794170379639
2025-12-09 11:53:29.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.002999972177446218 Training loss: 7.834774017333984
2025-12-09 11:53:29.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.002999966334731779 Training loss: 7.692677974700928
2025-12-09 11:53:29.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0029999599355770503 Training loss: 9.47362995147705
2025-12-09 11:53:29.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029999529799844054 Training loss: 7.911147117614746
2025-12-09 11:53:29.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029999454679564244 Training loss: 7.814628601074219
2025-12-09 11:53:29.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.002999937399495895 Training loss: 7.387720584869385
2025-12-09 11:53:29.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0029999287746058094 Training loss: 7.881276607513428
2025-12-09 11:53:29.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.002999919593289368 Training loss: 8.133663177490234
2025-12-09 11:53:29.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.002999909855549976 Training loss: 7.780333042144775
2025-12-09 11:53:29.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029998995613912463 Training loss: 7.948533058166504
2025-12-09 11:53:29.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.002999888710816997 Training loss: 7.710259914398193
2025-12-09 11:53:30.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.002999877303831254 Training loss: 8.070045471191406
2025-12-09 11:53:30.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002999865340438249 Training loss: 6.937975883483887
2025-12-09 11:53:30.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0029998528206424202 Training loss: 7.961223602294922
2025-12-09 11:53:30.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0029998397444484107 Training loss: 8.367616653442383
2025-12-09 11:53:30.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029998261118610726 Training loss: 7.753511905670166
2025-12-09 11:53:30.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.002999811922885463 Training loss: 8.192743301391602
2025-12-09 11:53:30.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0029997971775268454 Training loss: 7.8334760665893555
2025-12-09 11:53:30.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00299978187579069 Training loss: 7.707699775695801
2025-12-09 11:53:30.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0029997660176826735 Training loss: 7.714071750640869
2025-12-09 11:53:30.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.002999749603208678 Training loss: 7.931206703186035
2025-12-09 11:53:30.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.002999732632374793 Training loss: 7.651483535766602
2025-12-09 11:53:30.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002999715105187314 Training loss: 7.616082668304443
2025-12-09 11:53:30.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.002999697021652744 Training loss: 7.591127872467041
2025-12-09 11:53:31.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.002999678381777791 Training loss: 8.318432807922363
2025-12-09 11:53:31.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.002999659185569369 Training loss: 7.986443996429443
2025-12-09 11:53:31.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0029996394330345996 Training loss: 7.623082637786865
2025-12-09 11:53:31.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0029996191241808113 Training loss: 7.776973724365234
2025-12-09 11:53:31.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.002999598259015537 Training loss: 8.20831298828125
2025-12-09 11:53:31.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.002999576837546517 Training loss: 7.726274013519287
2025-12-09 11:53:31.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0029995548597816983 Training loss: 7.628843784332275
2025-12-09 11:53:31.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.002999532325729234 Training loss: 7.7275471687316895
2025-12-09 11:53:31.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0029995092353974837 Training loss: 7.60299015045166
2025-12-09 11:53:31.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.002999485588795013 Training loss: 7.398255825042725
2025-12-09 11:53:31.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0029994613859305936 Training loss: 7.848791122436523
2025-12-09 11:53:31.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0029994366268132045 Training loss: 7.065235614776611
2025-12-09 11:53:31.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0029994113114520304 Training loss: 7.587403774261475
2025-12-09 11:53:32.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0029993854398564627 Training loss: 7.821102142333984
2025-12-09 11:53:32.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0029993590120360987 Training loss: 7.736555576324463
2025-12-09 11:53:32.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0029993320280007423 Training loss: 7.679067611694336
2025-12-09 11:53:32.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.002999304487760404 Training loss: 7.6537370681762695
2025-12-09 11:53:32.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0029992763913253002 Training loss: 7.763614654541016
2025-12-09 11:53:32.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.002999247738705854 Training loss: 7.414008140563965
2025-12-09 11:53:32.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0029992185299126946 Training loss: 7.751524448394775
2025-12-09 11:53:32.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0029991887649566565 Training loss: 7.878465175628662
2025-12-09 11:53:32.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002999158443848783 Training loss: 7.286486625671387
2025-12-09 11:53:32.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0029991275666003212 Training loss: 7.94081974029541
2025-12-09 11:53:32.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0029990961332227264 Training loss: 7.542720317840576
2025-12-09 11:53:32.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.002999064143727659 Training loss: 7.5607099533081055
2025-12-09 11:53:33.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0029990315981269864 Training loss: 7.566500663757324
2025-12-09 11:53:33.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0029989984964327813 Training loss: 7.464481830596924
2025-12-09 11:53:33.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0029989648386573244 Training loss: 7.676898002624512
2025-12-09 11:53:33.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.002998930624813101 Training loss: 7.3926520347595215
2025-12-09 11:53:33.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.002998895854912803 Training loss: 7.805450439453125
2025-12-09 11:53:33.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0029988605289693296 Training loss: 7.84431266784668
2025-12-09 11:53:33.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0029988246469957857 Training loss: 7.799361705780029
2025-12-09 11:53:33.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002998788209005482 Training loss: 7.307791233062744
2025-12-09 11:53:33.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.002998751215011936 Training loss: 6.627615928649902
2025-12-09 11:53:33.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0029987136650288706 Training loss: 7.715376377105713
2025-12-09 11:53:33.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.002998675559070217 Training loss: 7.437880516052246
2025-12-09 11:53:33.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.00299863689715011 Training loss: 7.533684730529785
2025-12-09 11:53:33.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0029985976792828934 Training loss: 6.226654052734375
2025-12-09 11:53:34.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0029985579054831145 Training loss: 7.632509708404541
2025-12-09 11:53:34.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0029985175757655286 Training loss: 7.741230010986328
2025-12-09 11:53:34.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.002998476690145097 Training loss: 8.21214771270752
2025-12-09 11:53:34.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0029984352486369867 Training loss: 7.780457496643066
2025-12-09 11:53:34.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002998393251256571 Training loss: 7.870061874389648
2025-12-09 11:53:34.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0029983506980194303 Training loss: 8.609644889831543
2025-12-09 11:53:34.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0029983075889413497 Training loss: 7.663912296295166
2025-12-09 11:53:34.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0029982639240383217 Training loss: 8.006247520446777
2025-12-09 11:53:34.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0029982197033265445 Training loss: 8.084010124206543
2025-12-09 11:53:34.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0029981749268224228 Training loss: 7.6661696434021
2025-12-09 11:53:34.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.0029981295945425666 Training loss: 7.66474723815918
2025-12-09 11:53:34.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002998083706503794 Training loss: 7.395224571228027
2025-12-09 11:53:34.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.002998037262723127 Training loss: 7.40098762512207
2025-12-09 11:53:35.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.002997990263217795 Training loss: 7.022368907928467
2025-12-09 11:53:35.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0029979427080052334 Training loss: 7.327201843261719
2025-12-09 11:53:35.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.002997894597103084 Training loss: 6.903649806976318
2025-12-09 11:53:35.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0029978459305291943 Training loss: 7.596585273742676
2025-12-09 11:53:35.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0029977967083016175 Training loss: 7.626648902893066
2025-12-09 11:53:35.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.002997746930438614 Training loss: 7.425100803375244
2025-12-09 11:53:35.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0029976965969586494 Training loss: 7.494718074798584
2025-12-09 11:53:35.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0029976457078803964 Training loss: 7.691190719604492
2025-12-09 11:53:35.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0029975942632227332 Training loss: 7.41135311126709
2025-12-09 11:53:35.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002997542263004744 Training loss: 7.63411808013916
2025-12-09 11:53:35.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.002997489707245719 Training loss: 8.134411811828613
2025-12-09 11:53:35.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0029974365959651544 Training loss: 7.093667030334473
2025-12-09 11:53:36.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0029973829291827544 Training loss: 7.7945427894592285
2025-12-09 11:53:36.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.002997328706918426 Training loss: 7.564464569091797
2025-12-09 11:53:36.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0029972739291922843 Training loss: 7.595522880554199
2025-12-09 11:53:36.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0029972185960246514 Training loss: 7.290305137634277
2025-12-09 11:53:36.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0029971627074360523 Training loss: 7.324220657348633
2025-12-09 11:53:36.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0029971062634472205 Training loss: 7.424458980560303
2025-12-09 11:53:36.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0029970492640790957 Training loss: 7.6274333000183105
2025-12-09 11:53:36.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.002996991709352822 Training loss: 7.396675109863281
2025-12-09 11:53:36.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0029969335992897513 Training loss: 7.556243419647217
2025-12-09 11:53:36.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0029968749339114404 Training loss: 7.063990116119385
2025-12-09 11:53:36.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0029968157132396513 Training loss: 7.599918842315674
2025-12-09 11:53:36.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002996755937296354 Training loss: 7.488903045654297
2025-12-09 11:53:36.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.002996695606103723 Training loss: 7.44418478012085
2025-12-09 11:53:37.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0029966347196841393 Training loss: 7.424961090087891
2025-12-09 11:53:37.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.00299657327806019 Training loss: 7.5695343017578125
2025-12-09 11:53:37.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0029965112812546683 Training loss: 7.913699626922607
2025-12-09 11:53:37.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0029964487292905725 Training loss: 7.017045021057129
2025-12-09 11:53:37.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0029963856221911075 Training loss: 7.4006171226501465
2025-12-09 11:53:37.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.002996321959979685 Training loss: 7.370578765869141
2025-12-09 11:53:37.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00299625774267992 Training loss: 6.558045864105225
2025-12-09 11:53:37.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0029961929703156364 Training loss: 7.2680816650390625
2025-12-09 11:53:37.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.002996127642910863 Training loss: 7.276308059692383
2025-12-09 11:53:37.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0029960617604898325 Training loss: 7.578907012939453
2025-12-09 11:53:37.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.002995995323076986 Training loss: 7.415459156036377
2025-12-09 11:53:37.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.002995928330696971 Training loss: 7.4008917808532715
2025-12-09 11:53:37.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.002995860783374638 Training loss: 7.534653186798096
2025-12-09 11:53:38.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0029957926811350452 Training loss: 7.644033432006836
2025-12-09 11:53:38.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0029957240240034567 Training loss: 7.40102481842041
2025-12-09 11:53:38.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0029956548120053422 Training loss: 8.063419342041016
2025-12-09 11:53:38.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0029955850451663765 Training loss: 7.52872896194458
2025-12-09 11:53:38.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0029955147235124417 Training loss: 7.471928596496582
2025-12-09 11:53:38.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.002995443847069625 Training loss: 7.450699329376221
2025-12-09 11:53:38.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0029953724158642185 Training loss: 7.486439228057861
2025-12-09 11:53:38.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0029953004299227213 Training loss: 7.479004383087158
2025-12-09 11:53:38.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.002995227889271838 Training loss: 7.439903259277344
2025-12-09 11:53:38.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.002995154793938479 Training loss: 7.644002914428711
2025-12-09 11:53:38.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0029950811439497607 Training loss: 7.554532527923584
2025-12-09 11:53:38.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0029950069393330043 Training loss: 7.325230121612549
2025-12-09 11:53:38.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.002994932180115737 Training loss: 6.688649654388428
2025-12-09 11:53:39.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0029948568663256928 Training loss: 7.599237442016602
2025-12-09 11:53:39.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0029947809979908114 Training loss: 7.848843097686768
2025-12-09 11:53:39.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.002994704575139236 Training loss: 7.067373275756836
2025-12-09 11:53:39.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.002994627597799318 Training loss: 7.554126739501953
2025-12-09 11:53:39.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0029945500659996132 Training loss: 7.533163070678711
2025-12-09 11:53:39.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0029944719797688844 Training loss: 7.664332389831543
2025-12-09 11:53:39.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.002994393339136098 Training loss: 7.336575031280518
2025-12-09 11:53:39.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0029943141441304277 Training loss: 7.773313522338867
2025-12-09 11:53:39.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0029942343947812517 Training loss: 7.667043685913086
2025-12-09 11:53:39.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.002994154091118156 Training loss: 7.395887851715088
2025-12-09 11:53:39.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0029940732331709295 Training loss: 7.360229015350342
2025-12-09 11:53:39.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0029939918209695676 Training loss: 7.845003128051758
2025-12-09 11:53:40.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0029939098545442733 Training loss: 7.07450008392334
2025-12-09 11:53:40.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0029938273339254516 Training loss: 7.576327323913574
2025-12-09 11:53:40.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.002993744259143717 Training loss: 7.247929096221924
2025-12-09 11:53:40.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.002993660630229886 Training loss: 6.405491352081299
2025-12-09 11:53:40.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0029935764472149833 Training loss: 7.626428127288818
2025-12-09 11:53:40.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0029934917101302376 Training loss: 7.415420055389404
2025-12-09 11:53:40.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.002993406419007084 Training loss: 7.897877216339111
2025-12-09 11:53:40.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0029933205738771626 Training loss: 7.071279525756836
2025-12-09 11:53:40.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0029932341747723194 Training loss: 7.487879276275635
2025-12-09 11:53:40.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.002993147221724606 Training loss: 7.145799160003662
2025-12-09 11:53:40.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0029930597147662785 Training loss: 7.133472919464111
2025-12-09 11:53:40.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0029929716539297997 Training loss: 7.676650047302246
2025-12-09 11:53:40.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0029928830392478376 Training loss: 7.220711708068848
2025-12-09 11:53:41.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.002992793870753265 Training loss: 7.202521800994873
2025-12-09 11:53:41.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0029927041484791614 Training loss: 8.034334182739258
2025-12-09 11:53:41.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00299261387245881 Training loss: 7.625834941864014
2025-12-09 11:53:41.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0029925230427257006 Training loss: 7.547395706176758
2025-12-09 11:53:41.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0029924316593135285 Training loss: 7.2985663414001465
2025-12-09 11:53:41.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0029923397222561938 Training loss: 7.543284893035889
2025-12-09 11:53:41.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0029922472315878023 Training loss: 7.241466045379639
2025-12-09 11:53:41.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.002992154187342665 Training loss: 7.293763160705566
2025-12-09 11:53:41.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.002992060589555299 Training loss: 7.183499813079834
2025-12-09 11:53:41.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0029919664382604253 Training loss: 7.631886005401611
2025-12-09 11:53:41.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0029918717334929718 Training loss: 7.900401592254639
2025-12-09 11:53:41.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00299177647528807 Training loss: 7.248276710510254
2025-12-09 11:53:41.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.002991680663681059 Training loss: 7.306434154510498
2025-12-09 11:53:42.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.002991584298707481 Training loss: 7.787003993988037
2025-12-09 11:53:42.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.002991487380403084 Training loss: 8.380019187927246
2025-12-09 11:53:42.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.002991389908803823 Training loss: 7.5019378662109375
2025-12-09 11:53:42.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0029912918839458554 Training loss: 7.5975661277771
2025-12-09 11:53:42.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.002991193305865547 Training loss: 7.508146286010742
2025-12-09 11:53:42.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0029910941745994657 Training loss: 8.92702865600586
2025-12-09 11:53:42.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0029909944901843864 Training loss: 7.033946990966797
2025-12-09 11:53:42.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.002990894252657289 Training loss: 7.306068420410156
2025-12-09 11:53:42.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0029907934620553595 Training loss: 7.69537353515625
2025-12-09 11:53:42.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0029906921184159863 Training loss: 7.6212968826293945
2025-12-09 11:53:42.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029905902217767654 Training loss: 7.243485927581787
2025-12-09 11:53:42.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029904877721754976 Training loss: 7.613526344299316
2025-12-09 11:53:43.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002990384769650188 Training loss: 7.296475887298584
2025-12-09 11:53:43.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0029902812142390475 Training loss: 7.583534240722656
2025-12-09 11:53:43.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0029901771059804923 Training loss: 7.342926979064941
2025-12-09 11:53:43.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0029900724449131427 Training loss: 7.528458595275879
2025-12-09 11:53:43.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0029899672310758248 Training loss: 7.107953071594238
2025-12-09 11:53:43.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0029898614645075695 Training loss: 7.16855001449585
2025-12-09 11:53:43.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.002989755145247613 Training loss: 6.782660961151123
2025-12-09 11:53:43.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002989648273335397 Training loss: 7.256213665008545
2025-12-09 11:53:43.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0029895408488105667 Training loss: 7.444056987762451
2025-12-09 11:53:43.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0029894328717129737 Training loss: 7.33244514465332
2025-12-09 11:53:43.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0029893243420826736 Training loss: 7.621094226837158
2025-12-09 11:53:43.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.002989215259959928 Training loss: 7.657789707183838
2025-12-09 11:53:43.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002989105625385203 Training loss: 7.535585403442383
2025-12-09 11:53:44.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.002988995438399169 Training loss: 7.299041748046875
2025-12-09 11:53:44.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0029888846990427024 Training loss: 7.062422275543213
2025-12-09 11:53:44.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.002988773407356884 Training loss: 7.346871852874756
2025-12-09 11:53:44.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0029886615633829996 Training loss: 7.4923601150512695
2025-12-09 11:53:44.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.002988549167162539 Training loss: 7.374239444732666
2025-12-09 11:53:44.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0029884362187371986 Training loss: 7.898052215576172
2025-12-09 11:53:44.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0029883227181488783 Training loss: 6.48508358001709
2025-12-09 11:53:44.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.002988208665439683 Training loss: 7.494927883148193
2025-12-09 11:53:44.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0029880940606519233 Training loss: 7.455460071563721
2025-12-09 11:53:44.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.002987978903828114 Training loss: 7.266217231750488
2025-12-09 11:53:44.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0029878631950109734 Training loss: 7.331582069396973
2025-12-09 11:53:44.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0029877469342434273 Training loss: 6.510400295257568
2025-12-09 11:53:44.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.002987630121568604 Training loss: 7.395120620727539
2025-12-09 11:53:45.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.002987512757029838 Training loss: 6.743677616119385
2025-12-09 11:53:45.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0029873948406706667 Training loss: 7.727278232574463
2025-12-09 11:53:45.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0029872763725348342 Training loss: 8.118131637573242
2025-12-09 11:53:45.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0029871573526662884 Training loss: 7.434891700744629
2025-12-09 11:53:45.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0029870377811091822 Training loss: 7.247894287109375
2025-12-09 11:53:45.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.002986917657907872 Training loss: 7.402978897094727
2025-12-09 11:53:45.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.00298679698310692 Training loss: 7.501755237579346
2025-12-09 11:53:45.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.002986675756751093 Training loss: 6.887330055236816
2025-12-09 11:53:45.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0029865539788853624 Training loss: 7.548000335693359
2025-12-09 11:53:45.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0029864316495549037 Training loss: 7.392025947570801
2025-12-09 11:53:45.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0029863087688050973 Training loss: 7.326822280883789
2025-12-09 11:53:45.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.002986185336681528 Training loss: 7.269261837005615
2025-12-09 11:53:46.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0029860613532299847 Training loss: 7.311373233795166
2025-12-09 11:53:46.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0029859368184964627 Training loss: 7.200954914093018
2025-12-09 11:53:46.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.002985811732527159 Training loss: 7.082763671875
2025-12-09 11:53:46.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0029856860953684774 Training loss: 6.711154460906982
2025-12-09 11:53:46.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0029855599070670253 Training loss: 7.738734722137451
2025-12-09 11:53:46.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0029854331676696143 Training loss: 7.01794958114624
2025-12-09 11:53:46.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0029853058772232608 Training loss: 6.983828544616699
2025-12-09 11:53:46.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0029851780357751856 Training loss: 7.210479259490967
2025-12-09 11:53:46.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.002985049643372814 Training loss: 7.216610908508301
2025-12-09 11:53:46.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0029849207000637755 Training loss: 7.168489456176758
2025-12-09 11:53:46.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0029847912058959033 Training loss: 7.020786285400391
2025-12-09 11:53:46.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.002984661160917237 Training loss: 6.844659805297852
2025-12-09 11:53:46.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.002984530565176018 Training loss: 7.143846035003662
2025-12-09 11:53:47.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.002984399418720694 Training loss: 7.537593841552734
2025-12-09 11:53:47.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0029842677215999158 Training loss: 7.296660423278809
2025-12-09 11:53:47.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.002984135473862539 Training loss: 7.499332904815674
2025-12-09 11:53:47.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.002984002675557623 Training loss: 7.32333517074585
2025-12-09 11:53:47.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.002983869326734432 Training loss: 7.057345390319824
2025-12-09 11:53:47.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0029837354274424347 Training loss: 7.256861686706543
2025-12-09 11:53:47.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.002983600977731303 Training loss: 7.0426764488220215
2025-12-09 11:53:47.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0029834659776509136 Training loss: 6.866648197174072
2025-12-09 11:53:47.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0029833304272513473 Training loss: 7.046931743621826
2025-12-09 11:53:47.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.002983194326582889 Training loss: 7.160009384155273
2025-12-09 11:53:47.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0029830576756960285 Training loss: 7.1536664962768555
2025-12-09 11:53:47.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0029829204746414577 Training loss: 7.573050498962402
2025-12-09 11:53:47.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0029827827234700744 Training loss: 6.3262505531311035
2025-12-09 11:53:48.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00298264442223298 Training loss: 6.868226528167725
2025-12-09 11:53:48.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0029825055709814803 Training loss: 7.312450408935547
2025-12-09 11:53:48.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.002982366169767084 Training loss: 6.902043342590332
2025-12-09 11:53:48.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.002982226218641505 Training loss: 7.137598037719727
2025-12-09 11:53:48.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.002982085717656661 Training loss: 7.2062177658081055
2025-12-09 11:53:48.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0029819446668646723 Training loss: 7.120816230773926
2025-12-09 11:53:48.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0029818030663178656 Training loss: 7.039079666137695
2025-12-09 11:53:48.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00298166091606877 Training loss: 7.1836042404174805
2025-12-09 11:53:48.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0029815182161701185 Training loss: 7.697516918182373
2025-12-09 11:53:48.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0029813749666748484 Training loss: 7.299827575683594
2025-12-09 11:53:48.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0029812311676361003 Training loss: 7.447654724121094
2025-12-09 11:53:48.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00298108681910722 Training loss: 7.206648349761963
2025-12-09 11:53:49.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0029809419211417553 Training loss: 7.315396785736084
2025-12-09 11:53:49.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0029807964737934593 Training loss: 7.200397491455078
2025-12-09 11:53:49.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0029806504771162884 Training loss: 7.19415283203125
2025-12-09 11:53:49.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0029805039311644028 Training loss: 7.055984973907471
2025-12-09 11:53:49.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.002980356835992166 Training loss: 7.421419620513916
2025-12-09 11:53:49.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0029802091916541463 Training loss: 7.269465446472168
2025-12-09 11:53:49.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.002980060998205115 Training loss: 7.039222240447998
2025-12-09 11:53:49.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0029799122557000466 Training loss: 7.34372615814209
2025-12-09 11:53:49.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0029797629641941203 Training loss: 7.419201850891113
2025-12-09 11:53:49.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.002979613123742719 Training loss: 7.105193138122559
2025-12-09 11:53:49.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0029794627344014277 Training loss: 7.334209442138672
2025-12-09 11:53:49.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.002979311796226037 Training loss: 6.574466705322266
2025-12-09 11:53:49.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00297916030927254 Training loss: 7.326198101043701
2025-12-09 11:53:50.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0029790082735971337 Training loss: 7.38875150680542
2025-12-09 11:53:50.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0029788556892562184 Training loss: 6.776617050170898
2025-12-09 11:53:50.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.002978702556306398 Training loss: 7.089386940002441
2025-12-09 11:53:50.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.00297854887480448 Training loss: 7.783587455749512
2025-12-09 11:53:50.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.002978394644807475 Training loss: 7.687809944152832
2025-12-09 11:53:50.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0029782398663725984 Training loss: 7.338481903076172
2025-12-09 11:53:50.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0029780845395572676 Training loss: 6.748720645904541
2025-12-09 11:53:50.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0029779286644191043 Training loss: 7.248665809631348
2025-12-09 11:53:50.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.002977772241015933 Training loss: 7.557005405426025
2025-12-09 11:53:50.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.002977615269405782 Training loss: 7.0027079582214355
2025-12-09 11:53:50.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0029774577496468825 Training loss: 7.302324295043945
2025-12-09 11:53:50.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0029772996817976696 Training loss: 7.170278072357178
2025-12-09 11:53:50.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.002977141065916781 Training loss: 7.093092441558838
2025-12-09 11:53:51.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0029769819020630597 Training loss: 7.343667507171631
2025-12-09 11:53:51.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0029768221902955485 Training loss: 7.256009101867676
2025-12-09 11:53:51.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.002976661930673497 Training loss: 6.893094062805176
2025-12-09 11:53:51.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.002976501123256355 Training loss: 7.347696781158447
2025-12-09 11:53:51.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0029763397681037787 Training loss: 7.05135440826416
2025-12-09 11:53:51.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0029761778652756246 Training loss: 7.293687343597412
2025-12-09 11:53:51.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0029760154148319538 Training loss: 6.94558048248291
2025-12-09 11:53:51.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0029758524168330305 Training loss: 7.412813663482666
2025-12-09 11:53:51.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.0029756888713393216 Training loss: 7.007347583770752
2025-12-09 11:53:51.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.002975524778411498 Training loss: 7.024962425231934
2025-12-09 11:53:51.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0029753601381104318 Training loss: 7.287906646728516
2025-12-09 11:53:51.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0029751949504972 Training loss: 7.149062156677246
2025-12-09 11:53:52.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0029750292156330823 Training loss: 6.592287540435791
2025-12-09 11:53:52.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0029748629335795604 Training loss: 6.991415500640869
2025-12-09 11:53:52.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.002974696104398321 Training loss: 7.1899800300598145
2025-12-09 11:53:52.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.002974528728151251 Training loss: 7.157639026641846
2025-12-09 11:53:52.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0029743608049004424 Training loss: 6.737515449523926
2025-12-09 11:53:52.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.002974192334708189 Training loss: 7.421802997589111
2025-12-09 11:53:52.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.002974023317636989 Training loss: 6.9662346839904785
2025-12-09 11:53:52.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0029738537537495413 Training loss: 6.865285873413086
2025-12-09 11:53:52.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.0029736836431087494 Training loss: 6.882233619689941
2025-12-09 11:53:52.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0029735129857777188 Training loss: 6.7503533363342285
2025-12-09 11:53:52.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.002973341781819758 Training loss: 7.7165703773498535
2025-12-09 11:53:52.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.002973170031298378 Training loss: 7.263189792633057
2025-12-09 11:53:52.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0029729977342772937 Training loss: 7.115932464599609
2025-12-09 11:53:53.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.0029728248908204207 Training loss: 7.5175700187683105
2025-12-09 11:53:53.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.0029726515009918793 Training loss: 7.187154293060303
2025-12-09 11:53:53.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0029724775648559913 Training loss: 7.260032653808594
2025-12-09 11:53:53.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0029723030824772814 Training loss: 6.915391445159912
2025-12-09 11:53:53.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.002972128053920478 Training loss: 6.278204441070557
2025-12-09 11:53:53.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00297195247925051 Training loss: 7.225526809692383
2025-12-09 11:53:53.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.0029717763585325103 Training loss: 7.173048496246338
2025-12-09 11:53:53.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.002971599691831815 Training loss: 7.237122535705566
2025-12-09 11:53:53.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.0029714224792139607 Training loss: 7.221039295196533
2025-12-09 11:53:53.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.002971244720744689 Training loss: 7.294764995574951
2025-12-09 11:53:53.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.0029710664164899416 Training loss: 6.871382236480713
2025-12-09 11:53:53.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0029708875665158643 Training loss: 7.155046463012695
2025-12-09 11:53:53.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0029707081708888047 Training loss: 7.086852073669434
2025-12-09 11:53:54.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0029705282296753127 Training loss: 6.984673976898193
2025-12-09 11:53:54.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.002970347742942141 Training loss: 7.236988067626953
2025-12-09 11:53:54.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.002970166710756245 Training loss: 7.471110820770264
2025-12-09 11:53:54.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.002969985133184781 Training loss: 7.265893459320068
2025-12-09 11:53:54.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0029698030102951094 Training loss: 6.940718650817871
2025-12-09 11:53:54.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0029696203421547916 Training loss: 7.32488489151001
2025-12-09 11:53:54.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0029694371288315913 Training loss: 7.260299205780029
2025-12-09 11:53:54.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.0029692533703934757 Training loss: 6.90404748916626
2025-12-09 11:53:54.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.002969069066908613 Training loss: 6.817226886749268
2025-12-09 11:53:54.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0029688842184453744 Training loss: 7.859225273132324
2025-12-09 11:53:54.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.002968698825072332 Training loss: 7.54714298248291
2025-12-09 11:53:54.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0029685128868582626 Training loss: 6.855736255645752
2025-12-09 11:53:55.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0029683264038721418 Training loss: 7.069423198699951
2025-12-09 11:53:55.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0029681393761831487 Training loss: 7.094863414764404
2025-12-09 11:53:55.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.002967951803860666 Training loss: 7.472152233123779
2025-12-09 11:53:55.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.002967763686974276 Training loss: 7.025598049163818
2025-12-09 11:53:55.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.002967575025593765 Training loss: 8.617219924926758
2025-12-09 11:53:55.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.00296738581978912 Training loss: 6.807998180389404
2025-12-09 11:53:55.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0029671960696305306 Training loss: 7.171267032623291
2025-12-09 11:53:55.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.002967005775188388 Training loss: 7.14969539642334
2025-12-09 11:53:55.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0029668149365332853 Training loss: 7.323563575744629
2025-12-09 11:53:55.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.002966623553736018 Training loss: 7.139639377593994
2025-12-09 11:53:55.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0029664316268675824 Training loss: 7.451192378997803
2025-12-09 11:53:55.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0029662391559991783 Training loss: 8.019515991210938
2025-12-09 11:53:55.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0029660461412022057 Training loss: 7.650644302368164
2025-12-09 11:53:56.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.002965852582548267 Training loss: 7.219245910644531
2025-12-09 11:53:56.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0029656584801091668 Training loss: 7.354191303253174
2025-12-09 11:53:56.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0029654638339569107 Training loss: 7.197206020355225
2025-12-09 11:53:56.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.002965268644163706 Training loss: 6.67415189743042
2025-12-09 11:53:56.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0029650729108019625 Training loss: 6.589515686035156
2025-12-09 11:53:56.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.002964876633944291 Training loss: 6.955427646636963
2025-12-09 11:53:56.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0029646798136635038 Training loss: 7.195757865905762
2025-12-09 11:53:56.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.002964482450032615 Training loss: 6.976341247558594
2025-12-09 11:53:56.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.002964284543124841 Training loss: 7.347676753997803
2025-12-09 11:53:56.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0029640860930135976 Training loss: 6.965733528137207
2025-12-09 11:53:56.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.002963887099772505 Training loss: 7.16593074798584
2025-12-09 11:53:56.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.0029636875634753827 Training loss: 6.923715114593506
2025-12-09 11:53:56.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.002963487484196253 Training loss: 6.991359710693359
2025-12-09 11:53:57.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.002963286862009338 Training loss: 7.184243679046631
2025-12-09 11:53:57.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0029630856969890627 Training loss: 6.983364105224609
2025-12-09 11:53:57.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0029628839892100536 Training loss: 6.927122592926025
2025-12-09 11:53:57.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.002962681738747137 Training loss: 7.054836750030518
2025-12-09 11:53:57.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.002962478945675342 Training loss: 7.03665018081665
2025-12-09 11:53:57.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.002962275610069898 Training loss: 7.157291889190674
2025-12-09 11:53:57.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.002962071732006237 Training loss: 6.633443355560303
2025-12-09 11:53:57.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00296186731155999 Training loss: 7.326756954193115
2025-12-09 11:53:57.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0029616623488069923 Training loss: 7.261470794677734
2025-12-09 11:53:57.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0029614568438232768 Training loss: 6.782707691192627
2025-12-09 11:53:57.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.0029612507966850807 Training loss: 6.789678573608398
2025-12-09 11:53:57.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0029610442074688398 Training loss: 7.049905300140381
2025-12-09 11:53:58.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0029608370762511937 Training loss: 7.4415812492370605
2025-12-09 11:53:58.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0029606294031089804 Training loss: 7.124997138977051
2025-12-09 11:53:58.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.00296042118811924 Training loss: 7.20446252822876
2025-12-09 11:53:58.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.002960212431359215 Training loss: 7.164221286773682
2025-12-09 11:53:58.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0029600031329063466 Training loss: 7.2825775146484375
2025-12-09 11:53:58.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.002959793292838278 Training loss: 6.946348667144775
2025-12-09 11:53:58.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.002959582911232853 Training loss: 7.127441883087158
2025-12-09 11:53:58.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0029593719881681173 Training loss: 7.3820295333862305
2025-12-09 11:53:58.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.002959160523722316 Training loss: 6.8303937911987305
2025-12-09 11:53:58.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0029589485179738963 Training loss: 7.089783668518066
2025-12-09 11:53:58.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0029587359710015054 Training loss: 7.56169319152832
2025-12-09 11:53:58.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0029585228828839915 Training loss: 7.184954643249512
2025-12-09 11:53:58.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.002958309253700404 Training loss: 6.985814571380615
2025-12-09 11:53:59.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.002958095083529992 Training loss: 7.302738666534424
2025-12-09 11:53:59.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0029578803724522058 Training loss: 7.39024543762207
2025-12-09 11:53:59.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.002957665120546697 Training loss: 6.949093818664551
2025-12-09 11:53:59.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0029574493278933175 Training loss: 6.641403675079346
2025-12-09 11:53:59.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.002957232994572119 Training loss: 6.1934494972229
2025-12-09 11:53:59.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0029570161206633546 Training loss: 7.016007423400879
2025-12-09 11:53:59.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0029567987062474772 Training loss: 7.057834148406982
2025-12-09 11:53:59.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.002956580751405141 Training loss: 6.7354230880737305
2025-12-09 11:53:59.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.002956362256217201 Training loss: 6.915372848510742
2025-12-09 11:53:59.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0029561432207647113 Training loss: 7.419905662536621
2025-12-09 11:53:59.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.002955923645128927 Training loss: 7.161588668823242
2025-12-09 11:53:59.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0029557035293913047 Training loss: 7.322515487670898
2025-12-09 11:53:59.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.0029554828736334995 Training loss: 6.929101467132568
2025-12-09 11:54:00.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0029552616779373684 Training loss: 6.868518352508545
2025-12-09 11:54:00.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0029550399423849674 Training loss: 6.697653293609619
2025-12-09 11:54:00.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.002954817667058554 Training loss: 6.762097358703613
2025-12-09 11:54:00.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.002954594852040585 Training loss: 6.750809669494629
2025-12-09 11:54:00.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0029543714974137178 Training loss: 6.816134929656982
2025-12-09 11:54:00.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.00295414760326081 Training loss: 7.328139305114746
2025-12-09 11:54:00.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.002953923169664919 Training loss: 7.047082901000977
2025-12-09 11:54:00.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.002953698196709303 Training loss: 6.8714118003845215
2025-12-09 11:54:00.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.00295347268447742 Training loss: 7.056852340698242
2025-12-09 11:54:00.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.002953246633052928 Training loss: 7.0615034103393555
2025-12-09 11:54:00.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0029530200425196837 Training loss: 7.06398868560791
2025-12-09 11:54:00.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0029527929129617467 Training loss: 7.310015678405762
2025-12-09 11:54:01.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.002952565244463374 Training loss: 6.898496627807617
2025-12-09 11:54:01.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0029523370371090235 Training loss: 7.318864345550537
2025-12-09 11:54:01.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.002952108290983353 Training loss: 7.225074768066406
2025-12-09 11:54:01.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.002951879006171221 Training loss: 6.97988748550415
2025-12-09 11:54:01.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0029516491827576833 Training loss: 6.929690361022949
2025-12-09 11:54:01.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.0029514188208279984 Training loss: 7.682606220245361
2025-12-09 11:54:01.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0029511879204676223 Training loss: 7.28427791595459
2025-12-09 11:54:01.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0029509564817622133 Training loss: 7.009479999542236
2025-12-09 11:54:01.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0029507245047976265 Training loss: 6.923266887664795
2025-12-09 11:54:01.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.002950491989659918 Training loss: 6.869012355804443
2025-12-09 11:54:01.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0029502589364353454 Training loss: 7.066961288452148
2025-12-09 11:54:01.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0029500253452103622 Training loss: 7.0842695236206055
2025-12-09 11:54:01.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.0029497912160716235 Training loss: 7.368770122528076
2025-12-09 11:54:02.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.002949556549105985 Training loss: 7.297372817993164
2025-12-09 11:54:02.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0029493213444005 Training loss: 6.564399242401123
2025-12-09 11:54:02.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.002949085602042422 Training loss: 7.489241123199463
2025-12-09 11:54:02.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0029488493221192045 Training loss: 7.131577968597412
2025-12-09 11:54:02.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.002948612504718499 Training loss: 6.963879108428955
2025-12-09 11:54:02.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0029483751499281585 Training loss: 6.589960098266602
2025-12-09 11:54:02.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0029481372578362332 Training loss: 6.750088691711426
2025-12-09 11:54:02.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.002947898828530974 Training loss: 7.050100326538086
2025-12-09 11:54:02.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.00294765986210083 Training loss: 7.482784271240234
2025-12-09 11:54:02.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0029474203586344516 Training loss: 6.847475051879883
2025-12-09 11:54:02.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.0029471803182206857 Training loss: 6.714810371398926
2025-12-09 11:54:02.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.0029469397409485807 Training loss: 6.9443840980529785
2025-12-09 11:54:02.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0029466986269073825 Training loss: 7.10349702835083
2025-12-09 11:54:03.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.0029464569761865366 Training loss: 7.231997966766357
2025-12-09 11:54:03.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.002946214788875689 Training loss: 6.997532844543457
2025-12-09 11:54:03.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0029459720650646826 Training loss: 6.828283786773682
2025-12-09 11:54:03.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0029457288048435606 Training loss: 6.780557155609131
2025-12-09 11:54:03.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.002945485008302565 Training loss: 7.066738605499268
2025-12-09 11:54:03.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0029452406755321363 Training loss: 7.235119342803955
2025-12-09 11:54:03.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0029449958066229145 Training loss: 6.692873477935791
2025-12-09 11:54:03.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0029447504016657383 Training loss: 6.965122699737549
2025-12-09 11:54:03.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.002944504460751645 Training loss: 6.990239143371582
2025-12-09 11:54:03.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.002944257983971871 Training loss: 6.930241107940674
2025-12-09 11:54:03.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.002944010971417851 Training loss: 7.051540374755859
2025-12-09 11:54:03.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00294376342318122 Training loss: 7.016505241394043
2025-12-09 11:54:04.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.00294351533935381 Training loss: 6.6431684494018555
2025-12-09 11:54:04.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.002943266720027652 Training loss: 7.51872444152832
2025-12-09 11:54:04.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0029430175652949762 Training loss: 7.099221229553223
2025-12-09 11:54:04.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0029427678752482114 Training loss: 6.722705841064453
2025-12-09 11:54:04.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0029425176499799843 Training loss: 7.192796230316162
2025-12-09 11:54:04.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.002942266889583121 Training loss: 6.701643466949463
2025-12-09 11:54:04.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.0029420155941506454 Training loss: 6.762726783752441
2025-12-09 11:54:04.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00294176376377578 Training loss: 7.011488437652588
2025-12-09 11:54:04.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0029415113985519466 Training loss: 6.938356876373291
2025-12-09 11:54:04.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0029412584985727642 Training loss: 7.816811561584473
2025-12-09 11:54:04.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.002941005063932051 Training loss: 7.048575401306152
2025-12-09 11:54:04.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.002940751094723823 Training loss: 6.901191711425781
2025-12-09 11:54:04.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0029404965910422953 Training loss: 6.761001110076904
2025-12-09 11:54:05.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0029402415529818805 Training loss: 6.9240264892578125
2025-12-09 11:54:05.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.0029399859806371895 Training loss: 7.232253551483154
2025-12-09 11:54:05.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.002939729874103032 Training loss: 7.2680253982543945
2025-12-09 11:54:05.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.002939473233474415 Training loss: 6.748453140258789
2025-12-09 11:54:05.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.002939216058846544 Training loss: 6.653972148895264
2025-12-09 11:54:05.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0029389583503148234 Training loss: 6.931906223297119
2025-12-09 11:54:05.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.0029387001079748537 Training loss: 6.851944923400879
2025-12-09 11:54:05.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0029384413319224366 Training loss: 7.1054229736328125
2025-12-09 11:54:05.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.002938182022253568 Training loss: 7.0363359451293945
2025-12-09 11:54:05.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.002937922179064445 Training loss: 6.860860347747803
2025-12-09 11:54:05.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.00293766180245146 Training loss: 6.908502101898193
2025-12-09 11:54:05.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0029374008925112057 Training loss: 6.890663146972656
2025-12-09 11:54:05.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0029371394493404707 Training loss: 7.066931247711182
2025-12-09 11:54:06.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0029368774730362426 Training loss: 7.495790481567383
2025-12-09 11:54:06.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.002936614963695706 Training loss: 7.260051250457764
2025-12-09 11:54:06.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.002936351921416244 Training loss: 7.417135715484619
2025-12-09 11:54:06.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0029360883462954362 Training loss: 7.8352885246276855
2025-12-09 11:54:06.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.002935824238431062 Training loss: 6.645907402038574
2025-12-09 11:54:06.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0029355595979210962 Training loss: 6.761205196380615
2025-12-09 11:54:06.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.002935294424863712 Training loss: 6.975984573364258
2025-12-09 11:54:06.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.002935028719357281 Training loss: 7.058735370635986
2025-12-09 11:54:06.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0029347624815003704 Training loss: 6.955937385559082
2025-12-09 11:54:06.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.0029344957113917472 Training loss: 6.832052230834961
2025-12-09 11:54:06.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.002934228409130374 Training loss: 7.398099422454834
2025-12-09 11:54:06.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0029339605748154125 Training loss: 7.069407939910889
2025-12-09 11:54:07.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0029336922085462193 Training loss: 7.2382707595825195
2025-12-09 11:54:07.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.002933423310422351 Training loss: 7.158868312835693
2025-12-09 11:54:07.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00293315388054356 Training loss: 6.674002647399902
2025-12-09 11:54:07.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.002932883919009796 Training loss: 7.853985786437988
2025-12-09 11:54:07.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.002932613425921207 Training loss: 7.09785270690918
2025-12-09 11:54:07.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.002932342401378137 Training loss: 6.53255558013916
2025-12-09 11:54:07.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0029320708454811267 Training loss: 6.709263801574707
2025-12-09 11:54:07.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.002931798758330916 Training loss: 7.810985565185547
2025-12-09 11:54:07.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.002931526140028441 Training loss: 7.236917972564697
2025-12-09 11:54:07.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0029312529906748326 Training loss: 6.929712772369385
2025-12-09 11:54:07.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0029309793103714224 Training loss: 7.177783966064453
2025-12-09 11:54:07.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.002930705099219736 Training loss: 6.435698986053467
2025-12-09 11:54:07.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0029304303573214983 Training loss: 7.0298261642456055
2025-12-09 11:54:08.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0029301550847786293 Training loss: 7.0920867919921875
2025-12-09 11:54:08.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0029298792816932462 Training loss: 7.314302444458008
2025-12-09 11:54:08.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0029296029481676636 Training loss: 6.757560729980469
2025-12-09 11:54:08.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0029293260843043924 Training loss: 7.1356682777404785
2025-12-09 11:54:08.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0029290486902061397 Training loss: 7.010060787200928
2025-12-09 11:54:08.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0029287707659758117 Training loss: 7.4418840408325195
2025-12-09 11:54:08.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0029284923117165076 Training loss: 6.560851573944092
2025-12-09 11:54:08.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0029282133275315265 Training loss: 7.1984639167785645
2025-12-09 11:54:08.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.0029279338135243626 Training loss: 7.897568702697754
2025-12-09 11:54:08.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0029276537697987062 Training loss: 6.759417533874512
2025-12-09 11:54:08.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0029273731964584446 Training loss: 7.147840976715088
2025-12-09 11:54:08.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0029270920936076625 Training loss: 6.971058368682861
2025-12-09 11:54:08.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0029268104613506397 Training loss: 6.936478137969971
2025-12-09 11:54:09.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0029265282997918535 Training loss: 6.749828815460205
2025-12-09 11:54:09.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.0029262456090359762 Training loss: 7.439068794250488
2025-12-09 11:54:09.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.002925962389187877 Training loss: 7.087524890899658
2025-12-09 11:54:09.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0029256786403526226 Training loss: 7.300724029541016
2025-12-09 11:54:09.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0029253943626354737 Training loss: 6.3298492431640625
2025-12-09 11:54:09.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0029251095561418894 Training loss: 6.830812454223633
2025-12-09 11:54:09.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0029248242209775235 Training loss: 6.714680194854736
2025-12-09 11:54:09.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.002924538357248226 Training loss: 6.812134265899658
2025-12-09 11:54:09.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0029242519650600437 Training loss: 6.895529270172119
2025-12-09 11:54:09.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.002923965044519219 Training loss: 6.73838472366333
2025-12-09 11:54:09.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.002923677595732191 Training loss: 7.053770065307617
2025-12-09 11:54:09.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0029233896188055933 Training loss: 7.102532386779785
2025-12-09 11:54:10.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0029231011138462566 Training loss: 6.932579517364502
2025-12-09 11:54:10.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0029228120809612072 Training loss: 7.304733753204346
2025-12-09 11:54:10.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.002922522520257667 Training loss: 6.798766136169434
2025-12-09 11:54:10.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0029222324318430542 Training loss: 6.952324390411377
2025-12-09 11:54:10.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0029219418158249826 Training loss: 7.241913318634033
2025-12-09 11:54:10.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0029216506723112614 Training loss: 7.052742004394531
2025-12-09 11:54:10.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0029213590014098953 Training loss: 6.784222602844238
2025-12-09 11:54:10.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.002921066803229085 Training loss: 7.457218170166016
2025-12-09 11:54:10.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.002920774077877228 Training loss: 6.7718915939331055
2025-12-09 11:54:10.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0029204808254629146 Training loss: 6.1637372970581055
2025-12-09 11:54:10.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.002920187046094933 Training loss: 6.888757705688477
2025-12-09 11:54:10.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.002919892739882266 Training loss: 7.38470983505249
2025-12-09 11:54:10.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0029195979069340924 Training loss: 6.9919328689575195
2025-12-09 11:54:11.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0029193025473597855 Training loss: 6.3826141357421875
2025-12-09 11:54:11.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0029190066612689142 Training loss: 6.941226959228516
2025-12-09 11:54:11.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0029187102487712433 Training loss: 7.077340126037598
2025-12-09 11:54:11.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0029184133099767326 Training loss: 7.216209888458252
2025-12-09 11:54:11.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.0029181158449955364 Training loss: 6.9664764404296875
2025-12-09 11:54:11.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0029178178539380054 Training loss: 7.218052864074707
2025-12-09 11:54:11.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0029175193369146844 Training loss: 6.9672698974609375
2025-12-09 11:54:11.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.002917220294036315 Training loss: 7.309445381164551
2025-12-09 11:54:11.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0029169207254138314 Training loss: 6.882728099822998
2025-12-09 11:54:11.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0029166206311583647 Training loss: 6.948824882507324
2025-12-09 11:54:11.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.00291632001138124 Training loss: 6.850742816925049
2025-12-09 11:54:11.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0029160188661939783 Training loss: 7.021442413330078
2025-12-09 11:54:11.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.002915717195708295 Training loss: 7.04585599899292
2025-12-09 11:54:12.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0029154150000361 Training loss: 6.763498306274414
2025-12-09 11:54:12.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0029151122792894987 Training loss: 7.630409240722656
2025-12-09 11:54:12.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.00291480903358079 Training loss: 7.160463809967041
2025-12-09 11:54:12.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00291450526302247 Training loss: 7.094379425048828
2025-12-09 11:54:12.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0029142009677272274 Training loss: 6.9634552001953125
2025-12-09 11:54:12.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0029138961478079456 Training loss: 7.124644756317139
2025-12-09 11:54:12.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.002913590803377704 Training loss: 6.723509788513184
2025-12-09 11:54:12.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0029132849345497756 Training loss: 7.086480140686035
2025-12-09 11:54:12.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0029129785414376275 Training loss: 7.074717044830322
2025-12-09 11:54:12.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0029126716241549225 Training loss: 6.889821529388428
2025-12-09 11:54:12.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0029123641828155173 Training loss: 6.89312219619751
2025-12-09 11:54:12.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0029120562175334627 Training loss: 6.7610907554626465
2025-12-09 11:54:13.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0029117477284230043 Training loss: 7.021039962768555
2025-12-09 11:54:13.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0029114387155985814 Training loss: 7.146274566650391
2025-12-09 11:54:13.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0029111291791748283 Training loss: 6.878828048706055
2025-12-09 11:54:13.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.002910819119266574 Training loss: 6.831043720245361
2025-12-09 11:54:13.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0029105085359888397 Training loss: 7.2762532234191895
2025-12-09 11:54:13.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.0029101974294568427 Training loss: 6.987800121307373
2025-12-09 11:54:13.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0029098857997859936 Training loss: 7.060516357421875
2025-12-09 11:54:13.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0029095736470918974 Training loss: 6.566768169403076
2025-12-09 11:54:13.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0029092609714903525 Training loss: 6.9873809814453125
2025-12-09 11:54:13.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.002908947773097352 Training loss: 7.317965030670166
2025-12-09 11:54:13.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.002908634052029083 Training loss: 7.1645188331604
2025-12-09 11:54:13.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0029083198084019256 Training loss: 6.586503028869629
2025-12-09 11:54:13.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0029080050423324543 Training loss: 6.098776340484619
2025-12-09 11:54:14.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.002907689753937438 Training loss: 6.825014591217041
2025-12-09 11:54:14.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.0029073739433338377 Training loss: 6.783503532409668
2025-12-09 11:54:14.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0029070576106388106 Training loss: 7.234841823577881
2025-12-09 11:54:14.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.002906740755969705 Training loss: 6.974328517913818
2025-12-09 11:54:14.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.002906423379444064 Training loss: 6.607484817504883
2025-12-09 11:54:14.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0029061054811796248 Training loss: 6.871148586273193
2025-12-09 11:54:14.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0029057870612943177 Training loss: 6.627816200256348
2025-12-09 11:54:14.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0029054681199062664 Training loss: 6.819278717041016
2025-12-09 11:54:14.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.002905148657133788 Training loss: 6.748235702514648
2025-12-09 11:54:14.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0029048286730953927 Training loss: 6.880241870880127
2025-12-09 11:54:14.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.002904508167909785 Training loss: 6.680698394775391
2025-12-09 11:54:14.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.002904187141695863 Training loss: 6.809781074523926
2025-12-09 11:54:14.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.002903865594572716 Training loss: 6.849064350128174
2025-12-09 11:54:15.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.002903543526659628 Training loss: 7.1836934089660645
2025-12-09 11:54:15.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.0029032209380760766 Training loss: 7.295605659484863
2025-12-09 11:54:15.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0029028978289417323 Training loss: 6.877119064331055
2025-12-09 11:54:15.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.002902574199376457 Training loss: 7.000710487365723
2025-12-09 11:54:15.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.002902250049500309 Training loss: 6.785236835479736
2025-12-09 11:54:15.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0029019253794335363 Training loss: 6.308955192565918
2025-12-09 11:54:15.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0029016001892965817 Training loss: 6.174111366271973
2025-12-09 11:54:15.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.00290127447921008 Training loss: 6.977548599243164
2025-12-09 11:54:15.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.0029009482492948608 Training loss: 6.9700846672058105
2025-12-09 11:54:15.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.002900621499671944 Training loss: 7.251511096954346
2025-12-09 11:54:15.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0029002942304625435 Training loss: 7.0190348625183105
2025-12-09 11:54:15.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0028999664417880657 Training loss: 6.9905619621276855
2025-12-09 11:54:16.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.0028996381337701104 Training loss: 6.579888343811035
2025-12-09 11:54:16.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0028993093065304695 Training loss: 7.08418607711792
2025-12-09 11:54:16.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.002898979960191127 Training loss: 7.003323078155518
2025-12-09 11:54:16.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.002898650094874261 Training loss: 6.742406845092773
2025-12-09 11:54:16.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00289831971070224 Training loss: 6.80391263961792
2025-12-09 11:54:16.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.002897988807797627 Training loss: 6.764802932739258
2025-12-09 11:54:16.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.002897657386283176 Training loss: 6.996060371398926
2025-12-09 11:54:16.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0028973254462818345 Training loss: 7.070812225341797
2025-12-09 11:54:16.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.002896992987916741 Training loss: 6.612070083618164
2025-12-09 11:54:16.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0028966600113112277 Training loss: 5.8864970207214355
2025-12-09 11:54:16.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.002896326516588819 Training loss: 6.585468769073486
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 90.89 GiB memory in use. Of the allocated memory 89.10 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:07, 147.59it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 177.15it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:21, 122.75it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:09, 141.95it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:56, 173.90it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 214.35it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:52, 186.18it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 185.07it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 197.67it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 202.79it/s]Tokenizing texts:   2%|▏         | 241/10000 [00:01<00:47, 205.53it/s]Tokenizing texts:   3%|▎         | 263/10000 [00:01<00:47, 204.73it/s]Tokenizing texts:   3%|▎         | 292/10000 [00:01<00:42, 228.37it/s]Tokenizing texts:   3%|▎         | 316/10000 [00:01<00:42, 228.42it/s]Tokenizing texts:   3%|▎         | 340/10000 [00:01<00:52, 183.47it/s]Tokenizing texts:   4%|▎         | 364/10000 [00:01<00:48, 197.22it/s]Tokenizing texts:   4%|▍         | 386/10000 [00:02<00:52, 183.90it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 186.39it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:51, 185.64it/s]Tokenizing texts:   4%|▍         | 446/10000 [00:02<00:51, 187.24it/s]Tokenizing texts:   5%|▍         | 468/10000 [00:02<00:48, 195.87it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 218.13it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:39, 237.20it/s]Tokenizing texts:   6%|▌         | 551/10000 [00:02<00:50, 187.07it/s]Tokenizing texts:   6%|▌         | 580/10000 [00:02<00:44, 211.90it/s]Tokenizing texts:   6%|▌         | 604/10000 [00:03<00:44, 212.66it/s]Tokenizing texts:   6%|▋         | 627/10000 [00:03<00:46, 199.49it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 202.03it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 199.87it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 191.20it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 200.68it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 200.43it/s]Tokenizing texts:   8%|▊         | 767/10000 [00:03<00:43, 213.59it/s]Tokenizing texts:   8%|▊         | 789/10000 [00:04<00:45, 204.00it/s]Tokenizing texts:   8%|▊         | 813/10000 [00:04<00:43, 212.16it/s]Tokenizing texts:   8%|▊         | 835/10000 [00:04<00:43, 212.41it/s]Tokenizing texts:   9%|▊         | 862/10000 [00:04<00:40, 225.17it/s]Tokenizing texts:   9%|▉         | 885/10000 [00:04<00:41, 218.00it/s]Tokenizing texts:   9%|▉         | 911/10000 [00:04<00:39, 227.30it/s]Tokenizing texts:   9%|▉         | 943/10000 [00:04<00:35, 253.61it/s]Tokenizing texts:  10%|▉         | 974/10000 [00:04<00:34, 264.50it/s]Tokenizing texts:  10%|█         | 1004/10000 [00:04<00:33, 272.23it/s]Tokenizing texts:  10%|█         | 1036/10000 [00:04<00:31, 285.42it/s]Tokenizing texts:  11%|█         | 1065/10000 [00:05<00:40, 222.41it/s]Tokenizing texts:  11%|█         | 1090/10000 [00:05<00:40, 218.53it/s]Tokenizing texts:  11%|█         | 1116/10000 [00:05<00:39, 227.31it/s]Tokenizing texts:  11%|█▏        | 1140/10000 [00:05<00:45, 196.75it/s]Tokenizing texts:  12%|█▏        | 1165/10000 [00:05<00:43, 200.81it/s]Tokenizing texts:  12%|█▏        | 1193/10000 [00:05<00:41, 213.71it/s]Tokenizing texts:  12%|█▏        | 1216/10000 [00:05<00:40, 216.10it/s]Tokenizing texts:  12%|█▏        | 1239/10000 [00:05<00:41, 211.98it/s]Tokenizing texts:  13%|█▎        | 1261/10000 [00:06<00:41, 208.73it/s]Tokenizing texts:  13%|█▎        | 1295/10000 [00:06<00:35, 244.16it/s]Tokenizing texts:  13%|█▎        | 1320/10000 [00:06<00:39, 218.47it/s]Tokenizing texts:  13%|█▎        | 1343/10000 [00:06<00:42, 204.38it/s]Tokenizing texts:  14%|█▎        | 1370/10000 [00:06<00:40, 214.45it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:39, 218.13it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 235.33it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:33, 251.86it/s]Tokenizing texts:  15%|█▍        | 1487/10000 [00:07<00:31, 273.62it/s]Tokenizing texts:  15%|█▌        | 1515/10000 [00:07<00:31, 267.73it/s]Tokenizing texts:  15%|█▌        | 1543/10000 [00:07<00:33, 251.82it/s]Tokenizing texts:  16%|█▌        | 1575/10000 [00:07<00:31, 268.64it/s]Tokenizing texts:  16%|█▌        | 1603/10000 [00:07<00:33, 249.44it/s]Tokenizing texts:  16%|█▋        | 1629/10000 [00:07<00:40, 208.91it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 215.72it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 216.68it/s]Tokenizing texts:  17%|█▋        | 1703/10000 [00:07<00:37, 219.78it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:33, 243.64it/s]Tokenizing texts:  18%|█▊        | 1760/10000 [00:08<00:33, 243.90it/s]Tokenizing texts:  18%|█▊        | 1786/10000 [00:08<00:33, 244.82it/s]Tokenizing texts:  18%|█▊        | 1820/10000 [00:08<00:30, 271.12it/s]Tokenizing texts:  18%|█▊        | 1848/10000 [00:08<00:37, 217.69it/s]Tokenizing texts:  19%|█▊        | 1872/10000 [00:08<00:36, 220.65it/s]Tokenizing texts:  19%|█▉        | 1896/10000 [00:08<00:36, 225.02it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:08<00:35, 225.42it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 195.47it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:38, 211.04it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:35, 228.14it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 233.37it/s]Tokenizing texts:  21%|██        | 2055/10000 [00:09<00:32, 247.55it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:37, 208.45it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:37, 209.21it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:36, 214.21it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:09<00:38, 204.53it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:35, 221.90it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:31, 249.75it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:29, 260.04it/s]Tokenizing texts:  23%|██▎       | 2274/10000 [00:10<00:28, 270.69it/s]Tokenizing texts:  23%|██▎       | 2302/10000 [00:10<00:29, 260.41it/s]Tokenizing texts:  23%|██▎       | 2329/10000 [00:10<00:36, 210.02it/s]Tokenizing texts:  24%|██▎       | 2365/10000 [00:10<00:31, 245.64it/s]Tokenizing texts:  24%|██▍       | 2399/10000 [00:10<00:28, 266.68it/s]Tokenizing texts:  24%|██▍       | 2428/10000 [00:11<00:30, 246.57it/s]Tokenizing texts:  25%|██▍       | 2455/10000 [00:11<00:30, 249.85it/s]Tokenizing texts:  25%|██▍       | 2482/10000 [00:11<00:30, 245.18it/s]Tokenizing texts:  25%|██▌       | 2508/10000 [00:11<00:30, 244.37it/s]Tokenizing texts:  25%|██▌       | 2538/10000 [00:11<00:28, 257.56it/s]Tokenizing texts:  26%|██▌       | 2565/10000 [00:11<00:33, 221.87it/s]Tokenizing texts:  26%|██▌       | 2589/10000 [00:11<00:33, 220.75it/s]Tokenizing texts:  26%|██▌       | 2612/10000 [00:11<00:36, 203.07it/s]Tokenizing texts:  26%|██▋       | 2640/10000 [00:12<00:33, 222.17it/s]Tokenizing texts:  27%|██▋       | 2667/10000 [00:12<00:31, 230.06it/s]Tokenizing texts:  27%|██▋       | 2691/10000 [00:12<00:32, 222.54it/s]Tokenizing texts:  27%|██▋       | 2714/10000 [00:12<00:36, 202.17it/s]Tokenizing texts:  27%|██▋       | 2749/10000 [00:12<00:30, 238.99it/s]Tokenizing texts:  28%|██▊       | 2776/10000 [00:12<00:29, 242.00it/s]Tokenizing texts:  28%|██▊       | 2801/10000 [00:12<00:40, 178.75it/s]Tokenizing texts:  28%|██▊       | 2836/10000 [00:12<00:33, 216.56it/s]Tokenizing texts:  29%|██▊       | 2861/10000 [00:13<00:35, 198.99it/s]Tokenizing texts:  29%|██▉       | 2890/10000 [00:13<00:32, 219.10it/s]Tokenizing texts:  29%|██▉       | 2923/10000 [00:13<00:28, 245.65it/s]Tokenizing texts:  30%|██▉       | 2950/10000 [00:13<00:31, 224.76it/s]Tokenizing texts:  30%|██▉       | 2975/10000 [00:13<00:30, 227.58it/s]Tokenizing texts:  30%|███       | 3013/10000 [00:13<00:26, 264.31it/s]Tokenizing texts:  30%|███       | 3041/10000 [00:13<00:30, 228.43it/s]Tokenizing texts:  31%|███       | 3074/10000 [00:13<00:27, 253.37it/s]Tokenizing texts:  31%|███       | 3102/10000 [00:14<00:28, 243.17it/s]Tokenizing texts:  31%|███▏      | 3136/10000 [00:14<00:25, 266.62it/s]Tokenizing texts:  32%|███▏      | 3164/10000 [00:14<00:27, 253.03it/s]Tokenizing texts:  32%|███▏      | 3192/10000 [00:14<00:26, 259.84it/s]Tokenizing texts:  32%|███▏      | 3220/10000 [00:14<00:25, 263.86it/s]Tokenizing texts:  32%|███▏      | 3247/10000 [00:14<00:26, 256.78it/s]Tokenizing texts:  33%|███▎      | 3278/10000 [00:14<00:25, 267.98it/s]Tokenizing texts:  33%|███▎      | 3306/10000 [00:14<00:25, 260.64it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:14<00:26, 248.91it/s]Tokenizing texts:  34%|███▎      | 3361/10000 [00:15<00:25, 257.28it/s]Tokenizing texts:  34%|███▍      | 3387/10000 [00:15<00:26, 253.39it/s]Tokenizing texts:  34%|███▍      | 3413/10000 [00:15<00:25, 255.07it/s]Tokenizing texts:  35%|███▍      | 3453/10000 [00:15<00:22, 294.90it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:15<00:22, 290.14it/s]Tokenizing texts:  35%|███▌      | 3517/10000 [00:15<00:26, 245.97it/s]Tokenizing texts:  35%|███▌      | 3543/10000 [00:15<00:27, 233.10it/s]Tokenizing texts:  36%|███▌      | 3570/10000 [00:15<00:26, 241.10it/s]Tokenizing texts:  36%|███▌      | 3600/10000 [00:15<00:25, 252.81it/s]Tokenizing texts:  36%|███▋      | 3626/10000 [00:16<00:25, 251.57it/s]Tokenizing texts:  37%|███▋      | 3652/10000 [00:16<00:25, 249.81it/s]Tokenizing texts:  37%|███▋      | 3678/10000 [00:16<00:25, 246.69it/s]Tokenizing texts:  37%|███▋      | 3705/10000 [00:16<00:25, 250.81it/s]Tokenizing texts:  37%|███▋      | 3732/10000 [00:16<00:24, 255.56it/s]Tokenizing texts:  38%|███▊      | 3758/10000 [00:16<00:27, 225.28it/s]Tokenizing texts:  38%|███▊      | 3784/10000 [00:16<00:26, 234.28it/s]Tokenizing texts:  38%|███▊      | 3809/10000 [00:16<00:29, 206.47it/s]Tokenizing texts:  38%|███▊      | 3841/10000 [00:16<00:26, 235.14it/s]Tokenizing texts:  39%|███▊      | 3867/10000 [00:17<00:25, 241.41it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:17<00:24, 249.91it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:25, 234.39it/s]Tokenizing texts:  39%|███▉      | 3946/10000 [00:17<00:26, 229.57it/s]Tokenizing texts:  40%|███▉      | 3979/10000 [00:17<00:23, 256.30it/s]Tokenizing texts:  40%|████      | 4006/10000 [00:17<00:27, 219.21it/s]Tokenizing texts:  40%|████      | 4037/10000 [00:17<00:25, 237.22it/s]Tokenizing texts:  41%|████      | 4067/10000 [00:17<00:23, 253.18it/s]Tokenizing texts:  41%|████      | 4099/10000 [00:17<00:21, 270.77it/s]Tokenizing texts:  41%|████▏     | 4132/10000 [00:18<00:20, 286.33it/s]Tokenizing texts:  42%|████▏     | 4162/10000 [00:18<00:21, 272.11it/s]Tokenizing texts:  42%|████▏     | 4195/10000 [00:18<00:20, 285.83it/s]Tokenizing texts:  42%|████▏     | 4225/10000 [00:18<00:22, 260.65it/s]Tokenizing texts:  43%|████▎     | 4252/10000 [00:18<00:23, 246.44it/s]Tokenizing texts:  43%|████▎     | 4278/10000 [00:18<00:24, 236.28it/s]Tokenizing texts:  43%|████▎     | 4310/10000 [00:18<00:22, 257.48it/s]Tokenizing texts:  43%|████▎     | 4345/10000 [00:18<00:20, 279.60it/s]Tokenizing texts:  44%|████▎     | 4374/10000 [00:19<00:21, 263.05it/s]Tokenizing texts:  44%|████▍     | 4412/10000 [00:19<00:19, 292.03it/s]Tokenizing texts:  44%|████▍     | 4442/10000 [00:19<00:19, 280.58it/s]Tokenizing texts:  45%|████▍     | 4471/10000 [00:19<00:21, 257.00it/s]Tokenizing texts:  45%|████▍     | 4498/10000 [00:19<00:22, 241.22it/s]Tokenizing texts:  45%|████▌     | 4525/10000 [00:19<00:22, 248.03it/s]Tokenizing texts:  46%|████▌     | 4552/10000 [00:19<00:21, 253.02it/s]Tokenizing texts:  46%|████▌     | 4578/10000 [00:19<00:21, 250.52it/s]Tokenizing texts:  46%|████▌     | 4604/10000 [00:19<00:22, 238.60it/s]Tokenizing texts:  46%|████▋     | 4637/10000 [00:20<00:20, 261.78it/s]Tokenizing texts:  47%|████▋     | 4671/10000 [00:20<00:18, 282.10it/s]Tokenizing texts:  47%|████▋     | 4708/10000 [00:20<00:17, 304.55it/s]Tokenizing texts:  47%|████▋     | 4739/10000 [00:20<00:18, 292.21it/s]Tokenizing texts:  48%|████▊     | 4769/10000 [00:20<00:18, 282.01it/s]Tokenizing texts:  48%|████▊     | 4798/10000 [00:20<00:21, 245.03it/s]Tokenizing texts:  48%|████▊     | 4830/10000 [00:20<00:19, 261.07it/s]Tokenizing texts:  49%|████▊     | 4859/10000 [00:20<00:19, 266.23it/s]Tokenizing texts:  49%|████▉     | 4887/10000 [00:20<00:19, 268.06it/s]Tokenizing texts:  49%|████▉     | 4916/10000 [00:21<00:18, 272.57it/s]Tokenizing texts:  49%|████▉     | 4944/10000 [00:21<00:18, 271.48it/s]Tokenizing texts:  50%|████▉     | 4972/10000 [00:21<00:19, 261.90it/s]Tokenizing texts:  50%|█████     | 5011/10000 [00:21<00:17, 283.18it/s]Tokenizing texts:  50%|█████     | 5040/10000 [00:21<00:17, 280.79it/s]Tokenizing texts:  51%|█████     | 5069/10000 [00:21<00:19, 246.73it/s]Tokenizing texts:  51%|█████     | 5095/10000 [00:21<00:20, 243.36it/s]Tokenizing texts:  51%|█████     | 5122/10000 [00:21<00:20, 234.20it/s]Tokenizing texts:  52%|█████▏    | 5150/10000 [00:22<00:19, 244.01it/s]Tokenizing texts:  52%|█████▏    | 5178/10000 [00:22<00:19, 252.77it/s]Tokenizing texts:  52%|█████▏    | 5204/10000 [00:22<00:20, 237.43it/s]Tokenizing texts:  52%|█████▏    | 5231/10000 [00:22<00:19, 241.55it/s]Tokenizing texts:  53%|█████▎    | 5265/10000 [00:22<00:17, 265.76it/s]Tokenizing texts:  53%|█████▎    | 5292/10000 [00:22<00:18, 253.39it/s]Tokenizing texts:  53%|█████▎    | 5327/10000 [00:22<00:16, 279.25it/s]Tokenizing texts:  54%|█████▎    | 5359/10000 [00:22<00:16, 289.36it/s]Tokenizing texts:  54%|█████▍    | 5390/10000 [00:22<00:15, 293.35it/s]Tokenizing texts:  54%|█████▍    | 5423/10000 [00:22<00:15, 302.06it/s]Tokenizing texts:  55%|█████▍    | 5454/10000 [00:23<00:16, 272.73it/s]Tokenizing texts:  55%|█████▍    | 5482/10000 [00:23<00:17, 256.91it/s]Tokenizing texts:  55%|█████▌    | 5509/10000 [00:23<00:20, 216.85it/s]Tokenizing texts:  55%|█████▌    | 5542/10000 [00:23<00:18, 239.35it/s]Tokenizing texts:  56%|█████▌    | 5570/10000 [00:23<00:18, 241.87it/s]Tokenizing texts:  56%|█████▌    | 5600/10000 [00:23<00:17, 256.57it/s]Tokenizing texts:  56%|█████▋    | 5636/10000 [00:23<00:15, 283.04it/s]Tokenizing texts:  57%|█████▋    | 5672/10000 [00:23<00:14, 298.29it/s]Tokenizing texts:  57%|█████▋    | 5703/10000 [00:24<00:14, 297.65it/s]Tokenizing texts:  57%|█████▋    | 5734/10000 [00:24<00:15, 284.14it/s]Tokenizing texts:  58%|█████▊    | 5763/10000 [00:24<00:16, 258.82it/s]Tokenizing texts:  58%|█████▊    | 5790/10000 [00:24<00:19, 220.24it/s]Tokenizing texts:  58%|█████▊    | 5814/10000 [00:24<00:19, 217.62it/s]Tokenizing texts:  58%|█████▊    | 5837/10000 [00:24<00:19, 214.13it/s]Tokenizing texts:  59%|█████▊    | 5873/10000 [00:24<00:16, 251.37it/s]Tokenizing texts:  59%|█████▉    | 5900/10000 [00:24<00:17, 237.04it/s]Tokenizing texts:  59%|█████▉    | 5925/10000 [00:25<00:17, 235.99it/s]Tokenizing texts:  60%|█████▉    | 5954/10000 [00:25<00:16, 247.01it/s]Tokenizing texts:  60%|█████▉    | 5983/10000 [00:25<00:15, 257.70it/s]Tokenizing texts:  60%|██████    | 6010/10000 [00:25<00:17, 228.64it/s]Tokenizing texts:  60%|██████    | 6048/10000 [00:25<00:14, 267.42it/s]Tokenizing texts:  61%|██████    | 6076/10000 [00:25<00:14, 262.35it/s]Tokenizing texts:  61%|██████    | 6104/10000 [00:25<00:15, 256.70it/s]Tokenizing texts:  61%|██████▏   | 6138/10000 [00:25<00:13, 277.13it/s]Tokenizing texts:  62%|██████▏   | 6167/10000 [00:25<00:13, 277.15it/s]Tokenizing texts:  62%|██████▏   | 6196/10000 [00:26<00:13, 276.72it/s]Tokenizing texts:  62%|██████▏   | 6224/10000 [00:26<00:15, 247.02it/s]Tokenizing texts:  63%|██████▎   | 6254/10000 [00:26<00:14, 259.29it/s]Tokenizing texts:  63%|██████▎   | 6281/10000 [00:26<00:14, 251.83it/s]Tokenizing texts:  63%|██████▎   | 6307/10000 [00:26<00:14, 246.96it/s]Tokenizing texts:  63%|██████▎   | 6332/10000 [00:26<00:16, 223.60it/s]Tokenizing texts:  64%|██████▎   | 6359/10000 [00:26<00:15, 234.96it/s]Tokenizing texts:  64%|██████▍   | 6386/10000 [00:26<00:14, 243.53it/s]Tokenizing texts:  64%|██████▍   | 6414/10000 [00:26<00:14, 253.18it/s]Tokenizing texts:  64%|██████▍   | 6442/10000 [00:27<00:14, 253.95it/s]Tokenizing texts:  65%|██████▍   | 6468/10000 [00:27<00:14, 248.24it/s]Tokenizing texts:  65%|██████▍   | 6494/10000 [00:27<00:14, 248.54it/s]Tokenizing texts:  65%|██████▌   | 6519/10000 [00:27<00:15, 226.60it/s]Tokenizing texts:  66%|██████▌   | 6553/10000 [00:27<00:13, 256.13it/s]Tokenizing texts:  66%|██████▌   | 6592/10000 [00:27<00:11, 292.20it/s]Tokenizing texts:  66%|██████▌   | 6622/10000 [00:27<00:12, 262.58it/s]Tokenizing texts:  66%|██████▋   | 6650/10000 [00:27<00:12, 259.51it/s]Tokenizing texts:  67%|██████▋   | 6677/10000 [00:27<00:12, 260.86it/s]Tokenizing texts:  67%|██████▋   | 6704/10000 [00:28<00:13, 238.90it/s]Tokenizing texts:  67%|██████▋   | 6732/10000 [00:28<00:13, 243.93it/s]Tokenizing texts:  68%|██████▊   | 6759/10000 [00:28<00:12, 250.89it/s]Tokenizing texts:  68%|██████▊   | 6798/10000 [00:28<00:11, 287.69it/s]Tokenizing texts:  68%|██████▊   | 6830/10000 [00:28<00:10, 295.74it/s]Tokenizing texts:  69%|██████▊   | 6860/10000 [00:28<00:10, 293.90it/s]Tokenizing texts:  69%|██████▉   | 6890/10000 [00:28<00:11, 277.98it/s]Tokenizing texts:  69%|██████▉   | 6919/10000 [00:28<00:14, 218.11it/s]Tokenizing texts:  69%|██████▉   | 6945/10000 [00:29<00:13, 227.24it/s]Tokenizing texts:  70%|██████▉   | 6973/10000 [00:29<00:12, 239.04it/s]Tokenizing texts:  70%|███████   | 7000/10000 [00:29<00:12, 246.06it/s]Tokenizing texts:  70%|███████   | 7026/10000 [00:29<00:12, 236.45it/s]Tokenizing texts:  71%|███████   | 7051/10000 [00:29<00:12, 234.26it/s]Tokenizing texts:  71%|███████   | 7075/10000 [00:29<00:13, 213.43it/s]Tokenizing texts:  71%|███████   | 7099/10000 [00:29<00:13, 216.51it/s]Tokenizing texts:  71%|███████▏  | 7128/10000 [00:29<00:12, 234.84it/s]Tokenizing texts:  72%|███████▏  | 7153/10000 [00:29<00:12, 234.28it/s]Tokenizing texts:  72%|███████▏  | 7181/10000 [00:30<00:11, 246.99it/s]Tokenizing texts:  72%|███████▏  | 7207/10000 [00:30<00:11, 249.84it/s]Tokenizing texts:  72%|███████▏  | 7233/10000 [00:30<00:11, 247.27it/s]Tokenizing texts:  73%|███████▎  | 7264/10000 [00:30<00:10, 262.48it/s]Tokenizing texts:  73%|███████▎  | 7303/10000 [00:30<00:09, 298.60it/s]Tokenizing texts:  73%|███████▎  | 7335/10000 [00:30<00:08, 303.02it/s]Tokenizing texts:  74%|███████▎  | 7366/10000 [00:30<00:09, 264.25it/s]Tokenizing texts:  74%|███████▍  | 7394/10000 [00:30<00:09, 268.22it/s]Tokenizing texts:  74%|███████▍  | 7422/10000 [00:30<00:11, 223.43it/s]Tokenizing texts:  74%|███████▍  | 7447/10000 [00:31<00:12, 205.06it/s]Tokenizing texts:  75%|███████▍  | 7478/10000 [00:31<00:11, 228.32it/s]Tokenizing texts:  75%|███████▌  | 7515/10000 [00:31<00:09, 261.77it/s]Tokenizing texts:  75%|███████▌  | 7543/10000 [00:31<00:09, 265.73it/s]Tokenizing texts:  76%|███████▌  | 7571/10000 [00:31<00:09, 259.89it/s]Tokenizing texts:  76%|███████▌  | 7611/10000 [00:31<00:08, 289.05it/s]Tokenizing texts:  76%|███████▋  | 7641/10000 [00:31<00:08, 271.78it/s]Tokenizing texts:  77%|███████▋  | 7674/10000 [00:31<00:08, 285.92it/s]Tokenizing texts:  77%|███████▋  | 7704/10000 [00:31<00:07, 289.11it/s]Tokenizing texts:  77%|███████▋  | 7734/10000 [00:32<00:09, 248.28it/s]Tokenizing texts:  78%|███████▊  | 7761/10000 [00:32<00:09, 243.72it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 257.09it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 236.45it/s]Tokenizing texts:  78%|███████▊  | 7845/10000 [00:32<00:08, 244.01it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 256.16it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 253.25it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:32<00:07, 281.46it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:33<00:07, 279.20it/s]Tokenizing texts:  80%|███████▉  | 7999/10000 [00:33<00:06, 290.37it/s]Tokenizing texts:  80%|████████  | 8029/10000 [00:33<00:06, 286.82it/s]Tokenizing texts:  81%|████████  | 8058/10000 [00:33<00:11, 169.64it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 199.97it/s]Tokenizing texts:  81%|████████  | 8120/10000 [00:33<00:08, 208.92it/s]Tokenizing texts:  81%|████████▏ | 8149/10000 [00:33<00:08, 226.23it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:34<00:07, 238.24it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:34<00:07, 249.04it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 213.51it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 223.81it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 257.25it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 253.05it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:34<00:06, 256.68it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:34<00:06, 255.97it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:34<00:06, 231.92it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:35<00:06, 257.54it/s]Tokenizing texts:  85%|████████▍ | 8472/10000 [00:35<00:05, 263.18it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:35<00:05, 278.21it/s]Tokenizing texts:  85%|████████▌ | 8533/10000 [00:35<00:05, 260.42it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:35<00:05, 266.80it/s]Tokenizing texts:  86%|████████▌ | 8590/10000 [00:35<00:05, 261.93it/s]Tokenizing texts:  86%|████████▌ | 8624/10000 [00:35<00:04, 283.19it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:35<00:04, 284.80it/s]Tokenizing texts:  87%|████████▋ | 8687/10000 [00:35<00:04, 294.72it/s]Tokenizing texts:  87%|████████▋ | 8723/10000 [00:36<00:04, 310.13it/s]Tokenizing texts:  88%|████████▊ | 8755/10000 [00:36<00:04, 279.71it/s]Tokenizing texts:  88%|████████▊ | 8785/10000 [00:36<00:04, 282.90it/s]Tokenizing texts:  88%|████████▊ | 8814/10000 [00:36<00:05, 216.67it/s]Tokenizing texts:  88%|████████▊ | 8839/10000 [00:36<00:05, 218.53it/s]Tokenizing texts:  89%|████████▊ | 8872/10000 [00:36<00:04, 244.68it/s]Tokenizing texts:  89%|████████▉ | 8911/10000 [00:36<00:03, 280.39it/s]Tokenizing texts:  89%|████████▉ | 8941/10000 [00:36<00:03, 283.83it/s]Tokenizing texts:  90%|████████▉ | 8971/10000 [00:37<00:03, 275.28it/s]Tokenizing texts:  90%|█████████ | 9000/10000 [00:37<00:04, 247.52it/s]Tokenizing texts:  90%|█████████ | 9030/10000 [00:37<00:03, 258.77it/s]Tokenizing texts:  91%|█████████ | 9057/10000 [00:37<00:03, 250.45it/s]Tokenizing texts:  91%|█████████ | 9085/10000 [00:37<00:03, 256.33it/s]Tokenizing texts:  91%|█████████ | 9115/10000 [00:37<00:03, 266.83it/s]Tokenizing texts:  91%|█████████▏| 9146/10000 [00:37<00:03, 275.31it/s]Tokenizing texts:  92%|█████████▏| 9175/10000 [00:37<00:02, 278.77it/s]Tokenizing texts:  92%|█████████▏| 9205/10000 [00:37<00:02, 282.49it/s]Tokenizing texts:  92%|█████████▏| 9234/10000 [00:38<00:02, 274.02it/s]Tokenizing texts:  93%|█████████▎| 9262/10000 [00:38<00:02, 274.23it/s]Tokenizing texts:  93%|█████████▎| 9290/10000 [00:38<00:02, 254.10it/s]Tokenizing texts:  93%|█████████▎| 9316/10000 [00:38<00:02, 231.80it/s]Tokenizing texts:  93%|█████████▎| 9348/10000 [00:38<00:02, 254.38it/s]Tokenizing texts:  94%|█████████▍| 9375/10000 [00:38<00:02, 221.84it/s]Tokenizing texts:  94%|█████████▍| 9403/10000 [00:38<00:02, 229.96it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:38<00:02, 229.69it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:38<00:02, 234.18it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:39<00:02, 243.01it/s]Tokenizing texts:  95%|█████████▌| 9504/10000 [00:39<00:02, 182.67it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 222.64it/s]Tokenizing texts:  96%|█████████▌| 9566/10000 [00:39<00:01, 229.61it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 234.19it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 232.69it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 235.14it/s]Tokenizing texts:  97%|█████████▋| 9674/10000 [00:39<00:01, 241.47it/s]Tokenizing texts:  97%|█████████▋| 9699/10000 [00:40<00:01, 236.70it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:40<00:01, 239.13it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 206.83it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 215.30it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 260.62it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 253.26it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 258.19it/s]Tokenizing texts:  99%|█████████▉| 9907/10000 [00:40<00:00, 285.27it/s]Tokenizing texts:  99%|█████████▉| 9937/10000 [00:40<00:00, 282.28it/s]Tokenizing texts: 100%|█████████▉| 9968/10000 [00:41<00:00, 289.14it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 242.84it/s]
2025-12-09 11:55:17.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 12.063255310058594
2025-12-09 11:55:17.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 11.984463691711426
2025-12-09 11:55:17.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 11.958647727966309
2025-12-09 11:55:17.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 11.748620986938477
2025-12-09 11:55:17.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 11.376509666442871
2025-12-09 11:55:17.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 11.247518539428711
2025-12-09 11:55:17.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 10.877285957336426
2025-12-09 11:55:18.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 10.8001070022583
2025-12-09 11:55:18.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 10.534674644470215
2025-12-09 11:55:18.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 10.24853801727295
2025-12-09 11:55:18.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 9.917692184448242
2025-12-09 11:55:18.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 9.327902793884277
2025-12-09 11:55:18.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 9.123360633850098
2025-12-09 11:55:18.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 8.69895076751709
2025-12-09 11:55:18.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 8.600857734680176
2025-12-09 11:55:18.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 8.295791625976562
2025-12-09 11:55:18.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 7.934670448303223
2025-12-09 11:55:18.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 8.213921546936035
2025-12-09 11:55:18.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 7.761539459228516
2025-12-09 11:55:18.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 8.795857429504395
2025-12-09 11:55:19.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 8.233681678771973
2025-12-09 11:55:19.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 8.475003242492676
2025-12-09 11:55:19.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 8.806303024291992
2025-12-09 11:55:19.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 8.31022834777832
2025-12-09 11:55:19.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 8.440410614013672
2025-12-09 11:55:19.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 8.32324504852295
2025-12-09 11:55:19.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 7.915920734405518
2025-12-09 11:55:19.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 8.352940559387207
2025-12-09 11:55:19.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 8.296767234802246
2025-12-09 11:55:19.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 8.073149681091309
2025-12-09 11:55:19.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 8.077943801879883
2025-12-09 11:55:19.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 7.8735504150390625
2025-12-09 11:55:19.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 8.561635971069336
2025-12-09 11:55:20.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 8.129870414733887
2025-12-09 11:55:20.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 8.21534252166748
2025-12-09 11:55:20.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 8.371397972106934
2025-12-09 11:55:20.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 8.288712501525879
2025-12-09 11:55:20.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 8.2721586227417
2025-12-09 11:55:20.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 8.168195724487305
2025-12-09 11:55:20.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 8.315217971801758
2025-12-09 11:55:20.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 8.445841789245605
2025-12-09 11:55:20.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 8.319153785705566
2025-12-09 11:55:20.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 8.377486228942871
2025-12-09 11:55:20.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 8.298627853393555
2025-12-09 11:55:20.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 8.447206497192383
2025-12-09 11:55:20.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 8.34002685546875
2025-12-09 11:55:21.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 8.286623001098633
2025-12-09 11:55:21.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 7.168179035186768
2025-12-09 11:55:21.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 8.062989234924316
2025-12-09 11:55:21.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 8.379728317260742
2025-12-09 11:55:21.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 8.502148628234863
2025-12-09 11:55:21.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 8.302995681762695
2025-12-09 11:55:21.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 8.154354095458984
2025-12-09 11:55:21.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 8.138792037963867
2025-12-09 11:55:21.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 8.232065200805664
2025-12-09 11:55:21.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 9.041971206665039
2025-12-09 11:55:21.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 8.484880447387695
2025-12-09 11:55:21.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 8.374231338500977
2025-12-09 11:55:22.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 8.291810035705566
2025-12-09 11:55:22.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 8.031333923339844
2025-12-09 11:55:22.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 8.304935455322266
2025-12-09 11:55:22.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 8.272747039794922
2025-12-09 11:55:22.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 8.330089569091797
2025-12-09 11:55:22.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 8.040046691894531
2025-12-09 11:55:22.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 7.890986442565918
2025-12-09 11:55:22.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 7.9498162269592285
2025-12-09 11:55:22.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 8.037846565246582
2025-12-09 11:55:22.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 8.000903129577637
2025-12-09 11:55:22.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 8.247431755065918
2025-12-09 11:55:22.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 7.995804309844971
2025-12-09 11:55:22.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 8.29846477508545
2025-12-09 11:55:23.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 8.483443260192871
2025-12-09 11:55:23.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 8.348827362060547
2025-12-09 11:55:23.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 7.922191619873047
2025-12-09 11:55:23.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 8.266739845275879
2025-12-09 11:55:23.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 7.98421049118042
2025-12-09 11:55:23.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 8.139110565185547
2025-12-09 11:55:23.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 8.27150821685791
2025-12-09 11:55:23.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 8.009970664978027
2025-12-09 11:55:23.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 8.098555564880371
2025-12-09 11:55:23.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 8.964803695678711
2025-12-09 11:55:23.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 8.432161331176758
2025-12-09 11:55:23.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 8.59907341003418
2025-12-09 11:55:23.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 7.838232517242432
2025-12-09 11:55:24.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 8.13435173034668
2025-12-09 11:55:24.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 8.322112083435059
2025-12-09 11:55:24.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 8.416458129882812
2025-12-09 11:55:24.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 8.426653861999512
2025-12-09 11:55:24.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 8.385293006896973
2025-12-09 11:55:24.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 8.705780982971191
2025-12-09 11:55:24.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 8.011357307434082
2025-12-09 11:55:24.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 8.354504585266113
2025-12-09 11:55:24.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 8.102559089660645
2025-12-09 11:55:24.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 7.839563369750977
2025-12-09 11:55:24.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 7.943742275238037
2025-12-09 11:55:24.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 8.120260238647461
2025-12-09 11:55:24.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 7.992364406585693
2025-12-09 11:55:25.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 7.949121475219727
2025-12-09 11:55:25.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 8.070487022399902
2025-12-09 11:55:25.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 7.975766658782959
2025-12-09 11:55:25.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999999072578702 Training loss: 8.261590957641602
2025-12-09 11:55:25.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.009999996290315153 Training loss: 8.06386947631836
2025-12-09 11:55:25.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009999991653210385 Training loss: 7.864890098571777
2025-12-09 11:55:25.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009999985161266116 Training loss: 8.286579132080078
2025-12-09 11:55:25.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009999976814484758 Training loss: 7.955945014953613
2025-12-09 11:55:25.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009999966612869405 Training loss: 8.195731163024902
2025-12-09 11:55:25.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009999954556423843 Training loss: 7.40069055557251
2025-12-09 11:55:25.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.009999940645152541 Training loss: 7.764433860778809
2025-12-09 11:55:26.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.009999924879060665 Training loss: 8.193150520324707
2025-12-09 11:55:26.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.00999990725815406 Training loss: 8.528305053710938
2025-12-09 11:55:26.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009999887782439263 Training loss: 8.32723617553711
2025-12-09 11:55:26.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0099998664519235 Training loss: 8.124420166015625
2025-12-09 11:55:26.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009999843266614685 Training loss: 7.911625862121582
2025-12-09 11:55:26.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009999818226521415 Training loss: 8.024813652038574
2025-12-09 11:55:26.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.009999791331652984 Training loss: 7.943151473999023
2025-12-09 11:55:26.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009999762582019366 Training loss: 8.324020385742188
2025-12-09 11:55:26.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.009999731977631227 Training loss: 8.122705459594727
2025-12-09 11:55:26.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.00999969951849992 Training loss: 7.937477111816406
2025-12-09 11:55:26.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.009999665204637487 Training loss: 8.036410331726074
2025-12-09 11:55:26.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009999629036056657 Training loss: 8.261991500854492
2025-12-09 11:55:26.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.009999591012770847 Training loss: 7.872769355773926
2025-12-09 11:55:27.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.009999551134794164 Training loss: 7.931832313537598
2025-12-09 11:55:27.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0099995094021414 Training loss: 7.909985065460205
2025-12-09 11:55:27.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.009999465814828036 Training loss: 7.893203258514404
2025-12-09 11:55:27.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.009999420372870242 Training loss: 7.598515033721924
2025-12-09 11:55:27.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.009999373076284877 Training loss: 8.647554397583008
2025-12-09 11:55:27.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009999323925089485 Training loss: 7.917816162109375
2025-12-09 11:55:27.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009999272919302301 Training loss: 8.258798599243164
2025-12-09 11:55:27.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009999220058942245 Training loss: 8.160890579223633
2025-12-09 11:55:27.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009999165344028926 Training loss: 8.216623306274414
2025-12-09 11:55:27.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.009999108774582644 Training loss: 8.291814804077148
2025-12-09 11:55:27.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009999050350624381 Training loss: 8.099577903747559
2025-12-09 11:55:27.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.009998990072175813 Training loss: 7.747289657592773
2025-12-09 11:55:28.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009998927939259302 Training loss: 7.784843444824219
2025-12-09 11:55:28.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009998863951897896 Training loss: 7.8815789222717285
2025-12-09 11:55:28.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009998798110115333 Training loss: 7.906425952911377
2025-12-09 11:55:28.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.009998730413936037 Training loss: 7.728353023529053
2025-12-09 11:55:28.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.009998660863385123 Training loss: 7.761204719543457
2025-12-09 11:55:28.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00999858945848839 Training loss: 7.645893096923828
2025-12-09 11:55:28.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009998516199272327 Training loss: 7.629789352416992
2025-12-09 11:55:28.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009998441085764113 Training loss: 8.078169822692871
2025-12-09 11:55:28.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.009998364117991612 Training loss: 7.550654888153076
2025-12-09 11:55:28.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.009998285295983376 Training loss: 7.7021894454956055
2025-12-09 11:55:28.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009998204619768645 Training loss: 7.587409973144531
2025-12-09 11:55:28.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.00999812208937735 Training loss: 7.677079200744629
2025-12-09 11:55:28.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.009998037704840102 Training loss: 7.808053970336914
2025-12-09 11:55:29.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00999795146618821 Training loss: 7.7483673095703125
2025-12-09 11:55:29.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009997863373453663 Training loss: 8.357640266418457
2025-12-09 11:55:29.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.00999777342666914 Training loss: 7.653663158416748
2025-12-09 11:55:29.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009997681625868013 Training loss: 8.430500984191895
2025-12-09 11:55:29.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009997587971084335 Training loss: 8.395631790161133
2025-12-09 11:55:29.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009997492462352845 Training loss: 7.618196964263916
2025-12-09 11:55:29.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009997395099708982 Training loss: 8.309478759765625
2025-12-09 11:55:29.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.009997295883188855 Training loss: 7.910280227661133
2025-12-09 11:55:29.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009997194812829277 Training loss: 7.923718452453613
2025-12-09 11:55:29.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009997091888667738 Training loss: 7.7104811668396
2025-12-09 11:55:29.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009996987110742421 Training loss: 8.001173973083496
2025-12-09 11:55:29.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009996880479092198 Training loss: 7.736845970153809
2025-12-09 11:55:29.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.009996771993756622 Training loss: 7.69703483581543
2025-12-09 11:55:30.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.009996661654775939 Training loss: 7.741270542144775
2025-12-09 11:55:30.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.009996549462191081 Training loss: 7.505443572998047
2025-12-09 11:55:30.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00999643541604367 Training loss: 7.662580490112305
2025-12-09 11:55:30.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.00999631951637601 Training loss: 7.5269012451171875
2025-12-09 11:55:30.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.009996201763231098 Training loss: 7.623422622680664
2025-12-09 11:55:30.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.009996082156652618 Training loss: 7.7637481689453125
2025-12-09 11:55:30.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.009995960696684939 Training loss: 7.781864643096924
2025-12-09 11:55:30.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.009995837383373118 Training loss: 8.13175106048584
2025-12-09 11:55:30.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.009995712216762901 Training loss: 7.754611015319824
2025-12-09 11:55:30.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.009995585196900723 Training loss: 7.734020233154297
2025-12-09 11:55:30.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.009995456323833701 Training loss: 7.283416748046875
2025-12-09 11:55:30.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.009995325597609645 Training loss: 7.622228145599365
2025-12-09 11:55:30.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00999519301827705 Training loss: 8.358405113220215
2025-12-09 11:55:31.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.009995058585885095 Training loss: 7.877101421356201
2025-12-09 11:55:31.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.009994922300483657 Training loss: 7.5470967292785645
2025-12-09 11:55:31.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.00999478416212329 Training loss: 7.768487453460693
2025-12-09 11:55:31.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.009994644170855237 Training loss: 7.337655067443848
2025-12-09 11:55:31.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.009994502326731434 Training loss: 7.546895503997803
2025-12-09 11:55:31.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.009994358629804499 Training loss: 7.534764766693115
2025-12-09 11:55:31.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.009994213080127738 Training loss: 7.847391605377197
2025-12-09 11:55:31.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.009994065677755147 Training loss: 8.390033721923828
2025-12-09 11:55:31.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00999391642274141 Training loss: 7.531929016113281
2025-12-09 11:55:31.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00999376531514189 Training loss: 7.76187801361084
2025-12-09 11:55:31.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.009993612355012647 Training loss: 7.599546432495117
2025-12-09 11:55:31.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.009993457542410423 Training loss: 7.383874416351318
2025-12-09 11:55:32.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00999330087739265 Training loss: 7.452495574951172
2025-12-09 11:55:32.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.009993142360017445 Training loss: 7.498838901519775
2025-12-09 11:55:32.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.009992981990343614 Training loss: 7.474030494689941
2025-12-09 11:55:32.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.009992819768430647 Training loss: 8.18108081817627
2025-12-09 11:55:32.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.009992655694338725 Training loss: 7.547801494598389
2025-12-09 11:55:32.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.009992489768128714 Training loss: 7.43247652053833
2025-12-09 11:55:32.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.009992321989862165 Training loss: 7.8740386962890625
2025-12-09 11:55:32.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.009992152359601322 Training loss: 7.70432710647583
2025-12-09 11:55:32.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.00999198087740911 Training loss: 7.5174031257629395
2025-12-09 11:55:32.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.009991807543349147 Training loss: 7.1944804191589355
2025-12-09 11:55:32.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.009991632357485729 Training loss: 7.3987579345703125
2025-12-09 11:55:32.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.009991455319883848 Training loss: 7.473138809204102
2025-12-09 11:55:32.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.00999127643060918 Training loss: 7.763890266418457
2025-12-09 11:55:33.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.009991095689728087 Training loss: 7.2437591552734375
2025-12-09 11:55:33.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.009990913097307614 Training loss: 7.473475933074951
2025-12-09 11:55:33.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.009990728653415505 Training loss: 7.578348159790039
2025-12-09 11:55:33.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.009990542358120174 Training loss: 8.058544158935547
2025-12-09 11:55:33.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.009990354211490735 Training loss: 7.463674068450928
2025-12-09 11:55:33.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.009990164213596987 Training loss: 7.5904669761657715
2025-12-09 11:55:33.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.009989972364509407 Training loss: 6.4049201011657715
2025-12-09 11:55:33.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.009989778664299172 Training loss: 7.041526794433594
2025-12-09 11:55:33.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.009989583113038134 Training loss: 7.602558135986328
2025-12-09 11:55:33.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.009989385710798838 Training loss: 8.005163192749023
2025-12-09 11:55:33.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.009989186457654514 Training loss: 6.6269330978393555
2025-12-09 11:55:33.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.009988985353679076 Training loss: 7.5526909828186035
2025-12-09 11:55:33.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.009988782398947132 Training loss: 7.555957317352295
2025-12-09 11:55:34.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.009988577593533967 Training loss: 7.5728631019592285
2025-12-09 11:55:34.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00998837093751556 Training loss: 7.457029819488525
2025-12-09 11:55:34.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.009988162430968575 Training loss: 8.746326446533203
2025-12-09 11:55:34.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.009987952073970359 Training loss: 7.4252610206604
2025-12-09 11:55:34.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00998773986659895 Training loss: 7.692333221435547
2025-12-09 11:55:34.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.009987525808933069 Training loss: 7.2847981452941895
2025-12-09 11:55:34.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.009987309901052122 Training loss: 7.64419412612915
2025-12-09 11:55:34.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.009987092143036209 Training loss: 7.579955101013184
2025-12-09 11:55:34.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.009986872534966109 Training loss: 7.636409759521484
2025-12-09 11:55:34.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.009986651076923288 Training loss: 7.427708625793457
2025-12-09 11:55:34.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.009986427768989904 Training loss: 7.420014381408691
2025-12-09 11:55:34.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.009986202611248794 Training loss: 7.456103801727295
2025-12-09 11:55:35.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.009985975603783484 Training loss: 7.568571090698242
2025-12-09 11:55:35.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.00998574674667819 Training loss: 7.833445072174072
2025-12-09 11:55:35.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.009985516040017807 Training loss: 7.556896686553955
2025-12-09 11:55:35.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.009985283483887922 Training loss: 7.389524459838867
2025-12-09 11:55:35.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.009985049078374806 Training loss: 7.6837897300720215
2025-12-09 11:55:35.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.009984812823565416 Training loss: 7.83908224105835
2025-12-09 11:55:35.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.009984574719547395 Training loss: 7.602456092834473
2025-12-09 11:55:35.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.00998433476640907 Training loss: 7.46757173538208
2025-12-09 11:55:35.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00998409296423946 Training loss: 7.463108539581299
2025-12-09 11:55:35.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.009983849313128264 Training loss: 7.595710754394531
2025-12-09 11:55:35.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.009983603813165869 Training loss: 7.496494293212891
2025-12-09 11:55:35.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.009983356464443347 Training loss: 7.394896984100342
2025-12-09 11:55:35.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.009983107267052456 Training loss: 7.492064952850342
2025-12-09 11:55:36.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.009982856221085643 Training loss: 6.689952373504639
2025-12-09 11:55:36.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.009982603326636037 Training loss: 7.652336597442627
2025-12-09 11:55:36.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.009982348583797453 Training loss: 7.557606220245361
2025-12-09 11:55:36.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.009982091992664392 Training loss: 7.495641708374023
2025-12-09 11:55:36.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.009981833553332044 Training loss: 7.396645545959473
2025-12-09 11:55:36.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.009981573265896281 Training loss: 7.479889392852783
2025-12-09 11:55:36.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00998131113045366 Training loss: 7.270506858825684
2025-12-09 11:55:36.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.009981047147101425 Training loss: 7.421928882598877
2025-12-09 11:55:36.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.009980781315937506 Training loss: 7.201287269592285
2025-12-09 11:55:36.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.00998051363706052 Training loss: 7.344010829925537
2025-12-09 11:55:36.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.009980244110569764 Training loss: 7.33998966217041
2025-12-09 11:55:36.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.009979972736565226 Training loss: 7.1696367263793945
2025-12-09 11:55:36.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.009979699515147577 Training loss: 7.779980659484863
2025-12-09 11:55:37.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.009979424446418172 Training loss: 7.475347518920898
2025-12-09 11:55:37.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.009979147530479055 Training loss: 7.407486915588379
2025-12-09 11:55:37.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.009978868767432954 Training loss: 7.72120475769043
2025-12-09 11:55:37.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.009978588157383277 Training loss: 7.444549083709717
2025-12-09 11:55:37.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.009978305700434125 Training loss: 7.338507175445557
2025-12-09 11:55:37.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00997802139669028 Training loss: 7.762402534484863
2025-12-09 11:55:37.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.009977735246257209 Training loss: 7.154247283935547
2025-12-09 11:55:37.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.009977447249241066 Training loss: 7.3433613777160645
2025-12-09 11:55:37.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.009977157405748687 Training loss: 7.297767162322998
2025-12-09 11:55:37.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.009976865715887595 Training loss: 7.127902507781982
2025-12-09 11:55:37.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.009976572179765998 Training loss: 7.209758758544922
2025-12-09 11:55:37.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.009976276797492793 Training loss: 7.616316318511963
2025-12-09 11:55:38.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.009975979569177552 Training loss: 7.505348205566406
2025-12-09 11:55:38.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.009975680494930538 Training loss: 7.475014686584473
2025-12-09 11:55:38.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.0099753795748627 Training loss: 7.4884138107299805
2025-12-09 11:55:38.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.00997507680908567 Training loss: 7.407138824462891
2025-12-09 11:55:38.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.009974772197711762 Training loss: 7.440126895904541
2025-12-09 11:55:38.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.009974465740853979 Training loss: 7.482104301452637
2025-12-09 11:55:38.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.009974157438626008 Training loss: 7.358704090118408
2025-12-09 11:55:38.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.009973847291142217 Training loss: 7.1426849365234375
2025-12-09 11:55:38.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.009973535298517662 Training loss: 7.327328681945801
2025-12-09 11:55:38.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.009973221460868084 Training loss: 6.639787673950195
2025-12-09 11:55:38.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.009972905778309905 Training loss: 7.272516250610352
2025-12-09 11:55:38.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.009972588250960234 Training loss: 7.707494735717773
2025-12-09 11:55:38.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.009972268878936864 Training loss: 7.511810302734375
2025-12-09 11:55:39.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.009971947662358269 Training loss: 7.838357448577881
2025-12-09 11:55:39.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.009971624601343614 Training loss: 7.54288387298584
2025-12-09 11:55:39.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.009971299696012743 Training loss: 7.72575044631958
2025-12-09 11:55:39.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.009970972946486186 Training loss: 7.87484884262085
2025-12-09 11:55:39.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.009970644352885156 Training loss: 6.880894184112549
2025-12-09 11:55:39.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.009970313915331553 Training loss: 7.528165340423584
2025-12-09 11:55:39.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.009969981633947956 Training loss: 7.3944315910339355
2025-12-09 11:55:39.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.009969647508857631 Training loss: 7.36381196975708
2025-12-09 11:55:39.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.00996931154018453 Training loss: 7.966464996337891
2025-12-09 11:55:39.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.009968973728053289 Training loss: 7.6080756187438965
2025-12-09 11:55:39.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.009968634072589218 Training loss: 7.584969520568848
2025-12-09 11:55:39.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.009968292573918324 Training loss: 7.221831321716309
2025-12-09 11:55:40.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.009967949232167294 Training loss: 7.490912914276123
2025-12-09 11:55:40.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.009967604047463493 Training loss: 7.351847171783447
2025-12-09 11:55:40.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.009967257019934974 Training loss: 7.525744438171387
2025-12-09 11:55:40.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.009966908149710475 Training loss: 7.181379318237305
2025-12-09 11:55:40.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.009966557436919416 Training loss: 7.568030834197998
2025-12-09 11:55:40.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.009966204881691898 Training loss: 7.513513088226318
2025-12-09 11:55:40.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.00996585048415871 Training loss: 7.493380069732666
2025-12-09 11:55:40.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.009965494244451324 Training loss: 7.323841571807861
2025-12-09 11:55:40.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00996513616270189 Training loss: 7.934737205505371
2025-12-09 11:55:40.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.009964776239043245 Training loss: 8.333213806152344
2025-12-09 11:55:40.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.009964414473608912 Training loss: 7.0594682693481445
2025-12-09 11:55:40.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.009964050866533094 Training loss: 7.439661026000977
2025-12-09 11:55:40.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.009963685417950676 Training loss: 7.395275592803955
2025-12-09 11:55:41.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00996331812799723 Training loss: 6.863248348236084
2025-12-09 11:55:41.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.009962948996809008 Training loss: 7.290817737579346
2025-12-09 11:55:41.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.009962578024522948 Training loss: 7.259187698364258
2025-12-09 11:55:41.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.009962205211276665 Training loss: 7.681544780731201
2025-12-09 11:55:41.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.009961830557208463 Training loss: 7.005021572113037
2025-12-09 11:55:41.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.009961454062457329 Training loss: 7.414706707000732
2025-12-09 11:55:41.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.009961075727162927 Training loss: 7.098309516906738
2025-12-09 11:55:41.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.009960695551465611 Training loss: 7.2688469886779785
2025-12-09 11:55:41.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.009960313535506412 Training loss: 7.196478366851807
2025-12-09 11:55:41.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.009959929679427047 Training loss: 7.855969429016113
2025-12-09 11:55:41.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.009959543983369913 Training loss: 7.147617340087891
2025-12-09 11:55:41.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.009959156447478091 Training loss: 7.260180950164795
2025-12-09 11:55:41.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.009958767071895348 Training loss: 7.454639434814453
2025-12-09 11:55:42.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.009958375856766127 Training loss: 7.353964805603027
2025-12-09 11:55:42.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.009957982802235556 Training loss: 7.338326930999756
2025-12-09 11:55:42.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.009957587908449448 Training loss: 7.053083896636963
2025-12-09 11:55:42.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.009957191175554294 Training loss: 6.90659236907959
2025-12-09 11:55:42.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.009956792603697273 Training loss: 6.929520606994629
2025-12-09 11:55:42.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.009956392193026239 Training loss: 6.517426013946533
2025-12-09 11:55:42.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.009955989943689734 Training loss: 7.51852560043335
2025-12-09 11:55:42.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.009955585855836977 Training loss: 6.852766990661621
2025-12-09 11:55:42.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.009955179929617875 Training loss: 7.041172027587891
2025-12-09 11:55:42.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.009954772165183012 Training loss: 7.907735824584961
2025-12-09 11:55:42.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.009954362562683658 Training loss: 7.437585830688477
2025-12-09 11:55:42.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00995395112227176 Training loss: 7.336905002593994
2025-12-09 11:55:43.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00995353784409995 Training loss: 7.372098922729492
2025-12-09 11:55:43.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.009953122728321542 Training loss: 7.261094570159912
2025-12-09 11:55:43.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00995270577509053 Training loss: 7.616641998291016
2025-12-09 11:55:43.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.009952286984561591 Training loss: 7.307137489318848
2025-12-09 11:55:43.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.009951866356890084 Training loss: 7.526341438293457
2025-12-09 11:55:43.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.009951443892232048 Training loss: 7.2258124351501465
2025-12-09 11:55:43.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.009951019590744202 Training loss: 6.971292972564697
2025-12-09 11:55:43.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.009950593452583952 Training loss: 7.100450038909912
2025-12-09 11:55:43.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00995016547790938 Training loss: 7.082421779632568
2025-12-09 11:55:43.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.009949735666879251 Training loss: 7.009369373321533
2025-12-09 11:55:43.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.009949304019653011 Training loss: 6.999507904052734
2025-12-09 11:55:43.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00994887053639079 Training loss: 7.076481342315674
2025-12-09 11:55:43.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.009948435217253393 Training loss: 7.522009372711182
2025-12-09 11:55:44.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.009947998062402312 Training loss: 7.2952799797058105
2025-12-09 11:55:44.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.009947559071999719 Training loss: 7.22723913192749
2025-12-09 11:55:44.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.009947118246208461 Training loss: 7.1438984870910645
2025-12-09 11:55:44.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.009946675585192076 Training loss: 7.187896251678467
2025-12-09 11:55:44.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.009946231089114773 Training loss: 7.4475297927856445
2025-12-09 11:55:44.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.009945784758141448 Training loss: 8.069966316223145
2025-12-09 11:55:44.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.009945336592437678 Training loss: 7.634110450744629
2025-12-09 11:55:44.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.009944886592169713 Training loss: 7.4019060134887695
2025-12-09 11:55:44.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.009944434757504492 Training loss: 7.382606506347656
2025-12-09 11:55:44.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.009943981088609631 Training loss: 7.223486423492432
2025-12-09 11:55:44.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.009943525585653428 Training loss: 7.427748203277588
2025-12-09 11:55:44.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.009943068248804858 Training loss: 7.817005634307861
2025-12-09 11:55:44.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.009942609078233581 Training loss: 6.706047058105469
2025-12-09 11:55:45.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.009942148074109933 Training loss: 7.218790054321289
2025-12-09 11:55:45.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.009941685236604934 Training loss: 7.005025863647461
2025-12-09 11:55:45.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.009941220565890278 Training loss: 7.603968620300293
2025-12-09 11:55:45.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00994075406213835 Training loss: 7.257932186126709
2025-12-09 11:55:45.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.009940285725522203 Training loss: 7.105490207672119
2025-12-09 11:55:45.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.009939815556215575 Training loss: 7.388701915740967
2025-12-09 11:55:45.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.009939343554392886 Training loss: 7.561097621917725
2025-12-09 11:55:45.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.009938869720229233 Training loss: 7.284152030944824
2025-12-09 11:55:45.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.009938394053900394 Training loss: 7.435295104980469
2025-12-09 11:55:45.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.009937916555582828 Training loss: 7.398191452026367
2025-12-09 11:55:45.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.009937437225453668 Training loss: 7.412835597991943
2025-12-09 11:55:45.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.009936956063690734 Training loss: 7.280543327331543
2025-12-09 11:55:46.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.009936473070472518 Training loss: 6.316446781158447
2025-12-09 11:55:46.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.009935988245978198 Training loss: 5.538674354553223
2025-12-09 11:55:46.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.009935501590387627 Training loss: 7.224228858947754
2025-12-09 11:55:46.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.009935013103881342 Training loss: 7.023738384246826
2025-12-09 11:55:46.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.009934522786640554 Training loss: 7.04462194442749
2025-12-09 11:55:46.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.009934030638847154 Training loss: 7.432401657104492
2025-12-09 11:55:46.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.009933536660683716 Training loss: 7.1573896408081055
2025-12-09 11:55:46.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.009933040852333487 Training loss: 7.376941204071045
2025-12-09 11:55:46.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.009932543213980401 Training loss: 7.257607936859131
2025-12-09 11:55:46.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.009932043745809064 Training loss: 7.248316287994385
2025-12-09 11:55:46.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.009931542448004758 Training loss: 7.130856513977051
2025-12-09 11:55:46.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.009931039320753456 Training loss: 7.425638675689697
2025-12-09 11:55:46.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0099305343642418 Training loss: 7.321896553039551
2025-12-09 11:55:47.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.009930027578657113 Training loss: 6.742832660675049
2025-12-09 11:55:47.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.009929518964187393 Training loss: 7.097373008728027
2025-12-09 11:55:47.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.009929008521021325 Training loss: 7.064377784729004
2025-12-09 11:55:47.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.009928496249348266 Training loss: 7.32477331161499
2025-12-09 11:55:47.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.00992798214935825 Training loss: 7.29099178314209
2025-12-09 11:55:47.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.009927466221241995 Training loss: 7.404202938079834
2025-12-09 11:55:47.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.009926948465190892 Training loss: 7.490055561065674
2025-12-09 11:55:47.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.009926428881397015 Training loss: 7.692793369293213
2025-12-09 11:55:47.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.00992590747005311 Training loss: 6.8855061531066895
2025-12-09 11:55:47.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.009925384231352606 Training loss: 7.142307281494141
2025-12-09 11:55:47.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.009924859165489608 Training loss: 7.484642505645752
2025-12-09 11:55:47.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.009924332272658898 Training loss: 7.309149265289307
2025-12-09 11:55:47.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.009923803553055936 Training loss: 7.039489269256592
2025-12-09 11:55:48.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.009923273006876865 Training loss: 6.8761749267578125
2025-12-09 11:55:48.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.009922740634318495 Training loss: 7.318877220153809
2025-12-09 11:55:48.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.009922206435578323 Training loss: 7.196369171142578
2025-12-09 11:55:48.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.009921670410854518 Training loss: 7.1927971839904785
2025-12-09 11:55:48.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.009921132560345929 Training loss: 8.093311309814453
2025-12-09 11:55:48.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.009920592884252082 Training loss: 6.079619884490967
2025-12-09 11:55:48.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.009920051382773179 Training loss: 7.387023448944092
2025-12-09 11:55:48.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.009919508056110101 Training loss: 7.181493282318115
2025-12-09 11:55:48.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.009918962904464406 Training loss: 6.584753513336182
2025-12-09 11:55:48.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.009918415928038325 Training loss: 7.621324062347412
2025-12-09 11:55:48.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.009917867127034772 Training loss: 6.448436260223389
2025-12-09 11:55:48.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.009917316501657334 Training loss: 7.422311305999756
2025-12-09 11:55:49.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.009916764052110274 Training loss: 6.740147113800049
2025-12-09 11:55:49.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.009916209778598535 Training loss: 7.16002082824707
2025-12-09 11:55:49.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.009915653681327737 Training loss: 7.076687812805176
2025-12-09 11:55:49.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.009915095760504169 Training loss: 7.56663703918457
2025-12-09 11:55:49.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.009914536016334808 Training loss: 7.300405502319336
2025-12-09 11:55:49.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.009913974449027297 Training loss: 7.07456111907959
2025-12-09 11:55:49.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.009913411058789963 Training loss: 7.459012031555176
2025-12-09 11:55:49.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.009912845845831806 Training loss: 7.433396339416504
2025-12-09 11:55:49.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.009912278810362498 Training loss: 7.391078472137451
2025-12-09 11:55:49.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.009911709952592397 Training loss: 7.301466464996338
2025-12-09 11:55:49.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.009911139272732527 Training loss: 7.140738010406494
2025-12-09 11:55:49.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.009910566770994594 Training loss: 7.147168159484863
2025-12-09 11:55:49.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.00990999244759098 Training loss: 7.102423191070557
2025-12-09 11:55:50.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.009909416302734736 Training loss: 7.446854114532471
2025-12-09 11:55:50.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.009908838336639598 Training loss: 7.176449775695801
2025-12-09 11:55:50.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.00990825854951997 Training loss: 7.2059855461120605
2025-12-09 11:55:50.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.009907676941590938 Training loss: 7.129060745239258
2025-12-09 11:55:50.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.009907093513068259 Training loss: 7.3673553466796875
2025-12-09 11:55:50.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.009906508264168366 Training loss: 6.877706050872803
2025-12-09 11:55:50.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.009905921195108367 Training loss: 7.19063663482666
2025-12-09 11:55:50.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.00990533230610605 Training loss: 7.128118991851807
2025-12-09 11:55:50.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00990474159737987 Training loss: 7.211218357086182
2025-12-09 11:55:50.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.009904149069148962 Training loss: 7.742252826690674
2025-12-09 11:55:50.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.009903554721633139 Training loss: 7.25877571105957
2025-12-09 11:55:50.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.00990295855505288 Training loss: 7.667971611022949
2025-12-09 11:55:50.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.009902360569629348 Training loss: 7.386274337768555
2025-12-09 11:55:51.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.009901760765584376 Training loss: 7.091309070587158
2025-12-09 11:55:51.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.00990115914314047 Training loss: 7.73568868637085
2025-12-09 11:55:51.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.009900555702520816 Training loss: 7.9177632331848145
2025-12-09 11:55:51.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.00989995044394927 Training loss: 7.547481060028076
2025-12-09 11:55:51.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.009899343367650365 Training loss: 7.212563514709473
2025-12-09 11:55:51.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.009898734473849305 Training loss: 6.888923168182373
2025-12-09 11:55:51.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.00989812376277197 Training loss: 7.3718791007995605
2025-12-09 11:55:51.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00989751123464492 Training loss: 7.229673862457275
2025-12-09 11:55:51.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.009896896889695377 Training loss: 7.275705337524414
2025-12-09 11:55:51.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.009896280728151248 Training loss: 6.89022159576416
2025-12-09 11:55:51.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.009895662750241108 Training loss: 7.184666156768799
2025-12-09 11:55:51.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.009895042956194209 Training loss: 7.316910266876221
2025-12-09 11:55:52.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.009894421346240473 Training loss: 7.122453689575195
2025-12-09 11:55:52.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.009893797920610495 Training loss: 7.1011857986450195
2025-12-09 11:55:52.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.009893172679535553 Training loss: 7.576826572418213
2025-12-09 11:55:52.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.009892545623247586 Training loss: 7.201186180114746
2025-12-09 11:55:52.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.009891916751979217 Training loss: 6.924297332763672
2025-12-09 11:55:52.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.009891286065963734 Training loss: 7.502424240112305
2025-12-09 11:55:52.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0098906535654351 Training loss: 7.415647983551025
2025-12-09 11:55:52.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.00989001925062796 Training loss: 7.095068454742432
2025-12-09 11:55:52.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.009889383121777617 Training loss: 7.349898338317871
2025-12-09 11:55:52.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.00988874517912006 Training loss: 6.860998153686523
2025-12-09 11:55:52.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.009888105422891941 Training loss: 7.2267279624938965
2025-12-09 11:55:52.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.009887463853330595 Training loss: 7.5100603103637695
2025-12-09 11:55:52.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.009886820470674018 Training loss: 7.483155727386475
2025-12-09 11:55:53.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.00988617527516089 Training loss: 7.121877670288086
2025-12-09 11:55:53.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.009885528267030555 Training loss: 7.0699543952941895
2025-12-09 11:55:53.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.009884879446523035 Training loss: 7.506855010986328
2025-12-09 11:55:53.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00988422881387902 Training loss: 7.444037914276123
2025-12-09 11:55:53.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.009883576369339874 Training loss: 7.08614444732666
2025-12-09 11:55:53.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.009882922113147636 Training loss: 6.958566665649414
2025-12-09 11:55:53.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.009882266045545011 Training loss: 7.310863971710205
2025-12-09 11:55:53.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.009881608166775383 Training loss: 6.895522117614746
2025-12-09 11:55:53.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.009880948477082803 Training loss: 7.957093715667725
2025-12-09 11:55:53.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.009880286976711991 Training loss: 6.952081680297852
2025-12-09 11:55:53.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.00987962366590835 Training loss: 7.425051212310791
2025-12-09 11:55:53.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.009878958544917943 Training loss: 7.330679416656494
2025-12-09 11:55:54.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00987829161398751 Training loss: 6.972446918487549
2025-12-09 11:55:54.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00987762287336446 Training loss: 6.77932596206665
2025-12-09 11:55:54.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.009876952323296877 Training loss: 7.257870197296143
2025-12-09 11:55:54.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.009876279964033513 Training loss: 7.450436592102051
2025-12-09 11:55:54.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00987560579582379 Training loss: 6.995098114013672
2025-12-09 11:55:54.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.009874929818917806 Training loss: 7.075699806213379
2025-12-09 11:55:54.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.009874252033566327 Training loss: 7.014240741729736
2025-12-09 11:55:54.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.009873572440020791 Training loss: 7.252887725830078
2025-12-09 11:55:54.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.0098728910385333 Training loss: 7.462603569030762
2025-12-09 11:55:54.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.009872207829356642 Training loss: 7.457915782928467
2025-12-09 11:55:54.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.009871522812744256 Training loss: 6.9056267738342285
2025-12-09 11:55:54.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.009870835988950269 Training loss: 7.073171615600586
2025-12-09 11:55:54.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.009870147358229466 Training loss: 6.784204006195068
2025-12-09 11:55:55.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.009869456920837311 Training loss: 6.880091190338135
2025-12-09 11:55:55.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.009868764677029934 Training loss: 6.696523666381836
2025-12-09 11:55:55.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.009868070627064135 Training loss: 6.655722141265869
2025-12-09 11:55:55.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.009867374771197384 Training loss: 6.71453332901001
2025-12-09 11:55:55.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.009866677109687822 Training loss: 7.243475437164307
2025-12-09 11:55:55.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.009865977642794259 Training loss: 7.150756359100342
2025-12-09 11:55:55.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.009865276370776178 Training loss: 7.403965950012207
2025-12-09 11:55:55.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.009864573293893723 Training loss: 7.384838104248047
2025-12-09 11:55:55.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00986386841240772 Training loss: 7.1348958015441895
2025-12-09 11:55:55.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.009863161726579655 Training loss: 6.932092666625977
2025-12-09 11:55:55.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.009862453236671684 Training loss: 7.008739948272705
2025-12-09 11:55:55.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.009861742942946639 Training loss: 6.948450088500977
2025-12-09 11:55:55.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.009861030845668013 Training loss: 7.526383399963379
2025-12-09 11:55:56.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.009860316945099973 Training loss: 6.847995281219482
2025-12-09 11:55:56.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.009859601241507353 Training loss: 7.278702735900879
2025-12-09 11:55:56.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.009858883735155657 Training loss: 7.1369147300720215
2025-12-09 11:55:56.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.009858164426311058 Training loss: 6.309526443481445
2025-12-09 11:55:56.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.009857443315240397 Training loss: 6.9638566970825195
2025-12-09 11:55:56.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.00985672040221118 Training loss: 6.970827102661133
2025-12-09 11:55:56.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.00985599568749159 Training loss: 6.989009380340576
2025-12-09 11:55:56.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00985526917135047 Training loss: 7.352584362030029
2025-12-09 11:55:56.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.009854540854057337 Training loss: 6.9328227043151855
2025-12-09 11:55:56.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00985381073588237 Training loss: 7.270742893218994
2025-12-09 11:55:56.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.009853078817096423 Training loss: 7.246467113494873
2025-12-09 11:55:56.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.009852345097971017 Training loss: 6.669419288635254
2025-12-09 11:55:57.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.009851609578778332 Training loss: 6.909061908721924
2025-12-09 11:55:57.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.009850872259791229 Training loss: 6.624825477600098
2025-12-09 11:55:57.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.009850133141283225 Training loss: 7.049295425415039
2025-12-09 11:55:57.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.009849392223528514 Training loss: 6.941177845001221
2025-12-09 11:55:57.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00984864950680195 Training loss: 6.993532180786133
2025-12-09 11:55:57.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00984790499137906 Training loss: 7.293515205383301
2025-12-09 11:55:57.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.009847158677536034 Training loss: 7.214664936065674
2025-12-09 11:55:57.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00984641056554973 Training loss: 7.866571426391602
2025-12-09 11:55:57.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.009845660655697678 Training loss: 7.327062129974365
2025-12-09 11:55:57.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.009844908948258067 Training loss: 7.328981876373291
2025-12-09 11:55:57.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.009844155443509759 Training loss: 7.042409896850586
2025-12-09 11:55:57.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.009843400141732279 Training loss: 7.015265941619873
2025-12-09 11:55:57.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.009842643043205822 Training loss: 6.898802757263184
2025-12-09 11:55:58.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.009841884148211246 Training loss: 7.464487552642822
2025-12-09 11:55:58.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.009841123457030079 Training loss: 7.231400966644287
2025-12-09 11:55:58.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.009840360969944511 Training loss: 7.3331780433654785
2025-12-09 11:55:58.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.009839596687237401 Training loss: 7.020747661590576
2025-12-09 11:55:58.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.009838830609192278 Training loss: 6.348623275756836
2025-12-09 11:55:58.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.009838062736093327 Training loss: 7.226046562194824
2025-12-09 11:55:58.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.009837293068225408 Training loss: 7.493988037109375
2025-12-09 11:55:58.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.009836521605874044 Training loss: 7.294822692871094
2025-12-09 11:55:58.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.009835748349325421 Training loss: 6.863480567932129
2025-12-09 11:55:58.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.009834973298866394 Training loss: 7.248147010803223
2025-12-09 11:55:58.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.009834196454784484 Training loss: 7.423813343048096
2025-12-09 11:55:58.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.009833417817367874 Training loss: 7.108026027679443
2025-12-09 11:55:59.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.009832637386905412 Training loss: 7.03368616104126
2025-12-09 11:55:59.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.009831855163686617 Training loss: 6.968229293823242
2025-12-09 11:55:59.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.009831071148001667 Training loss: 6.509942531585693
2025-12-09 11:55:59.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.009830285340141407 Training loss: 7.395468711853027
2025-12-09 11:55:59.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.009829497740397349 Training loss: 6.174252510070801
2025-12-09 11:55:59.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.009828708349061663 Training loss: 7.301041126251221
2025-12-09 11:55:59.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.009827917166427195 Training loss: 8.660079956054688
2025-12-09 11:55:59.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.009827124192787444 Training loss: 7.381092548370361
2025-12-09 11:55:59.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.00982632942843658 Training loss: 7.53005838394165
2025-12-09 11:55:59.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.009825532873669433 Training loss: 7.217495918273926
2025-12-09 11:55:59.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.009824734528781505 Training loss: 7.232083320617676
2025-12-09 11:55:59.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.009823934394068952 Training loss: 7.371476650238037
2025-12-09 11:55:59.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.009823132469828601 Training loss: 7.252811908721924
2025-12-09 11:56:00.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.009822328756357942 Training loss: 7.0067009925842285
2025-12-09 11:56:00.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.009821523253955123 Training loss: 7.005199909210205
2025-12-09 11:56:00.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.009820715962918964 Training loss: 7.170434474945068
2025-12-09 11:56:00.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.009819906883548942 Training loss: 7.268289566040039
2025-12-09 11:56:00.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.009819096016145203 Training loss: 6.920798301696777
2025-12-09 11:56:00.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.00981828336100855 Training loss: 7.285030364990234
2025-12-09 11:56:00.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.009817468918440455 Training loss: 7.267150402069092
2025-12-09 11:56:00.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.009816652688743049 Training loss: 7.128125190734863
2025-12-09 11:56:00.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.009815834672219127 Training loss: 7.1963725090026855
2025-12-09 11:56:00.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00981501486917215 Training loss: 6.773182392120361
2025-12-09 11:56:00.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.009814193279906236 Training loss: 6.871586799621582
2025-12-09 11:56:00.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.00981336990472617 Training loss: 7.332981586456299
2025-12-09 11:56:00.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.0098125447439374 Training loss: 7.150868892669678
2025-12-09 11:56:01.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.009811717797846033 Training loss: 6.496454238891602
2025-12-09 11:56:01.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.00981088906675884 Training loss: 6.987274169921875
2025-12-09 11:56:01.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.009810058550983254 Training loss: 7.063376426696777
2025-12-09 11:56:01.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.009809226250827372 Training loss: 6.942118167877197
2025-12-09 11:56:01.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.009808392166599948 Training loss: 7.026293754577637
2025-12-09 11:56:01.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.009807556298610402 Training loss: 7.121580123901367
2025-12-09 11:56:01.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.009806718647168818 Training loss: 7.34888219833374
2025-12-09 11:56:01.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.009805879212585933 Training loss: 7.229014873504639
2025-12-09 11:56:01.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.009805037995173155 Training loss: 7.0859150886535645
2025-12-09 11:56:01.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.009804194995242549 Training loss: 6.713641166687012
2025-12-09 11:56:01.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.009803350213106837 Training loss: 6.762513160705566
2025-12-09 11:56:01.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.00980250364907941 Training loss: 7.578720569610596
2025-12-09 11:56:02.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.009801655303474318 Training loss: 7.411484718322754
2025-12-09 11:56:02.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.009800805176606269 Training loss: 7.395954132080078
2025-12-09 11:56:02.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.009799953268790632 Training loss: 7.332022190093994
2025-12-09 11:56:02.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.00979909958034344 Training loss: 6.975959777832031
2025-12-09 11:56:02.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.009798244111581382 Training loss: 6.813775539398193
2025-12-09 11:56:02.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.009797386862821814 Training loss: 7.016226291656494
2025-12-09 11:56:02.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.009796527834382745 Training loss: 7.335012435913086
2025-12-09 11:56:02.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.009795667026582846 Training loss: 6.909502983093262
2025-12-09 11:56:02.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.009794804439741454 Training loss: 7.027585029602051
2025-12-09 11:56:02.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.00979394007417856 Training loss: 7.693682670593262
2025-12-09 11:56:02.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.009793073930214816 Training loss: 6.9458699226379395
2025-12-09 11:56:02.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.009792206008171534 Training loss: 7.137169361114502
2025-12-09 11:56:02.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.009791336308370686 Training loss: 7.245114326477051
2025-12-09 11:56:03.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.009790464831134903 Training loss: 7.050073146820068
2025-12-09 11:56:03.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.009789591576787476 Training loss: 7.081058979034424
2025-12-09 11:56:03.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.009788716545652353 Training loss: 7.2676005363464355
2025-12-09 11:56:03.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.009787839738054147 Training loss: 7.00048828125
2025-12-09 11:56:03.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.009786961154318121 Training loss: 6.978905200958252
2025-12-09 11:56:03.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.009786080794770207 Training loss: 7.14905309677124
2025-12-09 11:56:03.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.009785198659736987 Training loss: 7.4001264572143555
2025-12-09 11:56:03.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.009784314749545706 Training loss: 6.86639928817749
2025-12-09 11:56:03.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00978342906452427 Training loss: 7.276663780212402
2025-12-09 11:56:03.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.009782541605001235 Training loss: 7.112419128417969
2025-12-09 11:56:03.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.009781652371305825 Training loss: 7.208958148956299
2025-12-09 11:56:03.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.009780761363767914 Training loss: 8.22702693939209
2025-12-09 11:56:04.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.009779868582718041 Training loss: 6.9339375495910645
2025-12-09 11:56:04.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.009778974028487398 Training loss: 7.501800060272217
2025-12-09 11:56:04.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.009778077701407838 Training loss: 7.175399303436279
2025-12-09 11:56:04.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.009777179601811866 Training loss: 7.14601993560791
2025-12-09 11:56:04.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.009776279730032653 Training loss: 7.3351545333862305
2025-12-09 11:56:04.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.009775378086404022 Training loss: 7.196518898010254
2025-12-09 11:56:04.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.009774474671260455 Training loss: 7.312355995178223
2025-12-09 11:56:04.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.00977356948493709 Training loss: 7.091278553009033
2025-12-09 11:56:04.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.00977266252776972 Training loss: 7.043217182159424
2025-12-09 11:56:04.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.009771753800094802 Training loss: 7.328257083892822
2025-12-09 11:56:04.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.009770843302249442 Training loss: 6.895500659942627
2025-12-09 11:56:04.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.009769931034571407 Training loss: 7.333732604980469
2025-12-09 11:56:04.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.00976901699739912 Training loss: 7.285698890686035
2025-12-09 11:56:05.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.009768101191071661 Training loss: 7.211524486541748
2025-12-09 11:56:05.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.009767183615928763 Training loss: 7.861427307128906
2025-12-09 11:56:05.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.009766264272310822 Training loss: 6.999772548675537
2025-12-09 11:56:05.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.009765343160558878 Training loss: 7.924746036529541
2025-12-09 11:56:05.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.009764420281014641 Training loss: 7.123581886291504
2025-12-09 11:56:05.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.009763495634020467 Training loss: 7.027801036834717
2025-12-09 11:56:05.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.009762569219919371 Training loss: 6.964165210723877
2025-12-09 11:56:05.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.009761641039055026 Training loss: 7.700946807861328
2025-12-09 11:56:05.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.009760711091771755 Training loss: 7.197073459625244
2025-12-09 11:56:05.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.009759779378414542 Training loss: 6.9793877601623535
2025-12-09 11:56:05.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.009758845899329021 Training loss: 7.17614221572876
2025-12-09 11:56:05.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.009757910654861483 Training loss: 7.229683876037598
2025-12-09 11:56:05.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.009756973645358876 Training loss: 7.1698317527771
2025-12-09 11:56:06.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.009756034871168799 Training loss: 6.969210147857666
2025-12-09 11:56:06.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.009755094332639512 Training loss: 7.2860589027404785
2025-12-09 11:56:06.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00975415203011992 Training loss: 7.205755233764648
2025-12-09 11:56:06.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.00975320796395959 Training loss: 7.3187360763549805
2025-12-09 11:56:06.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.009752262134508742 Training loss: 7.116626262664795
2025-12-09 11:56:06.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.009751314542118247 Training loss: 7.409585952758789
2025-12-09 11:56:06.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.009750365187139632 Training loss: 7.205497741699219
2025-12-09 11:56:06.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.009749414069925078 Training loss: 6.966934680938721
2025-12-09 11:56:06.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.00974846119082742 Training loss: 7.120182991027832
2025-12-09 11:56:06.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.009747506550200145 Training loss: 6.7997283935546875
2025-12-09 11:56:06.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.009746550148397396 Training loss: 7.234615802764893
2025-12-09 11:56:06.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.00974559198577397 Training loss: 6.952595233917236
2025-12-09 11:56:07.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.009744632062685311 Training loss: 6.686868667602539
2025-12-09 11:56:07.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.009743670379487522 Training loss: 6.975579261779785
2025-12-09 11:56:07.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.009742706936537358 Training loss: 6.965710163116455
2025-12-09 11:56:07.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.009741741734192224 Training loss: 7.891019821166992
2025-12-09 11:56:07.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.009740774772810181 Training loss: 7.502557277679443
2025-12-09 11:56:07.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.009739806052749942 Training loss: 7.120981693267822
2025-12-09 11:56:07.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.009738835574370872 Training loss: 7.251792907714844
2025-12-09 11:56:07.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.009737863338032985 Training loss: 7.112079620361328
2025-12-09 11:56:07.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.009736889344096951 Training loss: 6.772538185119629
2025-12-09 11:56:07.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.009735913592924092 Training loss: 6.816474437713623
2025-12-09 11:56:07.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.009734936084876382 Training loss: 7.0142130851745605
2025-12-09 11:56:07.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.009733956820316443 Training loss: 7.03582239151001
2025-12-09 11:56:07.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.009732975799607553 Training loss: 7.946193695068359
2025-12-09 11:56:08.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.00973199302311364 Training loss: 6.737399578094482
2025-12-09 11:56:08.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.009731008491199285 Training loss: 7.00885009765625
2025-12-09 11:56:08.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.009730022204229714 Training loss: 7.170485973358154
2025-12-09 11:56:08.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.009729034162570812 Training loss: 7.247425556182861
2025-12-09 11:56:08.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.009728044366589108 Training loss: 7.153532981872559
2025-12-09 11:56:08.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.009727052816651788 Training loss: 7.150241374969482
2025-12-09 11:56:08.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.009726059513126686 Training loss: 7.003635883331299
2025-12-09 11:56:08.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.009725064456382283 Training loss: 7.458043098449707
2025-12-09 11:56:08.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.009724067646787717 Training loss: 7.35405969619751
2025-12-09 11:56:08.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.00972306908471277 Training loss: 7.0142340660095215
2025-12-09 11:56:08.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.009722068770527881 Training loss: 7.168453693389893
2025-12-09 11:56:08.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.009721066704604134 Training loss: 7.077658176422119
2025-12-09 11:56:09.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.00972006288731326 Training loss: 7.24127197265625
2025-12-09 11:56:09.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.00971905731902765 Training loss: 6.82778787612915
2025-12-09 11:56:09.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.009718050000120333 Training loss: 7.277169227600098
2025-12-09 11:56:09.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.009717040930964996 Training loss: 7.243133068084717
2025-12-09 11:56:09.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.009716030111935968 Training loss: 6.971852779388428
2025-12-09 11:56:09.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.009715017543408233 Training loss: 7.422744274139404
2025-12-09 11:56:09.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.009714003225757424 Training loss: 7.000168323516846
2025-12-09 11:56:09.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.009712987159359818 Training loss: 6.811604976654053
2025-12-09 11:56:09.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.009711969344592347 Training loss: 7.1781415939331055
2025-12-09 11:56:09.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.009710949781832585 Training loss: 7.062676906585693
2025-12-09 11:56:09.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.009709928471458759 Training loss: 7.143895149230957
2025-12-09 11:56:09.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.009708905413849743 Training loss: 5.833603858947754
2025-12-09 11:56:09.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.009707880609385058 Training loss: 6.667603015899658
2025-12-09 11:56:10.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.009706854058444877 Training loss: 7.0289740562438965
2025-12-09 11:56:10.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.009705825761410014 Training loss: 7.244795799255371
2025-12-09 11:56:10.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.009704795718661938 Training loss: 7.071484088897705
2025-12-09 11:56:10.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.00970376393058276 Training loss: 7.338198661804199
2025-12-09 11:56:10.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.009702730397555245 Training loss: 7.124305248260498
2025-12-09 11:56:10.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.009701695119962798 Training loss: 7.349289417266846
2025-12-09 11:56:10.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.009700658098189475 Training loss: 7.263746738433838
2025-12-09 11:56:10.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.009699619332619978 Training loss: 7.195834636688232
2025-12-09 11:56:10.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.009698578823639658 Training loss: 7.1403703689575195
2025-12-09 11:56:10.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.009697536571634508 Training loss: 6.811283111572266
2025-12-09 11:56:10.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.009696492576991175 Training loss: 6.589407444000244
2025-12-09 11:56:10.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.009695446840096945 Training loss: 7.294337749481201
2025-12-09 11:56:10.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.009694399361339753 Training loss: 7.225009441375732
2025-12-09 11:56:11.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.009693350141108182 Training loss: 7.108267307281494
2025-12-09 11:56:11.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00969229917979146 Training loss: 7.345263481140137
2025-12-09 11:56:11.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.009691246477779459 Training loss: 7.612823009490967
2025-12-09 11:56:11.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0096901920354627 Training loss: 7.088704586029053
2025-12-09 11:56:11.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.009689135853232349 Training loss: 7.222228527069092
2025-12-09 11:56:11.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.009688077931480212 Training loss: 7.167717933654785
2025-12-09 11:56:11.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.009687018270598749 Training loss: 6.818683624267578
2025-12-09 11:56:11.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.009685956870981059 Training loss: 7.232278823852539
2025-12-09 11:56:11.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.009684893733020887 Training loss: 7.154414176940918
2025-12-09 11:56:11.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.009683828857112626 Training loss: 6.596502780914307
2025-12-09 11:56:11.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.009682762243651308 Training loss: 6.161585330963135
2025-12-09 11:56:11.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.009681693893032617 Training loss: 6.727372646331787
2025-12-09 11:56:12.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.009680623805652875 Training loss: 7.043707370758057
2025-12-09 11:56:12.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.009679551981909052 Training loss: 7.054770469665527
2025-12-09 11:56:12.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.00967847842219876 Training loss: 7.484165668487549
2025-12-09 11:56:12.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.009677403126920255 Training loss: 7.03688907623291
2025-12-09 11:56:12.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.00967632609647244 Training loss: 7.07042932510376
2025-12-09 11:56:12.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.009675247331254857 Training loss: 7.258984088897705
2025-12-09 11:56:12.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.009674166831667696 Training loss: 7.2662506103515625
2025-12-09 11:56:12.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.009673084598111788 Training loss: 6.933848857879639
2025-12-09 11:56:12.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.009672000630988605 Training loss: 6.736988544464111
2025-12-09 11:56:12.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.009670914930700268 Training loss: 6.871099472045898
2025-12-09 11:56:12.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.009669827497649537 Training loss: 7.255423545837402
2025-12-09 11:56:12.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.009668738332239813 Training loss: 7.132386684417725
2025-12-09 11:56:12.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.009667647434875144 Training loss: 7.45217227935791
2025-12-09 11:56:13.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.00966655480596022 Training loss: 6.95609188079834
2025-12-09 11:56:13.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.009665460445900368 Training loss: 6.481606483459473
2025-12-09 11:56:13.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.009664364355101564 Training loss: 7.234328269958496
2025-12-09 11:56:13.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.009663266533970424 Training loss: 7.062373638153076
2025-12-09 11:56:13.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.009662166982914203 Training loss: 7.104862689971924
2025-12-09 11:56:13.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.0096610657023408 Training loss: 7.610682010650635
2025-12-09 11:56:13.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.009659962692658756 Training loss: 7.147070407867432
2025-12-09 11:56:13.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.009658857954277254 Training loss: 7.078472137451172
2025-12-09 11:56:13.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.009657751487606114 Training loss: 6.751798629760742
2025-12-09 11:56:13.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.009656643293055805 Training loss: 8.017505645751953
2025-12-09 11:56:13.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.009655533371037426 Training loss: 6.932236671447754
2025-12-09 11:56:13.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.009654421721962729 Training loss: 7.140132427215576
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 90.89 GiB memory in use. Of the allocated memory 89.10 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:08, 145.68it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 175.42it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:21, 122.21it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:10, 141.41it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:57, 173.57it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 214.06it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:53, 185.67it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 184.67it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 197.70it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 202.76it/s]Tokenizing texts:   2%|▏         | 241/10000 [00:01<00:47, 205.44it/s]Tokenizing texts:   3%|▎         | 263/10000 [00:01<00:47, 204.66it/s]Tokenizing texts:   3%|▎         | 292/10000 [00:01<00:42, 228.35it/s]Tokenizing texts:   3%|▎         | 316/10000 [00:01<00:42, 228.49it/s]Tokenizing texts:   3%|▎         | 340/10000 [00:01<00:52, 183.57it/s]Tokenizing texts:   4%|▎         | 364/10000 [00:01<00:48, 197.25it/s]Tokenizing texts:   4%|▍         | 386/10000 [00:02<00:52, 183.93it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 186.44it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:51, 185.83it/s]Tokenizing texts:   4%|▍         | 446/10000 [00:02<00:50, 187.43it/s]Tokenizing texts:   5%|▍         | 468/10000 [00:02<00:48, 195.52it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 218.12it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:39, 237.17it/s]Tokenizing texts:   6%|▌         | 551/10000 [00:02<00:50, 187.27it/s]Tokenizing texts:   6%|▌         | 581/10000 [00:02<00:44, 211.31it/s]Tokenizing texts:   6%|▌         | 604/10000 [00:03<00:44, 213.20it/s]Tokenizing texts:   6%|▋         | 627/10000 [00:03<00:46, 199.90it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 202.13it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 199.74it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 191.22it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 200.67it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 200.06it/s]Tokenizing texts:   8%|▊         | 766/10000 [00:03<00:43, 210.92it/s]Tokenizing texts:   8%|▊         | 788/10000 [00:04<00:45, 203.02it/s]Tokenizing texts:   8%|▊         | 813/10000 [00:04<00:43, 212.78it/s]Tokenizing texts:   8%|▊         | 835/10000 [00:04<00:42, 213.29it/s]Tokenizing texts:   9%|▊         | 861/10000 [00:04<00:40, 226.40it/s]Tokenizing texts:   9%|▉         | 884/10000 [00:04<00:41, 218.45it/s]Tokenizing texts:   9%|▉         | 910/10000 [00:04<00:39, 228.71it/s]Tokenizing texts:   9%|▉         | 940/10000 [00:04<00:36, 248.51it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:04<00:33, 267.74it/s]Tokenizing texts:  10%|█         | 1001/10000 [00:04<00:33, 271.67it/s]Tokenizing texts:  10%|█         | 1032/10000 [00:04<00:31, 282.37it/s]Tokenizing texts:  11%|█         | 1061/10000 [00:05<00:39, 224.98it/s]Tokenizing texts:  11%|█         | 1086/10000 [00:05<00:39, 225.88it/s]Tokenizing texts:  11%|█         | 1110/10000 [00:05<00:38, 228.42it/s]Tokenizing texts:  11%|█▏        | 1134/10000 [00:05<00:45, 194.27it/s]Tokenizing texts:  12%|█▏        | 1159/10000 [00:05<00:43, 205.36it/s]Tokenizing texts:  12%|█▏        | 1184/10000 [00:05<00:40, 216.02it/s]Tokenizing texts:  12%|█▏        | 1207/10000 [00:05<00:43, 203.44it/s]Tokenizing texts:  12%|█▏        | 1235/10000 [00:05<00:41, 211.66it/s]Tokenizing texts:  13%|█▎        | 1257/10000 [00:06<00:42, 205.76it/s]Tokenizing texts:  13%|█▎        | 1292/10000 [00:06<00:35, 242.03it/s]Tokenizing texts:  13%|█▎        | 1317/10000 [00:06<00:37, 230.75it/s]Tokenizing texts:  13%|█▎        | 1341/10000 [00:06<00:41, 206.41it/s]Tokenizing texts:  14%|█▎        | 1368/10000 [00:06<00:39, 221.13it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:39, 216.31it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 233.96it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:34, 251.21it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:07<00:31, 274.18it/s]Tokenizing texts:  15%|█▌        | 1517/10000 [00:07<00:31, 268.49it/s]Tokenizing texts:  15%|█▌        | 1545/10000 [00:07<00:33, 252.75it/s]Tokenizing texts:  16%|█▌        | 1577/10000 [00:07<00:31, 270.63it/s]Tokenizing texts:  16%|█▌        | 1605/10000 [00:07<00:33, 251.78it/s]Tokenizing texts:  16%|█▋        | 1631/10000 [00:07<00:40, 208.10it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 215.41it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 216.99it/s]Tokenizing texts:  17%|█▋        | 1704/10000 [00:07<00:37, 222.71it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:33, 244.69it/s]Tokenizing texts:  18%|█▊        | 1761/10000 [00:08<00:33, 245.15it/s]Tokenizing texts:  18%|█▊        | 1786/10000 [00:08<00:33, 246.05it/s]Tokenizing texts:  18%|█▊        | 1820/10000 [00:08<00:30, 272.43it/s]Tokenizing texts:  18%|█▊        | 1848/10000 [00:08<00:37, 217.83it/s]Tokenizing texts:  19%|█▊        | 1872/10000 [00:08<00:36, 220.63it/s]Tokenizing texts:  19%|█▉        | 1896/10000 [00:08<00:35, 225.12it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:08<00:35, 225.78it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 195.84it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:37, 211.37it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:34, 228.66it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 234.04it/s]Tokenizing texts:  21%|██        | 2055/10000 [00:09<00:32, 247.85it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:37, 208.74it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:37, 209.27it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:36, 214.25it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:09<00:38, 205.05it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:35, 222.75it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:31, 250.83it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:29, 260.79it/s]Tokenizing texts:  23%|██▎       | 2274/10000 [00:10<00:28, 271.51it/s]Tokenizing texts:  23%|██▎       | 2302/10000 [00:10<00:29, 261.42it/s]Tokenizing texts:  23%|██▎       | 2329/10000 [00:10<00:36, 210.78it/s]Tokenizing texts:  24%|██▎       | 2364/10000 [00:10<00:31, 244.15it/s]Tokenizing texts:  24%|██▍       | 2399/10000 [00:10<00:28, 267.64it/s]Tokenizing texts:  24%|██▍       | 2428/10000 [00:11<00:30, 248.27it/s]Tokenizing texts:  25%|██▍       | 2455/10000 [00:11<00:29, 251.70it/s]Tokenizing texts:  25%|██▍       | 2482/10000 [00:11<00:30, 246.57it/s]Tokenizing texts:  25%|██▌       | 2508/10000 [00:11<00:30, 245.54it/s]Tokenizing texts:  25%|██▌       | 2538/10000 [00:11<00:28, 259.00it/s]Tokenizing texts:  26%|██▌       | 2565/10000 [00:11<00:33, 222.83it/s]Tokenizing texts:  26%|██▌       | 2589/10000 [00:11<00:33, 221.81it/s]Tokenizing texts:  26%|██▌       | 2612/10000 [00:11<00:36, 203.83it/s]Tokenizing texts:  26%|██▋       | 2640/10000 [00:11<00:32, 223.14it/s]Tokenizing texts:  27%|██▋       | 2667/10000 [00:12<00:31, 230.97it/s]Tokenizing texts:  27%|██▋       | 2691/10000 [00:12<00:32, 223.20it/s]Tokenizing texts:  27%|██▋       | 2714/10000 [00:12<00:35, 202.84it/s]Tokenizing texts:  27%|██▋       | 2749/10000 [00:12<00:30, 239.68it/s]Tokenizing texts:  28%|██▊       | 2776/10000 [00:12<00:29, 243.44it/s]Tokenizing texts:  28%|██▊       | 2802/10000 [00:12<00:39, 180.72it/s]Tokenizing texts:  28%|██▊       | 2838/10000 [00:12<00:32, 218.71it/s]Tokenizing texts:  29%|██▊       | 2863/10000 [00:13<00:35, 201.70it/s]Tokenizing texts:  29%|██▉       | 2891/10000 [00:13<00:32, 219.69it/s]Tokenizing texts:  29%|██▉       | 2923/10000 [00:13<00:28, 244.40it/s]Tokenizing texts:  30%|██▉       | 2950/10000 [00:13<00:31, 224.15it/s]Tokenizing texts:  30%|██▉       | 2975/10000 [00:13<00:30, 226.95it/s]Tokenizing texts:  30%|███       | 3012/10000 [00:13<00:26, 264.13it/s]Tokenizing texts:  30%|███       | 3040/10000 [00:13<00:30, 226.77it/s]Tokenizing texts:  31%|███       | 3074/10000 [00:13<00:27, 252.54it/s]Tokenizing texts:  31%|███       | 3101/10000 [00:13<00:28, 241.96it/s]Tokenizing texts:  31%|███▏      | 3135/10000 [00:14<00:25, 266.09it/s]Tokenizing texts:  32%|███▏      | 3163/10000 [00:14<00:27, 252.84it/s]Tokenizing texts:  32%|███▏      | 3190/10000 [00:14<00:26, 256.01it/s]Tokenizing texts:  32%|███▏      | 3219/10000 [00:14<00:25, 265.19it/s]Tokenizing texts:  32%|███▏      | 3247/10000 [00:14<00:26, 257.08it/s]Tokenizing texts:  33%|███▎      | 3278/10000 [00:14<00:25, 267.98it/s]Tokenizing texts:  33%|███▎      | 3306/10000 [00:14<00:25, 260.68it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:14<00:26, 248.37it/s]Tokenizing texts:  34%|███▎      | 3361/10000 [00:14<00:25, 256.98it/s]Tokenizing texts:  34%|███▍      | 3387/10000 [00:15<00:26, 252.64it/s]Tokenizing texts:  34%|███▍      | 3413/10000 [00:15<00:25, 254.58it/s]Tokenizing texts:  35%|███▍      | 3453/10000 [00:15<00:22, 294.12it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:15<00:22, 289.50it/s]Tokenizing texts:  35%|███▌      | 3517/10000 [00:15<00:26, 245.68it/s]Tokenizing texts:  35%|███▌      | 3543/10000 [00:15<00:27, 233.20it/s]Tokenizing texts:  36%|███▌      | 3570/10000 [00:15<00:26, 241.78it/s]Tokenizing texts:  36%|███▌      | 3600/10000 [00:15<00:25, 253.02it/s]Tokenizing texts:  36%|███▋      | 3626/10000 [00:16<00:25, 252.22it/s]Tokenizing texts:  37%|███▋      | 3652/10000 [00:16<00:25, 250.80it/s]Tokenizing texts:  37%|███▋      | 3678/10000 [00:16<00:25, 247.20it/s]Tokenizing texts:  37%|███▋      | 3705/10000 [00:16<00:25, 251.72it/s]Tokenizing texts:  37%|███▋      | 3732/10000 [00:16<00:24, 255.90it/s]Tokenizing texts:  38%|███▊      | 3758/10000 [00:16<00:27, 224.82it/s]Tokenizing texts:  38%|███▊      | 3784/10000 [00:16<00:26, 233.05it/s]Tokenizing texts:  38%|███▊      | 3808/10000 [00:16<00:30, 205.54it/s]Tokenizing texts:  38%|███▊      | 3840/10000 [00:16<00:26, 233.93it/s]Tokenizing texts:  39%|███▊      | 3867/10000 [00:17<00:25, 242.56it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:17<00:24, 251.01it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:25, 234.71it/s]Tokenizing texts:  39%|███▉      | 3946/10000 [00:17<00:26, 229.82it/s]Tokenizing texts:  40%|███▉      | 3979/10000 [00:17<00:23, 256.70it/s]Tokenizing texts:  40%|████      | 4006/10000 [00:17<00:27, 219.38it/s]Tokenizing texts:  40%|████      | 4037/10000 [00:17<00:25, 237.51it/s]Tokenizing texts:  41%|████      | 4067/10000 [00:17<00:23, 253.15it/s]Tokenizing texts:  41%|████      | 4099/10000 [00:17<00:21, 271.03it/s]Tokenizing texts:  41%|████▏     | 4132/10000 [00:18<00:20, 286.92it/s]Tokenizing texts:  42%|████▏     | 4162/10000 [00:18<00:21, 272.17it/s]Tokenizing texts:  42%|████▏     | 4195/10000 [00:18<00:20, 285.86it/s]Tokenizing texts:  42%|████▏     | 4225/10000 [00:18<00:22, 260.43it/s]Tokenizing texts:  43%|████▎     | 4252/10000 [00:18<00:23, 245.96it/s]Tokenizing texts:  43%|████▎     | 4278/10000 [00:18<00:24, 235.98it/s]Tokenizing texts:  43%|████▎     | 4310/10000 [00:18<00:22, 257.24it/s]Tokenizing texts:  43%|████▎     | 4345/10000 [00:18<00:20, 279.84it/s]Tokenizing texts:  44%|████▎     | 4374/10000 [00:19<00:21, 263.20it/s]Tokenizing texts:  44%|████▍     | 4412/10000 [00:19<00:19, 292.00it/s]Tokenizing texts:  44%|████▍     | 4442/10000 [00:19<00:19, 280.59it/s]Tokenizing texts:  45%|████▍     | 4471/10000 [00:19<00:21, 257.47it/s]Tokenizing texts:  45%|████▍     | 4498/10000 [00:19<00:22, 241.76it/s]Tokenizing texts:  45%|████▌     | 4525/10000 [00:19<00:22, 248.61it/s]Tokenizing texts:  46%|████▌     | 4552/10000 [00:19<00:21, 254.10it/s]Tokenizing texts:  46%|████▌     | 4578/10000 [00:19<00:21, 251.42it/s]Tokenizing texts:  46%|████▌     | 4604/10000 [00:19<00:22, 239.82it/s]Tokenizing texts:  46%|████▋     | 4637/10000 [00:20<00:20, 263.49it/s]Tokenizing texts:  47%|████▋     | 4671/10000 [00:20<00:18, 283.65it/s]Tokenizing texts:  47%|████▋     | 4708/10000 [00:20<00:17, 305.86it/s]Tokenizing texts:  47%|████▋     | 4739/10000 [00:20<00:17, 293.44it/s]Tokenizing texts:  48%|████▊     | 4769/10000 [00:20<00:18, 282.90it/s]Tokenizing texts:  48%|████▊     | 4798/10000 [00:20<00:21, 245.53it/s]Tokenizing texts:  48%|████▊     | 4830/10000 [00:20<00:19, 261.96it/s]Tokenizing texts:  49%|████▊     | 4859/10000 [00:20<00:19, 267.44it/s]Tokenizing texts:  49%|████▉     | 4887/10000 [00:20<00:19, 269.02it/s]Tokenizing texts:  49%|████▉     | 4916/10000 [00:21<00:18, 273.46it/s]Tokenizing texts:  49%|████▉     | 4944/10000 [00:21<00:18, 272.29it/s]Tokenizing texts:  50%|████▉     | 4972/10000 [00:21<00:19, 262.74it/s]Tokenizing texts:  50%|█████     | 5011/10000 [00:21<00:17, 283.95it/s]Tokenizing texts:  50%|█████     | 5040/10000 [00:21<00:17, 281.22it/s]Tokenizing texts:  51%|█████     | 5069/10000 [00:21<00:19, 247.18it/s]Tokenizing texts:  51%|█████     | 5095/10000 [00:21<00:20, 243.74it/s]Tokenizing texts:  51%|█████     | 5122/10000 [00:21<00:20, 234.78it/s]Tokenizing texts:  52%|█████▏    | 5150/10000 [00:21<00:19, 244.86it/s]Tokenizing texts:  52%|█████▏    | 5178/10000 [00:22<00:18, 253.97it/s]Tokenizing texts:  52%|█████▏    | 5204/10000 [00:22<00:20, 238.87it/s]Tokenizing texts:  52%|█████▏    | 5231/10000 [00:22<00:19, 242.69it/s]Tokenizing texts:  53%|█████▎    | 5265/10000 [00:22<00:17, 266.43it/s]Tokenizing texts:  53%|█████▎    | 5292/10000 [00:22<00:18, 253.75it/s]Tokenizing texts:  53%|█████▎    | 5327/10000 [00:22<00:16, 279.77it/s]Tokenizing texts:  54%|█████▎    | 5359/10000 [00:22<00:15, 290.51it/s]Tokenizing texts:  54%|█████▍    | 5390/10000 [00:22<00:15, 294.33it/s]Tokenizing texts:  54%|█████▍    | 5423/10000 [00:22<00:15, 302.44it/s]Tokenizing texts:  55%|█████▍    | 5454/10000 [00:23<00:16, 272.73it/s]Tokenizing texts:  55%|█████▍    | 5482/10000 [00:23<00:17, 256.41it/s]Tokenizing texts:  55%|█████▌    | 5509/10000 [00:23<00:20, 216.08it/s]Tokenizing texts:  55%|█████▌    | 5541/10000 [00:23<00:18, 240.32it/s]Tokenizing texts:  56%|█████▌    | 5568/10000 [00:23<00:17, 246.66it/s]Tokenizing texts:  56%|█████▌    | 5594/10000 [00:23<00:17, 246.87it/s]Tokenizing texts:  56%|█████▋    | 5628/10000 [00:23<00:16, 270.96it/s]Tokenizing texts:  57%|█████▋    | 5666/10000 [00:23<00:14, 300.07it/s]Tokenizing texts:  57%|█████▋    | 5697/10000 [00:23<00:14, 302.88it/s]Tokenizing texts:  57%|█████▋    | 5728/10000 [00:24<00:14, 285.25it/s]Tokenizing texts:  58%|█████▊    | 5758/10000 [00:24<00:16, 257.27it/s]Tokenizing texts:  58%|█████▊    | 5785/10000 [00:24<00:19, 216.02it/s]Tokenizing texts:  58%|█████▊    | 5809/10000 [00:24<00:19, 214.49it/s]Tokenizing texts:  58%|█████▊    | 5833/10000 [00:24<00:19, 218.41it/s]Tokenizing texts:  59%|█████▊    | 5865/10000 [00:24<00:17, 242.74it/s]Tokenizing texts:  59%|█████▉    | 5891/10000 [00:24<00:16, 247.01it/s]Tokenizing texts:  59%|█████▉    | 5917/10000 [00:24<00:16, 246.82it/s]Tokenizing texts:  59%|█████▉    | 5943/10000 [00:25<00:16, 238.83it/s]Tokenizing texts:  60%|█████▉    | 5971/10000 [00:25<00:16, 249.52it/s]Tokenizing texts:  60%|█████▉    | 5997/10000 [00:25<00:16, 245.32it/s]Tokenizing texts:  60%|██████    | 6022/10000 [00:25<00:16, 242.08it/s]Tokenizing texts:  61%|██████    | 6054/10000 [00:25<00:15, 261.08it/s]Tokenizing texts:  61%|██████    | 6081/10000 [00:25<00:14, 263.53it/s]Tokenizing texts:  61%|██████    | 6108/10000 [00:25<00:14, 259.85it/s]Tokenizing texts:  61%|██████▏   | 6140/10000 [00:25<00:14, 275.70it/s]Tokenizing texts:  62%|██████▏   | 6168/10000 [00:25<00:13, 275.62it/s]Tokenizing texts:  62%|██████▏   | 6196/10000 [00:26<00:13, 274.31it/s]Tokenizing texts:  62%|██████▏   | 6224/10000 [00:26<00:15, 243.90it/s]Tokenizing texts:  63%|██████▎   | 6253/10000 [00:26<00:14, 255.96it/s]Tokenizing texts:  63%|██████▎   | 6280/10000 [00:26<00:15, 247.20it/s]Tokenizing texts:  63%|██████▎   | 6306/10000 [00:26<00:15, 243.44it/s]Tokenizing texts:  63%|██████▎   | 6331/10000 [00:26<00:16, 220.70it/s]Tokenizing texts:  64%|██████▎   | 6358/10000 [00:26<00:15, 233.42it/s]Tokenizing texts:  64%|██████▍   | 6386/10000 [00:26<00:14, 243.14it/s]Tokenizing texts:  64%|██████▍   | 6414/10000 [00:26<00:14, 252.90it/s]Tokenizing texts:  64%|██████▍   | 6442/10000 [00:27<00:14, 253.80it/s]Tokenizing texts:  65%|██████▍   | 6468/10000 [00:27<00:14, 247.87it/s]Tokenizing texts:  65%|██████▍   | 6493/10000 [00:27<00:14, 246.37it/s]Tokenizing texts:  65%|██████▌   | 6518/10000 [00:27<00:15, 228.31it/s]Tokenizing texts:  66%|██████▌   | 6550/10000 [00:27<00:13, 250.87it/s]Tokenizing texts:  66%|██████▌   | 6589/10000 [00:27<00:11, 288.93it/s]Tokenizing texts:  66%|██████▌   | 6619/10000 [00:27<00:12, 265.77it/s]Tokenizing texts:  66%|██████▋   | 6648/10000 [00:27<00:12, 271.01it/s]Tokenizing texts:  67%|██████▋   | 6676/10000 [00:27<00:12, 257.73it/s]Tokenizing texts:  67%|██████▋   | 6703/10000 [00:28<00:13, 236.79it/s]Tokenizing texts:  67%|██████▋   | 6732/10000 [00:28<00:13, 243.20it/s]Tokenizing texts:  68%|██████▊   | 6760/10000 [00:28<00:12, 250.96it/s]Tokenizing texts:  68%|██████▊   | 6799/10000 [00:28<00:11, 286.12it/s]Tokenizing texts:  68%|██████▊   | 6831/10000 [00:28<00:10, 289.44it/s]Tokenizing texts:  69%|██████▊   | 6864/10000 [00:28<00:10, 300.71it/s]Tokenizing texts:  69%|██████▉   | 6895/10000 [00:28<00:13, 234.67it/s]Tokenizing texts:  69%|██████▉   | 6921/10000 [00:28<00:13, 227.55it/s]Tokenizing texts:  69%|██████▉   | 6947/10000 [00:29<00:13, 234.84it/s]Tokenizing texts:  70%|██████▉   | 6976/10000 [00:29<00:12, 248.62it/s]Tokenizing texts:  70%|███████   | 7002/10000 [00:29<00:12, 248.18it/s]Tokenizing texts:  70%|███████   | 7028/10000 [00:29<00:12, 241.06it/s]Tokenizing texts:  71%|███████   | 7053/10000 [00:29<00:12, 236.97it/s]Tokenizing texts:  71%|███████   | 7078/10000 [00:29<00:13, 213.37it/s]Tokenizing texts:  71%|███████   | 7100/10000 [00:29<00:13, 214.33it/s]Tokenizing texts:  71%|███████▏  | 7129/10000 [00:29<00:12, 231.23it/s]Tokenizing texts:  72%|███████▏  | 7153/10000 [00:29<00:12, 231.75it/s]Tokenizing texts:  72%|███████▏  | 7181/10000 [00:30<00:11, 245.23it/s]Tokenizing texts:  72%|███████▏  | 7207/10000 [00:30<00:11, 248.77it/s]Tokenizing texts:  72%|███████▏  | 7233/10000 [00:30<00:11, 246.06it/s]Tokenizing texts:  73%|███████▎  | 7263/10000 [00:30<00:10, 261.31it/s]Tokenizing texts:  73%|███████▎  | 7302/10000 [00:30<00:09, 295.80it/s]Tokenizing texts:  73%|███████▎  | 7335/10000 [00:30<00:08, 301.06it/s]Tokenizing texts:  74%|███████▎  | 7366/10000 [00:30<00:10, 263.05it/s]Tokenizing texts:  74%|███████▍  | 7394/10000 [00:30<00:09, 267.02it/s]Tokenizing texts:  74%|███████▍  | 7422/10000 [00:30<00:11, 222.66it/s]Tokenizing texts:  74%|███████▍  | 7446/10000 [00:31<00:12, 202.43it/s]Tokenizing texts:  75%|███████▍  | 7478/10000 [00:31<00:11, 227.56it/s]Tokenizing texts:  75%|███████▌  | 7515/10000 [00:31<00:09, 260.54it/s]Tokenizing texts:  75%|███████▌  | 7543/10000 [00:31<00:09, 264.34it/s]Tokenizing texts:  76%|███████▌  | 7571/10000 [00:31<00:09, 258.73it/s]Tokenizing texts:  76%|███████▌  | 7610/10000 [00:31<00:08, 293.79it/s]Tokenizing texts:  76%|███████▋  | 7641/10000 [00:31<00:08, 267.71it/s]Tokenizing texts:  77%|███████▋  | 7673/10000 [00:31<00:08, 281.20it/s]Tokenizing texts:  77%|███████▋  | 7703/10000 [00:32<00:08, 283.27it/s]Tokenizing texts:  77%|███████▋  | 7732/10000 [00:32<00:08, 265.86it/s]Tokenizing texts:  78%|███████▊  | 7760/10000 [00:32<00:09, 236.27it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 252.29it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 233.39it/s]Tokenizing texts:  78%|███████▊  | 7845/10000 [00:32<00:08, 241.89it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 254.47it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 252.46it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:32<00:07, 280.71it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:33<00:07, 278.28it/s]Tokenizing texts:  80%|███████▉  | 7999/10000 [00:33<00:06, 288.96it/s]Tokenizing texts:  80%|████████  | 8029/10000 [00:33<00:06, 284.90it/s]Tokenizing texts:  81%|████████  | 8058/10000 [00:33<00:11, 168.70it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 198.54it/s]Tokenizing texts:  81%|████████  | 8119/10000 [00:33<00:09, 206.12it/s]Tokenizing texts:  81%|████████▏ | 8148/10000 [00:33<00:08, 224.15it/s]Tokenizing texts:  82%|████████▏ | 8176/10000 [00:34<00:07, 237.71it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:34<00:07, 247.61it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 212.58it/s]Tokenizing texts:  83%|████████▎ | 8264/10000 [00:34<00:07, 220.96it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 256.44it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 252.17it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:34<00:06, 256.07it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:34<00:06, 255.04it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:35<00:06, 230.90it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:35<00:06, 256.58it/s]Tokenizing texts:  85%|████████▍ | 8472/10000 [00:35<00:05, 262.19it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:35<00:05, 277.01it/s]Tokenizing texts:  85%|████████▌ | 8533/10000 [00:35<00:05, 259.57it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:35<00:05, 265.88it/s]Tokenizing texts:  86%|████████▌ | 8590/10000 [00:35<00:05, 260.81it/s]Tokenizing texts:  86%|████████▌ | 8624/10000 [00:35<00:04, 282.08it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:35<00:04, 283.77it/s]Tokenizing texts:  87%|████████▋ | 8687/10000 [00:35<00:04, 293.51it/s]Tokenizing texts:  87%|████████▋ | 8722/10000 [00:36<00:04, 309.39it/s]Tokenizing texts:  88%|████████▊ | 8754/10000 [00:36<00:04, 278.22it/s]Tokenizing texts:  88%|████████▊ | 8785/10000 [00:36<00:04, 282.63it/s]Tokenizing texts:  88%|████████▊ | 8814/10000 [00:36<00:05, 216.17it/s]Tokenizing texts:  88%|████████▊ | 8839/10000 [00:36<00:05, 217.95it/s]Tokenizing texts:  89%|████████▊ | 8871/10000 [00:36<00:04, 242.17it/s]Tokenizing texts:  89%|████████▉ | 8910/10000 [00:36<00:03, 278.99it/s]Tokenizing texts:  89%|████████▉ | 8940/10000 [00:36<00:03, 284.02it/s]Tokenizing texts:  90%|████████▉ | 8970/10000 [00:37<00:03, 272.45it/s]Tokenizing texts:  90%|████████▉ | 8999/10000 [00:37<00:04, 244.96it/s]Tokenizing texts:  90%|█████████ | 9029/10000 [00:37<00:03, 258.89it/s]Tokenizing texts:  91%|█████████ | 9056/10000 [00:37<00:03, 247.27it/s]Tokenizing texts:  91%|█████████ | 9084/10000 [00:37<00:03, 255.39it/s]Tokenizing texts:  91%|█████████ | 9113/10000 [00:37<00:03, 264.45it/s]Tokenizing texts:  91%|█████████▏| 9143/10000 [00:37<00:03, 272.47it/s]Tokenizing texts:  92%|█████████▏| 9171/10000 [00:37<00:03, 273.73it/s]Tokenizing texts:  92%|█████████▏| 9204/10000 [00:37<00:02, 289.34it/s]Tokenizing texts:  92%|█████████▏| 9234/10000 [00:38<00:02, 270.32it/s]Tokenizing texts:  93%|█████████▎| 9262/10000 [00:38<00:02, 270.81it/s]Tokenizing texts:  93%|█████████▎| 9290/10000 [00:38<00:02, 251.45it/s]Tokenizing texts:  93%|█████████▎| 9316/10000 [00:38<00:02, 229.95it/s]Tokenizing texts:  93%|█████████▎| 9348/10000 [00:38<00:02, 252.50it/s]Tokenizing texts:  94%|█████████▎| 9374/10000 [00:38<00:02, 218.67it/s]Tokenizing texts:  94%|█████████▍| 9403/10000 [00:38<00:02, 228.63it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:38<00:02, 228.34it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:39<00:02, 232.54it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:39<00:02, 240.98it/s]Tokenizing texts:  95%|█████████▌| 9504/10000 [00:39<00:02, 181.87it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 221.35it/s]Tokenizing texts:  96%|█████████▌| 9565/10000 [00:39<00:01, 227.30it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 232.75it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 230.81it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 233.24it/s]Tokenizing texts:  97%|█████████▋| 9673/10000 [00:40<00:01, 237.36it/s]Tokenizing texts:  97%|█████████▋| 9698/10000 [00:40<00:01, 234.79it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:40<00:01, 238.21it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 206.17it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 214.35it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 259.07it/s]Tokenizing texts:  98%|█████████▊| 9840/10000 [00:40<00:00, 251.15it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 257.21it/s]Tokenizing texts:  99%|█████████▉| 9906/10000 [00:40<00:00, 281.95it/s]Tokenizing texts:  99%|█████████▉| 9935/10000 [00:41<00:00, 279.66it/s]Tokenizing texts: 100%|█████████▉| 9965/10000 [00:41<00:00, 285.16it/s]Tokenizing texts: 100%|█████████▉| 9998/10000 [00:41<00:00, 297.88it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 242.50it/s]
2025-12-09 11:57:14.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 12.036367416381836
2025-12-09 11:57:14.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 12.055171012878418
2025-12-09 11:57:14.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 12.067523956298828
2025-12-09 11:57:15.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 12.057825088500977
2025-12-09 11:57:15.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 12.030237197875977
2025-12-09 11:57:15.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 12.029718399047852
2025-12-09 11:57:15.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 12.045454025268555
2025-12-09 11:57:15.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 12.010782241821289
2025-12-09 11:57:15.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 12.030315399169922
2025-12-09 11:57:15.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 12.013559341430664
2025-12-09 11:57:15.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 12.040477752685547
2025-12-09 11:57:16.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 12.048235893249512
2025-12-09 11:57:16.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 12.039077758789062
2025-12-09 11:57:16.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 12.01540756225586
2025-12-09 11:57:16.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 11.999001502990723
2025-12-09 11:57:16.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 12.039562225341797
2025-12-09 11:57:16.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 12.008613586425781
2025-12-09 11:57:16.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 12.028059959411621
2025-12-09 11:57:17.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 12.044475555419922
2025-12-09 11:57:17.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 11.981945991516113
2025-12-09 11:57:17.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 12.031135559082031
2025-12-09 11:57:17.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 12.018762588500977
2025-12-09 11:57:17.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 11.977777481079102
2025-12-09 11:57:17.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 11.955656051635742
2025-12-09 11:57:17.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 12.021017074584961
2025-12-09 11:57:17.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 12.000387191772461
2025-12-09 11:57:18.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 11.943187713623047
2025-12-09 11:57:18.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 11.96904468536377
2025-12-09 11:57:18.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 11.985953330993652
2025-12-09 11:57:18.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 11.908731460571289
2025-12-09 11:57:18.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 11.920449256896973
2025-12-09 11:57:18.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 11.907462120056152
2025-12-09 11:57:19.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 11.948436737060547
2025-12-09 11:57:19.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 11.915759086608887
2025-12-09 11:57:19.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 11.917670249938965
2025-12-09 11:57:19.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 11.916658401489258
2025-12-09 11:57:19.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 11.897252082824707
2025-12-09 11:57:19.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 11.860623359680176
2025-12-09 11:57:19.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 11.905898094177246
2025-12-09 11:57:19.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 11.875168800354004
2025-12-09 11:57:20.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 11.82493782043457
2025-12-09 11:57:20.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 11.801054954528809
2025-12-09 11:57:20.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 11.786138534545898
2025-12-09 11:57:20.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 11.795989036560059
2025-12-09 11:57:20.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 11.760711669921875
2025-12-09 11:57:20.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 11.792525291442871
2025-12-09 11:57:20.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 11.684263229370117
2025-12-09 11:57:20.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 11.753244400024414
2025-12-09 11:57:21.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 11.67737865447998
2025-12-09 11:57:21.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 11.702920913696289
2025-12-09 11:57:21.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 11.756077766418457
2025-12-09 11:57:21.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 11.673077583312988
2025-12-09 11:57:21.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 11.614233016967773
2025-12-09 11:57:21.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 11.599508285522461
2025-12-09 11:57:21.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 11.520856857299805
2025-12-09 11:57:21.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 11.57048225402832
2025-12-09 11:57:22.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 11.49599838256836
2025-12-09 11:57:22.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 11.569892883300781
2025-12-09 11:57:22.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 11.554603576660156
2025-12-09 11:57:22.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 11.43634033203125
2025-12-09 11:57:22.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 11.410787582397461
2025-12-09 11:57:22.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 11.334914207458496
2025-12-09 11:57:22.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 11.442435264587402
2025-12-09 11:57:23.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 11.406007766723633
2025-12-09 11:57:23.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 11.267633438110352
2025-12-09 11:57:23.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 11.20435619354248
2025-12-09 11:57:23.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 11.193140029907227
2025-12-09 11:57:23.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 11.146675109863281
2025-12-09 11:57:23.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 11.294628143310547
2025-12-09 11:57:23.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 11.044686317443848
2025-12-09 11:57:23.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 11.113097190856934
2025-12-09 11:57:24.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 10.98831558227539
2025-12-09 11:57:24.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 11.068995475769043
2025-12-09 11:57:24.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 10.849246978759766
2025-12-09 11:57:24.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 11.029029846191406
2025-12-09 11:57:24.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 10.767083168029785
2025-12-09 11:57:24.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 11.064640045166016
2025-12-09 11:57:24.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 10.719366073608398
2025-12-09 11:57:25.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 10.711146354675293
2025-12-09 11:57:25.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 10.646607398986816
2025-12-09 11:57:25.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 10.598191261291504
2025-12-09 11:57:25.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 10.72862720489502
2025-12-09 11:57:25.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 10.4871187210083
2025-12-09 11:57:25.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 10.487870216369629
2025-12-09 11:57:25.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 10.389994621276855
2025-12-09 11:57:25.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 10.4053373336792
2025-12-09 11:57:26.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 10.323309898376465
2025-12-09 11:57:26.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 10.22712516784668
2025-12-09 11:57:26.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 10.226532936096191
2025-12-09 11:57:26.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 10.033377647399902
2025-12-09 11:57:26.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 10.285572052001953
2025-12-09 11:57:26.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 10.085925102233887
2025-12-09 11:57:26.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 10.088675498962402
2025-12-09 11:57:27.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 10.01040267944336
2025-12-09 11:57:27.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 10.011077880859375
2025-12-09 11:57:27.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 9.853168487548828
2025-12-09 11:57:27.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 9.963327407836914
2025-12-09 11:57:27.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 9.740853309631348
2025-12-09 11:57:27.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 10.144967079162598
2025-12-09 11:57:27.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 9.730020523071289
2025-12-09 11:57:27.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.999999072578703e-05 Training loss: 9.630147933959961
2025-12-09 11:57:28.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 9.999996290315153e-05 Training loss: 9.627694129943848
2025-12-09 11:57:28.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 9.999991653210385e-05 Training loss: 9.667829513549805
2025-12-09 11:57:28.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 9.999985161266117e-05 Training loss: 9.619974136352539
2025-12-09 11:57:28.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 9.999976814484758e-05 Training loss: 10.014693260192871
2025-12-09 11:57:28.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 9.999966612869405e-05 Training loss: 9.366647720336914
2025-12-09 11:57:28.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 9.999954556423843e-05 Training loss: 8.987460136413574
2025-12-09 11:57:28.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.999940645152541e-05 Training loss: 9.615381240844727
2025-12-09 11:57:28.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 9.999924879060665e-05 Training loss: 9.273347854614258
2025-12-09 11:57:29.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 9.999907258154059e-05 Training loss: 9.275617599487305
2025-12-09 11:57:29.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 9.999887782439263e-05 Training loss: 9.240117073059082
2025-12-09 11:57:29.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 9.999866451923501e-05 Training loss: 9.088343620300293
2025-12-09 11:57:29.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 9.999843266614685e-05 Training loss: 9.047431945800781
2025-12-09 11:57:29.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 9.999818226521415e-05 Training loss: 9.108661651611328
2025-12-09 11:57:29.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 9.999791331652984e-05 Training loss: 8.907038688659668
2025-12-09 11:57:29.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 9.999762582019365e-05 Training loss: 8.979951858520508
2025-12-09 11:57:30.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 9.999731977631227e-05 Training loss: 8.612056732177734
2025-12-09 11:57:30.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 9.999699518499921e-05 Training loss: 8.966264724731445
2025-12-09 11:57:30.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 9.999665204637487e-05 Training loss: 8.615577697753906
2025-12-09 11:57:30.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 9.999629036056657e-05 Training loss: 8.81956958770752
2025-12-09 11:57:30.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 9.999591012770848e-05 Training loss: 8.885807991027832
2025-12-09 11:57:30.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 9.999551134794164e-05 Training loss: 8.73504638671875
2025-12-09 11:57:30.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 9.999509402141401e-05 Training loss: 8.6514310836792
2025-12-09 11:57:30.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 9.999465814828036e-05 Training loss: 8.76676082611084
2025-12-09 11:57:31.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 9.999420372870242e-05 Training loss: 8.587395668029785
2025-12-09 11:57:31.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 9.999373076284877e-05 Training loss: 9.025795936584473
2025-12-09 11:57:31.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 9.999323925089486e-05 Training loss: 8.538195610046387
2025-12-09 11:57:31.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 9.999272919302301e-05 Training loss: 8.487483024597168
2025-12-09 11:57:31.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 9.999220058942245e-05 Training loss: 8.378059387207031
2025-12-09 11:57:31.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 9.999165344028926e-05 Training loss: 8.518070220947266
2025-12-09 11:57:31.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 9.999108774582645e-05 Training loss: 8.50197696685791
2025-12-09 11:57:31.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 9.999050350624382e-05 Training loss: 8.356917381286621
2025-12-09 11:57:32.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 9.998990072175813e-05 Training loss: 8.671404838562012
2025-12-09 11:57:32.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 9.998927939259303e-05 Training loss: 8.479084014892578
2025-12-09 11:57:32.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 9.998863951897897e-05 Training loss: 8.014450073242188
2025-12-09 11:57:32.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 9.998798110115333e-05 Training loss: 8.518638610839844
2025-12-09 11:57:32.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 9.998730413936037e-05 Training loss: 8.347447395324707
2025-12-09 11:57:32.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 9.998660863385123e-05 Training loss: 8.285057067871094
2025-12-09 11:57:32.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 9.99858945848839e-05 Training loss: 8.185754776000977
2025-12-09 11:57:33.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 9.998516199272327e-05 Training loss: 8.030879020690918
2025-12-09 11:57:33.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 9.998441085764113e-05 Training loss: 8.130260467529297
2025-12-09 11:57:33.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 9.998364117991612e-05 Training loss: 8.501321792602539
2025-12-09 11:57:33.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 9.998285295983376e-05 Training loss: 8.012680053710938
2025-12-09 11:57:33.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 9.998204619768646e-05 Training loss: 8.130764961242676
2025-12-09 11:57:33.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 9.998122089377349e-05 Training loss: 8.13751220703125
2025-12-09 11:57:33.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 9.998037704840102e-05 Training loss: 8.112275123596191
2025-12-09 11:57:33.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 9.99795146618821e-05 Training loss: 8.068370819091797
2025-12-09 11:57:34.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 9.997863373453663e-05 Training loss: 8.233158111572266
2025-12-09 11:57:34.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 9.997773426669142e-05 Training loss: 7.991042613983154
2025-12-09 11:57:34.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 9.997681625868013e-05 Training loss: 8.336577415466309
2025-12-09 11:57:34.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 9.997587971084335e-05 Training loss: 7.987461566925049
2025-12-09 11:57:34.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 9.997492462352846e-05 Training loss: 8.087884902954102
2025-12-09 11:57:34.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 9.997395099708982e-05 Training loss: 8.166196823120117
2025-12-09 11:57:34.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 9.997295883188856e-05 Training loss: 8.036577224731445
2025-12-09 11:57:35.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 9.997194812829276e-05 Training loss: 8.231599807739258
2025-12-09 11:57:35.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 9.997091888667738e-05 Training loss: 7.879671573638916
2025-12-09 11:57:35.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 9.996987110742422e-05 Training loss: 7.6072821617126465
2025-12-09 11:57:35.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 9.996880479092198e-05 Training loss: 7.719918727874756
2025-12-09 11:57:35.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 9.996771993756621e-05 Training loss: 7.8951616287231445
2025-12-09 11:57:35.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 9.996661654775938e-05 Training loss: 7.976646423339844
2025-12-09 11:57:35.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 9.996549462191082e-05 Training loss: 8.425024032592773
2025-12-09 11:57:35.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 9.99643541604367e-05 Training loss: 8.2512788772583
2025-12-09 11:57:36.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 9.99631951637601e-05 Training loss: 7.407425403594971
2025-12-09 11:57:36.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 9.996201763231099e-05 Training loss: 7.73447322845459
2025-12-09 11:57:36.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 9.996082156652618e-05 Training loss: 8.08664608001709
2025-12-09 11:57:36.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 9.995960696684939e-05 Training loss: 7.5427565574646
2025-12-09 11:57:36.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 9.995837383373119e-05 Training loss: 7.541266441345215
2025-12-09 11:57:36.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 9.995712216762902e-05 Training loss: 7.503180503845215
2025-12-09 11:57:36.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 9.995585196900723e-05 Training loss: 8.512582778930664
2025-12-09 11:57:36.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 9.9954563238337e-05 Training loss: 7.915310859680176
2025-12-09 11:57:37.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 9.995325597609645e-05 Training loss: 7.631989479064941
2025-12-09 11:57:37.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 9.99519301827705e-05 Training loss: 7.876887321472168
2025-12-09 11:57:37.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 9.995058585885095e-05 Training loss: 7.814043045043945
2025-12-09 11:57:37.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 9.994922300483656e-05 Training loss: 8.083382606506348
2025-12-09 11:57:37.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 9.99478416212329e-05 Training loss: 7.699570178985596
2025-12-09 11:57:37.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 9.994644170855237e-05 Training loss: 7.851292133331299
2025-12-09 11:57:37.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 9.994502326731434e-05 Training loss: 7.812880992889404
2025-12-09 11:57:38.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 9.994358629804499e-05 Training loss: 7.470415115356445
2025-12-09 11:57:38.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 9.994213080127739e-05 Training loss: 7.678866386413574
2025-12-09 11:57:38.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 9.994065677755147e-05 Training loss: 7.679513931274414
2025-12-09 11:57:38.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 9.993916422741409e-05 Training loss: 7.240365028381348
2025-12-09 11:57:38.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 9.99376531514189e-05 Training loss: 7.588984966278076
2025-12-09 11:57:38.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 9.993612355012647e-05 Training loss: 7.600711822509766
2025-12-09 11:57:38.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 9.993457542410424e-05 Training loss: 7.652915000915527
2025-12-09 11:57:38.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 9.993300877392651e-05 Training loss: 7.473021507263184
2025-12-09 11:57:39.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 9.993142360017446e-05 Training loss: 7.531064510345459
2025-12-09 11:57:39.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 9.992981990343614e-05 Training loss: 7.611759662628174
2025-12-09 11:57:39.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 9.992819768430647e-05 Training loss: 7.523959159851074
2025-12-09 11:57:39.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 9.992655694338725e-05 Training loss: 7.854403972625732
2025-12-09 11:57:39.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 9.992489768128713e-05 Training loss: 7.554591178894043
2025-12-09 11:57:39.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 9.992321989862166e-05 Training loss: 7.381766319274902
2025-12-09 11:57:39.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 9.992152359601322e-05 Training loss: 7.475583076477051
2025-12-09 11:57:39.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 9.99198087740911e-05 Training loss: 7.565568447113037
2025-12-09 11:57:40.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 9.991807543349146e-05 Training loss: 7.842536926269531
2025-12-09 11:57:40.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 9.99163235748573e-05 Training loss: 7.654867649078369
2025-12-09 11:57:40.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 9.99145531988385e-05 Training loss: 7.351114749908447
2025-12-09 11:57:40.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 9.99127643060918e-05 Training loss: 8.531035423278809
2025-12-09 11:57:40.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 9.991095689728087e-05 Training loss: 7.474036693572998
2025-12-09 11:57:40.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 9.990913097307614e-05 Training loss: 7.351780891418457
2025-12-09 11:57:40.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 9.990728653415504e-05 Training loss: 8.363186836242676
2025-12-09 11:57:41.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 9.990542358120174e-05 Training loss: 7.421618938446045
2025-12-09 11:57:41.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 9.990354211490735e-05 Training loss: 7.286938190460205
2025-12-09 11:57:41.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 9.990164213596986e-05 Training loss: 7.263092041015625
2025-12-09 11:57:41.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 9.989972364509408e-05 Training loss: 7.76845121383667
2025-12-09 11:57:41.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 9.989778664299172e-05 Training loss: 7.395857810974121
2025-12-09 11:57:41.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 9.989583113038135e-05 Training loss: 7.543057918548584
2025-12-09 11:57:41.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 9.989385710798837e-05 Training loss: 8.240867614746094
2025-12-09 11:57:41.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 9.989186457654513e-05 Training loss: 7.483976364135742
2025-12-09 11:57:42.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 9.988985353679077e-05 Training loss: 7.412898540496826
2025-12-09 11:57:42.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 9.988782398947131e-05 Training loss: 7.513901710510254
2025-12-09 11:57:42.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 9.988577593533967e-05 Training loss: 7.595154285430908
2025-12-09 11:57:42.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 9.988370937515561e-05 Training loss: 7.646005153656006
2025-12-09 11:57:42.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 9.988162430968575e-05 Training loss: 7.542147159576416
2025-12-09 11:57:42.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 9.987952073970359e-05 Training loss: 7.250340938568115
2025-12-09 11:57:42.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 9.98773986659895e-05 Training loss: 7.605935573577881
2025-12-09 11:57:43.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 9.987525808933068e-05 Training loss: 7.590118885040283
2025-12-09 11:57:43.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 9.987309901052121e-05 Training loss: 7.331213474273682
2025-12-09 11:57:43.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 9.98709214303621e-05 Training loss: 7.560491561889648
2025-12-09 11:57:43.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 9.986872534966109e-05 Training loss: 7.344175815582275
2025-12-09 11:57:43.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 9.986651076923288e-05 Training loss: 7.168997287750244
2025-12-09 11:57:43.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 9.986427768989903e-05 Training loss: 7.61029577255249
2025-12-09 11:57:43.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 9.986202611248793e-05 Training loss: 6.667059421539307
2025-12-09 11:57:43.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 9.985975603783484e-05 Training loss: 7.347821235656738
2025-12-09 11:57:44.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 9.98574674667819e-05 Training loss: 7.677192687988281
2025-12-09 11:57:44.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 9.985516040017807e-05 Training loss: 7.5960373878479
2025-12-09 11:57:44.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 9.985283483887923e-05 Training loss: 7.530910015106201
2025-12-09 11:57:44.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 9.985049078374806e-05 Training loss: 7.332867622375488
2025-12-09 11:57:44.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 9.984812823565417e-05 Training loss: 7.403707504272461
2025-12-09 11:57:44.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 9.984574719547395e-05 Training loss: 7.269711017608643
2025-12-09 11:57:44.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 9.984334766409071e-05 Training loss: 7.311942100524902
2025-12-09 11:57:44.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 9.98409296423946e-05 Training loss: 7.574908256530762
2025-12-09 11:57:45.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 9.983849313128264e-05 Training loss: 7.4487481117248535
2025-12-09 11:57:45.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 9.983603813165869e-05 Training loss: 7.1945061683654785
2025-12-09 11:57:45.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 9.983356464443347e-05 Training loss: 7.234274864196777
2025-12-09 11:57:45.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 9.983107267052457e-05 Training loss: 7.3845906257629395
2025-12-09 11:57:45.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 9.982856221085644e-05 Training loss: 6.814684867858887
2025-12-09 11:57:45.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 9.982603326636037e-05 Training loss: 7.083205699920654
2025-12-09 11:57:45.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 9.982348583797454e-05 Training loss: 7.080016613006592
2025-12-09 11:57:46.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 9.982091992664392e-05 Training loss: 7.145181179046631
2025-12-09 11:57:46.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 9.981833553332045e-05 Training loss: 7.786893844604492
2025-12-09 11:57:46.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 9.981573265896281e-05 Training loss: 7.403869152069092
2025-12-09 11:57:46.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 9.981311130453659e-05 Training loss: 6.931947231292725
2025-12-09 11:57:46.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 9.981047147101426e-05 Training loss: 7.173469066619873
2025-12-09 11:57:46.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 9.980781315937507e-05 Training loss: 7.367379665374756
2025-12-09 11:57:46.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 9.98051363706052e-05 Training loss: 7.399955749511719
2025-12-09 11:57:46.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 9.980244110569765e-05 Training loss: 7.139119625091553
2025-12-09 11:57:47.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 9.979972736565226e-05 Training loss: 7.285726070404053
2025-12-09 11:57:47.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 9.979699515147578e-05 Training loss: 6.53759241104126
2025-12-09 11:57:47.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 9.979424446418173e-05 Training loss: 6.995182514190674
2025-12-09 11:57:47.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 9.979147530479056e-05 Training loss: 7.436007499694824
2025-12-09 11:57:47.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 9.978868767432954e-05 Training loss: 7.520694732666016
2025-12-09 11:57:47.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 9.978588157383277e-05 Training loss: 7.252113342285156
2025-12-09 11:57:47.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 9.978305700434125e-05 Training loss: 7.330478191375732
2025-12-09 11:57:48.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 9.97802139669028e-05 Training loss: 7.400696754455566
2025-12-09 11:57:48.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 9.97773524625721e-05 Training loss: 7.146157264709473
2025-12-09 11:57:48.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 9.977447249241066e-05 Training loss: 7.3700408935546875
2025-12-09 11:57:48.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 9.977157405748687e-05 Training loss: 7.120455265045166
2025-12-09 11:57:48.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 9.976865715887595e-05 Training loss: 7.175747871398926
2025-12-09 11:57:48.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 9.976572179765999e-05 Training loss: 7.169113636016846
2025-12-09 11:57:48.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 9.976276797492793e-05 Training loss: 7.2894110679626465
2025-12-09 11:57:48.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 9.975979569177552e-05 Training loss: 7.2020697593688965
2025-12-09 11:57:49.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 9.975680494930538e-05 Training loss: 7.492094993591309
2025-12-09 11:57:49.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 9.9753795748627e-05 Training loss: 7.736581325531006
2025-12-09 11:57:49.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 9.975076809085669e-05 Training loss: 7.051456451416016
2025-12-09 11:57:49.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 9.974772197711761e-05 Training loss: 7.294482707977295
2025-12-09 11:57:49.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 9.97446574085398e-05 Training loss: 7.630684852600098
2025-12-09 11:57:49.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 9.974157438626008e-05 Training loss: 7.393791675567627
2025-12-09 11:57:49.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 9.973847291142218e-05 Training loss: 6.996034145355225
2025-12-09 11:57:50.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 9.973535298517663e-05 Training loss: 7.610676288604736
2025-12-09 11:57:50.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 9.973221460868086e-05 Training loss: 7.4989094734191895
2025-12-09 11:57:50.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 9.972905778309906e-05 Training loss: 7.485886573791504
2025-12-09 11:57:50.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 9.972588250960234e-05 Training loss: 7.3458943367004395
2025-12-09 11:57:50.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 9.972268878936863e-05 Training loss: 7.396247386932373
2025-12-09 11:57:50.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 9.97194766235827e-05 Training loss: 7.1093597412109375
2025-12-09 11:57:50.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 9.971624601343615e-05 Training loss: 7.2115912437438965
2025-12-09 11:57:50.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 9.971299696012743e-05 Training loss: 7.3473005294799805
2025-12-09 11:57:51.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 9.970972946486185e-05 Training loss: 7.517838001251221
2025-12-09 11:57:51.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 9.970644352885157e-05 Training loss: 7.313090801239014
2025-12-09 11:57:51.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 9.970313915331553e-05 Training loss: 7.161703109741211
2025-12-09 11:57:51.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 9.969981633947956e-05 Training loss: 7.454007625579834
2025-12-09 11:57:51.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 9.969647508857631e-05 Training loss: 7.296258449554443
2025-12-09 11:57:51.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 9.969311540184532e-05 Training loss: 7.2616496086120605
2025-12-09 11:57:51.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 9.968973728053288e-05 Training loss: 6.993233680725098
2025-12-09 11:57:52.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 9.968634072589218e-05 Training loss: 7.187115669250488
2025-12-09 11:57:52.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 9.968292573918325e-05 Training loss: 7.507740497589111
2025-12-09 11:57:52.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 9.967949232167294e-05 Training loss: 7.288326740264893
2025-12-09 11:57:52.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 9.967604047463493e-05 Training loss: 7.006239414215088
2025-12-09 11:57:52.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 9.967257019934975e-05 Training loss: 7.4978156089782715
2025-12-09 11:57:52.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 9.966908149710476e-05 Training loss: 6.494487762451172
2025-12-09 11:57:52.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 9.966557436919416e-05 Training loss: 7.371699810028076
2025-12-09 11:57:52.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 9.966204881691898e-05 Training loss: 6.712113857269287
2025-12-09 11:57:53.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 9.965850484158711e-05 Training loss: 6.246139049530029
2025-12-09 11:57:53.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 9.965494244451324e-05 Training loss: 7.376897811889648
2025-12-09 11:57:53.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 9.96513616270189e-05 Training loss: 7.149404048919678
2025-12-09 11:57:53.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 9.964776239043246e-05 Training loss: 7.258501052856445
2025-12-09 11:57:53.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 9.964414473608912e-05 Training loss: 6.396061420440674
2025-12-09 11:57:53.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 9.964050866533094e-05 Training loss: 7.078734397888184
2025-12-09 11:57:53.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 9.963685417950677e-05 Training loss: 7.144811630249023
2025-12-09 11:57:54.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 9.96331812799723e-05 Training loss: 6.866636753082275
2025-12-09 11:57:54.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 9.962948996809008e-05 Training loss: 7.034787178039551
2025-12-09 11:57:54.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 9.962578024522948e-05 Training loss: 7.228332996368408
2025-12-09 11:57:54.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 9.962205211276665e-05 Training loss: 7.348635673522949
2025-12-09 11:57:54.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 9.961830557208464e-05 Training loss: 8.055850982666016
2025-12-09 11:57:54.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 9.961454062457329e-05 Training loss: 6.955918312072754
2025-12-09 11:57:54.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 9.961075727162928e-05 Training loss: 7.023338317871094
2025-12-09 11:57:54.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 9.960695551465611e-05 Training loss: 7.170572757720947
2025-12-09 11:57:55.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 9.960313535506411e-05 Training loss: 7.15117073059082
2025-12-09 11:57:55.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 9.959929679427047e-05 Training loss: 7.104955196380615
2025-12-09 11:57:55.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 9.959543983369912e-05 Training loss: 6.717082977294922
2025-12-09 11:57:55.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 9.959156447478091e-05 Training loss: 7.102162837982178
2025-12-09 11:57:55.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 9.958767071895347e-05 Training loss: 7.352577209472656
2025-12-09 11:57:55.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 9.958375856766127e-05 Training loss: 6.979311466217041
2025-12-09 11:57:55.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 9.957982802235556e-05 Training loss: 7.097249984741211
2025-12-09 11:57:55.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 9.957587908449448e-05 Training loss: 7.166380882263184
2025-12-09 11:57:56.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 9.957191175554294e-05 Training loss: 7.167508602142334
2025-12-09 11:57:56.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 9.956792603697273e-05 Training loss: 7.595554351806641
2025-12-09 11:57:56.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 9.956392193026239e-05 Training loss: 7.486525535583496
2025-12-09 11:57:56.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 9.955989943689734e-05 Training loss: 7.3216352462768555
2025-12-09 11:57:56.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 9.955585855836978e-05 Training loss: 6.966623783111572
2025-12-09 11:57:56.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 9.955179929617875e-05 Training loss: 7.963873863220215
2025-12-09 11:57:56.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 9.954772165183013e-05 Training loss: 6.994626998901367
2025-12-09 11:57:57.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 9.954362562683658e-05 Training loss: 7.157822608947754
2025-12-09 11:57:57.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 9.95395112227176e-05 Training loss: 6.67520809173584
2025-12-09 11:57:57.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 9.95353784409995e-05 Training loss: 7.03367805480957
2025-12-09 11:57:57.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 9.953122728321542e-05 Training loss: 7.020949363708496
2025-12-09 11:57:57.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 9.952705775090529e-05 Training loss: 7.136720180511475
2025-12-09 11:57:57.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 9.952286984561592e-05 Training loss: 7.042449474334717
2025-12-09 11:57:57.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 9.951866356890084e-05 Training loss: 6.146900653839111
2025-12-09 11:57:57.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 9.951443892232047e-05 Training loss: 7.282291412353516
2025-12-09 11:57:58.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 9.951019590744203e-05 Training loss: 7.145144939422607
2025-12-09 11:57:58.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 9.950593452583952e-05 Training loss: 7.190146446228027
2025-12-09 11:57:58.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.95016547790938e-05 Training loss: 7.097532749176025
2025-12-09 11:57:58.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.949735666879252e-05 Training loss: 7.457273006439209
2025-12-09 11:57:58.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.949304019653012e-05 Training loss: 6.961640357971191
2025-12-09 11:57:58.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 9.94887053639079e-05 Training loss: 6.9899444580078125
2025-12-09 11:57:58.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 9.948435217253393e-05 Training loss: 7.453649044036865
2025-12-09 11:57:59.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 9.947998062402313e-05 Training loss: 7.13655424118042
2025-12-09 11:57:59.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 9.947559071999719e-05 Training loss: 7.217369556427002
2025-12-09 11:57:59.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 9.947118246208462e-05 Training loss: 7.461833477020264
2025-12-09 11:57:59.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 9.946675585192075e-05 Training loss: 6.987400054931641
2025-12-09 11:57:59.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 9.946231089114774e-05 Training loss: 7.052767276763916
2025-12-09 11:57:59.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 9.945784758141448e-05 Training loss: 6.945354461669922
2025-12-09 11:57:59.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 9.945336592437678e-05 Training loss: 7.324432849884033
2025-12-09 11:58:00.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 9.944886592169713e-05 Training loss: 7.026764392852783
2025-12-09 11:58:00.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 9.944434757504492e-05 Training loss: 6.759068489074707
2025-12-09 11:58:00.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 9.94398108860963e-05 Training loss: 6.904333591461182
2025-12-09 11:58:00.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 9.943525585653428e-05 Training loss: 7.403304100036621
2025-12-09 11:58:00.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 9.94306824880486e-05 Training loss: 7.277830123901367
2025-12-09 11:58:00.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 9.942609078233581e-05 Training loss: 7.14279317855835
2025-12-09 11:58:00.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 9.942148074109934e-05 Training loss: 7.287108421325684
2025-12-09 11:58:00.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 9.941685236604934e-05 Training loss: 7.071829795837402
2025-12-09 11:58:01.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 9.941220565890279e-05 Training loss: 6.903158187866211
2025-12-09 11:58:01.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 9.94075406213835e-05 Training loss: 7.42385721206665
2025-12-09 11:58:01.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 9.940285725522203e-05 Training loss: 6.968172073364258
2025-12-09 11:58:01.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 9.939815556215575e-05 Training loss: 7.854931354522705
2025-12-09 11:58:01.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 9.939343554392886e-05 Training loss: 6.8612141609191895
2025-12-09 11:58:01.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.938869720229234e-05 Training loss: 6.945045471191406
2025-12-09 11:58:01.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.938394053900395e-05 Training loss: 6.859571933746338
2025-12-09 11:58:01.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 9.937916555582828e-05 Training loss: 7.1368608474731445
2025-12-09 11:58:02.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 9.937437225453669e-05 Training loss: 7.174140453338623
2025-12-09 11:58:02.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 9.936956063690733e-05 Training loss: 7.827453136444092
2025-12-09 11:58:02.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 9.936473070472518e-05 Training loss: 6.966213226318359
2025-12-09 11:58:02.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 9.935988245978199e-05 Training loss: 7.364627838134766
2025-12-09 11:58:02.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 9.935501590387628e-05 Training loss: 7.28662633895874
2025-12-09 11:58:02.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 9.935013103881343e-05 Training loss: 6.816360950469971
2025-12-09 11:58:02.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 9.934522786640555e-05 Training loss: 6.90017032623291
2025-12-09 11:58:03.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 9.934030638847155e-05 Training loss: 6.914398670196533
2025-12-09 11:58:03.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 9.933536660683717e-05 Training loss: 7.420283317565918
2025-12-09 11:58:03.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 9.933040852333488e-05 Training loss: 7.159262180328369
2025-12-09 11:58:03.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 9.9325432139804e-05 Training loss: 6.904821872711182
2025-12-09 11:58:03.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 9.932043745809063e-05 Training loss: 6.791374683380127
2025-12-09 11:58:03.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 9.93154244800476e-05 Training loss: 6.848004341125488
2025-12-09 11:58:03.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.931039320753456e-05 Training loss: 6.802274227142334
2025-12-09 11:58:04.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 9.9305343642418e-05 Training loss: 6.1545329093933105
2025-12-09 11:58:04.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 9.930027578657113e-05 Training loss: 6.835149765014648
2025-12-09 11:58:04.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 9.929518964187395e-05 Training loss: 7.477100849151611
2025-12-09 11:58:04.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 9.929008521021325e-05 Training loss: 7.282159328460693
2025-12-09 11:58:04.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 9.928496249348265e-05 Training loss: 6.777239799499512
2025-12-09 11:58:04.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 9.92798214935825e-05 Training loss: 7.053509712219238
2025-12-09 11:58:04.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 9.927466221241996e-05 Training loss: 6.479123115539551
2025-12-09 11:58:04.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 9.926948465190892e-05 Training loss: 7.121148109436035
2025-12-09 11:58:05.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 9.926428881397015e-05 Training loss: 6.740291118621826
2025-12-09 11:58:05.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 9.925907470053111e-05 Training loss: 6.890580654144287
2025-12-09 11:58:05.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 9.925384231352606e-05 Training loss: 6.972421169281006
2025-12-09 11:58:05.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 9.924859165489608e-05 Training loss: 6.844672679901123
2025-12-09 11:58:05.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 9.924332272658898e-05 Training loss: 7.109844207763672
2025-12-09 11:58:05.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 9.923803553055937e-05 Training loss: 6.986715793609619
2025-12-09 11:58:05.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 9.923273006876865e-05 Training loss: 7.519983768463135
2025-12-09 11:58:05.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 9.922740634318495e-05 Training loss: 6.5456953048706055
2025-12-09 11:58:06.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 9.922206435578323e-05 Training loss: 6.97835636138916
2025-12-09 11:58:06.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 9.921670410854518e-05 Training loss: 6.6711201667785645
2025-12-09 11:58:06.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 9.92113256034593e-05 Training loss: 7.059864044189453
2025-12-09 11:58:06.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 9.920592884252082e-05 Training loss: 7.066532611846924
2025-12-09 11:58:06.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 9.920051382773179e-05 Training loss: 7.303539276123047
2025-12-09 11:58:06.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 9.919508056110102e-05 Training loss: 7.366818904876709
2025-12-09 11:58:06.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 9.918962904464407e-05 Training loss: 7.140793800354004
2025-12-09 11:58:07.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 9.918415928038325e-05 Training loss: 7.075247764587402
2025-12-09 11:58:07.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 9.917867127034772e-05 Training loss: 7.032847881317139
2025-12-09 11:58:07.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 9.917316501657334e-05 Training loss: 6.807161331176758
2025-12-09 11:58:07.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 9.916764052110274e-05 Training loss: 7.130211353302002
2025-12-09 11:58:07.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 9.916209778598535e-05 Training loss: 6.563636779785156
2025-12-09 11:58:07.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 9.915653681327737e-05 Training loss: 7.0649237632751465
2025-12-09 11:58:07.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 9.915095760504169e-05 Training loss: 6.621163368225098
2025-12-09 11:58:07.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 9.914536016334808e-05 Training loss: 7.585535049438477
2025-12-09 11:58:08.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 9.913974449027298e-05 Training loss: 6.911890029907227
2025-12-09 11:58:08.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 9.913411058789963e-05 Training loss: 7.0244669914245605
2025-12-09 11:58:08.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 9.912845845831805e-05 Training loss: 7.029130935668945
2025-12-09 11:58:08.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 9.912278810362498e-05 Training loss: 7.169125556945801
2025-12-09 11:58:08.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 9.911709952592397e-05 Training loss: 7.360884666442871
2025-12-09 11:58:08.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 9.911139272732526e-05 Training loss: 7.164026737213135
2025-12-09 11:58:08.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 9.910566770994594e-05 Training loss: 7.07269287109375
2025-12-09 11:58:09.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 9.909992447590979e-05 Training loss: 6.782312870025635
2025-12-09 11:58:09.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 9.909416302734736e-05 Training loss: 6.953774452209473
2025-12-09 11:58:09.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 9.908838336639597e-05 Training loss: 6.6797590255737305
2025-12-09 11:58:09.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 9.908258549519971e-05 Training loss: 7.082310676574707
2025-12-09 11:58:09.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 9.907676941590939e-05 Training loss: 7.613663673400879
2025-12-09 11:58:09.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 9.907093513068259e-05 Training loss: 6.992173671722412
2025-12-09 11:58:09.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 9.906508264168366e-05 Training loss: 7.442356586456299
2025-12-09 11:58:09.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 9.905921195108368e-05 Training loss: 7.163409233093262
2025-12-09 11:58:10.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 9.90533230610605e-05 Training loss: 6.903024673461914
2025-12-09 11:58:10.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 9.90474159737987e-05 Training loss: 6.8212971687316895
2025-12-09 11:58:10.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 9.904149069148963e-05 Training loss: 6.725666522979736
2025-12-09 11:58:10.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 9.903554721633139e-05 Training loss: 6.917799949645996
2025-12-09 11:58:10.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 9.902958555052882e-05 Training loss: 6.904211044311523
2025-12-09 11:58:10.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 9.902360569629349e-05 Training loss: 6.639390468597412
2025-12-09 11:58:10.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 9.901760765584375e-05 Training loss: 6.745560646057129
2025-12-09 11:58:11.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 9.901159143140471e-05 Training loss: 7.122737407684326
2025-12-09 11:58:11.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 9.900555702520816e-05 Training loss: 6.757495880126953
2025-12-09 11:58:11.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 9.89995044394927e-05 Training loss: 6.940248966217041
2025-12-09 11:58:11.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 9.899343367650365e-05 Training loss: 6.525867938995361
2025-12-09 11:58:11.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 9.898734473849305e-05 Training loss: 6.766597747802734
2025-12-09 11:58:11.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 9.898123762771971e-05 Training loss: 6.852272987365723
2025-12-09 11:58:11.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 9.89751123464492e-05 Training loss: 7.015379428863525
2025-12-09 11:58:11.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 9.896896889695378e-05 Training loss: 6.723658084869385
2025-12-09 11:58:12.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 9.896280728151248e-05 Training loss: 6.797286510467529
2025-12-09 11:58:12.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 9.895662750241108e-05 Training loss: 7.086207389831543
2025-12-09 11:58:12.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 9.89504295619421e-05 Training loss: 6.84187126159668
2025-12-09 11:58:12.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 9.894421346240473e-05 Training loss: 7.024752616882324
2025-12-09 11:58:12.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 9.893797920610496e-05 Training loss: 6.730753421783447
2025-12-09 11:58:12.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 9.893172679535553e-05 Training loss: 6.849332809448242
2025-12-09 11:58:12.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 9.892545623247586e-05 Training loss: 7.136153221130371
2025-12-09 11:58:13.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 9.891916751979218e-05 Training loss: 6.771117687225342
2025-12-09 11:58:13.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 9.891286065963734e-05 Training loss: 6.728635311126709
2025-12-09 11:58:13.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 9.890653565435101e-05 Training loss: 6.91156530380249
2025-12-09 11:58:13.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 9.89001925062796e-05 Training loss: 6.653972148895264
2025-12-09 11:58:13.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 9.889383121777617e-05 Training loss: 6.65187406539917
2025-12-09 11:58:13.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 9.88874517912006e-05 Training loss: 7.2303009033203125
2025-12-09 11:58:13.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 9.888105422891943e-05 Training loss: 6.5512542724609375
2025-12-09 11:58:13.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 9.887463853330594e-05 Training loss: 6.880410194396973
2025-12-09 11:58:14.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 9.886820470674018e-05 Training loss: 7.005861759185791
2025-12-09 11:58:14.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 9.88617527516089e-05 Training loss: 6.681546688079834
2025-12-09 11:58:14.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 9.885528267030557e-05 Training loss: 6.8335862159729
2025-12-09 11:58:14.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 9.884879446523035e-05 Training loss: 7.087092399597168
2025-12-09 11:58:14.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 9.88422881387902e-05 Training loss: 6.729354381561279
2025-12-09 11:58:14.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 9.883576369339875e-05 Training loss: 7.2489447593688965
2025-12-09 11:58:14.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 9.882922113147637e-05 Training loss: 7.604700565338135
2025-12-09 11:58:15.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 9.882266045545012e-05 Training loss: 7.311307907104492
2025-12-09 11:58:15.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 9.881608166775383e-05 Training loss: 7.120318412780762
2025-12-09 11:58:15.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 9.880948477082804e-05 Training loss: 6.555830001831055
2025-12-09 11:58:15.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 9.880286976711992e-05 Training loss: 7.0111846923828125
2025-12-09 11:58:15.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 9.87962366590835e-05 Training loss: 6.793332576751709
2025-12-09 11:58:15.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 9.878958544917942e-05 Training loss: 6.814214706420898
2025-12-09 11:58:15.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 9.87829161398751e-05 Training loss: 6.570484638214111
2025-12-09 11:58:15.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 9.87762287336446e-05 Training loss: 6.848060607910156
2025-12-09 11:58:16.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 9.876952323296877e-05 Training loss: 6.885112762451172
2025-12-09 11:58:16.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 9.876279964033512e-05 Training loss: 6.871891498565674
2025-12-09 11:58:16.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 9.87560579582379e-05 Training loss: 7.176794528961182
2025-12-09 11:58:16.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 9.874929818917806e-05 Training loss: 7.134247303009033
2025-12-09 11:58:16.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 9.874252033566327e-05 Training loss: 6.831822395324707
2025-12-09 11:58:16.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 9.87357244002079e-05 Training loss: 6.421219825744629
2025-12-09 11:58:16.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 9.8728910385333e-05 Training loss: 6.927199363708496
2025-12-09 11:58:16.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 9.872207829356641e-05 Training loss: 6.650744915008545
2025-12-09 11:58:17.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 9.871522812744256e-05 Training loss: 6.646908760070801
2025-12-09 11:58:17.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 9.870835988950268e-05 Training loss: 7.344549655914307
2025-12-09 11:58:17.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 9.870147358229467e-05 Training loss: 7.436823844909668
2025-12-09 11:58:17.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 9.869456920837312e-05 Training loss: 6.690093994140625
2025-12-09 11:58:17.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 9.868764677029934e-05 Training loss: 6.847703456878662
2025-12-09 11:58:17.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 9.868070627064135e-05 Training loss: 6.576296329498291
2025-12-09 11:58:17.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 9.867374771197383e-05 Training loss: 7.09814453125
2025-12-09 11:58:18.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 9.866677109687822e-05 Training loss: 6.858487129211426
2025-12-09 11:58:18.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 9.86597764279426e-05 Training loss: 6.946013927459717
2025-12-09 11:58:18.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 9.865276370776177e-05 Training loss: 7.082106590270996
2025-12-09 11:58:18.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 9.864573293893725e-05 Training loss: 6.631976127624512
2025-12-09 11:58:18.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 9.863868412407721e-05 Training loss: 6.9261956214904785
2025-12-09 11:58:18.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 9.863161726579655e-05 Training loss: 6.846278190612793
2025-12-09 11:58:18.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 9.862453236671685e-05 Training loss: 7.17305326461792
2025-12-09 11:58:18.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 9.861742942946639e-05 Training loss: 7.155831336975098
2025-12-09 11:58:19.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 9.861030845668014e-05 Training loss: 7.236730575561523
2025-12-09 11:58:19.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 9.860316945099973e-05 Training loss: 6.269207000732422
2025-12-09 11:58:19.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 9.859601241507353e-05 Training loss: 6.712277412414551
2025-12-09 11:58:19.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 9.858883735155657e-05 Training loss: 6.629607200622559
2025-12-09 11:58:19.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 9.858164426311059e-05 Training loss: 7.025750637054443
2025-12-09 11:58:19.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 9.857443315240397e-05 Training loss: 6.653988361358643
2025-12-09 11:58:19.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 9.856720402211182e-05 Training loss: 6.793638706207275
2025-12-09 11:58:20.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 9.855995687491591e-05 Training loss: 7.173320770263672
2025-12-09 11:58:20.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 9.855269171350471e-05 Training loss: 6.4739603996276855
2025-12-09 11:58:20.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 9.854540854057337e-05 Training loss: 7.299199104309082
2025-12-09 11:58:20.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 9.85381073588237e-05 Training loss: 7.289359092712402
2025-12-09 11:58:20.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 9.853078817096424e-05 Training loss: 7.002792835235596
2025-12-09 11:58:20.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 9.852345097971016e-05 Training loss: 7.134862422943115
2025-12-09 11:58:20.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 9.851609578778332e-05 Training loss: 7.0596818923950195
2025-12-09 11:58:20.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 9.850872259791228e-05 Training loss: 6.809314250946045
2025-12-09 11:58:21.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 9.850133141283226e-05 Training loss: 6.614190578460693
2025-12-09 11:58:21.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 9.849392223528514e-05 Training loss: 6.96136474609375
2025-12-09 11:58:21.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 9.84864950680195e-05 Training loss: 6.608044147491455
2025-12-09 11:58:21.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 9.84790499137906e-05 Training loss: 7.3006157875061035
2025-12-09 11:58:21.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 9.847158677536034e-05 Training loss: 7.152731895446777
2025-12-09 11:58:21.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 9.84641056554973e-05 Training loss: 6.986096382141113
2025-12-09 11:58:21.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 9.845660655697679e-05 Training loss: 6.960428237915039
2025-12-09 11:58:22.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 9.844908948258067e-05 Training loss: 6.831304550170898
2025-12-09 11:58:22.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 9.844155443509759e-05 Training loss: 6.908234596252441
2025-12-09 11:58:22.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 9.84340014173228e-05 Training loss: 7.026753902435303
2025-12-09 11:58:22.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 9.842643043205822e-05 Training loss: 7.0078630447387695
2025-12-09 11:58:22.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 9.841884148211247e-05 Training loss: 7.0872344970703125
2025-12-09 11:58:22.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 9.84112345703008e-05 Training loss: 6.685581684112549
2025-12-09 11:58:22.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 9.84036096994451e-05 Training loss: 6.6200151443481445
2025-12-09 11:58:22.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 9.839596687237403e-05 Training loss: 6.632592678070068
2025-12-09 11:58:23.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 9.838830609192277e-05 Training loss: 7.87579345703125
2025-12-09 11:58:23.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 9.838062736093328e-05 Training loss: 7.353219985961914
2025-12-09 11:58:23.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 9.837293068225408e-05 Training loss: 6.617096900939941
2025-12-09 11:58:23.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 9.836521605874044e-05 Training loss: 6.692335605621338
2025-12-09 11:58:23.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 9.835748349325422e-05 Training loss: 6.7921037673950195
2025-12-09 11:58:23.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 9.834973298866395e-05 Training loss: 7.442142009735107
2025-12-09 11:58:23.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 9.834196454784485e-05 Training loss: 6.86769962310791
2025-12-09 11:58:24.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 9.833417817367874e-05 Training loss: 6.889784336090088
2025-12-09 11:58:24.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 9.832637386905412e-05 Training loss: 7.121029376983643
2025-12-09 11:58:24.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 9.831855163686618e-05 Training loss: 6.961062908172607
2025-12-09 11:58:24.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 9.831071148001667e-05 Training loss: 6.9824910163879395
2025-12-09 11:58:24.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 9.830285340141408e-05 Training loss: 6.427636623382568
2025-12-09 11:58:24.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 9.829497740397349e-05 Training loss: 6.871574401855469
2025-12-09 11:58:24.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 9.828708349061664e-05 Training loss: 6.855907440185547
2025-12-09 11:58:24.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 9.827917166427195e-05 Training loss: 6.81779146194458
2025-12-09 11:58:25.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 9.827124192787444e-05 Training loss: 5.863516330718994
2025-12-09 11:58:25.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 9.82632942843658e-05 Training loss: 7.372718811035156
2025-12-09 11:58:25.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 9.825532873669435e-05 Training loss: 6.748238563537598
2025-12-09 11:58:25.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 9.824734528781505e-05 Training loss: 6.536512851715088
2025-12-09 11:58:25.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 9.823934394068952e-05 Training loss: 7.131877422332764
2025-12-09 11:58:25.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 9.823132469828601e-05 Training loss: 6.80369758605957
2025-12-09 11:58:25.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 9.822328756357942e-05 Training loss: 6.845089912414551
2025-12-09 11:58:26.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 9.821523253955122e-05 Training loss: 6.869593143463135
2025-12-09 11:58:26.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 9.820715962918964e-05 Training loss: 6.5875325202941895
2025-12-09 11:58:26.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 9.819906883548943e-05 Training loss: 6.9649858474731445
2025-12-09 11:58:26.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 9.819096016145203e-05 Training loss: 7.042397499084473
2025-12-09 11:58:26.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 9.81828336100855e-05 Training loss: 6.626626491546631
2025-12-09 11:58:26.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 9.817468918440454e-05 Training loss: 7.472657203674316
2025-12-09 11:58:26.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 9.816652688743049e-05 Training loss: 7.327666282653809
2025-12-09 11:58:26.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 9.815834672219127e-05 Training loss: 6.8741559982299805
2025-12-09 11:58:27.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 9.815014869172149e-05 Training loss: 6.812949180603027
2025-12-09 11:58:27.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 9.814193279906237e-05 Training loss: 7.3673481941223145
2025-12-09 11:58:27.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 9.81336990472617e-05 Training loss: 6.137852191925049
2025-12-09 11:58:27.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 9.8125447439374e-05 Training loss: 7.345660209655762
2025-12-09 11:58:27.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 9.811717797846033e-05 Training loss: 6.826103687286377
2025-12-09 11:58:27.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 9.81088906675884e-05 Training loss: 6.962007999420166
2025-12-09 11:58:27.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 9.810058550983254e-05 Training loss: 6.504824161529541
2025-12-09 11:58:27.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 9.809226250827371e-05 Training loss: 6.382596492767334
2025-12-09 11:58:28.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 9.808392166599948e-05 Training loss: 5.751303195953369
2025-12-09 11:58:28.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 9.807556298610404e-05 Training loss: 6.765973091125488
2025-12-09 11:58:28.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 9.806718647168818e-05 Training loss: 6.856298923492432
2025-12-09 11:58:28.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 9.805879212585933e-05 Training loss: 6.979004859924316
2025-12-09 11:58:28.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 9.805037995173155e-05 Training loss: 6.982579231262207
2025-12-09 11:58:28.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 9.804194995242548e-05 Training loss: 6.674861907958984
2025-12-09 11:58:28.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 9.803350213106836e-05 Training loss: 6.944133758544922
2025-12-09 11:58:29.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 9.802503649079411e-05 Training loss: 6.713926792144775
2025-12-09 11:58:29.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 9.801655303474318e-05 Training loss: 6.68861198425293
2025-12-09 11:58:29.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 9.80080517660627e-05 Training loss: 7.154351711273193
2025-12-09 11:58:29.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 9.799953268790633e-05 Training loss: 6.516645431518555
2025-12-09 11:58:29.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 9.799099580343441e-05 Training loss: 6.924363136291504
2025-12-09 11:58:29.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 9.798244111581382e-05 Training loss: 7.007630825042725
2025-12-09 11:58:29.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 9.797386862821813e-05 Training loss: 6.631745338439941
2025-12-09 11:58:29.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 9.796527834382745e-05 Training loss: 6.699329376220703
2025-12-09 11:58:30.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 9.795667026582847e-05 Training loss: 6.564159870147705
2025-12-09 11:58:30.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 9.794804439741456e-05 Training loss: 6.866763591766357
2025-12-09 11:58:30.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 9.79394007417856e-05 Training loss: 7.317873477935791
2025-12-09 11:58:30.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 9.793073930214817e-05 Training loss: 6.712386131286621
2025-12-09 11:58:30.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 9.792206008171533e-05 Training loss: 6.679441928863525
2025-12-09 11:58:30.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 9.791336308370687e-05 Training loss: 6.876049995422363
2025-12-09 11:58:30.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 9.790464831134903e-05 Training loss: 6.208228588104248
2025-12-09 11:58:31.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 9.789591576787476e-05 Training loss: 7.009117603302002
2025-12-09 11:58:31.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 9.788716545652353e-05 Training loss: 6.870438098907471
2025-12-09 11:58:31.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 9.787839738054146e-05 Training loss: 5.722658157348633
2025-12-09 11:58:31.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 9.786961154318121e-05 Training loss: 6.7608866691589355
2025-12-09 11:58:31.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 9.786080794770207e-05 Training loss: 6.514794826507568
2025-12-09 11:58:31.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 9.785198659736988e-05 Training loss: 6.5366387367248535
2025-12-09 11:58:31.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 9.784314749545707e-05 Training loss: 6.8431196212768555
2025-12-09 11:58:31.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 9.78342906452427e-05 Training loss: 6.74320125579834
2025-12-09 11:58:32.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 9.782541605001235e-05 Training loss: 7.152538299560547
2025-12-09 11:58:32.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 9.781652371305824e-05 Training loss: 5.9849467277526855
2025-12-09 11:58:32.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 9.780761363767914e-05 Training loss: 7.119777679443359
2025-12-09 11:58:32.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 9.779868582718041e-05 Training loss: 6.619010925292969
2025-12-09 11:58:32.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 9.778974028487398e-05 Training loss: 6.696942329406738
2025-12-09 11:58:32.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 9.778077701407837e-05 Training loss: 6.492534637451172
2025-12-09 11:58:32.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 9.777179601811867e-05 Training loss: 7.589662075042725
2025-12-09 11:58:33.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 9.776279730032654e-05 Training loss: 6.367037773132324
2025-12-09 11:58:33.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 9.775378086404023e-05 Training loss: 6.982028961181641
2025-12-09 11:58:33.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 9.774474671260457e-05 Training loss: 6.908581733703613
2025-12-09 11:58:33.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 9.77356948493709e-05 Training loss: 6.830528736114502
2025-12-09 11:58:33.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 9.77266252776972e-05 Training loss: 7.529296398162842
2025-12-09 11:58:33.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 9.771753800094803e-05 Training loss: 6.503760814666748
2025-12-09 11:58:33.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 9.770843302249443e-05 Training loss: 6.979055404663086
2025-12-09 11:58:33.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 9.769931034571408e-05 Training loss: 6.372653007507324
2025-12-09 11:58:34.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 9.76901699739912e-05 Training loss: 6.991969108581543
2025-12-09 11:58:34.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 9.768101191071661e-05 Training loss: 6.837868690490723
2025-12-09 11:58:34.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 9.767183615928765e-05 Training loss: 6.751780033111572
2025-12-09 11:58:34.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 9.766264272310822e-05 Training loss: 6.437783241271973
2025-12-09 11:58:34.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 9.765343160558879e-05 Training loss: 6.424837589263916
2025-12-09 11:58:34.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 9.764420281014642e-05 Training loss: 5.715467929840088
2025-12-09 11:58:34.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 9.763495634020467e-05 Training loss: 6.872989654541016
2025-12-09 11:58:35.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 9.762569219919372e-05 Training loss: 6.702643394470215
2025-12-09 11:58:35.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 9.761641039055026e-05 Training loss: 6.520061492919922
2025-12-09 11:58:35.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 9.760711091771755e-05 Training loss: 6.807140827178955
2025-12-09 11:58:35.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 9.759779378414542e-05 Training loss: 6.95611047744751
2025-12-09 11:58:35.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 9.758845899329021e-05 Training loss: 7.098390579223633
2025-12-09 11:58:35.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 9.757910654861483e-05 Training loss: 6.87119722366333
2025-12-09 11:58:35.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 9.756973645358876e-05 Training loss: 6.516587257385254
2025-12-09 11:58:35.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 9.7560348711688e-05 Training loss: 6.703334808349609
2025-12-09 11:58:36.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 9.755094332639512e-05 Training loss: 7.160772800445557
2025-12-09 11:58:36.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 9.754152030119921e-05 Training loss: 6.631410121917725
2025-12-09 11:58:36.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 9.75320796395959e-05 Training loss: 6.677167892456055
2025-12-09 11:58:36.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 9.752262134508742e-05 Training loss: 5.692044258117676
2025-12-09 11:58:36.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 9.751314542118246e-05 Training loss: 6.76406717300415
2025-12-09 11:58:36.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 9.750365187139632e-05 Training loss: 6.753148555755615
2025-12-09 11:58:36.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 9.749414069925078e-05 Training loss: 6.816843032836914
2025-12-09 11:58:37.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 9.74846119082742e-05 Training loss: 6.472335338592529
2025-12-09 11:58:37.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 9.747506550200146e-05 Training loss: 6.6498589515686035
2025-12-09 11:58:37.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 9.746550148397398e-05 Training loss: 7.209900856018066
2025-12-09 11:58:37.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 9.745591985773971e-05 Training loss: 7.070281028747559
2025-12-09 11:58:37.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 9.744632062685311e-05 Training loss: 6.873435974121094
2025-12-09 11:58:37.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 9.743670379487522e-05 Training loss: 6.667647838592529
2025-12-09 11:58:37.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 9.742706936537358e-05 Training loss: 7.135715007781982
2025-12-09 11:58:37.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 9.741741734192224e-05 Training loss: 6.843263149261475
2025-12-09 11:58:38.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 9.740774772810182e-05 Training loss: 6.8653669357299805
2025-12-09 11:58:38.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 9.739806052749943e-05 Training loss: 7.3989386558532715
2025-12-09 11:58:38.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 9.738835574370871e-05 Training loss: 6.671631336212158
2025-12-09 11:58:38.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 9.737863338032985e-05 Training loss: 6.887812614440918
2025-12-09 11:58:38.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 9.736889344096952e-05 Training loss: 5.561546325683594
2025-12-09 11:58:38.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 9.735913592924093e-05 Training loss: 6.182898044586182
2025-12-09 11:58:38.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 9.734936084876383e-05 Training loss: 6.693870544433594
2025-12-09 11:58:39.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 9.733956820316444e-05 Training loss: 6.984302520751953
2025-12-09 11:58:39.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 9.732975799607555e-05 Training loss: 6.761662483215332
2025-12-09 11:58:39.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 9.731993023113642e-05 Training loss: 7.061807155609131
2025-12-09 11:58:39.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 9.731008491199284e-05 Training loss: 6.682724475860596
2025-12-09 11:58:39.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 9.730022204229714e-05 Training loss: 6.593185901641846
2025-12-09 11:58:39.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 9.729034162570811e-05 Training loss: 6.700758457183838
2025-12-09 11:58:39.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 9.728044366589108e-05 Training loss: 6.698999881744385
2025-12-09 11:58:39.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 9.727052816651788e-05 Training loss: 6.805421352386475
2025-12-09 11:58:40.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 9.726059513126685e-05 Training loss: 6.035464286804199
2025-12-09 11:58:40.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 9.725064456382283e-05 Training loss: 7.1326727867126465
2025-12-09 11:58:40.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 9.724067646787717e-05 Training loss: 6.645845890045166
2025-12-09 11:58:40.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 9.723069084712772e-05 Training loss: 6.35807991027832
2025-12-09 11:58:40.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 9.722068770527883e-05 Training loss: 6.671358108520508
2025-12-09 11:58:40.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 9.721066704604134e-05 Training loss: 6.900519847869873
2025-12-09 11:58:40.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 9.720062887313261e-05 Training loss: 6.683053493499756
2025-12-09 11:58:41.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 9.71905731902765e-05 Training loss: 6.6826491355896
2025-12-09 11:58:41.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 9.718050000120334e-05 Training loss: 6.6714959144592285
2025-12-09 11:58:41.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 9.717040930964995e-05 Training loss: 6.70261287689209
2025-12-09 11:58:41.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 9.716030111935967e-05 Training loss: 6.703032493591309
2025-12-09 11:58:41.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 9.715017543408233e-05 Training loss: 7.437222003936768
2025-12-09 11:58:41.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 9.714003225757424e-05 Training loss: 7.342932224273682
2025-12-09 11:58:41.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 9.712987159359818e-05 Training loss: 6.440367698669434
2025-12-09 11:58:41.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 9.711969344592346e-05 Training loss: 6.536821365356445
2025-12-09 11:58:42.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 9.710949781832585e-05 Training loss: 6.580696105957031
2025-12-09 11:58:42.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 9.709928471458759e-05 Training loss: 7.221097946166992
2025-12-09 11:58:42.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 9.708905413849743e-05 Training loss: 6.471004009246826
2025-12-09 11:58:42.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 9.707880609385059e-05 Training loss: 6.932465553283691
2025-12-09 11:58:42.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 9.706854058444876e-05 Training loss: 6.7544660568237305
2025-12-09 11:58:42.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 9.705825761410014e-05 Training loss: 6.290647506713867
2025-12-09 11:58:42.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 9.704795718661939e-05 Training loss: 6.579319953918457
2025-12-09 11:58:43.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 9.703763930582761e-05 Training loss: 6.751395225524902
2025-12-09 11:58:43.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 9.702730397555247e-05 Training loss: 7.1127519607543945
2025-12-09 11:58:43.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 9.7016951199628e-05 Training loss: 6.697425365447998
2025-12-09 11:58:43.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 9.700658098189475e-05 Training loss: 7.026671886444092
2025-12-09 11:58:43.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 9.69961933261998e-05 Training loss: 6.724151134490967
2025-12-09 11:58:43.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 9.698578823639659e-05 Training loss: 6.697884559631348
2025-12-09 11:58:43.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 9.697536571634509e-05 Training loss: 6.724366188049316
2025-12-09 11:58:43.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 9.696492576991174e-05 Training loss: 6.921086311340332
2025-12-09 11:58:44.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 9.695446840096944e-05 Training loss: 6.786365509033203
2025-12-09 11:58:44.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 9.694399361339752e-05 Training loss: 6.901047229766846
2025-12-09 11:58:44.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 9.693350141108182e-05 Training loss: 6.753592014312744
2025-12-09 11:58:44.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 9.692299179791459e-05 Training loss: 6.497702598571777
2025-12-09 11:58:44.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 9.69124647777946e-05 Training loss: 7.245876789093018
2025-12-09 11:58:44.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 9.690192035462702e-05 Training loss: 7.310575008392334
2025-12-09 11:58:44.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 9.689135853232349e-05 Training loss: 6.710145473480225
2025-12-09 11:58:45.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 9.688077931480212e-05 Training loss: 6.765429496765137
2025-12-09 11:58:45.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 9.687018270598749e-05 Training loss: 7.647744655609131
2025-12-09 11:58:45.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 9.685956870981058e-05 Training loss: 6.33074426651001
2025-12-09 11:58:45.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 9.684893733020888e-05 Training loss: 7.040675163269043
2025-12-09 11:58:45.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 9.683828857112627e-05 Training loss: 6.341652870178223
2025-12-09 11:58:45.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 9.682762243651308e-05 Training loss: 6.763451099395752
2025-12-09 11:58:45.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 9.681693893032618e-05 Training loss: 7.440788269042969
2025-12-09 11:58:45.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 9.680623805652876e-05 Training loss: 6.551554203033447
2025-12-09 11:58:46.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 9.679551981909053e-05 Training loss: 6.743480682373047
2025-12-09 11:58:46.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 9.67847842219876e-05 Training loss: 6.352529048919678
2025-12-09 11:58:46.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 9.677403126920256e-05 Training loss: 6.533418655395508
2025-12-09 11:58:46.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 9.676326096472441e-05 Training loss: 6.148078918457031
2025-12-09 11:58:46.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 9.675247331254858e-05 Training loss: 6.525641441345215
2025-12-09 11:58:46.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 9.674166831667697e-05 Training loss: 6.286558151245117
2025-12-09 11:58:46.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 9.673084598111789e-05 Training loss: 6.643375396728516
2025-12-09 11:58:47.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 9.672000630988605e-05 Training loss: 6.721625328063965
2025-12-09 11:58:47.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 9.670914930700267e-05 Training loss: 6.87216854095459
2025-12-09 11:58:47.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 9.669827497649536e-05 Training loss: 6.382967472076416
2025-12-09 11:58:47.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 9.668738332239813e-05 Training loss: 6.4672160148620605
2025-12-09 11:58:47.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 9.667647434875145e-05 Training loss: 6.32840633392334
2025-12-09 11:58:47.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 9.66655480596022e-05 Training loss: 6.788444519042969
2025-12-09 11:58:47.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 9.665460445900368e-05 Training loss: 6.835482120513916
2025-12-09 11:58:47.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 9.664364355101565e-05 Training loss: 6.814599990844727
2025-12-09 11:58:48.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 9.663266533970424e-05 Training loss: 6.767668724060059
2025-12-09 11:58:48.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 9.662166982914203e-05 Training loss: 6.876009941101074
2025-12-09 11:58:48.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 9.661065702340801e-05 Training loss: 6.66388463973999
2025-12-09 11:58:48.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 9.659962692658758e-05 Training loss: 6.7456183433532715
2025-12-09 11:58:48.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 9.658857954277254e-05 Training loss: 6.428355693817139
2025-12-09 11:58:48.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 9.657751487606115e-05 Training loss: 6.497432231903076
2025-12-09 11:58:48.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 9.656643293055804e-05 Training loss: 6.81856632232666
2025-12-09 11:58:49.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 9.655533371037426e-05 Training loss: 7.335843563079834
2025-12-09 11:58:49.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 9.65442172196273e-05 Training loss: 6.729017734527588
2025-12-09 11:58:49.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 9.653308346244098e-05 Training loss: 6.623913764953613
2025-12-09 11:58:49.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 9.652193244294562e-05 Training loss: 7.500526428222656
2025-12-09 11:58:49.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 9.651076416527787e-05 Training loss: 6.596665859222412
2025-12-09 11:58:49.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 9.64995786335808e-05 Training loss: 6.621098041534424
2025-12-09 11:58:49.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 9.648837585200393e-05 Training loss: 6.800987243652344
2025-12-09 11:58:50.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 9.64771558247031e-05 Training loss: 6.792128562927246
2025-12-09 11:58:50.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 9.64659185558406e-05 Training loss: 6.625694751739502
2025-12-09 11:58:50.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 9.64546640495851e-05 Training loss: 6.761484622955322
2025-12-09 11:58:50.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 9.644339231011168e-05 Training loss: 6.613680362701416
2025-12-09 11:58:50.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 9.643210334160177e-05 Training loss: 6.399632930755615
2025-12-09 11:58:50.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 9.642079714824328e-05 Training loss: 6.523263931274414
2025-12-09 11:58:50.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 9.64094737342304e-05 Training loss: 6.756138324737549
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.37 GiB is free. Including non-PyTorch memory, this process has 90.85 GiB memory in use. Of the allocated memory 89.88 GiB is allocated by PyTorch, and 213.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:08, 145.23it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 175.36it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:21, 121.98it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:10, 141.19it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:57, 173.22it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 213.27it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:53, 184.96it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 184.05it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 197.01it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 201.91it/s]Tokenizing texts:   2%|▏         | 241/10000 [00:01<00:47, 204.80it/s]Tokenizing texts:   3%|▎         | 262/10000 [00:01<00:47, 204.67it/s]Tokenizing texts:   3%|▎         | 291/10000 [00:01<00:42, 227.81it/s]Tokenizing texts:   3%|▎         | 315/10000 [00:01<00:42, 226.67it/s]Tokenizing texts:   3%|▎         | 338/10000 [00:01<00:53, 181.85it/s]Tokenizing texts:   4%|▎         | 363/10000 [00:01<00:48, 197.76it/s]Tokenizing texts:   4%|▍         | 385/10000 [00:02<00:52, 184.04it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 185.46it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:51, 184.89it/s]Tokenizing texts:   4%|▍         | 446/10000 [00:02<00:51, 186.39it/s]Tokenizing texts:   5%|▍         | 468/10000 [00:02<00:48, 194.72it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 216.85it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:40, 235.72it/s]Tokenizing texts:   6%|▌         | 550/10000 [00:02<00:51, 184.80it/s]Tokenizing texts:   6%|▌         | 579/10000 [00:02<00:44, 209.99it/s]Tokenizing texts:   6%|▌         | 603/10000 [00:03<00:44, 209.40it/s]Tokenizing texts:   6%|▋         | 626/10000 [00:03<00:47, 199.32it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 200.81it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 198.80it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 190.30it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 199.46it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 199.10it/s]Tokenizing texts:   8%|▊         | 766/10000 [00:03<00:43, 209.89it/s]Tokenizing texts:   8%|▊         | 788/10000 [00:04<00:45, 201.99it/s]Tokenizing texts:   8%|▊         | 812/10000 [00:04<00:43, 212.10it/s]Tokenizing texts:   8%|▊         | 834/10000 [00:04<00:43, 209.45it/s]Tokenizing texts:   9%|▊         | 859/10000 [00:04<00:41, 220.52it/s]Tokenizing texts:   9%|▉         | 882/10000 [00:04<00:42, 214.25it/s]Tokenizing texts:   9%|▉         | 910/10000 [00:04<00:39, 229.53it/s]Tokenizing texts:   9%|▉         | 940/10000 [00:04<00:36, 248.51it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:04<00:33, 266.78it/s]Tokenizing texts:  10%|█         | 1001/10000 [00:04<00:33, 270.79it/s]Tokenizing texts:  10%|█         | 1032/10000 [00:04<00:31, 281.80it/s]Tokenizing texts:  11%|█         | 1061/10000 [00:05<00:39, 224.09it/s]Tokenizing texts:  11%|█         | 1086/10000 [00:05<00:39, 224.89it/s]Tokenizing texts:  11%|█         | 1110/10000 [00:05<00:39, 227.18it/s]Tokenizing texts:  11%|█▏        | 1134/10000 [00:05<00:45, 193.16it/s]Tokenizing texts:  12%|█▏        | 1159/10000 [00:05<00:43, 204.29it/s]Tokenizing texts:  12%|█▏        | 1184/10000 [00:05<00:40, 215.20it/s]Tokenizing texts:  12%|█▏        | 1207/10000 [00:05<00:43, 202.20it/s]Tokenizing texts:  12%|█▏        | 1235/10000 [00:06<00:41, 209.99it/s]Tokenizing texts:  13%|█▎        | 1257/10000 [00:06<00:42, 204.01it/s]Tokenizing texts:  13%|█▎        | 1291/10000 [00:06<00:36, 239.05it/s]Tokenizing texts:  13%|█▎        | 1316/10000 [00:06<00:37, 234.01it/s]Tokenizing texts:  13%|█▎        | 1340/10000 [00:06<00:42, 201.66it/s]Tokenizing texts:  14%|█▎        | 1368/10000 [00:06<00:39, 218.63it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:40, 214.05it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:37, 231.38it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:34, 248.76it/s]Tokenizing texts:  15%|█▍        | 1487/10000 [00:07<00:31, 270.90it/s]Tokenizing texts:  15%|█▌        | 1515/10000 [00:07<00:32, 265.12it/s]Tokenizing texts:  15%|█▌        | 1543/10000 [00:07<00:33, 250.01it/s]Tokenizing texts:  16%|█▌        | 1575/10000 [00:07<00:31, 267.65it/s]Tokenizing texts:  16%|█▌        | 1603/10000 [00:07<00:33, 248.48it/s]Tokenizing texts:  16%|█▋        | 1629/10000 [00:07<00:40, 208.23it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 214.88it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 215.76it/s]Tokenizing texts:  17%|█▋        | 1703/10000 [00:08<00:37, 219.13it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:33, 243.37it/s]Tokenizing texts:  18%|█▊        | 1760/10000 [00:08<00:33, 243.82it/s]Tokenizing texts:  18%|█▊        | 1786/10000 [00:08<00:33, 245.01it/s]Tokenizing texts:  18%|█▊        | 1820/10000 [00:08<00:30, 271.25it/s]Tokenizing texts:  18%|█▊        | 1848/10000 [00:08<00:37, 217.39it/s]Tokenizing texts:  19%|█▊        | 1872/10000 [00:08<00:36, 220.10it/s]Tokenizing texts:  19%|█▉        | 1896/10000 [00:08<00:36, 224.34it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:08<00:35, 224.73it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 194.12it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:38, 209.21it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:35, 226.37it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 232.21it/s]Tokenizing texts:  21%|██        | 2055/10000 [00:09<00:32, 246.24it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:38, 207.15it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:38, 207.77it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:36, 212.77it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:10<00:38, 203.42it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:35, 221.07it/s]Tokenizing texts:  22%|██▏       | 2213/10000 [00:10<00:31, 250.18it/s]Tokenizing texts:  22%|██▏       | 2242/10000 [00:10<00:29, 258.82it/s]Tokenizing texts:  23%|██▎       | 2271/10000 [00:10<00:28, 266.77it/s]Tokenizing texts:  23%|██▎       | 2299/10000 [00:10<00:29, 258.42it/s]Tokenizing texts:  23%|██▎       | 2326/10000 [00:10<00:35, 219.05it/s]Tokenizing texts:  24%|██▎       | 2354/10000 [00:10<00:33, 231.32it/s]Tokenizing texts:  24%|██▍       | 2389/10000 [00:10<00:29, 257.56it/s]Tokenizing texts:  24%|██▍       | 2418/10000 [00:11<00:28, 264.88it/s]Tokenizing texts:  24%|██▍       | 2446/10000 [00:11<00:30, 245.10it/s]Tokenizing texts:  25%|██▍       | 2475/10000 [00:11<00:29, 255.35it/s]Tokenizing texts:  25%|██▌       | 2502/10000 [00:11<00:30, 242.69it/s]Tokenizing texts:  25%|██▌       | 2534/10000 [00:11<00:28, 262.54it/s]Tokenizing texts:  26%|██▌       | 2561/10000 [00:11<00:32, 227.24it/s]Tokenizing texts:  26%|██▌       | 2585/10000 [00:11<00:33, 218.51it/s]Tokenizing texts:  26%|██▌       | 2608/10000 [00:11<00:34, 211.25it/s]Tokenizing texts:  26%|██▋       | 2630/10000 [00:12<00:34, 211.67it/s]Tokenizing texts:  27%|██▋       | 2660/10000 [00:12<00:31, 233.21it/s]Tokenizing texts:  27%|██▋       | 2684/10000 [00:12<00:34, 211.01it/s]Tokenizing texts:  27%|██▋       | 2706/10000 [00:12<00:36, 198.66it/s]Tokenizing texts:  27%|██▋       | 2738/10000 [00:12<00:31, 228.96it/s]Tokenizing texts:  28%|██▊       | 2768/10000 [00:12<00:29, 247.61it/s]Tokenizing texts:  28%|██▊       | 2794/10000 [00:12<00:39, 183.24it/s]Tokenizing texts:  28%|██▊       | 2827/10000 [00:12<00:33, 213.38it/s]Tokenizing texts:  29%|██▊       | 2852/10000 [00:13<00:33, 211.64it/s]Tokenizing texts:  29%|██▉       | 2876/10000 [00:13<00:33, 211.77it/s]Tokenizing texts:  29%|██▉       | 2903/10000 [00:13<00:31, 225.11it/s]Tokenizing texts:  29%|██▉       | 2935/10000 [00:13<00:28, 247.69it/s]Tokenizing texts:  30%|██▉       | 2961/10000 [00:13<00:30, 231.17it/s]Tokenizing texts:  30%|██▉       | 2986/10000 [00:13<00:30, 232.76it/s]Tokenizing texts:  30%|███       | 3021/10000 [00:13<00:26, 262.48it/s]Tokenizing texts:  30%|███       | 3048/10000 [00:13<00:30, 229.03it/s]Tokenizing texts:  31%|███       | 3081/10000 [00:13<00:27, 250.36it/s]Tokenizing texts:  31%|███       | 3108/10000 [00:14<00:27, 249.44it/s]Tokenizing texts:  31%|███▏      | 3139/10000 [00:14<00:26, 260.57it/s]Tokenizing texts:  32%|███▏      | 3166/10000 [00:14<00:26, 253.32it/s]Tokenizing texts:  32%|███▏      | 3196/10000 [00:14<00:25, 266.00it/s]Tokenizing texts:  32%|███▏      | 3224/10000 [00:14<00:25, 269.41it/s]Tokenizing texts:  33%|███▎      | 3252/10000 [00:14<00:26, 256.41it/s]Tokenizing texts:  33%|███▎      | 3281/10000 [00:14<00:25, 264.78it/s]Tokenizing texts:  33%|███▎      | 3308/10000 [00:14<00:25, 260.96it/s]Tokenizing texts:  33%|███▎      | 3335/10000 [00:14<00:26, 247.57it/s]Tokenizing texts:  34%|███▎      | 3365/10000 [00:15<00:25, 257.87it/s]Tokenizing texts:  34%|███▍      | 3392/10000 [00:15<00:26, 253.11it/s]Tokenizing texts:  34%|███▍      | 3420/10000 [00:15<00:25, 259.01it/s]Tokenizing texts:  35%|███▍      | 3460/10000 [00:15<00:22, 295.24it/s]Tokenizing texts:  35%|███▍      | 3490/10000 [00:15<00:23, 277.78it/s]Tokenizing texts:  35%|███▌      | 3519/10000 [00:15<00:26, 249.14it/s]Tokenizing texts:  35%|███▌      | 3545/10000 [00:15<00:27, 232.01it/s]Tokenizing texts:  36%|███▌      | 3574/10000 [00:15<00:26, 244.90it/s]Tokenizing texts:  36%|███▌      | 3602/10000 [00:16<00:25, 249.49it/s]Tokenizing texts:  36%|███▋      | 3632/10000 [00:16<00:24, 262.39it/s]Tokenizing texts:  37%|███▋      | 3659/10000 [00:16<00:25, 252.11it/s]Tokenizing texts:  37%|███▋      | 3685/10000 [00:16<00:25, 250.54it/s]Tokenizing texts:  37%|███▋      | 3711/10000 [00:16<00:24, 251.68it/s]Tokenizing texts:  37%|███▋      | 3737/10000 [00:16<00:25, 249.12it/s]Tokenizing texts:  38%|███▊      | 3763/10000 [00:16<00:27, 223.94it/s]Tokenizing texts:  38%|███▊      | 3787/10000 [00:16<00:27, 228.05it/s]Tokenizing texts:  38%|███▊      | 3811/10000 [00:16<00:29, 207.44it/s]Tokenizing texts:  38%|███▊      | 3842/10000 [00:17<00:26, 229.19it/s]Tokenizing texts:  39%|███▊      | 3870/10000 [00:17<00:25, 239.21it/s]Tokenizing texts:  39%|███▉      | 3896/10000 [00:17<00:25, 237.05it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:25, 237.04it/s]Tokenizing texts:  39%|███▉      | 3945/10000 [00:17<00:26, 230.38it/s]Tokenizing texts:  40%|███▉      | 3976/10000 [00:17<00:23, 251.84it/s]Tokenizing texts:  40%|████      | 4002/10000 [00:17<00:26, 224.92it/s]Tokenizing texts:  40%|████      | 4030/10000 [00:17<00:25, 238.40it/s]Tokenizing texts:  41%|████      | 4056/10000 [00:17<00:24, 244.07it/s]Tokenizing texts:  41%|████      | 4089/10000 [00:18<00:22, 266.37it/s]Tokenizing texts:  41%|████      | 4121/10000 [00:18<00:21, 279.95it/s]Tokenizing texts:  42%|████▏     | 4150/10000 [00:18<00:20, 281.19it/s]Tokenizing texts:  42%|████▏     | 4179/10000 [00:18<00:21, 275.70it/s]Tokenizing texts:  42%|████▏     | 4214/10000 [00:18<00:19, 296.74it/s]Tokenizing texts:  42%|████▏     | 4244/10000 [00:18<00:24, 235.14it/s]Tokenizing texts:  43%|████▎     | 4270/10000 [00:18<00:24, 236.38it/s]Tokenizing texts:  43%|████▎     | 4297/10000 [00:18<00:23, 242.12it/s]Tokenizing texts:  43%|████▎     | 4334/10000 [00:18<00:20, 274.09it/s]Tokenizing texts:  44%|████▎     | 4363/10000 [00:19<00:20, 275.16it/s]Tokenizing texts:  44%|████▍     | 4393/10000 [00:19<00:20, 280.31it/s]Tokenizing texts:  44%|████▍     | 4425/10000 [00:19<00:19, 290.36it/s]Tokenizing texts:  45%|████▍     | 4455/10000 [00:19<00:21, 259.74it/s]Tokenizing texts:  45%|████▍     | 4482/10000 [00:19<00:21, 261.15it/s]Tokenizing texts:  45%|████▌     | 4509/10000 [00:19<00:22, 243.17it/s]Tokenizing texts:  45%|████▌     | 4534/10000 [00:19<00:22, 240.73it/s]Tokenizing texts:  46%|████▌     | 4564/10000 [00:19<00:21, 255.92it/s]Tokenizing texts:  46%|████▌     | 4591/10000 [00:19<00:21, 250.24it/s]Tokenizing texts:  46%|████▌     | 4617/10000 [00:20<00:23, 233.51it/s]Tokenizing texts:  47%|████▋     | 4659/10000 [00:20<00:18, 282.83it/s]Tokenizing texts:  47%|████▋     | 4691/10000 [00:20<00:18, 292.89it/s]Tokenizing texts:  47%|████▋     | 4728/10000 [00:20<00:16, 312.07it/s]Tokenizing texts:  48%|████▊     | 4760/10000 [00:20<00:18, 277.09it/s]Tokenizing texts:  48%|████▊     | 4789/10000 [00:20<00:20, 250.21it/s]Tokenizing texts:  48%|████▊     | 4816/10000 [00:20<00:20, 254.77it/s]Tokenizing texts:  48%|████▊     | 4849/10000 [00:20<00:18, 273.96it/s]Tokenizing texts:  49%|████▉     | 4878/10000 [00:20<00:19, 264.42it/s]Tokenizing texts:  49%|████▉     | 4910/10000 [00:21<00:18, 270.68it/s]Tokenizing texts:  49%|████▉     | 4940/10000 [00:21<00:18, 276.47it/s]Tokenizing texts:  50%|████▉     | 4968/10000 [00:21<00:19, 260.89it/s]Tokenizing texts:  50%|█████     | 5006/10000 [00:21<00:17, 292.29it/s]Tokenizing texts:  50%|█████     | 5036/10000 [00:21<00:17, 277.02it/s]Tokenizing texts:  51%|█████     | 5065/10000 [00:21<00:19, 251.99it/s]Tokenizing texts:  51%|█████     | 5091/10000 [00:21<00:20, 235.92it/s]Tokenizing texts:  51%|█████     | 5120/10000 [00:21<00:19, 248.81it/s]Tokenizing texts:  51%|█████▏    | 5146/10000 [00:22<00:20, 235.61it/s]Tokenizing texts:  52%|█████▏    | 5175/10000 [00:22<00:19, 248.96it/s]Tokenizing texts:  52%|█████▏    | 5202/10000 [00:22<00:19, 244.01it/s]Tokenizing texts:  52%|█████▏    | 5227/10000 [00:22<00:19, 241.15it/s]Tokenizing texts:  53%|█████▎    | 5261/10000 [00:22<00:17, 265.48it/s]Tokenizing texts:  53%|█████▎    | 5288/10000 [00:22<00:19, 244.33it/s]Tokenizing texts:  53%|█████▎    | 5324/10000 [00:22<00:17, 274.70it/s]Tokenizing texts:  54%|█████▎    | 5358/10000 [00:22<00:16, 289.87it/s]Tokenizing texts:  54%|█████▍    | 5390/10000 [00:22<00:15, 292.47it/s]Tokenizing texts:  54%|█████▍    | 5423/10000 [00:23<00:15, 300.84it/s]Tokenizing texts:  55%|█████▍    | 5454/10000 [00:23<00:16, 272.32it/s]Tokenizing texts:  55%|█████▍    | 5482/10000 [00:23<00:17, 256.84it/s]Tokenizing texts:  55%|█████▌    | 5509/10000 [00:23<00:20, 217.15it/s]Tokenizing texts:  55%|█████▌    | 5541/10000 [00:23<00:18, 241.48it/s]Tokenizing texts:  56%|█████▌    | 5568/10000 [00:23<00:17, 247.80it/s]Tokenizing texts:  56%|█████▌    | 5594/10000 [00:23<00:17, 248.04it/s]Tokenizing texts:  56%|█████▋    | 5628/10000 [00:23<00:16, 271.99it/s]Tokenizing texts:  57%|█████▋    | 5666/10000 [00:23<00:14, 300.59it/s]Tokenizing texts:  57%|█████▋    | 5698/10000 [00:24<00:14, 305.63it/s]Tokenizing texts:  57%|█████▋    | 5730/10000 [00:24<00:14, 286.51it/s]Tokenizing texts:  58%|█████▊    | 5760/10000 [00:24<00:16, 255.30it/s]Tokenizing texts:  58%|█████▊    | 5787/10000 [00:24<00:19, 216.77it/s]Tokenizing texts:  58%|█████▊    | 5811/10000 [00:24<00:19, 216.57it/s]Tokenizing texts:  58%|█████▊    | 5834/10000 [00:24<00:19, 217.83it/s]Tokenizing texts:  59%|█████▊    | 5865/10000 [00:24<00:17, 241.51it/s]Tokenizing texts:  59%|█████▉    | 5891/10000 [00:24<00:16, 245.90it/s]Tokenizing texts:  59%|█████▉    | 5917/10000 [00:25<00:16, 245.46it/s]Tokenizing texts:  59%|█████▉    | 5943/10000 [00:25<00:17, 237.88it/s]Tokenizing texts:  60%|█████▉    | 5971/10000 [00:25<00:16, 248.93it/s]Tokenizing texts:  60%|█████▉    | 5997/10000 [00:25<00:16, 245.27it/s]Tokenizing texts:  60%|██████    | 6022/10000 [00:25<00:16, 242.15it/s]Tokenizing texts:  61%|██████    | 6054/10000 [00:25<00:15, 261.68it/s]Tokenizing texts:  61%|██████    | 6082/10000 [00:25<00:14, 265.97it/s]Tokenizing texts:  61%|██████    | 6109/10000 [00:25<00:15, 254.60it/s]Tokenizing texts:  61%|██████▏   | 6143/10000 [00:25<00:14, 274.68it/s]Tokenizing texts:  62%|██████▏   | 6171/10000 [00:26<00:13, 273.75it/s]Tokenizing texts:  62%|██████▏   | 6199/10000 [00:26<00:14, 270.02it/s]Tokenizing texts:  62%|██████▏   | 6227/10000 [00:26<00:15, 245.14it/s]Tokenizing texts:  63%|██████▎   | 6259/10000 [00:26<00:14, 263.36it/s]Tokenizing texts:  63%|██████▎   | 6286/10000 [00:26<00:14, 253.39it/s]Tokenizing texts:  63%|██████▎   | 6312/10000 [00:26<00:16, 227.25it/s]Tokenizing texts:  63%|██████▎   | 6336/10000 [00:26<00:16, 225.57it/s]Tokenizing texts:  64%|██████▎   | 6365/10000 [00:26<00:15, 240.99it/s]Tokenizing texts:  64%|██████▍   | 6390/10000 [00:26<00:14, 242.31it/s]Tokenizing texts:  64%|██████▍   | 6420/10000 [00:27<00:14, 254.79it/s]Tokenizing texts:  64%|██████▍   | 6447/10000 [00:27<00:13, 258.35it/s]Tokenizing texts:  65%|██████▍   | 6474/10000 [00:27<00:14, 245.55it/s]Tokenizing texts:  65%|██████▌   | 6501/10000 [00:27<00:13, 250.33it/s]Tokenizing texts:  65%|██████▌   | 6527/10000 [00:27<00:15, 224.62it/s]Tokenizing texts:  66%|██████▌   | 6567/10000 [00:27<00:12, 269.37it/s]Tokenizing texts:  66%|██████▌   | 6597/10000 [00:27<00:12, 276.82it/s]Tokenizing texts:  66%|██████▋   | 6626/10000 [00:27<00:12, 262.86it/s]Tokenizing texts:  67%|██████▋   | 6654/10000 [00:27<00:13, 256.88it/s]Tokenizing texts:  67%|██████▋   | 6683/10000 [00:28<00:12, 263.67it/s]Tokenizing texts:  67%|██████▋   | 6710/10000 [00:28<00:13, 247.34it/s]Tokenizing texts:  67%|██████▋   | 6736/10000 [00:28<00:13, 242.96it/s]Tokenizing texts:  68%|██████▊   | 6766/10000 [00:28<00:12, 258.48it/s]Tokenizing texts:  68%|██████▊   | 6801/10000 [00:28<00:11, 282.97it/s]Tokenizing texts:  68%|██████▊   | 6833/10000 [00:28<00:10, 293.18it/s]Tokenizing texts:  69%|██████▊   | 6865/10000 [00:28<00:10, 297.94it/s]Tokenizing texts:  69%|██████▉   | 6896/10000 [00:28<00:13, 234.23it/s]Tokenizing texts:  69%|██████▉   | 6922/10000 [00:29<00:13, 228.52it/s]Tokenizing texts:  69%|██████▉   | 6948/10000 [00:29<00:13, 231.45it/s]Tokenizing texts:  70%|██████▉   | 6977/10000 [00:29<00:12, 244.45it/s]Tokenizing texts:  70%|███████   | 7004/10000 [00:29<00:11, 250.76it/s]Tokenizing texts:  70%|███████   | 7030/10000 [00:29<00:12, 245.25it/s]Tokenizing texts:  71%|███████   | 7056/10000 [00:29<00:12, 235.11it/s]Tokenizing texts:  71%|███████   | 7080/10000 [00:29<00:13, 213.50it/s]Tokenizing texts:  71%|███████   | 7102/10000 [00:29<00:13, 211.24it/s]Tokenizing texts:  71%|███████▏  | 7133/10000 [00:29<00:12, 237.22it/s]Tokenizing texts:  72%|███████▏  | 7158/10000 [00:30<00:12, 234.44it/s]Tokenizing texts:  72%|███████▏  | 7188/10000 [00:30<00:11, 252.37it/s]Tokenizing texts:  72%|███████▏  | 7214/10000 [00:30<00:11, 246.00it/s]Tokenizing texts:  72%|███████▏  | 7242/10000 [00:30<00:10, 254.46it/s]Tokenizing texts:  73%|███████▎  | 7273/10000 [00:30<00:10, 268.87it/s]Tokenizing texts:  73%|███████▎  | 7308/10000 [00:30<00:09, 288.66it/s]Tokenizing texts:  73%|███████▎  | 7338/10000 [00:30<00:09, 280.65it/s]Tokenizing texts:  74%|███████▎  | 7367/10000 [00:30<00:09, 269.20it/s]Tokenizing texts:  74%|███████▍  | 7395/10000 [00:30<00:09, 271.46it/s]Tokenizing texts:  74%|███████▍  | 7423/10000 [00:31<00:11, 223.64it/s]Tokenizing texts:  74%|███████▍  | 7447/10000 [00:31<00:12, 205.36it/s]Tokenizing texts:  75%|███████▍  | 7478/10000 [00:31<00:11, 228.66it/s]Tokenizing texts:  75%|███████▌  | 7515/10000 [00:31<00:09, 262.33it/s]Tokenizing texts:  75%|███████▌  | 7543/10000 [00:31<00:09, 265.91it/s]Tokenizing texts:  76%|███████▌  | 7571/10000 [00:31<00:09, 260.25it/s]Tokenizing texts:  76%|███████▌  | 7610/10000 [00:31<00:08, 295.81it/s]Tokenizing texts:  76%|███████▋  | 7641/10000 [00:31<00:08, 269.72it/s]Tokenizing texts:  77%|███████▋  | 7674/10000 [00:31<00:08, 284.17it/s]Tokenizing texts:  77%|███████▋  | 7704/10000 [00:32<00:07, 287.47it/s]Tokenizing texts:  77%|███████▋  | 7734/10000 [00:32<00:09, 248.22it/s]Tokenizing texts:  78%|███████▊  | 7761/10000 [00:32<00:09, 244.05it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 257.68it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 237.12it/s]Tokenizing texts:  78%|███████▊  | 7845/10000 [00:32<00:08, 245.24it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 257.49it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 254.99it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:33<00:07, 283.48it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:33<00:07, 280.69it/s]Tokenizing texts:  80%|███████▉  | 7999/10000 [00:33<00:06, 291.84it/s]Tokenizing texts:  80%|████████  | 8029/10000 [00:33<00:06, 288.14it/s]Tokenizing texts:  81%|████████  | 8059/10000 [00:33<00:11, 171.23it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 199.75it/s]Tokenizing texts:  81%|████████  | 8120/10000 [00:33<00:09, 208.45it/s]Tokenizing texts:  81%|████████▏ | 8149/10000 [00:33<00:08, 225.64it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:34<00:07, 237.87it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:34<00:07, 248.98it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 213.40it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 223.73it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 257.15it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 252.82it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:34<00:06, 256.84it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:34<00:06, 256.27it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:35<00:06, 231.87it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:35<00:06, 257.53it/s]Tokenizing texts:  85%|████████▍ | 8472/10000 [00:35<00:05, 263.02it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:35<00:05, 277.76it/s]Tokenizing texts:  85%|████████▌ | 8533/10000 [00:35<00:05, 259.96it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:35<00:05, 266.32it/s]Tokenizing texts:  86%|████████▌ | 8590/10000 [00:35<00:05, 261.14it/s]Tokenizing texts:  86%|████████▌ | 8624/10000 [00:35<00:04, 282.57it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:35<00:04, 284.54it/s]Tokenizing texts:  87%|████████▋ | 8687/10000 [00:36<00:04, 294.41it/s]Tokenizing texts:  87%|████████▋ | 8723/10000 [00:36<00:04, 310.16it/s]Tokenizing texts:  88%|████████▊ | 8755/10000 [00:36<00:04, 280.60it/s]Tokenizing texts:  88%|████████▊ | 8785/10000 [00:36<00:04, 284.17it/s]Tokenizing texts:  88%|████████▊ | 8814/10000 [00:36<00:05, 217.09it/s]Tokenizing texts:  88%|████████▊ | 8839/10000 [00:36<00:05, 218.75it/s]Tokenizing texts:  89%|████████▊ | 8872/10000 [00:36<00:04, 244.86it/s]Tokenizing texts:  89%|████████▉ | 8911/10000 [00:36<00:03, 280.21it/s]Tokenizing texts:  89%|████████▉ | 8941/10000 [00:37<00:03, 282.03it/s]Tokenizing texts:  90%|████████▉ | 8971/10000 [00:37<00:03, 274.03it/s]Tokenizing texts:  90%|█████████ | 9000/10000 [00:37<00:04, 246.63it/s]Tokenizing texts:  90%|█████████ | 9030/10000 [00:37<00:03, 258.25it/s]Tokenizing texts:  91%|█████████ | 9057/10000 [00:37<00:03, 250.11it/s]Tokenizing texts:  91%|█████████ | 9085/10000 [00:37<00:03, 256.18it/s]Tokenizing texts:  91%|█████████ | 9115/10000 [00:37<00:03, 266.39it/s]Tokenizing texts:  91%|█████████▏| 9146/10000 [00:37<00:03, 274.81it/s]Tokenizing texts:  92%|█████████▏| 9175/10000 [00:37<00:02, 278.55it/s]Tokenizing texts:  92%|█████████▏| 9205/10000 [00:37<00:02, 282.48it/s]Tokenizing texts:  92%|█████████▏| 9234/10000 [00:38<00:02, 273.69it/s]Tokenizing texts:  93%|█████████▎| 9262/10000 [00:38<00:02, 273.68it/s]Tokenizing texts:  93%|█████████▎| 9290/10000 [00:38<00:02, 253.58it/s]Tokenizing texts:  93%|█████████▎| 9316/10000 [00:38<00:02, 231.36it/s]Tokenizing texts:  93%|█████████▎| 9348/10000 [00:38<00:02, 253.28it/s]Tokenizing texts:  94%|█████████▎| 9374/10000 [00:38<00:02, 219.56it/s]Tokenizing texts:  94%|█████████▍| 9403/10000 [00:38<00:02, 229.72it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:38<00:02, 229.43it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:39<00:02, 233.61it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:39<00:02, 242.65it/s]Tokenizing texts:  95%|█████████▌| 9504/10000 [00:39<00:02, 182.39it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 222.03it/s]Tokenizing texts:  96%|█████████▌| 9566/10000 [00:39<00:01, 228.57it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 232.75it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 230.93it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 233.62it/s]Tokenizing texts:  97%|█████████▋| 9673/10000 [00:40<00:01, 238.00it/s]Tokenizing texts:  97%|█████████▋| 9698/10000 [00:40<00:01, 235.60it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:40<00:01, 239.14it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 207.06it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 215.48it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 261.01it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 254.05it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 259.18it/s]Tokenizing texts:  99%|█████████▉| 9906/10000 [00:40<00:00, 283.99it/s]Tokenizing texts:  99%|█████████▉| 9936/10000 [00:41<00:00, 282.93it/s]Tokenizing texts: 100%|█████████▉| 9968/10000 [00:41<00:00, 291.18it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 242.39it/s]
2025-12-09 11:59:52.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 12.017276763916016
2025-12-09 11:59:52.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 12.014233589172363
2025-12-09 11:59:52.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 12.022150039672852
2025-12-09 11:59:53.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 12.003210067749023
2025-12-09 11:59:53.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 12.02867603302002
2025-12-09 11:59:53.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 11.990557670593262
2025-12-09 11:59:53.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 12.018766403198242
2025-12-09 11:59:53.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 11.999632835388184
2025-12-09 11:59:53.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 12.01817512512207
2025-12-09 11:59:53.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 12.012396812438965
2025-12-09 11:59:54.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 11.997751235961914
2025-12-09 11:59:54.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 11.989872932434082
2025-12-09 11:59:54.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 11.971329689025879
2025-12-09 11:59:54.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 11.95942211151123
2025-12-09 11:59:54.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 11.987467765808105
2025-12-09 11:59:54.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 11.9531888961792
2025-12-09 11:59:54.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 11.989813804626465
2025-12-09 11:59:54.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 11.925163269042969
2025-12-09 11:59:55.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 11.953319549560547
2025-12-09 11:59:55.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 11.888598442077637
2025-12-09 11:59:55.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 11.884206771850586
2025-12-09 11:59:55.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 11.958853721618652
2025-12-09 11:59:55.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 11.867239952087402
2025-12-09 11:59:55.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 11.815393447875977
2025-12-09 11:59:55.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 11.846590995788574
2025-12-09 11:59:55.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 11.830863952636719
2025-12-09 11:59:56.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 11.72628402709961
2025-12-09 11:59:56.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 11.796181678771973
2025-12-09 11:59:56.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 11.695066452026367
2025-12-09 11:59:56.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 11.724515914916992
2025-12-09 11:59:56.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 11.637164115905762
2025-12-09 11:59:56.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 11.628205299377441
2025-12-09 11:59:56.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 11.534814834594727
2025-12-09 11:59:57.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 11.605683326721191
2025-12-09 11:59:57.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 11.435103416442871
2025-12-09 11:59:57.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 11.39958667755127
2025-12-09 11:59:57.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 11.406482696533203
2025-12-09 11:59:57.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 11.315913200378418
2025-12-09 11:59:57.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 11.225587844848633
2025-12-09 11:59:57.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 11.308982849121094
2025-12-09 11:59:57.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 11.156816482543945
2025-12-09 11:59:58.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 11.023731231689453
2025-12-09 11:59:58.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 10.980108261108398
2025-12-09 11:59:58.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 10.802408218383789
2025-12-09 11:59:58.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 10.823062896728516
2025-12-09 11:59:58.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 10.800172805786133
2025-12-09 11:59:58.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 10.717365264892578
2025-12-09 11:59:58.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 10.689472198486328
2025-12-09 11:59:59.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 10.504203796386719
2025-12-09 11:59:59.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 10.550110816955566
2025-12-09 11:59:59.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 10.492928504943848
2025-12-09 11:59:59.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 10.329947471618652
2025-12-09 11:59:59.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 10.44078540802002
2025-12-09 11:59:59.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 10.120100021362305
2025-12-09 11:59:59.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 10.238626480102539
2025-12-09 11:59:59.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 10.031271934509277
2025-12-09 12:00:00.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 9.937215805053711
2025-12-09 12:00:00.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 10.180815696716309
2025-12-09 12:00:00.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 9.76114559173584
2025-12-09 12:00:00.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 9.801124572753906
2025-12-09 12:00:00.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 9.698225021362305
2025-12-09 12:00:00.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 9.512545585632324
2025-12-09 12:00:00.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 9.389293670654297
2025-12-09 12:00:00.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 9.380043983459473
2025-12-09 12:00:01.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 9.32768440246582
2025-12-09 12:00:01.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 9.210275650024414
2025-12-09 12:00:01.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 9.08169937133789
2025-12-09 12:00:01.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 9.50896167755127
2025-12-09 12:00:01.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 9.305347442626953
2025-12-09 12:00:01.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 8.940635681152344
2025-12-09 12:00:01.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 8.94219970703125
2025-12-09 12:00:02.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 8.775341033935547
2025-12-09 12:00:02.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 8.892553329467773
2025-12-09 12:00:02.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 8.82246208190918
2025-12-09 12:00:02.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 8.661765098571777
2025-12-09 12:00:02.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 8.507322311401367
2025-12-09 12:00:02.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 8.478124618530273
2025-12-09 12:00:02.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 8.307353019714355
2025-12-09 12:00:03.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 8.457712173461914
2025-12-09 12:00:03.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 8.137092590332031
2025-12-09 12:00:03.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 8.336108207702637
2025-12-09 12:00:03.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 8.147875785827637
2025-12-09 12:00:03.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 8.210332870483398
2025-12-09 12:00:03.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 7.9740424156188965
2025-12-09 12:00:03.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 7.976816654205322
2025-12-09 12:00:03.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 8.654913902282715
2025-12-09 12:00:04.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 8.027856826782227
2025-12-09 12:00:04.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 7.827661514282227
2025-12-09 12:00:04.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 8.17063045501709
2025-12-09 12:00:04.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 8.118560791015625
2025-12-09 12:00:04.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 8.113027572631836
2025-12-09 12:00:04.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 7.664613246917725
2025-12-09 12:00:04.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 7.601693630218506
2025-12-09 12:00:05.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 7.814143180847168
2025-12-09 12:00:05.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 7.749570846557617
2025-12-09 12:00:05.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 7.681154251098633
2025-12-09 12:00:05.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 7.375942707061768
2025-12-09 12:00:05.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 7.896535873413086
2025-12-09 12:00:05.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 8.019394874572754
2025-12-09 12:00:05.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 7.6782073974609375
2025-12-09 12:00:05.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999997217736103 Training loss: 7.822945594787598
2025-12-09 12:00:06.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029999988870945456 Training loss: 7.564026355743408
2025-12-09 12:00:06.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0002999997495963115 Training loss: 7.921164512634277
2025-12-09 12:00:06.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00029999955483798346 Training loss: 7.4398393630981445
2025-12-09 12:00:06.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0002999993044345427 Training loss: 7.5469231605529785
2025-12-09 12:00:06.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002999989983860821 Training loss: 7.6619343757629395
2025-12-09 12:00:06.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.00029999863669271526 Training loss: 7.573153972625732
2025-12-09 12:00:06.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0002999982193545762 Training loss: 7.601369380950928
2025-12-09 12:00:06.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0002999977463718199 Training loss: 7.776827812194824
2025-12-09 12:00:07.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.00029999721774462174 Training loss: 8.174052238464355
2025-12-09 12:00:07.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029999663347317785 Training loss: 7.613710403442383
2025-12-09 12:00:07.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.00029999599355770497 Training loss: 8.035562515258789
2025-12-09 12:00:07.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0002999952979984405 Training loss: 7.46183443069458
2025-12-09 12:00:07.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.00029999454679564244 Training loss: 7.538503646850586
2025-12-09 12:00:07.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0002999937399495895 Training loss: 7.684062480926514
2025-12-09 12:00:07.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00029999287746058093 Training loss: 7.67542839050293
2025-12-09 12:00:08.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029999195932893676 Training loss: 7.487673282623291
2025-12-09 12:00:08.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.00029999098555499756 Training loss: 8.365485191345215
2025-12-09 12:00:08.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002999899561391246 Training loss: 7.410362243652344
2025-12-09 12:00:08.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.00029998887108169967 Training loss: 7.692431926727295
2025-12-09 12:00:08.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0002999877303831254 Training loss: 7.746969223022461
2025-12-09 12:00:08.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00029998653404382487 Training loss: 7.446236610412598
2025-12-09 12:00:08.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.000299985282064242 Training loss: 8.1162691116333
2025-12-09 12:00:08.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00029998397444484104 Training loss: 8.049375534057617
2025-12-09 12:00:09.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002999826111861073 Training loss: 7.364995956420898
2025-12-09 12:00:09.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00029998119228854625 Training loss: 7.422381401062012
2025-12-09 12:00:09.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0002999797177526845 Training loss: 7.5230937004089355
2025-12-09 12:00:09.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.000299978187579069 Training loss: 7.672178745269775
2025-12-09 12:00:09.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0002999766017682673 Training loss: 7.269942283630371
2025-12-09 12:00:09.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029997496032086775 Training loss: 7.003915786743164
2025-12-09 12:00:09.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00029997326323747927 Training loss: 7.512111186981201
2025-12-09 12:00:10.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0002999715105187314 Training loss: 7.581761837005615
2025-12-09 12:00:10.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.00029996970216527436 Training loss: 7.590974807739258
2025-12-09 12:00:10.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.000299967838177779 Training loss: 7.303343296051025
2025-12-09 12:00:10.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.00029996591855693686 Training loss: 8.074216842651367
2025-12-09 12:00:10.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.00029996394330345996 Training loss: 7.61617374420166
2025-12-09 12:00:10.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002999619124180811 Training loss: 7.564840793609619
2025-12-09 12:00:10.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00029995982590155367 Training loss: 7.4469170570373535
2025-12-09 12:00:10.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00029995768375465164 Training loss: 7.584579944610596
2025-12-09 12:00:11.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0002999554859781698 Training loss: 7.121623516082764
2025-12-09 12:00:11.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.00029995323257292337 Training loss: 7.221843719482422
2025-12-09 12:00:11.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0002999509235397483 Training loss: 7.283834457397461
2025-12-09 12:00:11.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.00029994855887950124 Training loss: 7.644946098327637
2025-12-09 12:00:11.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.00029994613859305933 Training loss: 7.821969985961914
2025-12-09 12:00:11.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0002999436626813204 Training loss: 7.353585243225098
2025-12-09 12:00:11.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.000299941131145203 Training loss: 7.4362053871154785
2025-12-09 12:00:11.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0002999385439856462 Training loss: 7.502919673919678
2025-12-09 12:00:12.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0002999359012036099 Training loss: 7.132528305053711
2025-12-09 12:00:12.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0002999332028000742 Training loss: 7.708968639373779
2025-12-09 12:00:12.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002999304487760404 Training loss: 7.287742614746094
2025-12-09 12:00:12.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.00029992763913253 Training loss: 7.491179943084717
2025-12-09 12:00:12.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00029992477387058537 Training loss: 7.314788341522217
2025-12-09 12:00:12.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0002999218529912694 Training loss: 7.25926399230957
2025-12-09 12:00:12.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00029991887649566564 Training loss: 7.271883964538574
2025-12-09 12:00:13.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.00029991584438487825 Training loss: 7.258403301239014
2025-12-09 12:00:13.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0002999127566600321 Training loss: 6.944456577301025
2025-12-09 12:00:13.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.00029990961332227264 Training loss: 7.257098197937012
2025-12-09 12:00:13.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002999064143727659 Training loss: 7.387748718261719
2025-12-09 12:00:13.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.00029990315981269863 Training loss: 7.1419291496276855
2025-12-09 12:00:13.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0002998998496432781 Training loss: 7.343527317047119
2025-12-09 12:00:13.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0002998964838657324 Training loss: 7.173925876617432
2025-12-09 12:00:13.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0002998930624813101 Training loss: 7.1696696281433105
2025-12-09 12:00:14.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.00029988958549128026 Training loss: 6.954655647277832
2025-12-09 12:00:14.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00029988605289693295 Training loss: 7.626846790313721
2025-12-09 12:00:14.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0002998824646995785 Training loss: 7.048863410949707
2025-12-09 12:00:14.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.00029987882090054817 Training loss: 7.113448143005371
2025-12-09 12:00:14.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0002998751215011935 Training loss: 7.375546455383301
2025-12-09 12:00:14.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.000299871366502887 Training loss: 7.182054042816162
2025-12-09 12:00:14.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00029986755590702164 Training loss: 6.888533592224121
2025-12-09 12:00:15.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.000299863689715011 Training loss: 7.062204360961914
2025-12-09 12:00:15.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0002998597679282893 Training loss: 6.34267520904541
2025-12-09 12:00:15.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00029985579054831146 Training loss: 7.292207717895508
2025-12-09 12:00:15.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0002998517575765528 Training loss: 7.690987586975098
2025-12-09 12:00:15.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00029984766901450965 Training loss: 7.744146823883057
2025-12-09 12:00:15.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.00029984352486369867 Training loss: 7.155710220336914
2025-12-09 12:00:15.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00029983932512565707 Training loss: 7.416845798492432
2025-12-09 12:00:15.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.00029983506980194296 Training loss: 7.458015441894531
2025-12-09 12:00:16.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00029983075889413493 Training loss: 6.910699844360352
2025-12-09 12:00:16.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00029982639240383214 Training loss: 6.702212810516357
2025-12-09 12:00:16.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.00029982197033265437 Training loss: 7.251434326171875
2025-12-09 12:00:16.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00029981749268224225 Training loss: 7.214992046356201
2025-12-09 12:00:16.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00029981295945425665 Training loss: 7.254419326782227
2025-12-09 12:00:16.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.00029980837065037935 Training loss: 7.155343055725098
2025-12-09 12:00:16.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.00029980372627231265 Training loss: 7.0749406814575195
2025-12-09 12:00:16.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00029979902632177945 Training loss: 6.921314716339111
2025-12-09 12:00:17.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0002997942708005233 Training loss: 7.144604206085205
2025-12-09 12:00:17.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.00029978945971030835 Training loss: 6.962082386016846
2025-12-09 12:00:17.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0002997845930529194 Training loss: 6.878061294555664
2025-12-09 12:00:17.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.00029977967083016173 Training loss: 7.077265739440918
2025-12-09 12:00:17.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00029977469304386133 Training loss: 7.145606994628906
2025-12-09 12:00:17.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0002997696596958649 Training loss: 7.3023881912231445
2025-12-09 12:00:17.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0002997645707880396 Training loss: 7.7349700927734375
2025-12-09 12:00:18.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0002997594263222733 Training loss: 7.084598064422607
2025-12-09 12:00:18.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.00029975422630047435 Training loss: 7.221497535705566
2025-12-09 12:00:18.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.00029974897072457187 Training loss: 6.852949619293213
2025-12-09 12:00:18.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0002997436595965154 Training loss: 6.9160590171813965
2025-12-09 12:00:18.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0002997382929182754 Training loss: 7.702733039855957
2025-12-09 12:00:18.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00029973287069184255 Training loss: 7.380812644958496
2025-12-09 12:00:18.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0002997273929192284 Training loss: 6.937784194946289
2025-12-09 12:00:19.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0002997218596024651 Training loss: 6.207913398742676
2025-12-09 12:00:19.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.00029971627074360516 Training loss: 7.223640441894531
2025-12-09 12:00:19.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00029971062634472203 Training loss: 5.986891269683838
2025-12-09 12:00:19.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00029970492640790956 Training loss: 7.947351932525635
2025-12-09 12:00:19.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0002996991709352822 Training loss: 7.175192356109619
2025-12-09 12:00:19.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0002996933599289751 Training loss: 7.190242290496826
2025-12-09 12:00:19.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.000299687493391144 Training loss: 7.3318190574646
2025-12-09 12:00:19.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00029968157132396507 Training loss: 7.395473480224609
2025-12-09 12:00:20.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00029967559372963534 Training loss: 6.940025806427002
2025-12-09 12:00:20.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00029966956061037227 Training loss: 7.727866172790527
2025-12-09 12:00:20.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.00029966347196841393 Training loss: 6.684452533721924
2025-12-09 12:00:20.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.000299657327806019 Training loss: 6.898775100708008
2025-12-09 12:00:20.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0002996511281254668 Training loss: 7.6024346351623535
2025-12-09 12:00:20.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0002996448729290572 Training loss: 7.199158668518066
2025-12-09 12:00:20.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.00029963856221911075 Training loss: 7.090175151824951
2025-12-09 12:00:21.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00029963219599796843 Training loss: 6.725348949432373
2025-12-09 12:00:21.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.000299625774267992 Training loss: 6.950185775756836
2025-12-09 12:00:21.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0002996192970315636 Training loss: 6.9729437828063965
2025-12-09 12:00:21.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00029961276429108625 Training loss: 7.124135494232178
2025-12-09 12:00:21.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00029960617604898323 Training loss: 7.1713547706604
2025-12-09 12:00:21.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0002995995323076986 Training loss: 6.559902667999268
2025-12-09 12:00:21.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.00029959283306969705 Training loss: 7.1608195304870605
2025-12-09 12:00:21.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.00029958607833746375 Training loss: 6.7136359214782715
2025-12-09 12:00:22.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0002995792681135045 Training loss: 6.718952655792236
2025-12-09 12:00:22.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.00029957240240034564 Training loss: 6.798439979553223
2025-12-09 12:00:22.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0002995654812005342 Training loss: 6.981288909912109
2025-12-09 12:00:22.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0002995585045166376 Training loss: 7.012053489685059
2025-12-09 12:00:22.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00029955147235124417 Training loss: 7.228607654571533
2025-12-09 12:00:22.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00029954438470696247 Training loss: 6.548223495483398
2025-12-09 12:00:22.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0002995372415864218 Training loss: 5.943597793579102
2025-12-09 12:00:23.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0002995300429922721 Training loss: 7.049764633178711
2025-12-09 12:00:23.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00029952278892718376 Training loss: 6.905707359313965
2025-12-09 12:00:23.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0002995154793938479 Training loss: 6.76295804977417
2025-12-09 12:00:23.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.00029950811439497606 Training loss: 6.9565839767456055
2025-12-09 12:00:23.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0002995006939333004 Training loss: 6.805382251739502
2025-12-09 12:00:23.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.00029949321801157365 Training loss: 7.950512409210205
2025-12-09 12:00:23.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.00029948568663256927 Training loss: 7.048497676849365
2025-12-09 12:00:23.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0002994780997990811 Training loss: 7.039363384246826
2025-12-09 12:00:24.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0002994704575139236 Training loss: 6.888115882873535
2025-12-09 12:00:24.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00029946275977993175 Training loss: 7.001029014587402
2025-12-09 12:00:24.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0002994550065999613 Training loss: 7.11806058883667
2025-12-09 12:00:24.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0002994471979768884 Training loss: 7.161579608917236
2025-12-09 12:00:24.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00029943933391360974 Training loss: 6.504371166229248
2025-12-09 12:00:24.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00029943141441304274 Training loss: 6.617962837219238
2025-12-09 12:00:24.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.00029942343947812517 Training loss: 7.131515979766846
2025-12-09 12:00:24.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0002994154091118156 Training loss: 6.107207775115967
2025-12-09 12:00:25.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0002994073233170929 Training loss: 6.275119781494141
2025-12-09 12:00:25.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00029939918209695676 Training loss: 6.597204685211182
2025-12-09 12:00:25.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0002993909854544273 Training loss: 6.277313709259033
2025-12-09 12:00:25.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00029938273339254515 Training loss: 6.763093948364258
2025-12-09 12:00:25.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0002993744259143716 Training loss: 6.829300880432129
2025-12-09 12:00:25.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0002993660630229886 Training loss: 7.109520435333252
2025-12-09 12:00:25.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0002993576447214983 Training loss: 7.137240409851074
2025-12-09 12:00:26.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0002993491710130237 Training loss: 6.714609622955322
2025-12-09 12:00:26.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00029934064190070836 Training loss: 6.903271675109863
2025-12-09 12:00:26.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00029933205738771624 Training loss: 7.076664924621582
2025-12-09 12:00:26.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0002993234174772319 Training loss: 6.659549236297607
2025-12-09 12:00:26.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00029931472217246057 Training loss: 7.0342607498168945
2025-12-09 12:00:26.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0002993059714766278 Training loss: 6.066636085510254
2025-12-09 12:00:26.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00029929716539297993 Training loss: 6.806735515594482
2025-12-09 12:00:26.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00029928830392478376 Training loss: 7.439578533172607
2025-12-09 12:00:27.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0002992793870753265 Training loss: 6.881400108337402
2025-12-09 12:00:27.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0002992704148479161 Training loss: 7.194422245025635
2025-12-09 12:00:27.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00029926138724588097 Training loss: 7.158584117889404
2025-12-09 12:00:27.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.00029925230427257004 Training loss: 7.2395853996276855
2025-12-09 12:00:27.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0002992431659313528 Training loss: 6.77945613861084
2025-12-09 12:00:27.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.00029923397222561933 Training loss: 7.213230133056641
2025-12-09 12:00:27.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0002992247231587802 Training loss: 6.953975677490234
2025-12-09 12:00:28.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00029921541873426647 Training loss: 6.594226837158203
2025-12-09 12:00:28.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00029920605895552985 Training loss: 6.635535717010498
2025-12-09 12:00:28.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0002991966438260425 Training loss: 7.040811061859131
2025-12-09 12:00:28.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0002991871733492971 Training loss: 6.753504276275635
2025-12-09 12:00:28.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00029917764752880697 Training loss: 7.067911624908447
2025-12-09 12:00:28.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0002991680663681059 Training loss: 7.362040042877197
2025-12-09 12:00:28.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00029915842987074804 Training loss: 7.157418251037598
2025-12-09 12:00:28.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0002991487380403084 Training loss: 7.118253231048584
2025-12-09 12:00:29.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00029913899088038226 Training loss: 6.549434661865234
2025-12-09 12:00:29.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.00029912918839458555 Training loss: 6.925827980041504
2025-12-09 12:00:29.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00029911933058655464 Training loss: 6.5985636711120605
2025-12-09 12:00:29.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.00029910941745994653 Training loss: 6.962003231048584
2025-12-09 12:00:29.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.00029909944901843863 Training loss: 7.894905090332031
2025-12-09 12:00:29.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0002990894252657289 Training loss: 6.781303882598877
2025-12-09 12:00:29.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0002990793462055359 Training loss: 7.158175945281982
2025-12-09 12:00:30.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0002990692118415986 Training loss: 6.94621467590332
2025-12-09 12:00:30.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002990590221776765 Training loss: 6.757176876068115
2025-12-09 12:00:30.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0002990487772175497 Training loss: 6.827723503112793
2025-12-09 12:00:30.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00029903847696501876 Training loss: 7.409829139709473
2025-12-09 12:00:30.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.00029902812142390474 Training loss: 6.583880424499512
2025-12-09 12:00:30.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0002990177105980492 Training loss: 6.771885871887207
2025-12-09 12:00:30.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.00029900724449131424 Training loss: 7.373349189758301
2025-12-09 12:00:30.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.00029899672310758243 Training loss: 6.428901672363281
2025-12-09 12:00:31.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0002989861464507569 Training loss: 7.297654628753662
2025-12-09 12:00:31.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0002989755145247613 Training loss: 6.713200569152832
2025-12-09 12:00:31.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.00029896482733353965 Training loss: 6.67735481262207
2025-12-09 12:00:31.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00029895408488105665 Training loss: 6.996583938598633
2025-12-09 12:00:31.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0002989432871712973 Training loss: 7.061568260192871
2025-12-09 12:00:31.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0002989324342082673 Training loss: 7.690868377685547
2025-12-09 12:00:31.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00029892152599599275 Training loss: 6.922292232513428
2025-12-09 12:00:32.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.00029891056253852026 Training loss: 6.728631496429443
2025-12-09 12:00:32.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0002988995438399169 Training loss: 6.817065715789795
2025-12-09 12:00:32.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0002988884699042702 Training loss: 6.712889671325684
2025-12-09 12:00:32.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002988773407356884 Training loss: 6.44709587097168
2025-12-09 12:00:32.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0002988661563382999 Training loss: 7.103890895843506
2025-12-09 12:00:32.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0002988549167162539 Training loss: 7.513609409332275
2025-12-09 12:00:32.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.00029884362187371986 Training loss: 7.006128787994385
2025-12-09 12:00:32.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0002988322718148878 Training loss: 6.990085601806641
2025-12-09 12:00:33.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0002988208665439683 Training loss: 7.175356864929199
2025-12-09 12:00:33.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0002988094060651923 Training loss: 6.710684776306152
2025-12-09 12:00:33.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0002987978903828114 Training loss: 7.093545913696289
2025-12-09 12:00:33.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.00029878631950109734 Training loss: 7.0405964851379395
2025-12-09 12:00:33.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0002987746934243427 Training loss: 6.842047214508057
2025-12-09 12:00:33.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0002987630121568604 Training loss: 6.953717231750488
2025-12-09 12:00:33.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.00029875127570298376 Training loss: 6.759392261505127
2025-12-09 12:00:34.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0002987394840670666 Training loss: 6.142602443695068
2025-12-09 12:00:34.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0002987276372534834 Training loss: 7.113316535949707
2025-12-09 12:00:34.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0002987157352666288 Training loss: 6.678918838500977
2025-12-09 12:00:34.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0002987037781109182 Training loss: 6.444745063781738
2025-12-09 12:00:34.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00029869176579078714 Training loss: 6.755826473236084
2025-12-09 12:00:34.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.000298679698310692 Training loss: 7.067769527435303
2025-12-09 12:00:34.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00029866757567510927 Training loss: 6.811863899230957
2025-12-09 12:00:34.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0002986553978885362 Training loss: 6.969140529632568
2025-12-09 12:00:35.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00029864316495549037 Training loss: 6.810227870941162
2025-12-09 12:00:35.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0002986308768805097 Training loss: 5.967490196228027
2025-12-09 12:00:35.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00029861853366815275 Training loss: 6.808614253997803
2025-12-09 12:00:35.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00029860613532299845 Training loss: 6.7267165184021
2025-12-09 12:00:35.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00029859368184964624 Training loss: 6.747554302215576
2025-12-09 12:00:35.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00029858117325271585 Training loss: 7.392122745513916
2025-12-09 12:00:35.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.00029856860953684773 Training loss: 6.661529064178467
2025-12-09 12:00:35.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0002985559907067025 Training loss: 6.95978307723999
2025-12-09 12:00:36.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00029854331676696137 Training loss: 6.752152442932129
2025-12-09 12:00:36.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.000298530587722326 Training loss: 6.751728057861328
2025-12-09 12:00:36.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.00029851780357751853 Training loss: 6.682614326477051
2025-12-09 12:00:36.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00029850496433728136 Training loss: 6.63006067276001
2025-12-09 12:00:36.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0002984920700063775 Training loss: 6.877806663513184
2025-12-09 12:00:36.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.00029847912058959033 Training loss: 7.103487968444824
2025-12-09 12:00:36.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00029846611609172363 Training loss: 6.630029201507568
2025-12-09 12:00:37.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.00029845305651760175 Training loss: 7.579110145568848
2025-12-09 12:00:37.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00029843994187206933 Training loss: 6.6499247550964355
2025-12-09 12:00:37.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0002984267721599915 Training loss: 6.5617852210998535
2025-12-09 12:00:37.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0002984135473862538 Training loss: 6.881515979766846
2025-12-09 12:00:37.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0002984002675557622 Training loss: 6.543944835662842
2025-12-09 12:00:37.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0002983869326734432 Training loss: 6.714897155761719
2025-12-09 12:00:37.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0002983735427442434 Training loss: 6.949604034423828
2025-12-09 12:00:37.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.00029836009777313026 Training loss: 7.588409423828125
2025-12-09 12:00:38.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.00029834659776509134 Training loss: 6.888827800750732
2025-12-09 12:00:38.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0002983330427251347 Training loss: 6.862154483795166
2025-12-09 12:00:38.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0002983194326582889 Training loss: 6.784917831420898
2025-12-09 12:00:38.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0002983057675696028 Training loss: 6.580281734466553
2025-12-09 12:00:38.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0002982920474641457 Training loss: 7.280778408050537
2025-12-09 12:00:38.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0002982782723470074 Training loss: 7.199377059936523
2025-12-09 12:00:38.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.000298264442223298 Training loss: 6.732682704925537
2025-12-09 12:00:39.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00029825055709814795 Training loss: 7.268727779388428
2025-12-09 12:00:39.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00029823661697670834 Training loss: 6.57512903213501
2025-12-09 12:00:39.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00029822262186415046 Training loss: 6.341458797454834
2025-12-09 12:00:39.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00029820857176566606 Training loss: 6.5847673416137695
2025-12-09 12:00:39.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0002981944666864672 Training loss: 6.8834052085876465
2025-12-09 12:00:39.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0002981803066317865 Training loss: 7.025346755981445
2025-12-09 12:00:39.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00029816609160687697 Training loss: 7.021263122558594
2025-12-09 12:00:39.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0002981518216170118 Training loss: 7.221734523773193
2025-12-09 12:00:40.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002981374966674848 Training loss: 6.947574615478516
2025-12-09 12:00:40.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.00029812311676361003 Training loss: 5.895284175872803
2025-12-09 12:00:40.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00029810868191072195 Training loss: 6.25653600692749
2025-12-09 12:00:40.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.00029809419211417553 Training loss: 7.446513652801514
2025-12-09 12:00:40.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0002980796473793459 Training loss: 6.90146541595459
2025-12-09 12:00:40.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0002980650477116288 Training loss: 6.996707916259766
2025-12-09 12:00:40.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00029805039311644023 Training loss: 7.047536373138428
2025-12-09 12:00:41.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0002980356835992166 Training loss: 7.055732250213623
2025-12-09 12:00:41.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0002980209191654146 Training loss: 6.907588958740234
2025-12-09 12:00:41.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.00029800609982051147 Training loss: 6.754956245422363
2025-12-09 12:00:41.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0002979912255700046 Training loss: 7.339073657989502
2025-12-09 12:00:41.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.000297976296419412 Training loss: 6.6379170417785645
2025-12-09 12:00:41.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00029796131237427186 Training loss: 6.477149486541748
2025-12-09 12:00:41.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00029794627344014276 Training loss: 6.932147026062012
2025-12-09 12:00:41.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.00029793117962260366 Training loss: 6.608974933624268
2025-12-09 12:00:42.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.000297916030927254 Training loss: 6.618497371673584
2025-12-09 12:00:42.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0002979008273597133 Training loss: 7.411874771118164
2025-12-09 12:00:42.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0002978855689256218 Training loss: 6.744926929473877
2025-12-09 12:00:42.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.00029787025563063975 Training loss: 6.44092321395874
2025-12-09 12:00:42.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.0002978548874804479 Training loss: 6.456856727600098
2025-12-09 12:00:42.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.0002978394644807475 Training loss: 7.120935916900635
2025-12-09 12:00:42.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0002978239866372598 Training loss: 7.108285427093506
2025-12-09 12:00:43.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.00029780845395572673 Training loss: 7.113008499145508
2025-12-09 12:00:43.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0002977928664419104 Training loss: 6.912765026092529
2025-12-09 12:00:43.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0002977772241015933 Training loss: 6.974814414978027
2025-12-09 12:00:43.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.00029776152694057815 Training loss: 6.185475826263428
2025-12-09 12:00:43.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0002977457749646882 Training loss: 6.958452224731445
2025-12-09 12:00:43.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.00029772996817976693 Training loss: 6.598051071166992
2025-12-09 12:00:43.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.00029771410659167806 Training loss: 6.62130880355835
2025-12-09 12:00:43.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.00029769819020630594 Training loss: 6.494078159332275
2025-12-09 12:00:44.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0002976822190295548 Training loss: 6.744620323181152
2025-12-09 12:00:44.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.00029766619306734963 Training loss: 6.679957389831543
2025-12-09 12:00:44.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0002976501123256355 Training loss: 6.6014909744262695
2025-12-09 12:00:44.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.00029763397681037787 Training loss: 6.906907558441162
2025-12-09 12:00:44.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.00029761778652756245 Training loss: 7.221487998962402
2025-12-09 12:00:44.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.00029760154148319534 Training loss: 7.039102077484131
2025-12-09 12:00:44.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.000297585241683303 Training loss: 6.538576602935791
2025-12-09 12:00:45.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.00029756888713393213 Training loss: 6.761519908905029
2025-12-09 12:00:45.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.00029755247784114976 Training loss: 6.651091575622559
2025-12-09 12:00:45.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0002975360138110431 Training loss: 6.869318962097168
2025-12-09 12:00:45.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.00029751949504972 Training loss: 6.696396350860596
2025-12-09 12:00:45.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0002975029215633082 Training loss: 7.248980522155762
2025-12-09 12:00:45.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.000297486293357956 Training loss: 6.517058372497559
2025-12-09 12:00:45.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.00029746961043983206 Training loss: 6.7073140144348145
2025-12-09 12:00:45.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00029745287281512505 Training loss: 6.798377513885498
2025-12-09 12:00:46.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0002974360804900442 Training loss: 6.943686485290527
2025-12-09 12:00:46.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0002974192334708189 Training loss: 6.8597893714904785
2025-12-09 12:00:46.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.00029740233176369887 Training loss: 6.591212749481201
2025-12-09 12:00:46.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0002973853753749541 Training loss: 6.628645896911621
2025-12-09 12:00:46.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00029736836431087493 Training loss: 6.398437023162842
2025-12-09 12:00:46.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.00029735129857777183 Training loss: 6.707765102386475
2025-12-09 12:00:46.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.00029733417818197575 Training loss: 6.811399459838867
2025-12-09 12:00:47.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.00029731700312983776 Training loss: 6.555562973022461
2025-12-09 12:00:47.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0002972997734277293 Training loss: 6.678510665893555
2025-12-09 12:00:47.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.000297282489082042 Training loss: 6.787700176239014
2025-12-09 12:00:47.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00029726515009918786 Training loss: 6.5731096267700195
2025-12-09 12:00:47.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0002972477564855991 Training loss: 6.974290370941162
2025-12-09 12:00:47.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0002972303082477281 Training loss: 6.987683296203613
2025-12-09 12:00:47.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.00029721280539204774 Training loss: 6.633458614349365
2025-12-09 12:00:47.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.0002971952479250509 Training loss: 7.807102203369141
2025-12-09 12:00:48.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.000297177635853251 Training loss: 6.6467509269714355
2025-12-09 12:00:48.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0002971599691831815 Training loss: 6.821258068084717
2025-12-09 12:00:48.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00029714224792139605 Training loss: 6.8281569480896
2025-12-09 12:00:48.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0002971244720744688 Training loss: 6.665641784667969
2025-12-09 12:00:48.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00029710664164899413 Training loss: 6.440520763397217
2025-12-09 12:00:48.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0002970887566515864 Training loss: 6.922285556793213
2025-12-09 12:00:48.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0002970708170888804 Training loss: 6.9360270500183105
2025-12-09 12:00:49.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0002970528229675312 Training loss: 6.502284526824951
2025-12-09 12:00:49.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0002970347742942141 Training loss: 6.9281325340271
2025-12-09 12:00:49.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0002970166710756244 Training loss: 6.749300479888916
2025-12-09 12:00:49.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0002969985133184781 Training loss: 6.7211480140686035
2025-12-09 12:00:49.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0002969803010295109 Training loss: 6.658637523651123
2025-12-09 12:00:49.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0002969620342154791 Training loss: 7.637937545776367
2025-12-09 12:00:49.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0002969437128831591 Training loss: 6.3284783363342285
2025-12-09 12:00:49.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00029692533703934757 Training loss: 6.767683982849121
2025-12-09 12:00:50.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.00029690690669086127 Training loss: 6.610884189605713
2025-12-09 12:00:50.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0002968884218445374 Training loss: 6.517043113708496
2025-12-09 12:00:50.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.0002968698825072332 Training loss: 6.566539764404297
2025-12-09 12:00:50.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0002968512886858262 Training loss: 6.486649990081787
2025-12-09 12:00:50.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.00029683264038721414 Training loss: 6.8148064613342285
2025-12-09 12:00:50.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.00029681393761831485 Training loss: 6.758321285247803
2025-12-09 12:00:50.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0002967951803860665 Training loss: 6.99622106552124
2025-12-09 12:00:51.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0002967763686974276 Training loss: 6.537287712097168
2025-12-09 12:00:51.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.00029675750255937647 Training loss: 6.812067985534668
2025-12-09 12:00:51.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.000296738581978912 Training loss: 6.80968713760376
2025-12-09 12:00:51.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.00029671960696305304 Training loss: 6.3697075843811035
2025-12-09 12:00:51.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.00029670057751883874 Training loss: 7.253718376159668
2025-12-09 12:00:51.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0002966814936533285 Training loss: 6.376917839050293
2025-12-09 12:00:51.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.00029666235537360175 Training loss: 6.779952049255371
2025-12-09 12:00:51.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.00029664316268675824 Training loss: 6.6794304847717285
2025-12-09 12:00:52.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0002966239155999178 Training loss: 6.590425491333008
2025-12-09 12:00:52.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0002966046141202205 Training loss: 6.6775007247924805
2025-12-09 12:00:52.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0002965852582548267 Training loss: 6.99743127822876
2025-12-09 12:00:52.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.00029656584801091663 Training loss: 6.2806782722473145
2025-12-09 12:00:52.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.000296546383395691 Training loss: 6.46066427230835
2025-12-09 12:00:52.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00029652686441637054 Training loss: 6.82151460647583
2025-12-09 12:00:52.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.00029650729108019624 Training loss: 6.659484386444092
2025-12-09 12:00:53.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0002964876633944291 Training loss: 6.092236518859863
2025-12-09 12:00:53.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.00029646798136635034 Training loss: 7.143528938293457
2025-12-09 12:00:53.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.0002964482450032615 Training loss: 6.471934795379639
2025-12-09 12:00:53.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.00029642845431248406 Training loss: 6.759097099304199
2025-12-09 12:00:53.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0002964086093013597 Training loss: 6.861564636230469
2025-12-09 12:00:53.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.00029638870997725046 Training loss: 6.656829833984375
2025-12-09 12:00:53.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00029636875634753824 Training loss: 6.8517327308654785
2025-12-09 12:00:53.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00029634874841962525 Training loss: 6.415431976318359
2025-12-09 12:00:54.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00029632868620093375 Training loss: 6.452298641204834
2025-12-09 12:00:54.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0002963085696989063 Training loss: 6.563694953918457
2025-12-09 12:00:54.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.00029628839892100535 Training loss: 6.6003499031066895
2025-12-09 12:00:54.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00029626817387471365 Training loss: 6.354922771453857
2025-12-09 12:00:54.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.00029624789456753417 Training loss: 6.699944019317627
2025-12-09 12:00:54.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.00029622756100698976 Training loss: 6.899888038635254
2025-12-09 12:00:54.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.0002962071732006237 Training loss: 6.646925926208496
2025-12-09 12:00:55.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00029618673115599896 Training loss: 6.423149585723877
2025-12-09 12:00:55.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0002961662348806992 Training loss: 7.013546466827393
2025-12-09 12:00:55.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.00029614568438232766 Training loss: 6.659104824066162
2025-12-09 12:00:55.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.000296125079668508 Training loss: 6.39658260345459
2025-12-09 12:00:55.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.00029610442074688394 Training loss: 6.509881496429443
2025-12-09 12:00:55.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.00029608370762511935 Training loss: 6.559003829956055
2025-12-09 12:00:55.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.000296062940310898 Training loss: 6.078871250152588
2025-12-09 12:00:55.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.000296042118811924 Training loss: 6.983765602111816
2025-12-09 12:00:56.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0002960212431359215 Training loss: 7.1297688484191895
2025-12-09 12:00:56.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00029600031329063463 Training loss: 6.803799629211426
2025-12-09 12:00:56.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0002959793292838277 Training loss: 7.153914451599121
2025-12-09 12:00:56.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0002959582911232853 Training loss: 6.504130840301514
2025-12-09 12:00:56.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.00029593719881681167 Training loss: 6.674565315246582
2025-12-09 12:00:56.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00029591605237223157 Training loss: 6.6391682624816895
2025-12-09 12:00:56.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0002958948517973896 Training loss: 6.275681018829346
2025-12-09 12:00:57.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0002958735971001505 Training loss: 6.545169353485107
2025-12-09 12:00:57.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0002958522882883991 Training loss: 5.633846282958984
2025-12-09 12:00:57.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0002958309253700404 Training loss: 6.947949409484863
2025-12-09 12:00:57.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.00029580950835299914 Training loss: 6.625441551208496
2025-12-09 12:00:57.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0002957880372452206 Training loss: 6.708613872528076
2025-12-09 12:00:57.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0002957665120546697 Training loss: 6.591789722442627
2025-12-09 12:00:57.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0002957449327893317 Training loss: 7.192845821380615
2025-12-09 12:00:57.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.00029572329945721186 Training loss: 6.919058799743652
2025-12-09 12:00:58.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0002957016120663354 Training loss: 6.879348278045654
2025-12-09 12:00:58.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.00029567987062474767 Training loss: 6.878209114074707
2025-12-09 12:00:58.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00029565807514051406 Training loss: 6.1616034507751465
2025-12-09 12:00:58.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0002956362256217201 Training loss: 6.729837894439697
2025-12-09 12:00:58.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0002956143220764711 Training loss: 6.6906819343566895
2025-12-09 12:00:58.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.0002955923645128927 Training loss: 6.455505847930908
2025-12-09 12:00:58.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.00029557035293913044 Training loss: 6.91118049621582
2025-12-09 12:00:59.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.00029554828736334994 Training loss: 7.011110305786133
2025-12-09 12:00:59.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0002955261677937368 Training loss: 6.810476303100586
2025-12-09 12:00:59.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.00029550399423849673 Training loss: 6.484557151794434
2025-12-09 12:00:59.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0002954817667058554 Training loss: 6.714658737182617
2025-12-09 12:00:59.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00029545948520405844 Training loss: 6.8229851722717285
2025-12-09 12:00:59.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00029543714974137177 Training loss: 7.0380706787109375
2025-12-09 12:00:59.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.000295414760326081 Training loss: 6.656438827514648
2025-12-09 12:00:59.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.0002953923169664919 Training loss: 7.044652462005615
2025-12-09 12:01:00.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00029536981967093033 Training loss: 6.90962028503418
2025-12-09 12:01:00.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.00029534726844774196 Training loss: 6.736878871917725
2025-12-09 12:01:00.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.00029532466330529277 Training loss: 6.888649940490723
2025-12-09 12:01:00.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.00029530200425196835 Training loss: 6.774148464202881
2025-12-09 12:01:00.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.00029527929129617464 Training loss: 6.553130149841309
2025-12-09 12:01:00.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.00029525652444633736 Training loss: 6.9994916915893555
2025-12-09 12:01:00.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0002952337037109023 Training loss: 7.426202297210693
2025-12-09 12:01:01.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.0002952108290983353 Training loss: 6.5594868659973145
2025-12-09 12:01:01.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.00029518790061712204 Training loss: 6.241743564605713
2025-12-09 12:01:01.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0002951649182757683 Training loss: 6.630245685577393
2025-12-09 12:01:01.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.00029514188208279977 Training loss: 6.535955905914307
2025-12-09 12:01:01.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0002951187920467622 Training loss: 6.675178527832031
2025-12-09 12:01:01.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0002950956481762213 Training loss: 6.392228603363037
2025-12-09 12:01:01.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0002950724504797626 Training loss: 6.428028583526611
2025-12-09 12:01:01.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0002950491989659918 Training loss: 6.644340515136719
2025-12-09 12:01:02.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.00029502589364353447 Training loss: 6.666628837585449
2025-12-09 12:01:02.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.00029500253452103615 Training loss: 6.48818302154541
2025-12-09 12:01:02.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.00029497912160716234 Training loss: 7.919956207275391
2025-12-09 12:01:02.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0002949556549105985 Training loss: 6.822987079620361
2025-12-09 12:01:02.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.00029493213444005 Training loss: 6.4810991287231445
2025-12-09 12:01:02.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0002949085602042422 Training loss: 6.540131568908691
2025-12-09 12:01:02.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.00029488493221192043 Training loss: 6.6362409591674805
2025-12-09 12:01:03.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.00029486125047184985 Training loss: 6.688593864440918
2025-12-09 12:01:03.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0002948375149928158 Training loss: 5.852611541748047
2025-12-09 12:01:03.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0002948137257836233 Training loss: 6.5392374992370605
2025-12-09 12:01:03.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0002947898828530974 Training loss: 6.365372657775879
2025-12-09 12:01:03.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.000294765986210083 Training loss: 7.079968452453613
2025-12-09 12:01:03.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0002947420358634451 Training loss: 6.70639181137085
2025-12-09 12:01:03.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.00029471803182206855 Training loss: 6.657026767730713
2025-12-09 12:01:03.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.000294693974094858 Training loss: 6.692840099334717
2025-12-09 12:01:04.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0002946698626907382 Training loss: 6.8990912437438965
2025-12-09 12:01:04.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.00029464569761865366 Training loss: 6.507811546325684
2025-12-09 12:01:04.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.0002946214788875689 Training loss: 7.089452266693115
2025-12-09 12:01:04.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.00029459720650646824 Training loss: 6.292940139770508
2025-12-09 12:01:04.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.00029457288048435605 Training loss: 6.650278568267822
2025-12-09 12:01:04.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.00029454850083025644 Training loss: 7.036309242248535
2025-12-09 12:01:04.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0002945240675532136 Training loss: 6.466826915740967
2025-12-09 12:01:05.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0002944995806622914 Training loss: 6.294413089752197
2025-12-09 12:01:05.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0002944750401665738 Training loss: 7.050683498382568
2025-12-09 12:01:05.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00029445044607516447 Training loss: 6.929891586303711
2025-12-09 12:01:05.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.00029442579839718703 Training loss: 6.452652454376221
2025-12-09 12:01:05.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.0002944010971417851 Training loss: 6.59505558013916
2025-12-09 12:01:05.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.000294376342318122 Training loss: 6.840060234069824
2025-12-09 12:01:05.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.000294351533935381 Training loss: 6.942044258117676
2025-12-09 12:01:05.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.00029432667200276515 Training loss: 6.81189489364624
2025-12-09 12:01:06.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0002943017565294976 Training loss: 6.930415153503418
2025-12-09 12:01:06.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0002942767875248211 Training loss: 6.7939958572387695
2025-12-09 12:01:06.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0002942517649979984 Training loss: 6.865792274475098
2025-12-09 12:01:06.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.00029422668895831203 Training loss: 6.40138053894043
2025-12-09 12:01:06.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00029420155941506447 Training loss: 6.577569484710693
2025-12-09 12:01:06.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00029417637637757797 Training loss: 6.156454563140869
2025-12-09 12:01:06.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.00029415113985519463 Training loss: 6.197803020477295
2025-12-09 12:01:07.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0002941258498572764 Training loss: 7.021468639373779
2025-12-09 12:01:07.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.0002941005063932051 Training loss: 6.762009143829346
2025-12-09 12:01:07.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0002940751094723823 Training loss: 6.613066673278809
2025-12-09 12:01:07.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.00029404965910422953 Training loss: 6.416049003601074
2025-12-09 12:01:07.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.00029402415529818804 Training loss: 6.506036281585693
2025-12-09 12:01:07.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.00029399859806371895 Training loss: 6.630853652954102
2025-12-09 12:01:07.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.0002939729874103032 Training loss: 7.615969181060791
2025-12-09 12:01:07.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.00029394732334744146 Training loss: 6.692241668701172
2025-12-09 12:01:08.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.00029392160588465434 Training loss: 6.455888748168945
2025-12-09 12:01:08.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0002938958350314823 Training loss: 6.482207775115967
2025-12-09 12:01:08.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.00029387001079748536 Training loss: 6.586136341094971
2025-12-09 12:01:08.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0002938441331922436 Training loss: 6.553709030151367
2025-12-09 12:01:08.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0002938182022253568 Training loss: 5.880661964416504
2025-12-09 12:01:08.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0002937922179064445 Training loss: 6.211407661437988
2025-12-09 12:01:08.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.000293766180245146 Training loss: 6.299215316772461
2025-12-09 12:01:09.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.00029374008925112056 Training loss: 6.504745006561279
2025-12-09 12:01:09.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.00029371394493404705 Training loss: 7.041599273681641
2025-12-09 12:01:09.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.00029368774730362425 Training loss: 6.443864345550537
2025-12-09 12:01:09.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0002936614963695706 Training loss: 7.207455635070801
2025-12-09 12:01:09.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0002936351921416244 Training loss: 5.95761775970459
2025-12-09 12:01:09.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0002936088346295436 Training loss: 6.343687057495117
2025-12-09 12:01:09.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0002935824238431062 Training loss: 6.546266078948975
2025-12-09 12:01:09.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0002935559597921096 Training loss: 6.729245662689209
2025-12-09 12:01:10.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.00029352944248637117 Training loss: 6.764464855194092
2025-12-09 12:01:10.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00029350287193572806 Training loss: 6.392004489898682
2025-12-09 12:01:10.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.000293476248150037 Training loss: 6.790611743927002
2025-12-09 12:01:10.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.00029344957113917473 Training loss: 7.3041863441467285
2025-12-09 12:01:10.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0002934228409130374 Training loss: 6.534668445587158
2025-12-09 12:01:10.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0002933960574815412 Training loss: 6.413110256195068
2025-12-09 12:01:10.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0002933692208546219 Training loss: 6.887993812561035
2025-12-09 12:01:11.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.00029334233104223506 Training loss: 6.81685209274292
2025-12-09 12:01:11.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00029331538805435595 Training loss: 6.687434196472168
2025-12-09 12:01:11.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.00029328839190097955 Training loss: 7.154909133911133
2025-12-09 12:01:11.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.00029326134259212064 Training loss: 6.669170379638672
2025-12-09 12:01:11.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0002932342401378136 Training loss: 6.240015029907227
2025-12-09 12:01:11.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0002932070845481126 Training loss: 6.975734710693359
2025-12-09 12:01:11.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.00029317987583309156 Training loss: 7.019876480102539
2025-12-09 12:01:11.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.00029315261400284404 Training loss: 6.4877214431762695
2025-12-09 12:01:12.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0002931252990674832 Training loss: 6.861396312713623
2025-12-09 12:01:12.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0002930979310371422 Training loss: 6.418371200561523
2025-12-09 12:01:12.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0002930705099219736 Training loss: 6.357515811920166
2025-12-09 12:01:12.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0002930430357321498 Training loss: 6.688861846923828
2025-12-09 12:01:12.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0002930155084778629 Training loss: 7.235024929046631
2025-12-09 12:01:12.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0002929879281693246 Training loss: 6.358208656311035
2025-12-09 12:01:12.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0002929602948167663 Training loss: 6.311557769775391
2025-12-09 12:01:13.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0002929326084304392 Training loss: 6.5353474617004395
2025-12-09 12:01:13.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.00029290486902061396 Training loss: 6.3556132316589355
2025-12-09 12:01:13.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0002928770765975811 Training loss: 6.607030391693115
2025-12-09 12:01:13.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.00029284923117165075 Training loss: 5.5909857749938965
2025-12-09 12:01:13.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0002928213327531526 Training loss: 6.5181050300598145
2025-12-09 12:01:13.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.00029279338135243624 Training loss: 6.742288112640381
2025-12-09 12:01:13.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0002927653769798706 Training loss: 6.7202582359313965
2025-12-09 12:01:13.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.00029273731964584446 Training loss: 6.776366710662842
2025-12-09 12:01:14.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.00029270920936076624 Training loss: 6.625169277191162
2025-12-09 12:01:14.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.00029268104613506396 Training loss: 6.550739765167236
2025-12-09 12:01:14.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.00029265282997918533 Training loss: 6.306548595428467
2025-12-09 12:01:14.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00029262456090359756 Training loss: 7.355528354644775
2025-12-09 12:01:14.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0002925962389187877 Training loss: 6.676229476928711
2025-12-09 12:01:14.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0002925678640352622 Training loss: 6.540258884429932
2025-12-09 12:01:14.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.00029253943626354734 Training loss: 7.206689834594727
2025-12-09 12:01:15.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0002925109556141889 Training loss: 6.511402606964111
2025-12-09 12:01:15.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0002924824220977523 Training loss: 6.303092956542969
2025-12-09 12:01:15.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0002924538357248226 Training loss: 6.694511890411377
2025-12-09 12:01:15.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.00029242519650600436 Training loss: 6.763646125793457
2025-12-09 12:01:15.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.0002923965044519219 Training loss: 6.541094779968262
2025-12-09 12:01:15.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0002923677595732191 Training loss: 6.488090515136719
2025-12-09 12:01:15.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0002923389618805593 Training loss: 6.67152214050293
2025-12-09 12:01:15.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.00029231011138462564 Training loss: 6.443357944488525
2025-12-09 12:01:16.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0002922812080961207 Training loss: 6.6047797203063965
2025-12-09 12:01:16.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0002922522520257667 Training loss: 6.328556060791016
2025-12-09 12:01:16.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0002922232431843054 Training loss: 6.736841201782227
2025-12-09 12:01:16.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.00029219418158249824 Training loss: 6.27064323425293
2025-12-09 12:01:16.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0002921650672311261 Training loss: 6.51885986328125
2025-12-09 12:01:16.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0002921359001409895 Training loss: 5.989752292633057
2025-12-09 12:01:16.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0002921066803229085 Training loss: 6.7305827140808105
2025-12-09 12:01:17.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.00029207740778772277 Training loss: 6.618795871734619
2025-12-09 12:01:17.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.00029204808254629146 Training loss: 6.74566650390625
2025-12-09 12:01:17.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.00029201870460949326 Training loss: 7.412903785705566
2025-12-09 12:01:17.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.00029198927398822657 Training loss: 6.679183006286621
2025-12-09 12:01:17.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0002919597906934092 Training loss: 6.704006195068359
2025-12-09 12:01:17.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0002919302547359785 Training loss: 6.703843116760254
2025-12-09 12:01:17.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0002919006661268914 Training loss: 6.841831207275391
2025-12-09 12:01:17.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0002918710248771243 Training loss: 6.454733371734619
2025-12-09 12:01:18.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0002918413309976732 Training loss: 6.605524063110352
2025-12-09 12:01:18.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00029181158449955363 Training loss: 6.361297607421875
2025-12-09 12:01:18.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0002917817853938005 Training loss: 6.784152507781982
2025-12-09 12:01:18.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0002917519336914684 Training loss: 7.7667622566223145
2025-12-09 12:01:18.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.00029172202940363145 Training loss: 6.613851070404053
2025-12-09 12:01:18.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0002916920725413831 Training loss: 6.148787021636963
2025-12-09 12:01:18.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.00029166206311583644 Training loss: 6.886045932769775
2025-12-09 12:01:19.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.000291632001138124 Training loss: 6.526935577392578
2025-12-09 12:01:19.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0002916018866193978 Training loss: 6.916815757751465
2025-12-09 12:01:19.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0002915717195708295 Training loss: 6.82345724105835
2025-12-09 12:01:19.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.00029154150000360995 Training loss: 6.552038669586182
2025-12-09 12:01:19.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.00029151122792894985 Training loss: 6.676163673400879
2025-12-09 12:01:19.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.000291480903358079 Training loss: 6.322205066680908
2025-12-09 12:01:19.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00029145052630224696 Training loss: 6.571383953094482
2025-12-09 12:01:19.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0002914200967727227 Training loss: 6.479857921600342
2025-12-09 12:01:20.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00029138961478079455 Training loss: 6.495086193084717
2025-12-09 12:01:20.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.00029135908033777033 Training loss: 6.652370929718018
2025-12-09 12:01:20.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.00029132849345497755 Training loss: 6.543725967407227
2025-12-09 12:01:20.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.00029129785414376276 Training loss: 6.56861686706543
2025-12-09 12:01:20.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.00029126716241549224 Training loss: 6.753931999206543
2025-12-09 12:01:20.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0002912364182815517 Training loss: 6.595376491546631
2025-12-09 12:01:20.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.00029120562175334624 Training loss: 6.505898475646973
2025-12-09 12:01:21.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0002911747728423004 Training loss: 6.837916374206543
2025-12-09 12:01:21.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.00029114387155985814 Training loss: 6.638509273529053
2025-12-09 12:01:21.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0002911129179174828 Training loss: 6.668562889099121
2025-12-09 12:01:21.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.00029108191192665734 Training loss: 6.300370216369629
2025-12-09 12:01:21.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.00029105085359888396 Training loss: 6.628427505493164
2025-12-09 12:01:21.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.00029101974294568425 Training loss: 6.7580766677856445
2025-12-09 12:01:21.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0002909885799785993 Training loss: 6.897721290588379
2025-12-09 12:01:22.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0002909573647091897 Training loss: 6.199451446533203
2025-12-09 12:01:22.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.00029092609714903523 Training loss: 5.520115852355957
2025-12-09 12:01:22.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.00029089477730973517 Training loss: 6.0059099197387695
2025-12-09 12:01:22.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.0002908634052029083 Training loss: 6.503190994262695
2025-12-09 12:01:22.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0002908319808401925 Training loss: 6.72344446182251
2025-12-09 12:01:22.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0002908005042332454 Training loss: 6.374513626098633
2025-12-09 12:01:22.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00029076897539374375 Training loss: 6.233660697937012
2025-12-09 12:01:22.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.00029073739433338377 Training loss: 6.366774082183838
2025-12-09 12:01:23.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.000290705761063881 Training loss: 6.824740886688232
2025-12-09 12:01:23.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.00029067407559697046 Training loss: 6.360377788543701
2025-12-09 12:01:23.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0002906423379444063 Training loss: 6.090755462646484
2025-12-09 12:01:23.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.00029061054811796243 Training loss: 5.938483715057373
2025-12-09 12:01:23.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0002905787061294317 Training loss: 6.612460136413574
2025-12-09 12:01:23.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.00029054681199062657 Training loss: 6.465136528015137
2025-12-09 12:01:23.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.00029051486571337877 Training loss: 6.69902229309082
2025-12-09 12:01:24.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.00029048286730953924 Training loss: 6.477947235107422
2025-12-09 12:01:24.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0002904508167909785 Training loss: 6.0830888748168945
2025-12-09 12:01:24.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.00029041871416958623 Training loss: 7.334750652313232
2025-12-09 12:01:24.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00029038655945727153 Training loss: 6.911706924438477
2025-12-09 12:01:24.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0002903543526659628 Training loss: 6.424605846405029
2025-12-09 12:01:24.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00029032209380760765 Training loss: 6.860008239746094
2025-12-09 12:01:24.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0002902897828941732 Training loss: 6.675594806671143
2025-12-09 12:01:24.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0002902574199376457 Training loss: 6.521731376647949
2025-12-09 12:01:25.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.00029022500495003086 Training loss: 6.5252814292907715
2025-12-09 12:01:25.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0002901925379433536 Training loss: 6.590757846832275
2025-12-09 12:01:25.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0002901600189296581 Training loss: 6.5281805992126465
2025-12-09 12:01:25.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.000290127447921008 Training loss: 6.544780254364014
2025-12-09 12:01:25.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.00029009482492948607 Training loss: 6.504819869995117
2025-12-09 12:01:25.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.00029006214996719437 Training loss: 6.309567451477051
2025-12-09 12:01:25.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0002900294230462543 Training loss: 7.658801078796387
2025-12-09 12:01:26.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.00028999664417880654 Training loss: 6.242201805114746
2025-12-09 12:01:26.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.000289963813377011 Training loss: 6.7849202156066895
2025-12-09 12:01:26.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0002899309306530469 Training loss: 6.358148574829102
2025-12-09 12:01:26.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.0002898979960191127 Training loss: 6.5138325691223145
2025-12-09 12:01:26.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.000289865009487426 Training loss: 6.422249794006348
2025-12-09 12:01:26.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00028983197107022396 Training loss: 6.185622215270996
2025-12-09 12:01:26.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0002897988807797627 Training loss: 6.162075519561768
2025-12-09 12:01:26.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.00028976573862831757 Training loss: 6.689298152923584
2025-12-09 12:01:27.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0002897325446281834 Training loss: 6.454525947570801
2025-12-09 12:01:27.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0002896992987916741 Training loss: 6.3265700340271
2025-12-09 12:01:27.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.00028966600113112276 Training loss: 6.477938652038574
2025-12-09 12:01:27.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.00028963265165888187 Training loss: 6.591029167175293
2025-12-09 12:01:27.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.00028959925038732294 Training loss: 7.733788967132568
2025-12-09 12:01:27.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.00028956579732883684 Training loss: 6.176237106323242
2025-12-09 12:01:27.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.00028953229249583355 Training loss: 6.156928539276123
2025-12-09 12:01:28.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.0002894987359007424 Training loss: 6.974931716918945
2025-12-09 12:01:28.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.00028946512755601174 Training loss: 6.057158470153809
2025-12-09 12:01:28.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0002894314674741092 Training loss: 6.154262065887451
2025-12-09 12:01:28.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.00028939775566752177 Training loss: 6.2636518478393555
2025-12-09 12:01:28.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.00028936399214875524 Training loss: 7.497303009033203
2025-12-09 12:01:28.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.000289330176930335 Training loss: 6.58074951171875
2025-12-09 12:01:28.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.0002892963100248053 Training loss: 6.721785068511963
2025-12-09 12:01:28.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0002892623914447298 Training loss: 6.234133720397949
2025-12-09 12:01:29.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.00028922842120269115 Training loss: 6.179486274719238
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.30 GiB is free. Including non-PyTorch memory, this process has 90.85 GiB memory in use. Of the allocated memory 89.88 GiB is allocated by PyTorch, and 213.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:08, 146.56it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 176.37it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:21, 122.22it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:10, 141.46it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:57, 173.60it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 213.94it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:53, 185.62it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 184.77it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 197.40it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 202.50it/s]Tokenizing texts:   2%|▏         | 241/10000 [00:01<00:47, 205.25it/s]Tokenizing texts:   3%|▎         | 263/10000 [00:01<00:47, 204.23it/s]Tokenizing texts:   3%|▎         | 292/10000 [00:01<00:42, 227.41it/s]Tokenizing texts:   3%|▎         | 316/10000 [00:01<00:42, 227.63it/s]Tokenizing texts:   3%|▎         | 340/10000 [00:01<00:52, 182.72it/s]Tokenizing texts:   4%|▎         | 364/10000 [00:01<00:49, 196.38it/s]Tokenizing texts:   4%|▍         | 386/10000 [00:02<00:52, 182.98it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 185.57it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:51, 185.19it/s]Tokenizing texts:   4%|▍         | 446/10000 [00:02<00:51, 186.83it/s]Tokenizing texts:   5%|▍         | 468/10000 [00:02<00:48, 195.26it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 217.66it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:40, 236.54it/s]Tokenizing texts:   6%|▌         | 550/10000 [00:02<00:50, 185.73it/s]Tokenizing texts:   6%|▌         | 580/10000 [00:02<00:44, 211.46it/s]Tokenizing texts:   6%|▌         | 603/10000 [00:03<00:44, 210.36it/s]Tokenizing texts:   6%|▋         | 626/10000 [00:03<00:46, 199.86it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 201.80it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 199.39it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 190.64it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 199.90it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 199.16it/s]Tokenizing texts:   8%|▊         | 766/10000 [00:03<00:44, 209.75it/s]Tokenizing texts:   8%|▊         | 788/10000 [00:04<00:45, 201.37it/s]Tokenizing texts:   8%|▊         | 813/10000 [00:04<00:43, 211.40it/s]Tokenizing texts:   8%|▊         | 835/10000 [00:04<00:43, 211.57it/s]Tokenizing texts:   9%|▊         | 861/10000 [00:04<00:40, 224.99it/s]Tokenizing texts:   9%|▉         | 884/10000 [00:04<00:42, 217.00it/s]Tokenizing texts:   9%|▉         | 910/10000 [00:04<00:39, 227.64it/s]Tokenizing texts:   9%|▉         | 940/10000 [00:04<00:36, 247.44it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:04<00:33, 266.95it/s]Tokenizing texts:  10%|█         | 1001/10000 [00:04<00:33, 271.30it/s]Tokenizing texts:  10%|█         | 1032/10000 [00:04<00:31, 282.28it/s]Tokenizing texts:  11%|█         | 1061/10000 [00:05<00:39, 224.43it/s]Tokenizing texts:  11%|█         | 1086/10000 [00:05<00:39, 225.25it/s]Tokenizing texts:  11%|█         | 1110/10000 [00:05<00:39, 227.64it/s]Tokenizing texts:  11%|█▏        | 1134/10000 [00:05<00:45, 193.60it/s]Tokenizing texts:  12%|█▏        | 1159/10000 [00:05<00:43, 204.60it/s]Tokenizing texts:  12%|█▏        | 1184/10000 [00:05<00:40, 215.62it/s]Tokenizing texts:  12%|█▏        | 1207/10000 [00:05<00:43, 203.10it/s]Tokenizing texts:  12%|█▏        | 1235/10000 [00:05<00:41, 210.95it/s]Tokenizing texts:  13%|█▎        | 1257/10000 [00:06<00:42, 204.91it/s]Tokenizing texts:  13%|█▎        | 1291/10000 [00:06<00:36, 240.31it/s]Tokenizing texts:  13%|█▎        | 1316/10000 [00:06<00:36, 235.15it/s]Tokenizing texts:  13%|█▎        | 1341/10000 [00:06<00:42, 204.23it/s]Tokenizing texts:  14%|█▎        | 1368/10000 [00:06<00:39, 218.87it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:40, 214.39it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:37, 231.75it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:34, 248.81it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:07<00:31, 271.85it/s]Tokenizing texts:  15%|█▌        | 1516/10000 [00:07<00:31, 265.95it/s]Tokenizing texts:  15%|█▌        | 1544/10000 [00:07<00:33, 249.35it/s]Tokenizing texts:  16%|█▌        | 1577/10000 [00:07<00:31, 270.42it/s]Tokenizing texts:  16%|█▌        | 1605/10000 [00:07<00:33, 251.02it/s]Tokenizing texts:  16%|█▋        | 1631/10000 [00:07<00:40, 207.27it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 214.82it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 216.17it/s]Tokenizing texts:  17%|█▋        | 1703/10000 [00:08<00:37, 219.44it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:33, 243.69it/s]Tokenizing texts:  18%|█▊        | 1760/10000 [00:08<00:33, 243.89it/s]Tokenizing texts:  18%|█▊        | 1786/10000 [00:08<00:33, 244.92it/s]Tokenizing texts:  18%|█▊        | 1820/10000 [00:08<00:30, 271.01it/s]Tokenizing texts:  18%|█▊        | 1848/10000 [00:08<00:37, 217.20it/s]Tokenizing texts:  19%|█▊        | 1872/10000 [00:08<00:36, 219.99it/s]Tokenizing texts:  19%|█▉        | 1896/10000 [00:08<00:36, 224.37it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:08<00:35, 224.94it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 194.74it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:38, 210.23it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:35, 227.59it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 232.65it/s]Tokenizing texts:  21%|██        | 2055/10000 [00:09<00:32, 246.62it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:38, 207.51it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:37, 208.08it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:36, 213.25it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:10<00:38, 203.91it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:35, 221.57it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:31, 249.54it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:29, 259.37it/s]Tokenizing texts:  23%|██▎       | 2274/10000 [00:10<00:28, 269.96it/s]Tokenizing texts:  23%|██▎       | 2302/10000 [00:10<00:29, 259.60it/s]Tokenizing texts:  23%|██▎       | 2329/10000 [00:10<00:36, 209.93it/s]Tokenizing texts:  24%|██▎       | 2365/10000 [00:10<00:31, 245.70it/s]Tokenizing texts:  24%|██▍       | 2399/10000 [00:10<00:28, 266.97it/s]Tokenizing texts:  24%|██▍       | 2428/10000 [00:11<00:30, 247.04it/s]Tokenizing texts:  25%|██▍       | 2455/10000 [00:11<00:30, 250.56it/s]Tokenizing texts:  25%|██▍       | 2482/10000 [00:11<00:30, 245.43it/s]Tokenizing texts:  25%|██▌       | 2508/10000 [00:11<00:30, 245.02it/s]Tokenizing texts:  25%|██▌       | 2538/10000 [00:11<00:28, 258.72it/s]Tokenizing texts:  26%|██▌       | 2565/10000 [00:11<00:33, 222.54it/s]Tokenizing texts:  26%|██▌       | 2589/10000 [00:11<00:33, 221.14it/s]Tokenizing texts:  26%|██▌       | 2612/10000 [00:11<00:36, 203.35it/s]Tokenizing texts:  26%|██▋       | 2640/10000 [00:12<00:33, 222.25it/s]Tokenizing texts:  27%|██▋       | 2667/10000 [00:12<00:31, 230.59it/s]Tokenizing texts:  27%|██▋       | 2691/10000 [00:12<00:32, 223.14it/s]Tokenizing texts:  27%|██▋       | 2714/10000 [00:12<00:35, 202.43it/s]Tokenizing texts:  27%|██▋       | 2749/10000 [00:12<00:30, 239.21it/s]Tokenizing texts:  28%|██▊       | 2776/10000 [00:12<00:29, 242.70it/s]Tokenizing texts:  28%|██▊       | 2801/10000 [00:12<00:40, 177.35it/s]Tokenizing texts:  28%|██▊       | 2836/10000 [00:12<00:33, 214.97it/s]Tokenizing texts:  29%|██▊       | 2861/10000 [00:13<00:36, 197.73it/s]Tokenizing texts:  29%|██▉       | 2890/10000 [00:13<00:32, 218.16it/s]Tokenizing texts:  29%|██▉       | 2923/10000 [00:13<00:28, 245.12it/s]Tokenizing texts:  30%|██▉       | 2950/10000 [00:13<00:31, 224.47it/s]Tokenizing texts:  30%|██▉       | 2975/10000 [00:13<00:30, 227.11it/s]Tokenizing texts:  30%|███       | 3013/10000 [00:13<00:26, 264.50it/s]Tokenizing texts:  30%|███       | 3041/10000 [00:13<00:30, 228.51it/s]Tokenizing texts:  31%|███       | 3074/10000 [00:13<00:27, 252.94it/s]Tokenizing texts:  31%|███       | 3101/10000 [00:14<00:28, 242.30it/s]Tokenizing texts:  31%|███▏      | 3135/10000 [00:14<00:25, 266.77it/s]Tokenizing texts:  32%|███▏      | 3163/10000 [00:14<00:26, 253.37it/s]Tokenizing texts:  32%|███▏      | 3190/10000 [00:14<00:26, 256.18it/s]Tokenizing texts:  32%|███▏      | 3219/10000 [00:14<00:25, 265.26it/s]Tokenizing texts:  32%|███▏      | 3247/10000 [00:14<00:26, 257.39it/s]Tokenizing texts:  33%|███▎      | 3278/10000 [00:14<00:25, 268.65it/s]Tokenizing texts:  33%|███▎      | 3306/10000 [00:14<00:25, 261.22it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:14<00:26, 247.79it/s]Tokenizing texts:  34%|███▎      | 3361/10000 [00:15<00:26, 254.82it/s]Tokenizing texts:  34%|███▍      | 3387/10000 [00:15<00:26, 250.34it/s]Tokenizing texts:  34%|███▍      | 3413/10000 [00:15<00:26, 252.33it/s]Tokenizing texts:  35%|███▍      | 3452/10000 [00:15<00:22, 291.47it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:15<00:22, 287.39it/s]Tokenizing texts:  35%|███▌      | 3516/10000 [00:15<00:26, 243.71it/s]Tokenizing texts:  35%|███▌      | 3542/10000 [00:15<00:28, 229.80it/s]Tokenizing texts:  36%|███▌      | 3569/10000 [00:15<00:27, 238.16it/s]Tokenizing texts:  36%|███▌      | 3600/10000 [00:15<00:25, 251.55it/s]Tokenizing texts:  36%|███▋      | 3626/10000 [00:16<00:25, 249.84it/s]Tokenizing texts:  37%|███▋      | 3652/10000 [00:16<00:25, 248.04it/s]Tokenizing texts:  37%|███▋      | 3678/10000 [00:16<00:25, 245.35it/s]Tokenizing texts:  37%|███▋      | 3705/10000 [00:16<00:25, 249.77it/s]Tokenizing texts:  37%|███▋      | 3732/10000 [00:16<00:24, 253.90it/s]Tokenizing texts:  38%|███▊      | 3758/10000 [00:16<00:27, 223.80it/s]Tokenizing texts:  38%|███▊      | 3784/10000 [00:16<00:26, 232.57it/s]Tokenizing texts:  38%|███▊      | 3808/10000 [00:16<00:30, 204.72it/s]Tokenizing texts:  38%|███▊      | 3839/10000 [00:17<00:26, 231.37it/s]Tokenizing texts:  39%|███▊      | 3867/10000 [00:17<00:25, 241.45it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:17<00:24, 249.70it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:26, 233.56it/s]Tokenizing texts:  39%|███▉      | 3945/10000 [00:17<00:26, 227.93it/s]Tokenizing texts:  40%|███▉      | 3976/10000 [00:17<00:24, 249.21it/s]Tokenizing texts:  40%|████      | 4002/10000 [00:17<00:26, 223.17it/s]Tokenizing texts:  40%|████      | 4030/10000 [00:17<00:25, 237.06it/s]Tokenizing texts:  41%|████      | 4056/10000 [00:17<00:24, 243.10it/s]Tokenizing texts:  41%|████      | 4089/10000 [00:18<00:22, 265.06it/s]Tokenizing texts:  41%|████      | 4121/10000 [00:18<00:21, 278.48it/s]Tokenizing texts:  42%|████▏     | 4150/10000 [00:18<00:20, 279.35it/s]Tokenizing texts:  42%|████▏     | 4179/10000 [00:18<00:21, 273.90it/s]Tokenizing texts:  42%|████▏     | 4214/10000 [00:18<00:19, 295.51it/s]Tokenizing texts:  42%|████▏     | 4244/10000 [00:18<00:24, 231.63it/s]Tokenizing texts:  43%|████▎     | 4270/10000 [00:18<00:24, 229.32it/s]Tokenizing texts:  43%|████▎     | 4295/10000 [00:18<00:24, 233.38it/s]Tokenizing texts:  43%|████▎     | 4327/10000 [00:18<00:22, 255.51it/s]Tokenizing texts:  44%|████▎     | 4358/10000 [00:19<00:21, 266.23it/s]Tokenizing texts:  44%|████▍     | 4386/10000 [00:19<00:20, 267.47it/s]Tokenizing texts:  44%|████▍     | 4415/10000 [00:19<00:20, 273.68it/s]Tokenizing texts:  44%|████▍     | 4443/10000 [00:19<00:20, 271.58it/s]Tokenizing texts:  45%|████▍     | 4471/10000 [00:19<00:22, 246.81it/s]Tokenizing texts:  45%|████▍     | 4497/10000 [00:19<00:23, 232.74it/s]Tokenizing texts:  45%|████▌     | 4523/10000 [00:19<00:23, 237.16it/s]Tokenizing texts:  46%|████▌     | 4552/10000 [00:19<00:22, 247.11it/s]Tokenizing texts:  46%|████▌     | 4578/10000 [00:19<00:22, 245.07it/s]Tokenizing texts:  46%|████▌     | 4603/10000 [00:20<00:23, 232.00it/s]Tokenizing texts:  46%|████▋     | 4636/10000 [00:20<00:20, 258.46it/s]Tokenizing texts:  47%|████▋     | 4669/10000 [00:20<00:19, 277.47it/s]Tokenizing texts:  47%|████▋     | 4705/10000 [00:20<00:17, 299.33it/s]Tokenizing texts:  47%|████▋     | 4736/10000 [00:20<00:17, 299.33it/s]Tokenizing texts:  48%|████▊     | 4767/10000 [00:20<00:18, 275.76it/s]Tokenizing texts:  48%|████▊     | 4796/10000 [00:20<00:21, 245.90it/s]Tokenizing texts:  48%|████▊     | 4826/10000 [00:20<00:20, 257.43it/s]Tokenizing texts:  49%|████▊     | 4859/10000 [00:20<00:19, 262.76it/s]Tokenizing texts:  49%|████▉     | 4887/10000 [00:21<00:19, 265.01it/s]Tokenizing texts:  49%|████▉     | 4915/10000 [00:21<00:18, 268.97it/s]Tokenizing texts:  49%|████▉     | 4943/10000 [00:21<00:18, 267.54it/s]Tokenizing texts:  50%|████▉     | 4970/10000 [00:21<00:19, 257.24it/s]Tokenizing texts:  50%|█████     | 5008/10000 [00:21<00:17, 291.45it/s]Tokenizing texts:  50%|█████     | 5038/10000 [00:21<00:18, 273.00it/s]Tokenizing texts:  51%|█████     | 5066/10000 [00:21<00:20, 244.04it/s]Tokenizing texts:  51%|█████     | 5092/10000 [00:21<00:20, 237.79it/s]Tokenizing texts:  51%|█████     | 5120/10000 [00:21<00:19, 248.76it/s]Tokenizing texts:  51%|█████▏    | 5146/10000 [00:22<00:20, 235.61it/s]Tokenizing texts:  52%|█████▏    | 5175/10000 [00:22<00:19, 249.53it/s]Tokenizing texts:  52%|█████▏    | 5202/10000 [00:22<00:19, 244.45it/s]Tokenizing texts:  52%|█████▏    | 5227/10000 [00:22<00:19, 241.53it/s]Tokenizing texts:  53%|█████▎    | 5261/10000 [00:22<00:17, 265.81it/s]Tokenizing texts:  53%|█████▎    | 5288/10000 [00:22<00:19, 244.31it/s]Tokenizing texts:  53%|█████▎    | 5324/10000 [00:22<00:17, 274.78it/s]Tokenizing texts:  54%|█████▎    | 5358/10000 [00:22<00:16, 289.51it/s]Tokenizing texts:  54%|█████▍    | 5390/10000 [00:22<00:15, 291.85it/s]Tokenizing texts:  54%|█████▍    | 5423/10000 [00:23<00:15, 299.83it/s]Tokenizing texts:  55%|█████▍    | 5454/10000 [00:23<00:16, 271.17it/s]Tokenizing texts:  55%|█████▍    | 5482/10000 [00:23<00:17, 255.59it/s]Tokenizing texts:  55%|█████▌    | 5509/10000 [00:23<00:20, 215.56it/s]Tokenizing texts:  55%|█████▌    | 5541/10000 [00:23<00:18, 239.81it/s]Tokenizing texts:  56%|█████▌    | 5568/10000 [00:23<00:17, 246.23it/s]Tokenizing texts:  56%|█████▌    | 5594/10000 [00:23<00:17, 247.03it/s]Tokenizing texts:  56%|█████▋    | 5628/10000 [00:23<00:16, 271.36it/s]Tokenizing texts:  57%|█████▋    | 5666/10000 [00:24<00:14, 300.42it/s]Tokenizing texts:  57%|█████▋    | 5698/10000 [00:24<00:14, 305.72it/s]Tokenizing texts:  57%|█████▋    | 5730/10000 [00:24<00:14, 286.78it/s]Tokenizing texts:  58%|█████▊    | 5760/10000 [00:24<00:16, 255.39it/s]Tokenizing texts:  58%|█████▊    | 5787/10000 [00:24<00:19, 216.83it/s]Tokenizing texts:  58%|█████▊    | 5811/10000 [00:24<00:19, 216.94it/s]Tokenizing texts:  58%|█████▊    | 5834/10000 [00:24<00:19, 218.24it/s]Tokenizing texts:  59%|█████▊    | 5866/10000 [00:24<00:16, 244.17it/s]Tokenizing texts:  59%|█████▉    | 5892/10000 [00:25<00:16, 247.88it/s]Tokenizing texts:  59%|█████▉    | 5918/10000 [00:25<00:16, 245.63it/s]Tokenizing texts:  59%|█████▉    | 5944/10000 [00:25<00:17, 237.89it/s]Tokenizing texts:  60%|█████▉    | 5973/10000 [00:25<00:15, 251.93it/s]Tokenizing texts:  60%|█████▉    | 5999/10000 [00:25<00:16, 249.55it/s]Tokenizing texts:  60%|██████    | 6025/10000 [00:25<00:16, 245.11it/s]Tokenizing texts:  61%|██████    | 6056/10000 [00:25<00:15, 261.74it/s]Tokenizing texts:  61%|██████    | 6084/10000 [00:25<00:14, 262.09it/s]Tokenizing texts:  61%|██████    | 6111/10000 [00:25<00:15, 257.65it/s]Tokenizing texts:  61%|██████▏   | 6143/10000 [00:25<00:14, 274.01it/s]Tokenizing texts:  62%|██████▏   | 6171/10000 [00:26<00:14, 273.38it/s]Tokenizing texts:  62%|██████▏   | 6199/10000 [00:26<00:14, 269.65it/s]Tokenizing texts:  62%|██████▏   | 6227/10000 [00:26<00:15, 244.27it/s]Tokenizing texts:  63%|██████▎   | 6259/10000 [00:26<00:14, 262.83it/s]Tokenizing texts:  63%|██████▎   | 6286/10000 [00:26<00:14, 253.54it/s]Tokenizing texts:  63%|██████▎   | 6312/10000 [00:26<00:16, 227.23it/s]Tokenizing texts:  63%|██████▎   | 6336/10000 [00:26<00:16, 225.41it/s]Tokenizing texts:  64%|██████▎   | 6365/10000 [00:26<00:15, 240.57it/s]Tokenizing texts:  64%|██████▍   | 6390/10000 [00:27<00:14, 241.72it/s]Tokenizing texts:  64%|██████▍   | 6420/10000 [00:27<00:14, 254.75it/s]Tokenizing texts:  64%|██████▍   | 6448/10000 [00:27<00:13, 259.95it/s]Tokenizing texts:  65%|██████▍   | 6475/10000 [00:27<00:14, 247.97it/s]Tokenizing texts:  65%|██████▌   | 6501/10000 [00:27<00:13, 250.10it/s]Tokenizing texts:  65%|██████▌   | 6527/10000 [00:27<00:15, 223.96it/s]Tokenizing texts:  66%|██████▌   | 6567/10000 [00:27<00:12, 268.75it/s]Tokenizing texts:  66%|██████▌   | 6597/10000 [00:27<00:12, 276.32it/s]Tokenizing texts:  66%|██████▋   | 6626/10000 [00:27<00:12, 262.61it/s]Tokenizing texts:  67%|██████▋   | 6654/10000 [00:28<00:13, 257.16it/s]Tokenizing texts:  67%|██████▋   | 6683/10000 [00:28<00:12, 264.23it/s]Tokenizing texts:  67%|██████▋   | 6710/10000 [00:28<00:13, 247.79it/s]Tokenizing texts:  67%|██████▋   | 6736/10000 [00:28<00:13, 243.39it/s]Tokenizing texts:  68%|██████▊   | 6767/10000 [00:28<00:12, 259.85it/s]Tokenizing texts:  68%|██████▊   | 6802/10000 [00:28<00:11, 284.96it/s]Tokenizing texts:  68%|██████▊   | 6834/10000 [00:28<00:10, 293.83it/s]Tokenizing texts:  69%|██████▊   | 6866/10000 [00:28<00:10, 300.71it/s]Tokenizing texts:  69%|██████▉   | 6897/10000 [00:28<00:13, 235.30it/s]Tokenizing texts:  69%|██████▉   | 6923/10000 [00:29<00:13, 229.10it/s]Tokenizing texts:  69%|██████▉   | 6948/10000 [00:29<00:13, 231.18it/s]Tokenizing texts:  70%|██████▉   | 6977/10000 [00:29<00:12, 244.82it/s]Tokenizing texts:  70%|███████   | 7005/10000 [00:29<00:11, 253.26it/s]Tokenizing texts:  70%|███████   | 7032/10000 [00:29<00:11, 248.34it/s]Tokenizing texts:  71%|███████   | 7058/10000 [00:29<00:12, 236.53it/s]Tokenizing texts:  71%|███████   | 7083/10000 [00:29<00:13, 213.22it/s]Tokenizing texts:  71%|███████   | 7105/10000 [00:29<00:13, 213.93it/s]Tokenizing texts:  71%|███████▏  | 7135/10000 [00:29<00:12, 236.41it/s]Tokenizing texts:  72%|███████▏  | 7160/10000 [00:30<00:11, 236.72it/s]Tokenizing texts:  72%|███████▏  | 7190/10000 [00:30<00:11, 252.58it/s]Tokenizing texts:  72%|███████▏  | 7216/10000 [00:30<00:11, 246.06it/s]Tokenizing texts:  72%|███████▏  | 7244/10000 [00:30<00:10, 253.45it/s]Tokenizing texts:  73%|███████▎  | 7277/10000 [00:30<00:09, 275.17it/s]Tokenizing texts:  73%|███████▎  | 7311/10000 [00:30<00:09, 292.01it/s]Tokenizing texts:  73%|███████▎  | 7341/10000 [00:30<00:09, 280.74it/s]Tokenizing texts:  74%|███████▎  | 7370/10000 [00:30<00:09, 271.64it/s]Tokenizing texts:  74%|███████▍  | 7398/10000 [00:30<00:10, 252.27it/s]Tokenizing texts:  74%|███████▍  | 7424/10000 [00:31<00:11, 226.50it/s]Tokenizing texts:  74%|███████▍  | 7448/10000 [00:31<00:12, 204.18it/s]Tokenizing texts:  75%|███████▍  | 7481/10000 [00:31<00:10, 229.93it/s]Tokenizing texts:  75%|███████▌  | 7518/10000 [00:31<00:09, 261.85it/s]Tokenizing texts:  75%|███████▌  | 7546/10000 [00:31<00:09, 265.11it/s]Tokenizing texts:  76%|███████▌  | 7574/10000 [00:31<00:09, 264.72it/s]Tokenizing texts:  76%|███████▌  | 7611/10000 [00:31<00:08, 292.88it/s]Tokenizing texts:  76%|███████▋  | 7641/10000 [00:31<00:08, 273.72it/s]Tokenizing texts:  77%|███████▋  | 7674/10000 [00:32<00:08, 287.43it/s]Tokenizing texts:  77%|███████▋  | 7704/10000 [00:32<00:07, 289.93it/s]Tokenizing texts:  77%|███████▋  | 7734/10000 [00:32<00:09, 249.17it/s]Tokenizing texts:  78%|███████▊  | 7761/10000 [00:32<00:09, 244.47it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 258.05it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 237.12it/s]Tokenizing texts:  78%|███████▊  | 7845/10000 [00:32<00:08, 245.30it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 257.63it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 254.45it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:33<00:07, 282.53it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:33<00:07, 279.96it/s]Tokenizing texts:  80%|███████▉  | 7999/10000 [00:33<00:06, 290.53it/s]Tokenizing texts:  80%|████████  | 8029/10000 [00:33<00:06, 286.66it/s]Tokenizing texts:  81%|████████  | 8058/10000 [00:33<00:11, 169.79it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 199.65it/s]Tokenizing texts:  81%|████████  | 8120/10000 [00:33<00:09, 208.25it/s]Tokenizing texts:  81%|████████▏ | 8148/10000 [00:34<00:08, 224.42it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:34<00:07, 237.76it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:34<00:07, 248.53it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 213.03it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 223.45it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 256.76it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 252.70it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:34<00:06, 256.96it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:34<00:06, 256.08it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:35<00:06, 231.63it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:35<00:06, 257.00it/s]Tokenizing texts:  85%|████████▍ | 8472/10000 [00:35<00:05, 262.87it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:35<00:05, 277.89it/s]Tokenizing texts:  85%|████████▌ | 8533/10000 [00:35<00:05, 260.15it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:35<00:05, 266.57it/s]Tokenizing texts:  86%|████████▌ | 8590/10000 [00:35<00:05, 261.66it/s]Tokenizing texts:  86%|████████▌ | 8624/10000 [00:35<00:04, 283.19it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:35<00:04, 284.91it/s]Tokenizing texts:  87%|████████▋ | 8687/10000 [00:36<00:04, 294.61it/s]Tokenizing texts:  87%|████████▋ | 8722/10000 [00:36<00:04, 310.29it/s]Tokenizing texts:  88%|████████▊ | 8754/10000 [00:36<00:04, 278.53it/s]Tokenizing texts:  88%|████████▊ | 8785/10000 [00:36<00:04, 282.72it/s]Tokenizing texts:  88%|████████▊ | 8814/10000 [00:36<00:05, 216.36it/s]Tokenizing texts:  88%|████████▊ | 8839/10000 [00:36<00:05, 218.26it/s]Tokenizing texts:  89%|████████▊ | 8871/10000 [00:36<00:04, 242.73it/s]Tokenizing texts:  89%|████████▉ | 8910/10000 [00:36<00:03, 279.66it/s]Tokenizing texts:  89%|████████▉ | 8941/10000 [00:37<00:03, 283.33it/s]Tokenizing texts:  90%|████████▉ | 8971/10000 [00:37<00:03, 275.57it/s]Tokenizing texts:  90%|█████████ | 9000/10000 [00:37<00:04, 247.65it/s]Tokenizing texts:  90%|█████████ | 9030/10000 [00:37<00:03, 258.87it/s]Tokenizing texts:  91%|█████████ | 9057/10000 [00:37<00:03, 250.04it/s]Tokenizing texts:  91%|█████████ | 9085/10000 [00:37<00:03, 256.16it/s]Tokenizing texts:  91%|█████████ | 9115/10000 [00:37<00:03, 266.62it/s]Tokenizing texts:  91%|█████████▏| 9146/10000 [00:37<00:03, 275.07it/s]Tokenizing texts:  92%|█████████▏| 9175/10000 [00:37<00:02, 278.70it/s]Tokenizing texts:  92%|█████████▏| 9205/10000 [00:38<00:02, 282.49it/s]Tokenizing texts:  92%|█████████▏| 9234/10000 [00:38<00:02, 274.14it/s]Tokenizing texts:  93%|█████████▎| 9262/10000 [00:38<00:02, 274.35it/s]Tokenizing texts:  93%|█████████▎| 9290/10000 [00:38<00:02, 253.86it/s]Tokenizing texts:  93%|█████████▎| 9316/10000 [00:38<00:02, 231.53it/s]Tokenizing texts:  93%|█████████▎| 9348/10000 [00:38<00:02, 253.90it/s]Tokenizing texts:  94%|█████████▍| 9375/10000 [00:38<00:02, 221.18it/s]Tokenizing texts:  94%|█████████▍| 9403/10000 [00:38<00:02, 229.53it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:39<00:02, 229.50it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:39<00:02, 233.97it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:39<00:02, 243.03it/s]Tokenizing texts:  95%|█████████▌| 9504/10000 [00:39<00:02, 181.85it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 221.69it/s]Tokenizing texts:  96%|█████████▌| 9566/10000 [00:39<00:01, 229.08it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 234.19it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 232.94it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:40<00:01, 235.70it/s]Tokenizing texts:  97%|█████████▋| 9674/10000 [00:40<00:01, 241.76it/s]Tokenizing texts:  97%|█████████▋| 9699/10000 [00:40<00:01, 236.86it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:40<00:01, 239.16it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 206.36it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 214.39it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 259.74it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 253.01it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 259.14it/s]Tokenizing texts:  99%|█████████▉| 9907/10000 [00:41<00:00, 286.47it/s]Tokenizing texts:  99%|█████████▉| 9937/10000 [00:41<00:00, 283.66it/s]Tokenizing texts: 100%|█████████▉| 9968/10000 [00:41<00:00, 291.04it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 242.04it/s]
2025-12-09 12:02:30.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 12.036738395690918
2025-12-09 12:02:30.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 12.02636432647705
2025-12-09 12:02:30.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 12.018957138061523
2025-12-09 12:02:30.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 12.01760196685791
2025-12-09 12:02:30.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 12.037823677062988
2025-12-09 12:02:30.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 12.0309476852417
2025-12-09 12:02:30.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 11.991897583007812
2025-12-09 12:02:31.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 11.987770080566406
2025-12-09 12:02:31.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 11.986248016357422
2025-12-09 12:02:31.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 11.993215560913086
2025-12-09 12:02:31.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 12.000655174255371
2025-12-09 12:02:31.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 11.919689178466797
2025-12-09 12:02:31.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 11.95062255859375
2025-12-09 12:02:31.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 11.895785331726074
2025-12-09 12:02:31.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 11.834922790527344
2025-12-09 12:02:32.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 11.837024688720703
2025-12-09 12:02:32.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 11.860803604125977
2025-12-09 12:02:32.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 11.792881965637207
2025-12-09 12:02:32.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 11.659875869750977
2025-12-09 12:02:32.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 11.634784698486328
2025-12-09 12:02:32.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 11.56049919128418
2025-12-09 12:02:32.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 11.453001976013184
2025-12-09 12:02:32.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 11.292013168334961
2025-12-09 12:02:33.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 11.25639820098877
2025-12-09 12:02:33.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 11.1045503616333
2025-12-09 12:02:33.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 10.975249290466309
2025-12-09 12:02:33.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 11.030375480651855
2025-12-09 12:02:33.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 10.757047653198242
2025-12-09 12:02:33.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 10.614514350891113
2025-12-09 12:02:33.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 10.483620643615723
2025-12-09 12:02:33.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 10.360397338867188
2025-12-09 12:02:34.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 10.395875930786133
2025-12-09 12:02:34.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 10.118990898132324
2025-12-09 12:02:34.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 9.899840354919434
2025-12-09 12:02:34.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 9.773759841918945
2025-12-09 12:02:34.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 9.82335376739502
2025-12-09 12:02:34.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 9.529322624206543
2025-12-09 12:02:34.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 9.801416397094727
2025-12-09 12:02:34.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 9.269388198852539
2025-12-09 12:02:35.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 9.103315353393555
2025-12-09 12:02:35.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 9.104509353637695
2025-12-09 12:02:35.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 8.767719268798828
2025-12-09 12:02:35.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 8.628226280212402
2025-12-09 12:02:35.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 8.666290283203125
2025-12-09 12:02:35.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 8.34058952331543
2025-12-09 12:02:35.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 8.234347343444824
2025-12-09 12:02:35.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 8.425394058227539
2025-12-09 12:02:36.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 7.940915584564209
2025-12-09 12:02:36.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 8.199346542358398
2025-12-09 12:02:36.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 7.914145469665527
2025-12-09 12:02:36.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 7.928239345550537
2025-12-09 12:02:36.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 8.227005958557129
2025-12-09 12:02:36.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 8.675325393676758
2025-12-09 12:02:36.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 8.032774925231934
2025-12-09 12:02:36.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 7.828540325164795
2025-12-09 12:02:37.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 7.708550453186035
2025-12-09 12:02:37.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 7.577162742614746
2025-12-09 12:02:37.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 7.897932052612305
2025-12-09 12:02:37.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 7.851442337036133
2025-12-09 12:02:37.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 8.068791389465332
2025-12-09 12:02:37.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 7.603549480438232
2025-12-09 12:02:37.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 7.741434574127197
2025-12-09 12:02:37.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 7.835379600524902
2025-12-09 12:02:38.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 7.887382984161377
2025-12-09 12:02:38.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 7.909825325012207
2025-12-09 12:02:38.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 7.296028137207031
2025-12-09 12:02:38.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 8.836092948913574
2025-12-09 12:02:38.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 7.792585372924805
2025-12-09 12:02:38.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 7.784920692443848
2025-12-09 12:02:38.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 7.1685333251953125
2025-12-09 12:02:39.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 7.522121429443359
2025-12-09 12:02:39.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 7.726876258850098
2025-12-09 12:02:39.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 7.613091468811035
2025-12-09 12:02:39.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 7.4364333152771
2025-12-09 12:02:39.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 7.757497787475586
2025-12-09 12:02:39.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 7.767163276672363
2025-12-09 12:02:39.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 7.890933513641357
2025-12-09 12:02:39.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 7.639053821563721
2025-12-09 12:02:40.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 7.800591468811035
2025-12-09 12:02:40.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 7.693432331085205
2025-12-09 12:02:40.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 8.238616943359375
2025-12-09 12:02:40.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 7.626345157623291
2025-12-09 12:02:40.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 7.510646343231201
2025-12-09 12:02:40.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 7.437651634216309
2025-12-09 12:02:40.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 7.579714775085449
2025-12-09 12:02:40.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 8.069463729858398
2025-12-09 12:02:41.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 7.801568508148193
2025-12-09 12:02:41.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 7.333349704742432
2025-12-09 12:02:41.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 7.359031677246094
2025-12-09 12:02:41.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 7.200258255004883
2025-12-09 12:02:41.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 7.886744976043701
2025-12-09 12:02:41.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 6.820188999176025
2025-12-09 12:02:41.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 7.892992973327637
2025-12-09 12:02:41.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 7.521602630615234
2025-12-09 12:02:42.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 7.365626811981201
2025-12-09 12:02:42.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 7.669881820678711
2025-12-09 12:02:42.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 7.361945629119873
2025-12-09 12:02:42.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 7.155930519104004
2025-12-09 12:02:42.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 7.18398904800415
2025-12-09 12:02:42.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 6.31261682510376
2025-12-09 12:02:42.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009999999072578703 Training loss: 7.514684200286865
2025-12-09 12:02:42.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0009999996290315154 Training loss: 7.555935382843018
2025-12-09 12:02:43.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0009999991653210384 Training loss: 7.120645523071289
2025-12-09 12:02:43.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0009999985161266117 Training loss: 7.374326229095459
2025-12-09 12:02:43.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0009999976814484759 Training loss: 7.324737548828125
2025-12-09 12:02:43.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0009999966612869405 Training loss: 7.482791900634766
2025-12-09 12:02:43.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0009999954556423843 Training loss: 7.2624711990356445
2025-12-09 12:02:43.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0009999940645152542 Training loss: 7.3493781089782715
2025-12-09 12:02:43.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0009999924879060664 Training loss: 7.205756664276123
2025-12-09 12:02:43.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.000999990725815406 Training loss: 7.00725793838501
2025-12-09 12:02:44.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0009999887782439264 Training loss: 7.390416622161865
2025-12-09 12:02:44.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.00099998664519235 Training loss: 7.263221263885498
2025-12-09 12:02:44.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0009999843266614685 Training loss: 7.034301280975342
2025-12-09 12:02:44.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0009999818226521416 Training loss: 7.4698662757873535
2025-12-09 12:02:44.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0009999791331652982 Training loss: 7.288463592529297
2025-12-09 12:02:44.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0009999762582019365 Training loss: 6.652538299560547
2025-12-09 12:02:44.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.0009999731977631226 Training loss: 7.2399115562438965
2025-12-09 12:02:44.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0009999699518499921 Training loss: 6.98307466506958
2025-12-09 12:02:45.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0009999665204637486 Training loss: 7.082936763763428
2025-12-09 12:02:45.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0009999629036056656 Training loss: 7.420849800109863
2025-12-09 12:02:45.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0009999591012770847 Training loss: 7.248884677886963
2025-12-09 12:02:45.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0009999551134794165 Training loss: 6.9309234619140625
2025-12-09 12:02:45.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00099995094021414 Training loss: 6.948024749755859
2025-12-09 12:02:45.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0009999465814828036 Training loss: 7.3201446533203125
2025-12-09 12:02:45.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0009999420372870244 Training loss: 7.280089855194092
2025-12-09 12:02:45.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0009999373076284876 Training loss: 7.564404487609863
2025-12-09 12:02:46.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0009999323925089486 Training loss: 7.216577529907227
2025-12-09 12:02:46.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00099992729193023 Training loss: 7.2674689292907715
2025-12-09 12:02:46.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0009999220058942244 Training loss: 7.557126045227051
2025-12-09 12:02:46.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0009999165344028926 Training loss: 7.2423930168151855
2025-12-09 12:02:46.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0009999108774582644 Training loss: 7.150573253631592
2025-12-09 12:02:46.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0009999050350624381 Training loss: 7.539736747741699
2025-12-09 12:02:46.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0009998990072175814 Training loss: 7.04423189163208
2025-12-09 12:02:47.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0009998927939259303 Training loss: 6.621234893798828
2025-12-09 12:02:47.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0009998863951897897 Training loss: 7.258621692657471
2025-12-09 12:02:47.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0009998798110115333 Training loss: 7.264430522918701
2025-12-09 12:02:47.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0009998730413936037 Training loss: 7.03380012512207
2025-12-09 12:02:47.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0009998660863385124 Training loss: 7.172884941101074
2025-12-09 12:02:47.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0009998589458488389 Training loss: 7.309265613555908
2025-12-09 12:02:47.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0009998516199272328 Training loss: 6.357180118560791
2025-12-09 12:02:47.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0009998441085764113 Training loss: 6.89383602142334
2025-12-09 12:02:48.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0009998364117991612 Training loss: 7.134244918823242
2025-12-09 12:02:48.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0009998285295983375 Training loss: 7.007519245147705
2025-12-09 12:02:48.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0009998204619768645 Training loss: 7.323397636413574
2025-12-09 12:02:48.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0009998122089377348 Training loss: 7.029985427856445
2025-12-09 12:02:48.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0009998037704840102 Training loss: 6.964159965515137
2025-12-09 12:02:48.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.000999795146618821 Training loss: 7.127078056335449
2025-12-09 12:02:48.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0009997863373453664 Training loss: 7.4188127517700195
2025-12-09 12:02:48.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.000999777342666914 Training loss: 7.1843109130859375
2025-12-09 12:02:49.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0009997681625868013 Training loss: 6.838796615600586
2025-12-09 12:02:49.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0009997587971084334 Training loss: 7.101072311401367
2025-12-09 12:02:49.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0009997492462352846 Training loss: 7.141707897186279
2025-12-09 12:02:49.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0009997395099708981 Training loss: 6.897332668304443
2025-12-09 12:02:49.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0009997295883188856 Training loss: 7.053903579711914
2025-12-09 12:02:49.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0009997194812829276 Training loss: 6.880746841430664
2025-12-09 12:02:49.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0009997091888667737 Training loss: 6.995784759521484
2025-12-09 12:02:49.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.000999698711074242 Training loss: 6.873544216156006
2025-12-09 12:02:50.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0009996880479092197 Training loss: 7.023534774780273
2025-12-09 12:02:50.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.000999677199375662 Training loss: 6.821414470672607
2025-12-09 12:02:50.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0009996661654775938 Training loss: 6.832840919494629
2025-12-09 12:02:50.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.000999654946219108 Training loss: 7.055845260620117
2025-12-09 12:02:50.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.000999643541604367 Training loss: 6.89979362487793
2025-12-09 12:02:50.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.000999631951637601 Training loss: 7.099783420562744
2025-12-09 12:02:50.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0009996201763231099 Training loss: 7.677933692932129
2025-12-09 12:02:50.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0009996082156652618 Training loss: 6.602964878082275
2025-12-09 12:02:51.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.000999596069668494 Training loss: 6.865091800689697
2025-12-09 12:02:51.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.000999583738337312 Training loss: 6.913546562194824
2025-12-09 12:02:51.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0009995712216762903 Training loss: 6.180954456329346
2025-12-09 12:02:51.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0009995585196900722 Training loss: 7.060033321380615
2025-12-09 12:02:51.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.00099954563238337 Training loss: 6.950040817260742
2025-12-09 12:02:51.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0009995325597609644 Training loss: 6.660050868988037
2025-12-09 12:02:51.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.000999519301827705 Training loss: 7.143718242645264
2025-12-09 12:02:51.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0009995058585885095 Training loss: 6.540797710418701
2025-12-09 12:02:52.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0009994922300483656 Training loss: 6.557721138000488
2025-12-09 12:02:52.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.000999478416212329 Training loss: 6.888328552246094
2025-12-09 12:02:52.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.0009994644170855237 Training loss: 6.574862957000732
2025-12-09 12:02:52.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0009994502326731434 Training loss: 5.484664440155029
2025-12-09 12:02:52.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0009994358629804498 Training loss: 7.277833938598633
2025-12-09 12:02:52.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0009994213080127738 Training loss: 7.047826290130615
2025-12-09 12:02:52.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0009994065677755147 Training loss: 7.049299716949463
2025-12-09 12:02:53.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0009993916422741409 Training loss: 7.0731916427612305
2025-12-09 12:02:53.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.000999376531514189 Training loss: 7.2615065574646
2025-12-09 12:02:53.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0009993612355012646 Training loss: 7.214653015136719
2025-12-09 12:02:53.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0009993457542410422 Training loss: 6.9679694175720215
2025-12-09 12:02:53.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.000999330087739265 Training loss: 6.694363594055176
2025-12-09 12:02:53.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0009993142360017445 Training loss: 6.91012716293335
2025-12-09 12:02:53.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0009992981990343613 Training loss: 7.116302013397217
2025-12-09 12:02:53.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0009992819768430649 Training loss: 7.042596340179443
2025-12-09 12:02:54.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0009992655694338725 Training loss: 6.5011091232299805
2025-12-09 12:02:54.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0009992489768128714 Training loss: 6.840176105499268
2025-12-09 12:02:54.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0009992321989862165 Training loss: 7.073214530944824
2025-12-09 12:02:54.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0009992152359601322 Training loss: 6.88706636428833
2025-12-09 12:02:54.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.000999198087740911 Training loss: 6.853440284729004
2025-12-09 12:02:54.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0009991807543349145 Training loss: 7.084873676300049
2025-12-09 12:02:54.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.000999163235748573 Training loss: 6.971322059631348
2025-12-09 12:02:54.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0009991455319883849 Training loss: 6.7652812004089355
2025-12-09 12:02:55.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0009991276430609181 Training loss: 6.711162090301514
2025-12-09 12:02:55.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0009991095689728087 Training loss: 7.068493843078613
2025-12-09 12:02:55.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0009990913097307613 Training loss: 7.03703498840332
2025-12-09 12:02:55.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0009990728653415503 Training loss: 6.979121208190918
2025-12-09 12:02:55.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0009990542358120174 Training loss: 6.697172164916992
2025-12-09 12:02:55.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0009990354211490736 Training loss: 6.676518440246582
2025-12-09 12:02:55.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0009990164213596986 Training loss: 7.258223056793213
2025-12-09 12:02:55.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0009989972364509408 Training loss: 7.310309886932373
2025-12-09 12:02:56.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0009989778664299172 Training loss: 6.819150447845459
2025-12-09 12:02:56.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0009989583113038133 Training loss: 7.311183452606201
2025-12-09 12:02:56.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0009989385710798837 Training loss: 7.003556251525879
2025-12-09 12:02:56.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.0009989186457654514 Training loss: 7.473839282989502
2025-12-09 12:02:56.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0009988985353679076 Training loss: 7.069533348083496
2025-12-09 12:02:56.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0009988782398947132 Training loss: 6.571652889251709
2025-12-09 12:02:56.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0009988577593533967 Training loss: 6.988352298736572
2025-12-09 12:02:56.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.000998837093751556 Training loss: 6.62106466293335
2025-12-09 12:02:57.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0009988162430968576 Training loss: 7.119531154632568
2025-12-09 12:02:57.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.000998795207397036 Training loss: 7.450992107391357
2025-12-09 12:02:57.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.000998773986659895 Training loss: 6.72132682800293
2025-12-09 12:02:57.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0009987525808933069 Training loss: 6.98527717590332
2025-12-09 12:02:57.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0009987309901052122 Training loss: 6.761155128479004
2025-12-09 12:02:57.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.000998709214303621 Training loss: 6.796388149261475
2025-12-09 12:02:57.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.000998687253496611 Training loss: 6.541527271270752
2025-12-09 12:02:57.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0009986651076923287 Training loss: 6.850901126861572
2025-12-09 12:02:58.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0009986427768989903 Training loss: 6.703541278839111
2025-12-09 12:02:58.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0009986202611248793 Training loss: 7.034614562988281
2025-12-09 12:02:58.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0009985975603783483 Training loss: 6.966471195220947
2025-12-09 12:02:58.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.000998574674667819 Training loss: 6.786764621734619
2025-12-09 12:02:58.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0009985516040017807 Training loss: 6.825815200805664
2025-12-09 12:02:58.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0009985283483887923 Training loss: 6.788971900939941
2025-12-09 12:02:58.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0009985049078374806 Training loss: 6.663442134857178
2025-12-09 12:02:58.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0009984812823565416 Training loss: 6.878453254699707
2025-12-09 12:02:59.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0009984574719547395 Training loss: 6.833503723144531
2025-12-09 12:02:59.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.000998433476640907 Training loss: 6.888825416564941
2025-12-09 12:02:59.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0009984092964239462 Training loss: 7.331724643707275
2025-12-09 12:02:59.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0009983849313128263 Training loss: 6.2507710456848145
2025-12-09 12:02:59.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0009983603813165868 Training loss: 6.723475933074951
2025-12-09 12:02:59.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0009983356464443346 Training loss: 7.062877655029297
2025-12-09 12:02:59.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0009983107267052458 Training loss: 6.658452987670898
2025-12-09 12:03:00.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0009982856221085643 Training loss: 7.55913782119751
2025-12-09 12:03:00.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0009982603326636036 Training loss: 7.175309658050537
2025-12-09 12:03:00.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0009982348583797453 Training loss: 6.67604923248291
2025-12-09 12:03:00.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0009982091992664392 Training loss: 6.451696395874023
2025-12-09 12:03:00.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0009981833553332044 Training loss: 6.896419525146484
2025-12-09 12:03:00.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0009981573265896281 Training loss: 6.506529808044434
2025-12-09 12:03:00.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.000998131113045366 Training loss: 6.5095906257629395
2025-12-09 12:03:00.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0009981047147101425 Training loss: 6.88517427444458
2025-12-09 12:03:01.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0009980781315937506 Training loss: 7.00541877746582
2025-12-09 12:03:01.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.000998051363706052 Training loss: 6.661247253417969
2025-12-09 12:03:01.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0009980244110569766 Training loss: 6.756035327911377
2025-12-09 12:03:01.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0009979972736565226 Training loss: 6.5670270919799805
2025-12-09 12:03:01.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0009979699515147579 Training loss: 6.104981422424316
2025-12-09 12:03:01.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0009979424446418172 Training loss: 6.921023368835449
2025-12-09 12:03:01.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0009979147530479056 Training loss: 6.740763187408447
2025-12-09 12:03:01.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0009978868767432953 Training loss: 6.626884937286377
2025-12-09 12:03:02.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0009978588157383277 Training loss: 6.661087512969971
2025-12-09 12:03:02.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0009978305700434125 Training loss: 6.886436939239502
2025-12-09 12:03:02.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.000997802139669028 Training loss: 7.256704330444336
2025-12-09 12:03:02.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0009977735246257209 Training loss: 6.624797821044922
2025-12-09 12:03:02.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0009977447249241065 Training loss: 6.65022611618042
2025-12-09 12:03:02.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.0009977157405748687 Training loss: 6.744448661804199
2025-12-09 12:03:02.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0009976865715887596 Training loss: 7.167446136474609
2025-12-09 12:03:02.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0009976572179766 Training loss: 6.879934787750244
2025-12-09 12:03:03.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0009976276797492793 Training loss: 6.565054893493652
2025-12-09 12:03:03.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0009975979569177551 Training loss: 7.115324974060059
2025-12-09 12:03:03.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0009975680494930539 Training loss: 7.355686664581299
2025-12-09 12:03:03.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00099753795748627 Training loss: 6.6200690269470215
2025-12-09 12:03:03.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0009975076809085669 Training loss: 6.771648406982422
2025-12-09 12:03:03.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0009974772197711762 Training loss: 7.137760162353516
2025-12-09 12:03:03.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.000997446574085398 Training loss: 6.4746503829956055
2025-12-09 12:03:03.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0009974157438626008 Training loss: 5.891612529754639
2025-12-09 12:03:04.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0009973847291142217 Training loss: 6.957813262939453
2025-12-09 12:03:04.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0009973535298517663 Training loss: 6.72880220413208
2025-12-09 12:03:04.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0009973221460868086 Training loss: 6.754021167755127
2025-12-09 12:03:04.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0009972905778309906 Training loss: 7.253818988800049
2025-12-09 12:03:04.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0009972588250960234 Training loss: 6.7972846031188965
2025-12-09 12:03:04.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0009972268878936862 Training loss: 6.838903903961182
2025-12-09 12:03:04.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.000997194766235827 Training loss: 7.057294845581055
2025-12-09 12:03:04.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0009971624601343614 Training loss: 6.346435070037842
2025-12-09 12:03:05.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0009971299696012743 Training loss: 6.321258544921875
2025-12-09 12:03:05.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009970972946486186 Training loss: 6.602957248687744
2025-12-09 12:03:05.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0009970644352885157 Training loss: 6.617111682891846
2025-12-09 12:03:05.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0009970313915331553 Training loss: 6.549430847167969
2025-12-09 12:03:05.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009969981633947955 Training loss: 6.563640594482422
2025-12-09 12:03:05.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0009969647508857632 Training loss: 6.743109703063965
2025-12-09 12:03:05.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.000996931154018453 Training loss: 6.758645057678223
2025-12-09 12:03:06.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009968973728053288 Training loss: 6.976416110992432
2025-12-09 12:03:06.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0009968634072589219 Training loss: 7.179847240447998
2025-12-09 12:03:06.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0009968292573918325 Training loss: 6.568514347076416
2025-12-09 12:03:06.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0009967949232167295 Training loss: 6.834122657775879
2025-12-09 12:03:06.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0009967604047463492 Training loss: 6.69897985458374
2025-12-09 12:03:06.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0009967257019934974 Training loss: 6.484487056732178
2025-12-09 12:03:06.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0009966908149710476 Training loss: 6.756567478179932
2025-12-09 12:03:06.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0009966557436919415 Training loss: 6.572447776794434
2025-12-09 12:03:07.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.00099662048816919 Training loss: 6.504932403564453
2025-12-09 12:03:07.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.000996585048415871 Training loss: 6.9161152839660645
2025-12-09 12:03:07.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0009965494244451323 Training loss: 6.706542491912842
2025-12-09 12:03:07.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0009965136162701888 Training loss: 7.468760013580322
2025-12-09 12:03:07.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0009964776239043244 Training loss: 6.756986618041992
2025-12-09 12:03:07.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0009964414473608912 Training loss: 6.908231258392334
2025-12-09 12:03:07.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0009964050866533092 Training loss: 6.390747547149658
2025-12-09 12:03:07.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0009963685417950677 Training loss: 6.681149959564209
2025-12-09 12:03:08.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.000996331812799723 Training loss: 7.193514347076416
2025-12-09 12:03:08.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0009962948996809007 Training loss: 6.610032081604004
2025-12-09 12:03:08.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0009962578024522947 Training loss: 6.553871154785156
2025-12-09 12:03:08.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0009962205211276665 Training loss: 6.729949474334717
2025-12-09 12:03:08.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0009961830557208464 Training loss: 6.4343438148498535
2025-12-09 12:03:08.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.000996145406245733 Training loss: 7.115503311157227
2025-12-09 12:03:08.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0009961075727162928 Training loss: 6.810252666473389
2025-12-09 12:03:08.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0009960695551465611 Training loss: 6.6158552169799805
2025-12-09 12:03:09.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.000996031353550641 Training loss: 6.8779473304748535
2025-12-09 12:03:09.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0009959929679427047 Training loss: 6.637306213378906
2025-12-09 12:03:09.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0009959543983369913 Training loss: 6.6347336769104
2025-12-09 12:03:09.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.000995915644747809 Training loss: 6.637045860290527
2025-12-09 12:03:09.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0009958767071895347 Training loss: 6.737940311431885
2025-12-09 12:03:09.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0009958375856766127 Training loss: 6.74445104598999
2025-12-09 12:03:09.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0009957982802235555 Training loss: 6.655014991760254
2025-12-09 12:03:09.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0009957587908449449 Training loss: 6.346701145172119
2025-12-09 12:03:10.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0009957191175554295 Training loss: 6.980985641479492
2025-12-09 12:03:10.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0009956792603697273 Training loss: 6.929077625274658
2025-12-09 12:03:10.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.000995639219302624 Training loss: 6.7176361083984375
2025-12-09 12:03:10.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0009955989943689733 Training loss: 6.87581205368042
2025-12-09 12:03:10.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0009955585855836978 Training loss: 6.929023742675781
2025-12-09 12:03:10.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0009955179929617875 Training loss: 6.689305782318115
2025-12-09 12:03:10.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0009954772165183012 Training loss: 6.791595935821533
2025-12-09 12:03:11.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0009954362562683658 Training loss: 7.045203685760498
2025-12-09 12:03:11.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.000995395112227176 Training loss: 6.652148246765137
2025-12-09 12:03:11.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.000995353784409995 Training loss: 5.790577411651611
2025-12-09 12:03:11.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0009953122728321542 Training loss: 6.926495552062988
2025-12-09 12:03:11.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0009952705775090529 Training loss: 6.6461381912231445
2025-12-09 12:03:11.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0009952286984561591 Training loss: 6.465615749359131
2025-12-09 12:03:11.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0009951866356890083 Training loss: 6.742863178253174
2025-12-09 12:03:11.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0009951443892232048 Training loss: 7.383542537689209
2025-12-09 12:03:12.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0009951019590744203 Training loss: 7.826157569885254
2025-12-09 12:03:12.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0009950593452583952 Training loss: 6.987085819244385
2025-12-09 12:03:12.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0009950165477909379 Training loss: 6.665283679962158
2025-12-09 12:03:12.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0009949735666879252 Training loss: 6.912450790405273
2025-12-09 12:03:12.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.000994930401965301 Training loss: 6.783586502075195
2025-12-09 12:03:12.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.000994887053639079 Training loss: 6.82138204574585
2025-12-09 12:03:12.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0009948435217253394 Training loss: 6.612948417663574
2025-12-09 12:03:12.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0009947998062402312 Training loss: 6.449861526489258
2025-12-09 12:03:13.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.000994755907199972 Training loss: 5.801375389099121
2025-12-09 12:03:13.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0009947118246208461 Training loss: 6.454620361328125
2025-12-09 12:03:13.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0009946675585192075 Training loss: 6.646521091461182
2025-12-09 12:03:13.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0009946231089114773 Training loss: 6.713217735290527
2025-12-09 12:03:13.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.000994578475814145 Training loss: 6.851408004760742
2025-12-09 12:03:13.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0009945336592437678 Training loss: 7.60982084274292
2025-12-09 12:03:13.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0009944886592169711 Training loss: 7.006700038909912
2025-12-09 12:03:13.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.000994443475750449 Training loss: 6.617615699768066
2025-12-09 12:03:14.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.000994398108860963 Training loss: 6.445258617401123
2025-12-09 12:03:14.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0009943525585653428 Training loss: 6.739959239959717
2025-12-09 12:03:14.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0009943068248804859 Training loss: 6.099700450897217
2025-12-09 12:03:14.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.000994260907823358 Training loss: 6.741822719573975
2025-12-09 12:03:14.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0009942148074109933 Training loss: 6.836730480194092
2025-12-09 12:03:14.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0009941685236604934 Training loss: 6.84077262878418
2025-12-09 12:03:14.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.0009941220565890278 Training loss: 6.596435070037842
2025-12-09 12:03:14.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.000994075406213835 Training loss: 6.433582305908203
2025-12-09 12:03:15.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0009940285725522201 Training loss: 6.747947692871094
2025-12-09 12:03:15.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0009939815556215576 Training loss: 6.598106861114502
2025-12-09 12:03:15.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0009939343554392886 Training loss: 6.798794269561768
2025-12-09 12:03:15.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.0009938869720229233 Training loss: 6.68513822555542
2025-12-09 12:03:15.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0009938394053900395 Training loss: 6.300459384918213
2025-12-09 12:03:15.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0009937916555582827 Training loss: 6.6517815589904785
2025-12-09 12:03:15.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.000993743722545367 Training loss: 6.703118801116943
2025-12-09 12:03:15.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0009936956063690734 Training loss: 6.3554229736328125
2025-12-09 12:03:16.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0009936473070472518 Training loss: 6.882552146911621
2025-12-09 12:03:16.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0009935988245978198 Training loss: 6.703530788421631
2025-12-09 12:03:16.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0009935501590387628 Training loss: 6.910624027252197
2025-12-09 12:03:16.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0009935013103881344 Training loss: 6.729982376098633
2025-12-09 12:03:16.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0009934522786640555 Training loss: 6.500086784362793
2025-12-09 12:03:16.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0009934030638847156 Training loss: 6.846634387969971
2025-12-09 12:03:16.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0009933536660683717 Training loss: 7.3070878982543945
2025-12-09 12:03:16.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0009933040852333488 Training loss: 7.087226867675781
2025-12-09 12:03:17.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00099325432139804 Training loss: 7.150514602661133
2025-12-09 12:03:17.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0009932043745809064 Training loss: 6.274015426635742
2025-12-09 12:03:17.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.000993154244800476 Training loss: 6.670828819274902
2025-12-09 12:03:17.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.0009931039320753457 Training loss: 6.832633972167969
2025-12-09 12:03:17.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00099305343642418 Training loss: 6.2933149337768555
2025-12-09 12:03:17.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0009930027578657114 Training loss: 6.556875705718994
2025-12-09 12:03:17.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0009929518964187393 Training loss: 6.5515313148498535
2025-12-09 12:03:17.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0009929008521021325 Training loss: 6.835233211517334
2025-12-09 12:03:18.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.0009928496249348266 Training loss: 7.311041355133057
2025-12-09 12:03:18.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.000992798214935825 Training loss: 7.048940181732178
2025-12-09 12:03:18.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0009927466221241995 Training loss: 6.962950706481934
2025-12-09 12:03:18.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0009926948465190893 Training loss: 6.581498146057129
2025-12-09 12:03:18.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0009926428881397015 Training loss: 6.305028915405273
2025-12-09 12:03:18.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0009925907470053111 Training loss: 6.422947406768799
2025-12-09 12:03:18.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.0009925384231352606 Training loss: 6.871475696563721
2025-12-09 12:03:19.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0009924859165489608 Training loss: 6.6776814460754395
2025-12-09 12:03:19.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0009924332272658897 Training loss: 6.732749938964844
2025-12-09 12:03:19.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.0009923803553055937 Training loss: 6.423061847686768
2025-12-09 12:03:19.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0009923273006876864 Training loss: 6.569746971130371
2025-12-09 12:03:19.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0009922740634318494 Training loss: 7.094375133514404
2025-12-09 12:03:19.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.0009922206435578323 Training loss: 6.990843296051025
2025-12-09 12:03:19.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0009921670410854518 Training loss: 6.835886001586914
2025-12-09 12:03:19.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0009921132560345928 Training loss: 6.617934703826904
2025-12-09 12:03:20.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0009920592884252082 Training loss: 6.589437007904053
2025-12-09 12:03:20.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0009920051382773178 Training loss: 7.052041053771973
2025-12-09 12:03:20.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.00099195080561101 Training loss: 6.554508209228516
2025-12-09 12:03:20.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.0009918962904464407 Training loss: 7.179710388183594
2025-12-09 12:03:20.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0009918415928038325 Training loss: 6.453556537628174
2025-12-09 12:03:20.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.000991786712703477 Training loss: 6.961930274963379
2025-12-09 12:03:20.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0009917316501657334 Training loss: 6.633442401885986
2025-12-09 12:03:20.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0009916764052110274 Training loss: 6.538117408752441
2025-12-09 12:03:21.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0009916209778598536 Training loss: 6.53241491317749
2025-12-09 12:03:21.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0009915653681327736 Training loss: 6.836720943450928
2025-12-09 12:03:21.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.000991509576050417 Training loss: 6.298804759979248
2025-12-09 12:03:21.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0009914536016334807 Training loss: 6.660128593444824
2025-12-09 12:03:21.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0009913974449027297 Training loss: 6.573736667633057
2025-12-09 12:03:21.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0009913411058789963 Training loss: 6.468829154968262
2025-12-09 12:03:21.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0009912845845831805 Training loss: 6.0087971687316895
2025-12-09 12:03:21.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00099122788103625 Training loss: 6.561327934265137
2025-12-09 12:03:22.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0009911709952592396 Training loss: 6.72817850112915
2025-12-09 12:03:22.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0009911139272732526 Training loss: 6.421874046325684
2025-12-09 12:03:22.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.0009910566770994593 Training loss: 6.9131598472595215
2025-12-09 12:03:22.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0009909992447590978 Training loss: 7.801738739013672
2025-12-09 12:03:22.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.0009909416302734736 Training loss: 6.818877696990967
2025-12-09 12:03:22.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.0009908838336639598 Training loss: 6.745377063751221
2025-12-09 12:03:22.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.000990825854951997 Training loss: 6.232327461242676
2025-12-09 12:03:22.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0009907676941590937 Training loss: 6.878891944885254
2025-12-09 12:03:23.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0009907093513068259 Training loss: 6.4163126945495605
2025-12-09 12:03:23.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.0009906508264168365 Training loss: 6.347307205200195
2025-12-09 12:03:23.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.0009905921195108368 Training loss: 6.593333721160889
2025-12-09 12:03:23.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0009905332306106049 Training loss: 6.610775470733643
2025-12-09 12:03:23.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.000990474159737987 Training loss: 6.1939496994018555
2025-12-09 12:03:23.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0009904149069148963 Training loss: 6.496598720550537
2025-12-09 12:03:23.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.000990355472163314 Training loss: 6.78249454498291
2025-12-09 12:03:23.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0009902958555052881 Training loss: 6.616597652435303
2025-12-09 12:03:24.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0009902360569629348 Training loss: 6.598101615905762
2025-12-09 12:03:24.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0009901760765584375 Training loss: 7.046745777130127
2025-12-09 12:03:24.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.000990115914314047 Training loss: 6.718498229980469
2025-12-09 12:03:24.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0009900555702520816 Training loss: 6.860625743865967
2025-12-09 12:03:24.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.000989995044394927 Training loss: 7.100838661193848
2025-12-09 12:03:24.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0009899343367650365 Training loss: 6.6148905754089355
2025-12-09 12:03:24.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0009898734473849304 Training loss: 6.536470890045166
2025-12-09 12:03:25.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0009898123762771972 Training loss: 7.065069675445557
2025-12-09 12:03:25.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.000989751123464492 Training loss: 6.761075973510742
2025-12-09 12:03:25.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0009896896889695376 Training loss: 6.513956546783447
2025-12-09 12:03:25.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0009896280728151248 Training loss: 6.822356700897217
2025-12-09 12:03:25.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.0009895662750241108 Training loss: 6.746079444885254
2025-12-09 12:03:25.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0009895042956194209 Training loss: 6.603967189788818
2025-12-09 12:03:25.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0009894421346240473 Training loss: 6.528581142425537
2025-12-09 12:03:25.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0009893797920610496 Training loss: 6.723762035369873
2025-12-09 12:03:26.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0009893172679535552 Training loss: 6.841834545135498
2025-12-09 12:03:26.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0009892545623247585 Training loss: 7.47447395324707
2025-12-09 12:03:26.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0009891916751979218 Training loss: 6.604770660400391
2025-12-09 12:03:26.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0009891286065963733 Training loss: 6.616488456726074
2025-12-09 12:03:26.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0009890653565435101 Training loss: 6.409182548522949
2025-12-09 12:03:26.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0009890019250627959 Training loss: 7.317986011505127
2025-12-09 12:03:26.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0009889383121777617 Training loss: 7.12453556060791
2025-12-09 12:03:26.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.000988874517912006 Training loss: 6.842548370361328
2025-12-09 12:03:27.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0009888105422891941 Training loss: 6.871623992919922
2025-12-09 12:03:27.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0009887463853330593 Training loss: 7.00991153717041
2025-12-09 12:03:27.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.000988682047067402 Training loss: 6.517697334289551
2025-12-09 12:03:27.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.000988617527516089 Training loss: 6.929253578186035
2025-12-09 12:03:27.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0009885528267030556 Training loss: 6.830944538116455
2025-12-09 12:03:27.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0009884879446523036 Training loss: 6.434349060058594
2025-12-09 12:03:27.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.000988422881387902 Training loss: 6.710525035858154
2025-12-09 12:03:27.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0009883576369339876 Training loss: 6.806839942932129
2025-12-09 12:03:28.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0009882922113147636 Training loss: 6.444363594055176
2025-12-09 12:03:28.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0009882266045545011 Training loss: 6.238527297973633
2025-12-09 12:03:28.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.0009881608166775384 Training loss: 6.811755657196045
2025-12-09 12:03:28.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0009880948477082802 Training loss: 6.66322135925293
2025-12-09 12:03:28.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0009880286976711992 Training loss: 6.434731483459473
2025-12-09 12:03:28.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.000987962366590835 Training loss: 6.579520225524902
2025-12-09 12:03:28.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.0009878958544917943 Training loss: 6.265983581542969
2025-12-09 12:03:28.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.0009878291613987509 Training loss: 6.5606536865234375
2025-12-09 12:03:29.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.000987762287336446 Training loss: 6.626275539398193
2025-12-09 12:03:29.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0009876952323296876 Training loss: 6.556122303009033
2025-12-09 12:03:29.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0009876279964033511 Training loss: 6.401126384735107
2025-12-09 12:03:29.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.000987560579582379 Training loss: 6.600388526916504
2025-12-09 12:03:29.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0009874929818917805 Training loss: 6.612752437591553
2025-12-09 12:03:29.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0009874252033566326 Training loss: 5.4670257568359375
2025-12-09 12:03:29.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.000987357244002079 Training loss: 6.899438381195068
2025-12-09 12:03:29.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00098728910385333 Training loss: 6.511354446411133
2025-12-09 12:03:30.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.000987220782935664 Training loss: 6.437246322631836
2025-12-09 12:03:30.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0009871522812744257 Training loss: 6.566300868988037
2025-12-09 12:03:30.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.0009870835988950268 Training loss: 6.104409694671631
2025-12-09 12:03:30.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0009870147358229467 Training loss: 6.538071632385254
2025-12-09 12:03:30.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0009869456920837312 Training loss: 6.752614974975586
2025-12-09 12:03:30.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0009868764677029933 Training loss: 6.431708812713623
2025-12-09 12:03:30.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0009868070627064133 Training loss: 6.329833030700684
2025-12-09 12:03:31.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0009867374771197384 Training loss: 6.782079219818115
2025-12-09 12:03:31.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0009866677109687822 Training loss: 6.5290632247924805
2025-12-09 12:03:31.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0009865977642794259 Training loss: 6.717098236083984
2025-12-09 12:03:31.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0009865276370776177 Training loss: 6.334634780883789
2025-12-09 12:03:31.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0009864573293893724 Training loss: 6.339054107666016
2025-12-09 12:03:31.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.000986386841240772 Training loss: 6.514306545257568
2025-12-09 12:03:31.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0009863161726579655 Training loss: 6.447267532348633
2025-12-09 12:03:31.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0009862453236671685 Training loss: 6.696974754333496
2025-12-09 12:03:32.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.000986174294294664 Training loss: 6.915733337402344
2025-12-09 12:03:32.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0009861030845668014 Training loss: 6.37598991394043
2025-12-09 12:03:32.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0009860316945099973 Training loss: 6.434442520141602
2025-12-09 12:03:32.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0009859601241507354 Training loss: 6.6362833976745605
2025-12-09 12:03:32.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0009858883735155658 Training loss: 6.5298237800598145
2025-12-09 12:03:32.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0009858164426311058 Training loss: 6.831437587738037
2025-12-09 12:03:32.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0009857443315240395 Training loss: 5.808970928192139
2025-12-09 12:03:32.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.000985672040221118 Training loss: 6.654963493347168
2025-12-09 12:03:33.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.000985599568749159 Training loss: 6.431772232055664
2025-12-09 12:03:33.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.000985526917135047 Training loss: 6.516060829162598
2025-12-09 12:03:33.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0009854540854057337 Training loss: 6.624401569366455
2025-12-09 12:03:33.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.000985381073588237 Training loss: 6.5749030113220215
2025-12-09 12:03:33.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.0009853078817096423 Training loss: 7.427049160003662
2025-12-09 12:03:33.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0009852345097971016 Training loss: 6.764505863189697
2025-12-09 12:03:33.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.0009851609578778332 Training loss: 6.568978309631348
2025-12-09 12:03:33.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0009850872259791227 Training loss: 6.3745198249816895
2025-12-09 12:03:34.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0009850133141283226 Training loss: 7.027958869934082
2025-12-09 12:03:34.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0009849392223528514 Training loss: 6.478643417358398
2025-12-09 12:03:34.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.000984864950680195 Training loss: 6.716550350189209
2025-12-09 12:03:34.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.000984790499137906 Training loss: 6.354851722717285
2025-12-09 12:03:34.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0009847158677536033 Training loss: 6.483170032501221
2025-12-09 12:03:34.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.000984641056554973 Training loss: 6.2334184646606445
2025-12-09 12:03:34.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.0009845660655697678 Training loss: 6.58173942565918
2025-12-09 12:03:34.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0009844908948258067 Training loss: 6.46934175491333
2025-12-09 12:03:35.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.000984415544350976 Training loss: 6.366387367248535
2025-12-09 12:03:35.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.000984340014173228 Training loss: 6.232658863067627
2025-12-09 12:03:35.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0009842643043205823 Training loss: 6.900472164154053
2025-12-09 12:03:35.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0009841884148211247 Training loss: 6.818996906280518
2025-12-09 12:03:35.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.000984112345703008 Training loss: 6.374688625335693
2025-12-09 12:03:35.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.000984036096994451 Training loss: 6.73113489151001
2025-12-09 12:03:35.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0009839596687237402 Training loss: 6.496989727020264
2025-12-09 12:03:35.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0009838830609192278 Training loss: 6.607463359832764
2025-12-09 12:03:36.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.0009838062736093327 Training loss: 6.716217517852783
2025-12-09 12:03:36.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0009837293068225407 Training loss: 6.12101936340332
2025-12-09 12:03:36.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0009836521605874043 Training loss: 6.792470932006836
2025-12-09 12:03:36.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0009835748349325422 Training loss: 7.698047161102295
2025-12-09 12:03:36.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0009834973298866393 Training loss: 6.68784761428833
2025-12-09 12:03:36.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0009834196454784484 Training loss: 6.585480690002441
2025-12-09 12:03:36.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0009833417817367873 Training loss: 6.361832618713379
2025-12-09 12:03:36.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.0009832637386905413 Training loss: 6.376302719116211
2025-12-09 12:03:37.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0009831855163686617 Training loss: 6.359853744506836
2025-12-09 12:03:37.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0009831071148001668 Training loss: 6.45081901550293
2025-12-09 12:03:37.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0009830285340141408 Training loss: 5.481302738189697
2025-12-09 12:03:37.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0009829497740397348 Training loss: 6.5897345542907715
2025-12-09 12:03:37.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0009828708349061664 Training loss: 7.659575462341309
2025-12-09 12:03:37.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0009827917166427196 Training loss: 6.7214860916137695
2025-12-09 12:03:37.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0009827124192787445 Training loss: 6.700921535491943
2025-12-09 12:03:38.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.000982632942843658 Training loss: 6.8971734046936035
2025-12-09 12:03:38.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0009825532873669433 Training loss: 6.096462249755859
2025-12-09 12:03:38.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0009824734528781505 Training loss: 6.854891777038574
2025-12-09 12:03:38.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.0009823934394068952 Training loss: 6.454542636871338
2025-12-09 12:03:38.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.0009823132469828602 Training loss: 6.471283435821533
2025-12-09 12:03:38.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.000982232875635794 Training loss: 6.587871074676514
2025-12-09 12:03:38.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.0009821523253955122 Training loss: 6.9285197257995605
2025-12-09 12:03:38.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.0009820715962918964 Training loss: 6.332165241241455
2025-12-09 12:03:39.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0009819906883548942 Training loss: 6.723578453063965
2025-12-09 12:03:39.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0009819096016145203 Training loss: 7.062783241271973
2025-12-09 12:03:39.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.000981828336100855 Training loss: 6.3791375160217285
2025-12-09 12:03:39.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0009817468918440454 Training loss: 6.393143177032471
2025-12-09 12:03:39.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0009816652688743048 Training loss: 6.718340873718262
2025-12-09 12:03:39.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0009815834672219127 Training loss: 6.040677070617676
2025-12-09 12:03:39.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.000981501486917215 Training loss: 6.389911651611328
2025-12-09 12:03:39.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0009814193279906237 Training loss: 6.158276557922363
2025-12-09 12:03:40.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.000981336990472617 Training loss: 6.454792499542236
2025-12-09 12:03:40.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00098125447439374 Training loss: 6.271609783172607
2025-12-09 12:03:40.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0009811717797846033 Training loss: 6.8715057373046875
2025-12-09 12:03:40.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.000981088906675884 Training loss: 6.241182327270508
2025-12-09 12:03:40.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0009810058550983253 Training loss: 5.875990867614746
2025-12-09 12:03:40.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.000980922625082737 Training loss: 7.108219146728516
2025-12-09 12:03:40.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0009808392166599947 Training loss: 6.455345153808594
2025-12-09 12:03:40.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.0009807556298610403 Training loss: 6.829875946044922
2025-12-09 12:03:41.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.0009806718647168817 Training loss: 6.9094557762146
2025-12-09 12:03:41.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.0009805879212585933 Training loss: 6.791544437408447
2025-12-09 12:03:41.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0009805037995173154 Training loss: 6.530855655670166
2025-12-09 12:03:41.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0009804194995242548 Training loss: 6.286531925201416
2025-12-09 12:03:41.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.0009803350213106836 Training loss: 5.172741413116455
2025-12-09 12:03:41.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.000980250364907941 Training loss: 6.398804187774658
2025-12-09 12:03:41.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0009801655303474318 Training loss: 6.6673383712768555
2025-12-09 12:03:41.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.000980080517660627 Training loss: 6.169914245605469
2025-12-09 12:03:42.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.0009799953268790633 Training loss: 6.4816107749938965
2025-12-09 12:03:42.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.000979909958034344 Training loss: 6.407988548278809
2025-12-09 12:03:42.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0009798244111581382 Training loss: 6.297486782073975
2025-12-09 12:03:42.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0009797386862821812 Training loss: 6.447037696838379
2025-12-09 12:03:42.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0009796527834382745 Training loss: 6.216250896453857
2025-12-09 12:03:42.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.0009795667026582847 Training loss: 6.63519811630249
2025-12-09 12:03:42.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0009794804439741454 Training loss: 6.792272090911865
2025-12-09 12:03:43.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.000979394007417856 Training loss: 6.628424644470215
2025-12-09 12:03:43.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0009793073930214817 Training loss: 7.024183750152588
2025-12-09 12:03:43.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0009792206008171535 Training loss: 6.265568733215332
2025-12-09 12:03:43.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0009791336308370687 Training loss: 6.3925251960754395
2025-12-09 12:03:43.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0009790464831134903 Training loss: 6.29970645904541
2025-12-09 12:03:43.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0009789591576787476 Training loss: 6.51332950592041
2025-12-09 12:03:43.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0009788716545652352 Training loss: 6.366156578063965
2025-12-09 12:03:43.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0009787839738054146 Training loss: 6.510836124420166
2025-12-09 12:03:44.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0009786961154318121 Training loss: 6.486940860748291
2025-12-09 12:03:44.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0009786080794770206 Training loss: 6.471288681030273
2025-12-09 12:03:44.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0009785198659736987 Training loss: 6.464500427246094
2025-12-09 12:03:44.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0009784314749545706 Training loss: 6.460035800933838
2025-12-09 12:03:44.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0009783429064524269 Training loss: 6.706320285797119
2025-12-09 12:03:44.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0009782541605001234 Training loss: 6.1624860763549805
2025-12-09 12:03:44.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.0009781652371305826 Training loss: 6.392887115478516
2025-12-09 12:03:44.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0009780761363767914 Training loss: 5.668431758880615
2025-12-09 12:03:45.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.000977986858271804 Training loss: 5.888003826141357
2025-12-09 12:03:45.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0009778974028487398 Training loss: 6.439932823181152
2025-12-09 12:03:45.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0009778077701407836 Training loss: 6.4065327644348145
2025-12-09 12:03:45.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0009777179601811866 Training loss: 6.697622776031494
2025-12-09 12:03:45.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0009776279730032654 Training loss: 6.546161651611328
2025-12-09 12:03:45.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.0009775378086404024 Training loss: 6.404727935791016
2025-12-09 12:03:45.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0009774474671260455 Training loss: 6.410513877868652
2025-12-09 12:03:45.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.000977356948493709 Training loss: 6.660482883453369
2025-12-09 12:03:46.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.000977266252776972 Training loss: 6.481603145599365
2025-12-09 12:03:46.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0009771753800094803 Training loss: 6.51197624206543
2025-12-09 12:03:46.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0009770843302249442 Training loss: 5.965072154998779
2025-12-09 12:03:46.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0009769931034571409 Training loss: 5.990983963012695
2025-12-09 12:03:46.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0009769016997399121 Training loss: 6.268945217132568
2025-12-09 12:03:46.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.000976810119107166 Training loss: 6.4428558349609375
2025-12-09 12:03:46.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0009767183615928764 Training loss: 6.5886969566345215
2025-12-09 12:03:47.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.000976626427231082 Training loss: 6.14658260345459
2025-12-09 12:03:47.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0009765343160558879 Training loss: 6.5717058181762695
2025-12-09 12:03:47.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0009764420281014641 Training loss: 6.20658016204834
2025-12-09 12:03:47.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0009763495634020466 Training loss: 7.0991997718811035
2025-12-09 12:03:47.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0009762569219919371 Training loss: 6.461499214172363
2025-12-09 12:03:47.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0009761641039055025 Training loss: 6.7398200035095215
2025-12-09 12:03:47.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0009760711091771755 Training loss: 6.706345081329346
2025-12-09 12:03:47.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.0009759779378414542 Training loss: 5.737017631530762
2025-12-09 12:03:48.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0009758845899329021 Training loss: 6.1762919425964355
2025-12-09 12:03:48.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0009757910654861482 Training loss: 7.0765485763549805
2025-12-09 12:03:48.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0009756973645358876 Training loss: 6.635476112365723
2025-12-09 12:03:48.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0009756034871168799 Training loss: 6.2325286865234375
2025-12-09 12:03:48.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0009755094332639511 Training loss: 6.036492824554443
2025-12-09 12:03:48.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.0009754152030119921 Training loss: 6.118531227111816
2025-12-09 12:03:48.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0009753207963959591 Training loss: 6.420346736907959
2025-12-09 12:03:48.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0009752262134508741 Training loss: 6.549444675445557
2025-12-09 12:03:49.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0009751314542118246 Training loss: 7.0090179443359375
2025-12-09 12:03:49.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0009750365187139631 Training loss: 6.332983493804932
2025-12-09 12:03:49.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0009749414069925077 Training loss: 6.655776023864746
2025-12-09 12:03:49.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0009748461190827421 Training loss: 6.58729362487793
2025-12-09 12:03:49.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0009747506550200146 Training loss: 6.607231140136719
2025-12-09 12:03:49.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.0009746550148397397 Training loss: 6.578985691070557
2025-12-09 12:03:49.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0009745591985773971 Training loss: 6.663816452026367
2025-12-09 12:03:49.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0009744632062685312 Training loss: 5.973597526550293
2025-12-09 12:03:50.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0009743670379487523 Training loss: 6.869509220123291
2025-12-09 12:03:50.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0009742706936537357 Training loss: 6.368684768676758
2025-12-09 12:03:50.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0009741741734192224 Training loss: 7.446974754333496
2025-12-09 12:03:50.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0009740774772810182 Training loss: 6.31230354309082
2025-12-09 12:03:50.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0009739806052749942 Training loss: 6.295523166656494
2025-12-09 12:03:50.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0009738835574370871 Training loss: 6.206315994262695
2025-12-09 12:03:50.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0009737863338032984 Training loss: 6.16643762588501
2025-12-09 12:03:50.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0009736889344096951 Training loss: 6.316180229187012
2025-12-09 12:03:51.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0009735913592924093 Training loss: 6.139079570770264
2025-12-09 12:03:51.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0009734936084876383 Training loss: 6.252739429473877
2025-12-09 12:03:51.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0009733956820316443 Training loss: 7.315304756164551
2025-12-09 12:03:51.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0009732975799607554 Training loss: 6.714629173278809
2025-12-09 12:03:51.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0009731993023113641 Training loss: 6.46361780166626
2025-12-09 12:03:51.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0009731008491199284 Training loss: 6.249499320983887
2025-12-09 12:03:51.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0009730022204229714 Training loss: 6.591714382171631
2025-12-09 12:03:51.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0009729034162570811 Training loss: 6.546329498291016
2025-12-09 12:03:52.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0009728044366589108 Training loss: 6.791184902191162
2025-12-09 12:03:52.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.0009727052816651788 Training loss: 6.547806739807129
2025-12-09 12:03:52.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0009726059513126685 Training loss: 6.521251201629639
2025-12-09 12:03:52.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0009725064456382282 Training loss: 6.15297794342041
2025-12-09 12:03:52.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0009724067646787717 Training loss: 6.5335211753845215
2025-12-09 12:03:52.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0009723069084712771 Training loss: 6.502094745635986
2025-12-09 12:03:52.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0009722068770527882 Training loss: 6.454440593719482
2025-12-09 12:03:53.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0009721066704604133 Training loss: 6.202004432678223
2025-12-09 12:03:53.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0009720062887313262 Training loss: 5.766631603240967
2025-12-09 12:03:53.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.000971905731902765 Training loss: 6.461292266845703
2025-12-09 12:03:53.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0009718050000120333 Training loss: 6.213130950927734
2025-12-09 12:03:53.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0009717040930964996 Training loss: 6.608403205871582
2025-12-09 12:03:53.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0009716030111935968 Training loss: 6.170299053192139
2025-12-09 12:03:53.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0009715017543408234 Training loss: 6.081792831420898
2025-12-09 12:03:53.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0009714003225757424 Training loss: 7.25713586807251
2025-12-09 12:03:54.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0009712987159359818 Training loss: 6.605785846710205
2025-12-09 12:03:54.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.0009711969344592346 Training loss: 6.485659122467041
2025-12-09 12:03:54.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0009710949781832585 Training loss: 6.735185146331787
2025-12-09 12:03:54.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0009709928471458759 Training loss: 6.64725923538208
2025-12-09 12:03:54.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0009708905413849743 Training loss: 6.357053756713867
2025-12-09 12:03:54.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0009707880609385058 Training loss: 6.333167552947998
2025-12-09 12:03:54.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0009706854058444876 Training loss: 6.217065334320068
2025-12-09 12:03:54.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0009705825761410014 Training loss: 6.37624979019165
2025-12-09 12:03:55.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0009704795718661938 Training loss: 6.2714972496032715
2025-12-09 12:03:55.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.000970376393058276 Training loss: 6.4054999351501465
2025-12-09 12:03:55.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.0009702730397555246 Training loss: 6.570096969604492
2025-12-09 12:03:55.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0009701695119962799 Training loss: 6.004060745239258
2025-12-09 12:03:55.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.0009700658098189476 Training loss: 6.496342182159424
2025-12-09 12:03:55.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0009699619332619979 Training loss: 6.663668155670166
2025-12-09 12:03:55.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0009698578823639658 Training loss: 6.240267753601074
2025-12-09 12:03:55.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0009697536571634509 Training loss: 5.968716144561768
2025-12-09 12:03:56.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0009696492576991174 Training loss: 6.512424468994141
2025-12-09 12:03:56.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.0009695446840096944 Training loss: 6.470773696899414
2025-12-09 12:03:56.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0009694399361339751 Training loss: 6.281362533569336
2025-12-09 12:03:56.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0009693350141108182 Training loss: 6.877420425415039
2025-12-09 12:03:56.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.000969229917979146 Training loss: 5.347050189971924
2025-12-09 12:03:56.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.000969124647777946 Training loss: 6.491990089416504
2025-12-09 12:03:56.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0009690192035462701 Training loss: 6.284433364868164
2025-12-09 12:03:56.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0009689135853232349 Training loss: 6.214897632598877
2025-12-09 12:03:57.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0009688077931480212 Training loss: 6.217370986938477
2025-12-09 12:03:57.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0009687018270598749 Training loss: 6.146960258483887
2025-12-09 12:03:57.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0009685956870981059 Training loss: 6.185265064239502
2025-12-09 12:03:57.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0009684893733020888 Training loss: 6.552245616912842
2025-12-09 12:03:57.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0009683828857112626 Training loss: 6.986239433288574
2025-12-09 12:03:57.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0009682762243651309 Training loss: 7.167060852050781
2025-12-09 12:03:57.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0009681693893032617 Training loss: 6.203007221221924
2025-12-09 12:03:57.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.0009680623805652876 Training loss: 6.72571325302124
2025-12-09 12:03:58.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.0009679551981909053 Training loss: 6.186853408813477
2025-12-09 12:03:58.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.000967847842219876 Training loss: 6.282524108886719
2025-12-09 12:03:58.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.0009677403126920255 Training loss: 6.351428031921387
2025-12-09 12:03:58.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0009676326096472441 Training loss: 6.617810249328613
2025-12-09 12:03:58.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0009675247331254858 Training loss: 5.6135711669921875
2025-12-09 12:03:58.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.0009674166831667697 Training loss: 6.246216773986816
2025-12-09 12:03:58.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0009673084598111788 Training loss: 6.3827362060546875
2025-12-09 12:03:59.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0009672000630988605 Training loss: 6.292949199676514
2025-12-09 12:03:59.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.0009670914930700268 Training loss: 6.364292144775391
2025-12-09 12:03:59.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.0009669827497649536 Training loss: 6.628636837005615
2025-12-09 12:03:59.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.0009668738332239813 Training loss: 6.250635623931885
2025-12-09 12:03:59.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0009667647434875144 Training loss: 6.63932991027832
2025-12-09 12:03:59.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0009666554805960219 Training loss: 6.331051349639893
2025-12-09 12:03:59.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.0009665460445900368 Training loss: 6.329664707183838
2025-12-09 12:03:59.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0009664364355101565 Training loss: 6.075947284698486
2025-12-09 12:04:00.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.0009663266533970423 Training loss: 6.295769214630127
2025-12-09 12:04:00.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0009662166982914202 Training loss: 5.876245498657227
2025-12-09 12:04:00.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00096610657023408 Training loss: 6.127959728240967
2025-12-09 12:04:00.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0009659962692658757 Training loss: 6.027772903442383
2025-12-09 12:04:00.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0009658857954277254 Training loss: 6.833125591278076
2025-12-09 12:04:00.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0009657751487606115 Training loss: 6.274590015411377
2025-12-09 12:04:00.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0009656643293055805 Training loss: 6.136888027191162
2025-12-09 12:04:00.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0009655533371037426 Training loss: 6.1777472496032715
2025-12-09 12:04:01.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0009654421721962729 Training loss: 6.400061130523682
2025-12-09 12:04:01.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0009653308346244099 Training loss: 5.823729038238525
2025-12-09 12:04:01.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.0009652193244294562 Training loss: 6.394837856292725
2025-12-09 12:04:01.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.0009651076416527786 Training loss: 6.328973770141602
2025-12-09 12:04:01.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.000964995786335808 Training loss: 6.716628551483154
2025-12-09 12:04:01.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.0009648837585200391 Training loss: 5.863666534423828
2025-12-09 12:04:01.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0009647715582470309 Training loss: 6.745628833770752
2025-12-09 12:04:01.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.000964659185558406 Training loss: 6.315053939819336
2025-12-09 12:04:02.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.000964546640495851 Training loss: 6.107921123504639
2025-12-09 12:04:02.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.0009644339231011168 Training loss: 6.061578750610352
2025-12-09 12:04:02.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.0009643210334160178 Training loss: 6.147593975067139
2025-12-09 12:04:02.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0009642079714824328 Training loss: 5.984225273132324
2025-12-09 12:04:02.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.0009640947373423039 Training loss: 5.943513870239258
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.31 GiB is free. Including non-PyTorch memory, this process has 90.85 GiB memory in use. Of the allocated memory 89.88 GiB is allocated by PyTorch, and 213.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:08, 145.55it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 175.61it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:21, 122.46it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:10, 141.78it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:56, 173.75it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 213.82it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:53, 185.30it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 184.54it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 197.07it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 201.96it/s]Tokenizing texts:   2%|▏         | 240/10000 [00:01<00:47, 203.83it/s]Tokenizing texts:   3%|▎         | 261/10000 [00:01<00:48, 202.83it/s]Tokenizing texts:   3%|▎         | 291/10000 [00:01<00:42, 227.50it/s]Tokenizing texts:   3%|▎         | 315/10000 [00:01<00:42, 226.01it/s]Tokenizing texts:   3%|▎         | 338/10000 [00:01<00:53, 181.49it/s]Tokenizing texts:   4%|▎         | 363/10000 [00:01<00:48, 197.74it/s]Tokenizing texts:   4%|▍         | 385/10000 [00:02<00:52, 183.94it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 185.46it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:51, 185.04it/s]Tokenizing texts:   4%|▍         | 446/10000 [00:02<00:51, 186.53it/s]Tokenizing texts:   5%|▍         | 468/10000 [00:02<00:48, 195.06it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 217.17it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:40, 235.98it/s]Tokenizing texts:   6%|▌         | 550/10000 [00:02<00:50, 185.80it/s]Tokenizing texts:   6%|▌         | 580/10000 [00:02<00:44, 211.45it/s]Tokenizing texts:   6%|▌         | 603/10000 [00:03<00:44, 209.78it/s]Tokenizing texts:   6%|▋         | 626/10000 [00:03<00:47, 199.27it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 201.63it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 199.75it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 191.11it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 200.44it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 200.08it/s]Tokenizing texts:   8%|▊         | 766/10000 [00:03<00:43, 210.87it/s]Tokenizing texts:   8%|▊         | 788/10000 [00:04<00:45, 202.89it/s]Tokenizing texts:   8%|▊         | 813/10000 [00:04<00:43, 212.54it/s]Tokenizing texts:   8%|▊         | 835/10000 [00:04<00:42, 213.32it/s]Tokenizing texts:   9%|▊         | 862/10000 [00:04<00:40, 226.11it/s]Tokenizing texts:   9%|▉         | 885/10000 [00:04<00:41, 219.85it/s]Tokenizing texts:   9%|▉         | 911/10000 [00:04<00:39, 228.63it/s]Tokenizing texts:   9%|▉         | 944/10000 [00:04<00:35, 256.33it/s]Tokenizing texts:  10%|▉         | 974/10000 [00:04<00:34, 265.28it/s]Tokenizing texts:  10%|█         | 1004/10000 [00:04<00:32, 273.43it/s]Tokenizing texts:  10%|█         | 1036/10000 [00:04<00:31, 286.67it/s]Tokenizing texts:  11%|█         | 1065/10000 [00:05<00:39, 223.95it/s]Tokenizing texts:  11%|█         | 1090/10000 [00:05<00:40, 219.80it/s]Tokenizing texts:  11%|█         | 1116/10000 [00:05<00:38, 229.21it/s]Tokenizing texts:  11%|█▏        | 1141/10000 [00:05<00:44, 197.30it/s]Tokenizing texts:  12%|█▏        | 1165/10000 [00:05<00:43, 201.77it/s]Tokenizing texts:  12%|█▏        | 1193/10000 [00:05<00:41, 214.26it/s]Tokenizing texts:  12%|█▏        | 1216/10000 [00:05<00:40, 216.98it/s]Tokenizing texts:  12%|█▏        | 1239/10000 [00:05<00:41, 212.66it/s]Tokenizing texts:  13%|█▎        | 1261/10000 [00:06<00:41, 209.55it/s]Tokenizing texts:  13%|█▎        | 1296/10000 [00:06<00:35, 247.14it/s]Tokenizing texts:  13%|█▎        | 1322/10000 [00:06<00:40, 216.17it/s]Tokenizing texts:  13%|█▎        | 1345/10000 [00:06<00:42, 203.79it/s]Tokenizing texts:  14%|█▎        | 1370/10000 [00:06<00:40, 214.10it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:39, 218.07it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 235.51it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:33, 252.72it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:07<00:30, 275.45it/s]Tokenizing texts:  15%|█▌        | 1516/10000 [00:07<00:31, 268.77it/s]Tokenizing texts:  15%|█▌        | 1544/10000 [00:07<00:33, 251.63it/s]Tokenizing texts:  16%|█▌        | 1577/10000 [00:07<00:30, 272.82it/s]Tokenizing texts:  16%|█▌        | 1605/10000 [00:07<00:33, 253.01it/s]Tokenizing texts:  16%|█▋        | 1631/10000 [00:07<00:40, 208.61it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 216.10it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 217.45it/s]Tokenizing texts:  17%|█▋        | 1704/10000 [00:07<00:37, 223.12it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:33, 244.52it/s]Tokenizing texts:  18%|█▊        | 1761/10000 [00:08<00:33, 245.00it/s]Tokenizing texts:  18%|█▊        | 1786/10000 [00:08<00:33, 245.68it/s]Tokenizing texts:  18%|█▊        | 1820/10000 [00:08<00:30, 271.90it/s]Tokenizing texts:  18%|█▊        | 1848/10000 [00:08<00:37, 218.29it/s]Tokenizing texts:  19%|█▊        | 1872/10000 [00:08<00:36, 221.19it/s]Tokenizing texts:  19%|█▉        | 1896/10000 [00:08<00:35, 225.58it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:08<00:35, 226.30it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 195.90it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:37, 211.54it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:35, 228.30it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 234.04it/s]Tokenizing texts:  21%|██        | 2055/10000 [00:09<00:31, 248.72it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:37, 209.82it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:37, 210.31it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:36, 215.38it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:09<00:38, 205.66it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:35, 223.39it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:31, 251.08it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:29, 260.04it/s]Tokenizing texts:  23%|██▎       | 2275/10000 [00:10<00:28, 269.73it/s]Tokenizing texts:  23%|██▎       | 2303/10000 [00:10<00:29, 257.48it/s]Tokenizing texts:  23%|██▎       | 2330/10000 [00:10<00:36, 209.25it/s]Tokenizing texts:  24%|██▎       | 2370/10000 [00:10<00:30, 253.99it/s]Tokenizing texts:  24%|██▍       | 2401/10000 [00:10<00:28, 267.28it/s]Tokenizing texts:  24%|██▍       | 2430/10000 [00:11<00:31, 242.44it/s]Tokenizing texts:  25%|██▍       | 2461/10000 [00:11<00:29, 259.37it/s]Tokenizing texts:  25%|██▍       | 2489/10000 [00:11<00:30, 249.91it/s]Tokenizing texts:  25%|██▌       | 2516/10000 [00:11<00:29, 252.83it/s]Tokenizing texts:  25%|██▌       | 2544/10000 [00:11<00:28, 259.86it/s]Tokenizing texts:  26%|██▌       | 2571/10000 [00:11<00:32, 227.45it/s]Tokenizing texts:  26%|██▌       | 2595/10000 [00:11<00:34, 216.11it/s]Tokenizing texts:  26%|██▌       | 2618/10000 [00:11<00:35, 206.73it/s]Tokenizing texts:  26%|██▋       | 2646/10000 [00:11<00:32, 225.40it/s]Tokenizing texts:  27%|██▋       | 2670/10000 [00:12<00:32, 227.28it/s]Tokenizing texts:  27%|██▋       | 2694/10000 [00:12<00:36, 198.12it/s]Tokenizing texts:  27%|██▋       | 2718/10000 [00:12<00:34, 208.31it/s]Tokenizing texts:  28%|██▊       | 2757/10000 [00:12<00:28, 252.62it/s]Tokenizing texts:  28%|██▊       | 2784/10000 [00:12<00:30, 239.05it/s]Tokenizing texts:  28%|██▊       | 2809/10000 [00:12<00:37, 190.97it/s]Tokenizing texts:  28%|██▊       | 2841/10000 [00:12<00:32, 218.73it/s]Tokenizing texts:  29%|██▊       | 2865/10000 [00:13<00:35, 201.81it/s]Tokenizing texts:  29%|██▉       | 2893/10000 [00:13<00:32, 220.50it/s]Tokenizing texts:  29%|██▉       | 2924/10000 [00:13<00:29, 237.24it/s]Tokenizing texts:  29%|██▉       | 2949/10000 [00:13<00:31, 226.69it/s]Tokenizing texts:  30%|██▉       | 2973/10000 [00:13<00:31, 224.68it/s]Tokenizing texts:  30%|███       | 3012/10000 [00:13<00:26, 267.35it/s]Tokenizing texts:  30%|███       | 3040/10000 [00:13<00:30, 229.42it/s]Tokenizing texts:  31%|███       | 3074/10000 [00:13<00:27, 254.81it/s]Tokenizing texts:  31%|███       | 3101/10000 [00:13<00:28, 243.88it/s]Tokenizing texts:  31%|███▏      | 3135/10000 [00:14<00:25, 268.74it/s]Tokenizing texts:  32%|███▏      | 3163/10000 [00:14<00:26, 254.73it/s]Tokenizing texts:  32%|███▏      | 3190/10000 [00:14<00:26, 257.43it/s]Tokenizing texts:  32%|███▏      | 3219/10000 [00:14<00:25, 266.22it/s]Tokenizing texts:  32%|███▏      | 3247/10000 [00:14<00:26, 258.25it/s]Tokenizing texts:  33%|███▎      | 3278/10000 [00:14<00:24, 269.05it/s]Tokenizing texts:  33%|███▎      | 3306/10000 [00:14<00:25, 261.21it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:14<00:26, 248.54it/s]Tokenizing texts:  34%|███▎      | 3362/10000 [00:14<00:25, 259.31it/s]Tokenizing texts:  34%|███▍      | 3389/10000 [00:15<00:26, 249.73it/s]Tokenizing texts:  34%|███▍      | 3416/10000 [00:15<00:25, 254.52it/s]Tokenizing texts:  35%|███▍      | 3457/10000 [00:15<00:22, 295.25it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:15<00:22, 289.38it/s]Tokenizing texts:  35%|███▌      | 3517/10000 [00:15<00:26, 245.46it/s]Tokenizing texts:  35%|███▌      | 3543/10000 [00:15<00:27, 233.21it/s]Tokenizing texts:  36%|███▌      | 3570/10000 [00:15<00:26, 241.96it/s]Tokenizing texts:  36%|███▌      | 3600/10000 [00:15<00:25, 254.59it/s]Tokenizing texts:  36%|███▋      | 3627/10000 [00:16<00:24, 254.99it/s]Tokenizing texts:  37%|███▋      | 3653/10000 [00:16<00:25, 250.70it/s]Tokenizing texts:  37%|███▋      | 3679/10000 [00:16<00:25, 249.07it/s]Tokenizing texts:  37%|███▋      | 3705/10000 [00:16<00:24, 251.97it/s]Tokenizing texts:  37%|███▋      | 3732/10000 [00:16<00:24, 256.92it/s]Tokenizing texts:  38%|███▊      | 3758/10000 [00:16<00:27, 226.04it/s]Tokenizing texts:  38%|███▊      | 3784/10000 [00:16<00:26, 235.07it/s]Tokenizing texts:  38%|███▊      | 3809/10000 [00:16<00:29, 207.43it/s]Tokenizing texts:  38%|███▊      | 3842/10000 [00:16<00:26, 232.88it/s]Tokenizing texts:  39%|███▊      | 3870/10000 [00:17<00:25, 243.03it/s]Tokenizing texts:  39%|███▉      | 3896/10000 [00:17<00:25, 240.40it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:25, 239.53it/s]Tokenizing texts:  39%|███▉      | 3946/10000 [00:17<00:25, 233.00it/s]Tokenizing texts:  40%|███▉      | 3979/10000 [00:17<00:23, 259.57it/s]Tokenizing texts:  40%|████      | 4006/10000 [00:17<00:27, 220.69it/s]Tokenizing texts:  40%|████      | 4037/10000 [00:17<00:24, 238.89it/s]Tokenizing texts:  41%|████      | 4067/10000 [00:17<00:23, 254.54it/s]Tokenizing texts:  41%|████      | 4099/10000 [00:17<00:21, 272.02it/s]Tokenizing texts:  41%|████▏     | 4132/10000 [00:18<00:20, 287.54it/s]Tokenizing texts:  42%|████▏     | 4162/10000 [00:18<00:21, 272.60it/s]Tokenizing texts:  42%|████▏     | 4195/10000 [00:18<00:20, 286.66it/s]Tokenizing texts:  42%|████▏     | 4225/10000 [00:18<00:22, 261.80it/s]Tokenizing texts:  43%|████▎     | 4252/10000 [00:18<00:23, 247.86it/s]Tokenizing texts:  43%|████▎     | 4278/10000 [00:18<00:24, 237.77it/s]Tokenizing texts:  43%|████▎     | 4310/10000 [00:18<00:21, 259.05it/s]Tokenizing texts:  43%|████▎     | 4345/10000 [00:18<00:20, 281.69it/s]Tokenizing texts:  44%|████▎     | 4374/10000 [00:18<00:21, 264.76it/s]Tokenizing texts:  44%|████▍     | 4412/10000 [00:19<00:19, 293.22it/s]Tokenizing texts:  44%|████▍     | 4442/10000 [00:19<00:19, 281.75it/s]Tokenizing texts:  45%|████▍     | 4471/10000 [00:19<00:21, 258.66it/s]Tokenizing texts:  45%|████▍     | 4498/10000 [00:19<00:22, 242.93it/s]Tokenizing texts:  45%|████▌     | 4525/10000 [00:19<00:21, 249.57it/s]Tokenizing texts:  46%|████▌     | 4553/10000 [00:19<00:21, 249.77it/s]Tokenizing texts:  46%|████▌     | 4581/10000 [00:19<00:21, 257.27it/s]Tokenizing texts:  46%|████▌     | 4608/10000 [00:19<00:22, 244.96it/s]Tokenizing texts:  46%|████▋     | 4641/10000 [00:20<00:20, 267.81it/s]Tokenizing texts:  47%|████▋     | 4677/10000 [00:20<00:18, 292.15it/s]Tokenizing texts:  47%|████▋     | 4713/10000 [00:20<00:17, 309.33it/s]Tokenizing texts:  47%|████▋     | 4745/10000 [00:20<00:19, 274.60it/s]Tokenizing texts:  48%|████▊     | 4776/10000 [00:20<00:19, 274.34it/s]Tokenizing texts:  48%|████▊     | 4805/10000 [00:20<00:20, 253.69it/s]Tokenizing texts:  48%|████▊     | 4837/10000 [00:20<00:19, 265.62it/s]Tokenizing texts:  49%|████▊     | 4867/10000 [00:20<00:18, 273.65it/s]Tokenizing texts:  49%|████▉     | 4899/10000 [00:20<00:17, 283.56it/s]Tokenizing texts:  49%|████▉     | 4928/10000 [00:21<00:19, 266.13it/s]Tokenizing texts:  50%|████▉     | 4956/10000 [00:21<00:18, 267.10it/s]Tokenizing texts:  50%|████▉     | 4990/10000 [00:21<00:17, 284.25it/s]Tokenizing texts:  50%|█████     | 5022/10000 [00:21<00:16, 293.36it/s]Tokenizing texts:  51%|█████     | 5052/10000 [00:21<00:19, 257.49it/s]Tokenizing texts:  51%|█████     | 5079/10000 [00:21<00:20, 234.58it/s]Tokenizing texts:  51%|█████     | 5109/10000 [00:21<00:19, 247.99it/s]Tokenizing texts:  51%|█████▏    | 5135/10000 [00:21<00:20, 232.66it/s]Tokenizing texts:  52%|█████▏    | 5166/10000 [00:21<00:19, 249.21it/s]Tokenizing texts:  52%|█████▏    | 5193/10000 [00:22<00:18, 254.31it/s]Tokenizing texts:  52%|█████▏    | 5219/10000 [00:22<00:19, 242.15it/s]Tokenizing texts:  53%|█████▎    | 5253/10000 [00:22<00:17, 267.39it/s]Tokenizing texts:  53%|█████▎    | 5281/10000 [00:22<00:18, 258.48it/s]Tokenizing texts:  53%|█████▎    | 5310/10000 [00:22<00:17, 266.95it/s]Tokenizing texts:  53%|█████▎    | 5345/10000 [00:22<00:16, 289.08it/s]Tokenizing texts:  54%|█████▍    | 5379/10000 [00:22<00:15, 300.45it/s]Tokenizing texts:  54%|█████▍    | 5410/10000 [00:22<00:15, 294.54it/s]Tokenizing texts:  54%|█████▍    | 5440/10000 [00:22<00:16, 276.41it/s]Tokenizing texts:  55%|█████▍    | 5470/10000 [00:23<00:16, 279.56it/s]Tokenizing texts:  55%|█████▍    | 5499/10000 [00:23<00:21, 211.34it/s]Tokenizing texts:  55%|█████▌    | 5536/10000 [00:23<00:18, 246.10it/s]Tokenizing texts:  56%|█████▌    | 5564/10000 [00:23<00:17, 247.22it/s]Tokenizing texts:  56%|█████▌    | 5591/10000 [00:23<00:17, 248.20it/s]Tokenizing texts:  56%|█████▋    | 5625/10000 [00:23<00:16, 272.07it/s]Tokenizing texts:  57%|█████▋    | 5662/10000 [00:23<00:14, 298.05it/s]Tokenizing texts:  57%|█████▋    | 5693/10000 [00:23<00:14, 301.19it/s]Tokenizing texts:  57%|█████▋    | 5724/10000 [00:24<00:14, 299.56it/s]Tokenizing texts:  58%|█████▊    | 5755/10000 [00:24<00:16, 262.32it/s]Tokenizing texts:  58%|█████▊    | 5783/10000 [00:24<00:17, 244.98it/s]Tokenizing texts:  58%|█████▊    | 5809/10000 [00:24<00:20, 208.79it/s]Tokenizing texts:  58%|█████▊    | 5833/10000 [00:24<00:19, 213.99it/s]Tokenizing texts:  59%|█████▊    | 5865/10000 [00:24<00:17, 238.87it/s]Tokenizing texts:  59%|█████▉    | 5891/10000 [00:24<00:16, 244.24it/s]Tokenizing texts:  59%|█████▉    | 5917/10000 [00:24<00:16, 244.95it/s]Tokenizing texts:  59%|█████▉    | 5943/10000 [00:25<00:17, 238.14it/s]Tokenizing texts:  60%|█████▉    | 5972/10000 [00:25<00:16, 250.93it/s]Tokenizing texts:  60%|█████▉    | 5998/10000 [00:25<00:16, 248.54it/s]Tokenizing texts:  60%|██████    | 6024/10000 [00:25<00:16, 244.10it/s]Tokenizing texts:  61%|██████    | 6056/10000 [00:25<00:14, 263.28it/s]Tokenizing texts:  61%|██████    | 6084/10000 [00:25<00:14, 264.08it/s]Tokenizing texts:  61%|██████    | 6111/10000 [00:25<00:14, 260.17it/s]Tokenizing texts:  61%|██████▏   | 6144/10000 [00:25<00:13, 279.37it/s]Tokenizing texts:  62%|██████▏   | 6173/10000 [00:25<00:13, 277.31it/s]Tokenizing texts:  62%|██████▏   | 6201/10000 [00:25<00:14, 265.15it/s]Tokenizing texts:  62%|██████▏   | 6228/10000 [00:26<00:15, 248.91it/s]Tokenizing texts:  63%|██████▎   | 6260/10000 [00:26<00:14, 267.08it/s]Tokenizing texts:  63%|██████▎   | 6288/10000 [00:26<00:14, 254.37it/s]Tokenizing texts:  63%|██████▎   | 6314/10000 [00:26<00:15, 232.04it/s]Tokenizing texts:  63%|██████▎   | 6338/10000 [00:26<00:15, 230.80it/s]Tokenizing texts:  64%|██████▎   | 6366/10000 [00:26<00:15, 240.17it/s]Tokenizing texts:  64%|██████▍   | 6393/10000 [00:26<00:14, 248.12it/s]Tokenizing texts:  64%|██████▍   | 6421/10000 [00:26<00:13, 257.06it/s]Tokenizing texts:  64%|██████▍   | 6449/10000 [00:26<00:13, 261.70it/s]Tokenizing texts:  65%|██████▍   | 6476/10000 [00:27<00:14, 235.13it/s]Tokenizing texts:  65%|██████▌   | 6506/10000 [00:27<00:14, 233.49it/s]Tokenizing texts:  65%|██████▌   | 6531/10000 [00:27<00:14, 237.50it/s]Tokenizing texts:  66%|██████▌   | 6572/10000 [00:27<00:12, 284.16it/s]Tokenizing texts:  66%|██████▌   | 6602/10000 [00:27<00:12, 272.28it/s]Tokenizing texts:  66%|██████▋   | 6630/10000 [00:27<00:12, 265.44it/s]Tokenizing texts:  67%|██████▋   | 6658/10000 [00:27<00:12, 268.22it/s]Tokenizing texts:  67%|██████▋   | 6686/10000 [00:27<00:12, 266.08it/s]Tokenizing texts:  67%|██████▋   | 6713/10000 [00:28<00:12, 255.16it/s]Tokenizing texts:  67%|██████▋   | 6739/10000 [00:28<00:13, 247.08it/s]Tokenizing texts:  68%|██████▊   | 6770/10000 [00:28<00:12, 264.04it/s]Tokenizing texts:  68%|██████▊   | 6805/10000 [00:28<00:11, 286.23it/s]Tokenizing texts:  68%|██████▊   | 6837/10000 [00:28<00:10, 294.31it/s]Tokenizing texts:  69%|██████▊   | 6869/10000 [00:28<00:10, 290.45it/s]Tokenizing texts:  69%|██████▉   | 6899/10000 [00:28<00:13, 238.06it/s]Tokenizing texts:  69%|██████▉   | 6925/10000 [00:28<00:13, 233.35it/s]Tokenizing texts:  70%|██████▉   | 6950/10000 [00:28<00:13, 234.13it/s]Tokenizing texts:  70%|██████▉   | 6978/10000 [00:29<00:12, 243.97it/s]Tokenizing texts:  70%|███████   | 7009/10000 [00:29<00:11, 259.90it/s]Tokenizing texts:  70%|███████   | 7036/10000 [00:29<00:11, 250.32it/s]Tokenizing texts:  71%|███████   | 7062/10000 [00:29<00:13, 211.74it/s]Tokenizing texts:  71%|███████   | 7086/10000 [00:29<00:13, 217.52it/s]Tokenizing texts:  71%|███████   | 7112/10000 [00:29<00:12, 227.33it/s]Tokenizing texts:  71%|███████▏  | 7142/10000 [00:29<00:11, 246.59it/s]Tokenizing texts:  72%|███████▏  | 7168/10000 [00:29<00:11, 236.54it/s]Tokenizing texts:  72%|███████▏  | 7195/10000 [00:29<00:11, 245.15it/s]Tokenizing texts:  72%|███████▏  | 7224/10000 [00:30<00:10, 255.67it/s]Tokenizing texts:  72%|███████▎  | 7250/10000 [00:30<00:11, 243.65it/s]Tokenizing texts:  73%|███████▎  | 7295/10000 [00:30<00:09, 299.37it/s]Tokenizing texts:  73%|███████▎  | 7328/10000 [00:30<00:08, 307.09it/s]Tokenizing texts:  74%|███████▎  | 7360/10000 [00:30<00:09, 275.10it/s]Tokenizing texts:  74%|███████▍  | 7389/10000 [00:30<00:09, 263.77it/s]Tokenizing texts:  74%|███████▍  | 7417/10000 [00:30<00:11, 222.55it/s]Tokenizing texts:  74%|███████▍  | 7442/10000 [00:30<00:12, 206.67it/s]Tokenizing texts:  75%|███████▍  | 7474/10000 [00:31<00:10, 232.76it/s]Tokenizing texts:  75%|███████▌  | 7509/10000 [00:31<00:09, 261.18it/s]Tokenizing texts:  75%|███████▌  | 7538/10000 [00:31<00:09, 268.44it/s]Tokenizing texts:  76%|███████▌  | 7567/10000 [00:31<00:09, 258.54it/s]Tokenizing texts:  76%|███████▌  | 7605/10000 [00:31<00:08, 290.97it/s]Tokenizing texts:  76%|███████▋  | 7636/10000 [00:31<00:08, 275.86it/s]Tokenizing texts:  77%|███████▋  | 7665/10000 [00:31<00:08, 277.91it/s]Tokenizing texts:  77%|███████▋  | 7698/10000 [00:31<00:07, 291.90it/s]Tokenizing texts:  77%|███████▋  | 7728/10000 [00:31<00:08, 262.46it/s]Tokenizing texts:  78%|███████▊  | 7756/10000 [00:32<00:09, 237.00it/s]Tokenizing texts:  78%|███████▊  | 7788/10000 [00:32<00:08, 256.90it/s]Tokenizing texts:  78%|███████▊  | 7815/10000 [00:32<00:09, 237.97it/s]Tokenizing texts:  78%|███████▊  | 7844/10000 [00:32<00:08, 244.93it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 257.95it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 254.95it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:32<00:07, 282.91it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:32<00:07, 280.81it/s]Tokenizing texts:  80%|███████▉  | 7999/10000 [00:33<00:06, 291.38it/s]Tokenizing texts:  80%|████████  | 8029/10000 [00:33<00:06, 287.23it/s]Tokenizing texts:  81%|████████  | 8058/10000 [00:33<00:11, 170.00it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 199.93it/s]Tokenizing texts:  81%|████████  | 8120/10000 [00:33<00:09, 208.81it/s]Tokenizing texts:  81%|████████▏ | 8149/10000 [00:33<00:08, 226.12it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:33<00:07, 238.44it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:34<00:07, 249.44it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 214.10it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 224.56it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 258.39it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 254.20it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:34<00:06, 258.23it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:34<00:06, 257.41it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:34<00:06, 232.75it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:34<00:06, 258.39it/s]Tokenizing texts:  85%|████████▍ | 8472/10000 [00:35<00:05, 264.00it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:35<00:05, 279.12it/s]Tokenizing texts:  85%|████████▌ | 8533/10000 [00:35<00:05, 262.12it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:35<00:05, 268.77it/s]Tokenizing texts:  86%|████████▌ | 8590/10000 [00:35<00:05, 263.74it/s]Tokenizing texts:  86%|████████▋ | 8625/10000 [00:35<00:04, 287.13it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:35<00:04, 285.91it/s]Tokenizing texts:  87%|████████▋ | 8688/10000 [00:35<00:04, 284.40it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:35<00:04, 285.94it/s]Tokenizing texts:  88%|████████▊ | 8757/10000 [00:36<00:04, 292.11it/s]Tokenizing texts:  88%|████████▊ | 8787/10000 [00:36<00:04, 292.79it/s]Tokenizing texts:  88%|████████▊ | 8817/10000 [00:36<00:05, 220.55it/s]Tokenizing texts:  88%|████████▊ | 8842/10000 [00:36<00:05, 222.19it/s]Tokenizing texts:  89%|████████▉ | 8876/10000 [00:36<00:04, 249.24it/s]Tokenizing texts:  89%|████████▉ | 8914/10000 [00:36<00:03, 274.27it/s]Tokenizing texts:  89%|████████▉ | 8946/10000 [00:36<00:03, 285.08it/s]Tokenizing texts:  90%|████████▉ | 8976/10000 [00:36<00:03, 282.33it/s]Tokenizing texts:  90%|█████████ | 9006/10000 [00:37<00:04, 245.31it/s]Tokenizing texts:  90%|█████████ | 9038/10000 [00:37<00:03, 261.90it/s]Tokenizing texts:  91%|█████████ | 9066/10000 [00:37<00:03, 251.39it/s]Tokenizing texts:  91%|█████████ | 9098/10000 [00:37<00:03, 268.15it/s]Tokenizing texts:  91%|█████████▏| 9126/10000 [00:37<00:03, 258.28it/s]Tokenizing texts:  92%|█████████▏| 9158/10000 [00:37<00:03, 270.23it/s]Tokenizing texts:  92%|█████████▏| 9191/10000 [00:37<00:02, 285.84it/s]Tokenizing texts:  92%|█████████▏| 9221/10000 [00:37<00:02, 285.07it/s]Tokenizing texts:  92%|█████████▎| 9250/10000 [00:37<00:02, 278.04it/s]Tokenizing texts:  93%|█████████▎| 9279/10000 [00:38<00:02, 247.47it/s]Tokenizing texts:  93%|█████████▎| 9310/10000 [00:38<00:02, 257.34it/s]Tokenizing texts:  93%|█████████▎| 9337/10000 [00:38<00:02, 248.05it/s]Tokenizing texts:  94%|█████████▎| 9363/10000 [00:38<00:03, 211.81it/s]Tokenizing texts:  94%|█████████▍| 9400/10000 [00:38<00:02, 248.98it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:38<00:02, 227.36it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:38<00:02, 232.02it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:38<00:02, 241.13it/s]Tokenizing texts:  95%|█████████▌| 9504/10000 [00:39<00:02, 183.04it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 221.79it/s]Tokenizing texts:  96%|█████████▌| 9566/10000 [00:39<00:01, 229.13it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 233.44it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 231.83it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 234.66it/s]Tokenizing texts:  97%|█████████▋| 9674/10000 [00:39<00:01, 240.84it/s]Tokenizing texts:  97%|█████████▋| 9699/10000 [00:39<00:01, 236.60it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:40<00:01, 239.20it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 207.80it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 216.22it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 261.17it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 254.18it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 259.38it/s]Tokenizing texts:  99%|█████████▉| 9907/10000 [00:40<00:00, 286.23it/s]Tokenizing texts:  99%|█████████▉| 9937/10000 [00:40<00:00, 283.41it/s]Tokenizing texts: 100%|█████████▉| 9968/10000 [00:40<00:00, 290.67it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 243.78it/s]
2025-12-09 12:05:02.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 11.98813247680664
2025-12-09 12:05:03.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 12.020317077636719
2025-12-09 12:05:03.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 12.033624649047852
2025-12-09 12:05:03.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 12.005095481872559
2025-12-09 12:05:03.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 12.001751899719238
2025-12-09 12:05:03.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 11.996685028076172
2025-12-09 12:05:03.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 11.931672096252441
2025-12-09 12:05:03.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 11.921452522277832
2025-12-09 12:05:03.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 11.826909065246582
2025-12-09 12:05:04.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 11.865828514099121
2025-12-09 12:05:04.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 11.724854469299316
2025-12-09 12:05:04.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 11.614480018615723
2025-12-09 12:05:04.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 11.625214576721191
2025-12-09 12:05:04.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 11.446258544921875
2025-12-09 12:05:04.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 11.374361991882324
2025-12-09 12:05:04.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 11.055753707885742
2025-12-09 12:05:04.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 10.87437915802002
2025-12-09 12:05:05.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 10.909364700317383
2025-12-09 12:05:05.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 10.469635009765625
2025-12-09 12:05:05.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 10.037959098815918
2025-12-09 12:05:05.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 9.900455474853516
2025-12-09 12:05:05.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 9.990485191345215
2025-12-09 12:05:05.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 9.535585403442383
2025-12-09 12:05:05.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 9.310676574707031
2025-12-09 12:05:05.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 9.177281379699707
2025-12-09 12:05:06.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 9.009505271911621
2025-12-09 12:05:06.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 8.584514617919922
2025-12-09 12:05:06.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 8.662240982055664
2025-12-09 12:05:06.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 8.357013702392578
2025-12-09 12:05:06.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 8.188308715820312
2025-12-09 12:05:06.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 8.414538383483887
2025-12-09 12:05:06.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 8.390768051147461
2025-12-09 12:05:06.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 7.885852336883545
2025-12-09 12:05:07.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 7.9445953369140625
2025-12-09 12:05:07.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 7.992331027984619
2025-12-09 12:05:07.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 8.24866008758545
2025-12-09 12:05:07.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 8.662283897399902
2025-12-09 12:05:07.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 8.074654579162598
2025-12-09 12:05:07.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 8.109217643737793
2025-12-09 12:05:07.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 7.965890884399414
2025-12-09 12:05:07.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 8.189459800720215
2025-12-09 12:05:08.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 7.842183589935303
2025-12-09 12:05:08.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 8.327285766601562
2025-12-09 12:05:08.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 8.489036560058594
2025-12-09 12:05:08.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 8.092158317565918
2025-12-09 12:05:08.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 7.6095147132873535
2025-12-09 12:05:08.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 7.578208923339844
2025-12-09 12:05:08.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 7.249727725982666
2025-12-09 12:05:08.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 7.911329746246338
2025-12-09 12:05:09.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 7.829705715179443
2025-12-09 12:05:09.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 7.940003395080566
2025-12-09 12:05:09.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 7.680728435516357
2025-12-09 12:05:09.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 7.6409382820129395
2025-12-09 12:05:09.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 7.4993462562561035
2025-12-09 12:05:09.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 7.598169803619385
2025-12-09 12:05:09.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 6.730731964111328
2025-12-09 12:05:09.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 8.407793045043945
2025-12-09 12:05:10.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 7.621516704559326
2025-12-09 12:05:10.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 7.604882717132568
2025-12-09 12:05:10.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 7.5927042961120605
2025-12-09 12:05:10.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 7.0711259841918945
2025-12-09 12:05:10.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 7.033199310302734
2025-12-09 12:05:10.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 7.571732521057129
2025-12-09 12:05:10.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 7.5596089363098145
2025-12-09 12:05:10.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 7.225132465362549
2025-12-09 12:05:10.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 6.966214656829834
2025-12-09 12:05:11.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 7.603925704956055
2025-12-09 12:05:11.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 7.737853050231934
2025-12-09 12:05:11.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 7.299221992492676
2025-12-09 12:05:11.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 7.369157314300537
2025-12-09 12:05:11.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 7.340426921844482
2025-12-09 12:05:11.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 7.222747325897217
2025-12-09 12:05:11.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 7.451982021331787
2025-12-09 12:05:11.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 7.374504089355469
2025-12-09 12:05:12.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 8.08111572265625
2025-12-09 12:05:12.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 8.207511901855469
2025-12-09 12:05:12.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 7.2296833992004395
2025-12-09 12:05:12.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 7.442329406738281
2025-12-09 12:05:12.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 7.636672496795654
2025-12-09 12:05:12.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 7.433643817901611
2025-12-09 12:05:12.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 7.646920680999756
2025-12-09 12:05:12.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 7.607159614562988
2025-12-09 12:05:13.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 8.273204803466797
2025-12-09 12:05:13.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 6.974884033203125
2025-12-09 12:05:13.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 7.209382057189941
2025-12-09 12:05:13.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 7.208792209625244
2025-12-09 12:05:13.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 7.687203407287598
2025-12-09 12:05:13.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 7.039453029632568
2025-12-09 12:05:13.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 7.280423641204834
2025-12-09 12:05:13.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 7.048717021942139
2025-12-09 12:05:14.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 7.38893461227417
2025-12-09 12:05:14.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 7.068789482116699
2025-12-09 12:05:14.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 7.514190673828125
2025-12-09 12:05:14.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 7.105762004852295
2025-12-09 12:05:14.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 7.182415008544922
2025-12-09 12:05:14.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 7.177349090576172
2025-12-09 12:05:14.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 7.424893379211426
2025-12-09 12:05:14.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 6.7787933349609375
2025-12-09 12:05:15.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 7.701885223388672
2025-12-09 12:05:15.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 7.05919075012207
2025-12-09 12:05:15.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999997217736107 Training loss: 7.086114883422852
2025-12-09 12:05:15.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002999998887094546 Training loss: 7.598290920257568
2025-12-09 12:05:15.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029999974959631155 Training loss: 7.5694355964660645
2025-12-09 12:05:15.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0029999955483798347 Training loss: 7.1781182289123535
2025-12-09 12:05:15.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0029999930443454273 Training loss: 6.992284297943115
2025-12-09 12:05:15.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.002999989983860821 Training loss: 7.246082305908203
2025-12-09 12:05:16.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029999863669271528 Training loss: 7.459183692932129
2025-12-09 12:05:16.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0029999821935457623 Training loss: 7.429252624511719
2025-12-09 12:05:16.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0029999774637181993 Training loss: 7.095832347869873
2025-12-09 12:05:16.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.002999972177446218 Training loss: 7.317591190338135
2025-12-09 12:05:16.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.002999966334731779 Training loss: 7.615779876708984
2025-12-09 12:05:16.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0029999599355770503 Training loss: 6.770733833312988
2025-12-09 12:05:16.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029999529799844054 Training loss: 7.502468109130859
2025-12-09 12:05:16.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029999454679564244 Training loss: 7.484908580780029
2025-12-09 12:05:16.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.002999937399495895 Training loss: 6.646720886230469
2025-12-09 12:05:17.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0029999287746058094 Training loss: 7.510659217834473
2025-12-09 12:05:17.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.002999919593289368 Training loss: 7.504597187042236
2025-12-09 12:05:17.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.002999909855549976 Training loss: 6.8735456466674805
2025-12-09 12:05:17.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029998995613912463 Training loss: 7.484748840332031
2025-12-09 12:05:17.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.002999888710816997 Training loss: 7.469799518585205
2025-12-09 12:05:17.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.002999877303831254 Training loss: 7.0493340492248535
2025-12-09 12:05:17.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002999865340438249 Training loss: 7.920214653015137
2025-12-09 12:05:17.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0029998528206424202 Training loss: 6.873972415924072
2025-12-09 12:05:18.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0029998397444484107 Training loss: 7.196199417114258
2025-12-09 12:05:18.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029998261118610726 Training loss: 7.917972087860107
2025-12-09 12:05:18.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.002999811922885463 Training loss: 6.8861517906188965
2025-12-09 12:05:18.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0029997971775268454 Training loss: 7.283421516418457
2025-12-09 12:05:18.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00299978187579069 Training loss: 7.357768535614014
2025-12-09 12:05:18.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0029997660176826735 Training loss: 6.943240165710449
2025-12-09 12:05:18.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.002999749603208678 Training loss: 7.233653545379639
2025-12-09 12:05:18.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.002999732632374793 Training loss: 7.9168596267700195
2025-12-09 12:05:19.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002999715105187314 Training loss: 7.243625164031982
2025-12-09 12:05:19.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.002999697021652744 Training loss: 6.353390693664551
2025-12-09 12:05:19.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.002999678381777791 Training loss: 7.287785053253174
2025-12-09 12:05:19.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.002999659185569369 Training loss: 6.828713893890381
2025-12-09 12:05:19.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0029996394330345996 Training loss: 6.904609203338623
2025-12-09 12:05:19.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0029996191241808113 Training loss: 7.045434474945068
2025-12-09 12:05:19.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.002999598259015537 Training loss: 7.082573890686035
2025-12-09 12:05:19.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.002999576837546517 Training loss: 6.936346054077148
2025-12-09 12:05:20.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0029995548597816983 Training loss: 7.18700647354126
2025-12-09 12:05:20.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.002999532325729234 Training loss: 7.107272624969482
2025-12-09 12:05:20.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0029995092353974837 Training loss: 8.086138725280762
2025-12-09 12:05:20.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.002999485588795013 Training loss: 7.249373912811279
2025-12-09 12:05:20.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0029994613859305936 Training loss: 7.223480224609375
2025-12-09 12:05:20.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0029994366268132045 Training loss: 6.80667781829834
2025-12-09 12:05:20.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0029994113114520304 Training loss: 6.744720935821533
2025-12-09 12:05:20.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0029993854398564627 Training loss: 7.405220031738281
2025-12-09 12:05:21.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0029993590120360987 Training loss: 7.144330978393555
2025-12-09 12:05:21.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0029993320280007423 Training loss: 7.1497392654418945
2025-12-09 12:05:21.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.002999304487760404 Training loss: 6.602901458740234
2025-12-09 12:05:21.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0029992763913253002 Training loss: 7.206544876098633
2025-12-09 12:05:21.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.002999247738705854 Training loss: 6.98017692565918
2025-12-09 12:05:21.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0029992185299126946 Training loss: 6.835174560546875
2025-12-09 12:05:21.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0029991887649566565 Training loss: 7.12421178817749
2025-12-09 12:05:21.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002999158443848783 Training loss: 7.790450572967529
2025-12-09 12:05:22.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0029991275666003212 Training loss: 6.811736106872559
2025-12-09 12:05:22.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0029990961332227264 Training loss: 7.060420036315918
2025-12-09 12:05:22.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.002999064143727659 Training loss: 7.154933452606201
2025-12-09 12:05:22.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0029990315981269864 Training loss: 7.296911716461182
2025-12-09 12:05:22.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0029989984964327813 Training loss: 7.282171726226807
2025-12-09 12:05:22.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0029989648386573244 Training loss: 7.152685165405273
2025-12-09 12:05:22.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.002998930624813101 Training loss: 7.012467384338379
2025-12-09 12:05:22.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.002998895854912803 Training loss: 7.233880519866943
2025-12-09 12:05:22.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0029988605289693296 Training loss: 7.140772342681885
2025-12-09 12:05:23.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0029988246469957857 Training loss: 7.3661675453186035
2025-12-09 12:05:23.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002998788209005482 Training loss: 7.312524318695068
2025-12-09 12:05:23.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.002998751215011936 Training loss: 6.516375541687012
2025-12-09 12:05:23.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0029987136650288706 Training loss: 6.929812431335449
2025-12-09 12:05:23.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.002998675559070217 Training loss: 6.8609724044799805
2025-12-09 12:05:23.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.00299863689715011 Training loss: 6.895111560821533
2025-12-09 12:05:23.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0029985976792828934 Training loss: 7.254406929016113
2025-12-09 12:05:23.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0029985579054831145 Training loss: 6.654031276702881
2025-12-09 12:05:24.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0029985175757655286 Training loss: 7.010246753692627
2025-12-09 12:05:24.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.002998476690145097 Training loss: 7.15181303024292
2025-12-09 12:05:24.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0029984352486369867 Training loss: 6.963212966918945
2025-12-09 12:05:24.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002998393251256571 Training loss: 7.211718559265137
2025-12-09 12:05:24.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0029983506980194303 Training loss: 7.077202320098877
2025-12-09 12:05:24.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0029983075889413497 Training loss: 6.9745378494262695
2025-12-09 12:05:24.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0029982639240383217 Training loss: 7.192485809326172
2025-12-09 12:05:24.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0029982197033265445 Training loss: 6.301922798156738
2025-12-09 12:05:25.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0029981749268224228 Training loss: 7.0376763343811035
2025-12-09 12:05:25.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.0029981295945425666 Training loss: 6.540614128112793
2025-12-09 12:05:25.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002998083706503794 Training loss: 6.860949993133545
2025-12-09 12:05:25.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.002998037262723127 Training loss: 6.9038825035095215
2025-12-09 12:05:25.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.002997990263217795 Training loss: 6.875637054443359
2025-12-09 12:05:25.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0029979427080052334 Training loss: 7.065805435180664
2025-12-09 12:05:25.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.002997894597103084 Training loss: 6.904855728149414
2025-12-09 12:05:25.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0029978459305291943 Training loss: 7.1725263595581055
2025-12-09 12:05:26.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0029977967083016175 Training loss: 6.729778289794922
2025-12-09 12:05:26.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.002997746930438614 Training loss: 6.934265613555908
2025-12-09 12:05:26.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0029976965969586494 Training loss: 6.941344261169434
2025-12-09 12:05:26.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0029976457078803964 Training loss: 7.19559383392334
2025-12-09 12:05:26.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0029975942632227332 Training loss: 6.979376792907715
2025-12-09 12:05:26.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002997542263004744 Training loss: 7.3300981521606445
2025-12-09 12:05:26.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.002997489707245719 Training loss: 7.249724388122559
2025-12-09 12:05:26.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0029974365959651544 Training loss: 6.929182052612305
2025-12-09 12:05:27.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0029973829291827544 Training loss: 6.691507816314697
2025-12-09 12:05:27.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.002997328706918426 Training loss: 6.938060283660889
2025-12-09 12:05:27.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0029972739291922843 Training loss: 6.9993085861206055
2025-12-09 12:05:27.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0029972185960246514 Training loss: 7.4263105392456055
2025-12-09 12:05:27.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0029971627074360523 Training loss: 7.228862762451172
2025-12-09 12:05:27.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0029971062634472205 Training loss: 6.750738620758057
2025-12-09 12:05:27.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0029970492640790957 Training loss: 7.2763166427612305
2025-12-09 12:05:27.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.002996991709352822 Training loss: 6.641522407531738
2025-12-09 12:05:28.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0029969335992897513 Training loss: 6.886725902557373
2025-12-09 12:05:28.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0029968749339114404 Training loss: 7.2846503257751465
2025-12-09 12:05:28.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0029968157132396513 Training loss: 7.206923484802246
2025-12-09 12:05:28.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002996755937296354 Training loss: 7.082550048828125
2025-12-09 12:05:28.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.002996695606103723 Training loss: 6.808716297149658
2025-12-09 12:05:28.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0029966347196841393 Training loss: 6.572547912597656
2025-12-09 12:05:28.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.00299657327806019 Training loss: 6.8004279136657715
2025-12-09 12:05:28.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0029965112812546683 Training loss: 7.0698957443237305
2025-12-09 12:05:28.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0029964487292905725 Training loss: 6.591031551361084
2025-12-09 12:05:29.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0029963856221911075 Training loss: 6.883304595947266
2025-12-09 12:05:29.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.002996321959979685 Training loss: 6.934451103210449
2025-12-09 12:05:29.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00299625774267992 Training loss: 7.29334020614624
2025-12-09 12:05:29.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0029961929703156364 Training loss: 6.379184722900391
2025-12-09 12:05:29.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.002996127642910863 Training loss: 6.92647647857666
2025-12-09 12:05:29.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0029960617604898325 Training loss: 7.028370380401611
2025-12-09 12:05:29.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.002995995323076986 Training loss: 7.0811543464660645
2025-12-09 12:05:29.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.002995928330696971 Training loss: 6.890685558319092
2025-12-09 12:05:30.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.002995860783374638 Training loss: 7.706643104553223
2025-12-09 12:05:30.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0029957926811350452 Training loss: 6.878013610839844
2025-12-09 12:05:30.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0029957240240034567 Training loss: 6.7413740158081055
2025-12-09 12:05:30.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0029956548120053422 Training loss: 7.170962810516357
2025-12-09 12:05:30.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0029955850451663765 Training loss: 6.870187759399414
2025-12-09 12:05:30.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0029955147235124417 Training loss: 7.636745929718018
2025-12-09 12:05:30.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.002995443847069625 Training loss: 7.187682151794434
2025-12-09 12:05:30.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0029953724158642185 Training loss: 6.975300312042236
2025-12-09 12:05:31.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0029953004299227213 Training loss: 7.082615375518799
2025-12-09 12:05:31.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.002995227889271838 Training loss: 6.888105392456055
2025-12-09 12:05:31.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.002995154793938479 Training loss: 7.163969039916992
2025-12-09 12:05:31.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0029950811439497607 Training loss: 6.54183292388916
2025-12-09 12:05:31.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0029950069393330043 Training loss: 7.248185157775879
2025-12-09 12:05:31.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.002994932180115737 Training loss: 6.550455570220947
2025-12-09 12:05:31.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0029948568663256928 Training loss: 7.025433540344238
2025-12-09 12:05:31.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0029947809979908114 Training loss: 7.061462879180908
2025-12-09 12:05:32.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.002994704575139236 Training loss: 7.30240535736084
2025-12-09 12:05:32.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.002994627597799318 Training loss: 7.129021167755127
2025-12-09 12:05:32.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0029945500659996132 Training loss: 6.7972092628479
2025-12-09 12:05:32.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0029944719797688844 Training loss: 6.6796650886535645
2025-12-09 12:05:32.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.002994393339136098 Training loss: 7.10182523727417
2025-12-09 12:05:32.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0029943141441304277 Training loss: 6.628809452056885
2025-12-09 12:05:32.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0029942343947812517 Training loss: 6.81244421005249
2025-12-09 12:05:32.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.002994154091118156 Training loss: 6.9828081130981445
2025-12-09 12:05:33.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0029940732331709295 Training loss: 6.6157708168029785
2025-12-09 12:05:33.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0029939918209695676 Training loss: 7.071329116821289
2025-12-09 12:05:33.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0029939098545442733 Training loss: 6.941823959350586
2025-12-09 12:05:33.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0029938273339254516 Training loss: 6.677472114562988
2025-12-09 12:05:33.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.002993744259143717 Training loss: 6.711587429046631
2025-12-09 12:05:33.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.002993660630229886 Training loss: 6.82505464553833
2025-12-09 12:05:33.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0029935764472149833 Training loss: 7.037490367889404
2025-12-09 12:05:33.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0029934917101302376 Training loss: 7.223073482513428
2025-12-09 12:05:34.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.002993406419007084 Training loss: 7.10621976852417
2025-12-09 12:05:34.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0029933205738771626 Training loss: 6.809376239776611
2025-12-09 12:05:34.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0029932341747723194 Training loss: 6.802060604095459
2025-12-09 12:05:34.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.002993147221724606 Training loss: 6.593541622161865
2025-12-09 12:05:34.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0029930597147662785 Training loss: 6.7338409423828125
2025-12-09 12:05:34.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0029929716539297997 Training loss: 6.879372596740723
2025-12-09 12:05:34.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0029928830392478376 Training loss: 7.171481132507324
2025-12-09 12:05:34.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.002992793870753265 Training loss: 6.703210353851318
2025-12-09 12:05:34.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0029927041484791614 Training loss: 6.11191463470459
2025-12-09 12:05:35.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00299261387245881 Training loss: 6.605636119842529
2025-12-09 12:05:35.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0029925230427257006 Training loss: 6.912030220031738
2025-12-09 12:05:35.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0029924316593135285 Training loss: 6.914347171783447
2025-12-09 12:05:35.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0029923397222561938 Training loss: 6.732860088348389
2025-12-09 12:05:35.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0029922472315878023 Training loss: 6.930025100708008
2025-12-09 12:05:35.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.002992154187342665 Training loss: 6.907292366027832
2025-12-09 12:05:35.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.002992060589555299 Training loss: 6.37298059463501
2025-12-09 12:05:35.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0029919664382604253 Training loss: 6.750514984130859
2025-12-09 12:05:36.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0029918717334929718 Training loss: 6.665954113006592
2025-12-09 12:05:36.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00299177647528807 Training loss: 6.813029766082764
2025-12-09 12:05:36.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.002991680663681059 Training loss: 5.885826587677002
2025-12-09 12:05:36.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.002991584298707481 Training loss: 6.899281978607178
2025-12-09 12:05:36.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.002991487380403084 Training loss: 7.518653392791748
2025-12-09 12:05:36.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.002991389908803823 Training loss: 7.222137928009033
2025-12-09 12:05:36.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0029912918839458554 Training loss: 6.804786205291748
2025-12-09 12:05:36.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.002991193305865547 Training loss: 7.185172080993652
2025-12-09 12:05:37.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0029910941745994657 Training loss: 6.8946380615234375
2025-12-09 12:05:37.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0029909944901843864 Training loss: 7.345077037811279
2025-12-09 12:05:37.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.002990894252657289 Training loss: 7.333892822265625
2025-12-09 12:05:37.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0029907934620553595 Training loss: 6.997591495513916
2025-12-09 12:05:37.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0029906921184159863 Training loss: 6.679353713989258
2025-12-09 12:05:37.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029905902217767654 Training loss: 6.816385269165039
2025-12-09 12:05:37.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029904877721754976 Training loss: 7.0955424308776855
2025-12-09 12:05:37.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002990384769650188 Training loss: 6.82263708114624
2025-12-09 12:05:38.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0029902812142390475 Training loss: 6.762483596801758
2025-12-09 12:05:38.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0029901771059804923 Training loss: 6.780555725097656
2025-12-09 12:05:38.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0029900724449131427 Training loss: 6.386040210723877
2025-12-09 12:05:38.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0029899672310758248 Training loss: 6.883411407470703
2025-12-09 12:05:38.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0029898614645075695 Training loss: 7.01676607131958
2025-12-09 12:05:38.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.002989755145247613 Training loss: 6.611011981964111
2025-12-09 12:05:38.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002989648273335397 Training loss: 7.019101142883301
2025-12-09 12:05:38.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0029895408488105667 Training loss: 7.096496105194092
2025-12-09 12:05:39.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0029894328717129737 Training loss: 6.5327582359313965
2025-12-09 12:05:39.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0029893243420826736 Training loss: 6.870201110839844
2025-12-09 12:05:39.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.002989215259959928 Training loss: 6.733623504638672
2025-12-09 12:05:39.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002989105625385203 Training loss: 6.927218437194824
2025-12-09 12:05:39.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.002988995438399169 Training loss: 6.964476585388184
2025-12-09 12:05:39.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0029888846990427024 Training loss: 6.757341384887695
2025-12-09 12:05:39.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.002988773407356884 Training loss: 7.1066389083862305
2025-12-09 12:05:39.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0029886615633829996 Training loss: 6.322603225708008
2025-12-09 12:05:40.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.002988549167162539 Training loss: 6.427431583404541
2025-12-09 12:05:40.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0029884362187371986 Training loss: 6.963607311248779
2025-12-09 12:05:40.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0029883227181488783 Training loss: 6.9178547859191895
2025-12-09 12:05:40.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.002988208665439683 Training loss: 6.541426181793213
2025-12-09 12:05:40.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0029880940606519233 Training loss: 6.805783748626709
2025-12-09 12:05:40.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.002987978903828114 Training loss: 6.969445705413818
2025-12-09 12:05:40.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0029878631950109734 Training loss: 6.805370807647705
2025-12-09 12:05:40.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0029877469342434273 Training loss: 7.025158882141113
2025-12-09 12:05:41.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.002987630121568604 Training loss: 6.805266380310059
2025-12-09 12:05:41.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.002987512757029838 Training loss: 6.598630905151367
2025-12-09 12:05:41.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0029873948406706667 Training loss: 6.352892875671387
2025-12-09 12:05:41.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0029872763725348342 Training loss: 6.993279933929443
2025-12-09 12:05:41.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0029871573526662884 Training loss: 6.742519378662109
2025-12-09 12:05:41.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0029870377811091822 Training loss: 6.9930009841918945
2025-12-09 12:05:41.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.002986917657907872 Training loss: 6.519716262817383
2025-12-09 12:05:41.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.00298679698310692 Training loss: 7.770962715148926
2025-12-09 12:05:41.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.002986675756751093 Training loss: 7.010343074798584
2025-12-09 12:05:42.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0029865539788853624 Training loss: 6.959561347961426
2025-12-09 12:05:42.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0029864316495549037 Training loss: 6.989856719970703
2025-12-09 12:05:42.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0029863087688050973 Training loss: 6.9069952964782715
2025-12-09 12:05:42.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.002986185336681528 Training loss: 6.645585536956787
2025-12-09 12:05:42.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0029860613532299847 Training loss: 6.952005863189697
2025-12-09 12:05:42.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0029859368184964627 Training loss: 6.6477203369140625
2025-12-09 12:05:42.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.002985811732527159 Training loss: 7.0222320556640625
2025-12-09 12:05:42.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0029856860953684774 Training loss: 6.6990485191345215
2025-12-09 12:05:43.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0029855599070670253 Training loss: 6.454990386962891
2025-12-09 12:05:43.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0029854331676696143 Training loss: 6.622445106506348
2025-12-09 12:05:43.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0029853058772232608 Training loss: 6.689058780670166
2025-12-09 12:05:43.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0029851780357751856 Training loss: 6.2664594650268555
2025-12-09 12:05:43.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.002985049643372814 Training loss: 6.52475118637085
2025-12-09 12:05:43.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0029849207000637755 Training loss: 6.678162574768066
2025-12-09 12:05:43.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0029847912058959033 Training loss: 6.927253723144531
2025-12-09 12:05:43.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.002984661160917237 Training loss: 6.8126726150512695
2025-12-09 12:05:44.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.002984530565176018 Training loss: 7.009664058685303
2025-12-09 12:05:44.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.002984399418720694 Training loss: 6.624544620513916
2025-12-09 12:05:44.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0029842677215999158 Training loss: 6.5386271476745605
2025-12-09 12:05:44.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.002984135473862539 Training loss: 6.407007694244385
2025-12-09 12:05:44.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.002984002675557623 Training loss: 6.570512294769287
2025-12-09 12:05:44.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.002983869326734432 Training loss: 6.532273292541504
2025-12-09 12:05:44.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0029837354274424347 Training loss: 6.729857921600342
2025-12-09 12:05:44.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.002983600977731303 Training loss: 6.378969669342041
2025-12-09 12:05:45.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0029834659776509136 Training loss: 7.510632514953613
2025-12-09 12:05:45.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0029833304272513473 Training loss: 6.683416843414307
2025-12-09 12:05:45.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.002983194326582889 Training loss: 6.808837413787842
2025-12-09 12:05:45.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0029830576756960285 Training loss: 6.818580627441406
2025-12-09 12:05:45.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0029829204746414577 Training loss: 6.602586269378662
2025-12-09 12:05:45.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0029827827234700744 Training loss: 5.905323505401611
2025-12-09 12:05:45.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00298264442223298 Training loss: 6.948307514190674
2025-12-09 12:05:45.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0029825055709814803 Training loss: 7.455341815948486
2025-12-09 12:05:46.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.002982366169767084 Training loss: 6.869459629058838
2025-12-09 12:05:46.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.002982226218641505 Training loss: 6.885716915130615
2025-12-09 12:05:46.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.002982085717656661 Training loss: 6.678483486175537
2025-12-09 12:05:46.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0029819446668646723 Training loss: 6.679804801940918
2025-12-09 12:05:46.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0029818030663178656 Training loss: 6.837352752685547
2025-12-09 12:05:46.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00298166091606877 Training loss: 6.841495990753174
2025-12-09 12:05:46.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0029815182161701185 Training loss: 6.787259101867676
2025-12-09 12:05:46.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0029813749666748484 Training loss: 6.735292911529541
2025-12-09 12:05:47.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0029812311676361003 Training loss: 6.574674129486084
2025-12-09 12:05:47.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00298108681910722 Training loss: 6.6183695793151855
2025-12-09 12:05:47.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0029809419211417553 Training loss: 6.583162307739258
2025-12-09 12:05:47.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0029807964737934593 Training loss: 6.841292858123779
2025-12-09 12:05:47.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0029806504771162884 Training loss: 6.667411804199219
2025-12-09 12:05:47.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0029805039311644028 Training loss: 6.444013595581055
2025-12-09 12:05:47.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.002980356835992166 Training loss: 7.527584075927734
2025-12-09 12:05:47.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0029802091916541463 Training loss: 6.606066703796387
2025-12-09 12:05:48.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.002980060998205115 Training loss: 6.430929660797119
2025-12-09 12:05:48.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0029799122557000466 Training loss: 6.374064922332764
2025-12-09 12:05:48.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0029797629641941203 Training loss: 6.807084560394287
2025-12-09 12:05:48.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.002979613123742719 Training loss: 6.848287105560303
2025-12-09 12:05:48.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0029794627344014277 Training loss: 6.897701740264893
2025-12-09 12:05:48.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.002979311796226037 Training loss: 6.638086318969727
2025-12-09 12:05:48.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00297916030927254 Training loss: 7.404041767120361
2025-12-09 12:05:48.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0029790082735971337 Training loss: 6.88470983505249
2025-12-09 12:05:49.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0029788556892562184 Training loss: 6.763189792633057
2025-12-09 12:05:49.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.002978702556306398 Training loss: 6.6315836906433105
2025-12-09 12:05:49.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.00297854887480448 Training loss: 6.436560153961182
2025-12-09 12:05:49.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.002978394644807475 Training loss: 6.867447853088379
2025-12-09 12:05:49.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0029782398663725984 Training loss: 6.524511814117432
2025-12-09 12:05:49.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0029780845395572676 Training loss: 6.580059051513672
2025-12-09 12:05:49.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0029779286644191043 Training loss: 6.952443599700928
2025-12-09 12:05:49.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.002977772241015933 Training loss: 6.746650218963623
2025-12-09 12:05:50.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.002977615269405782 Training loss: 6.474046230316162
2025-12-09 12:05:50.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0029774577496468825 Training loss: 6.598649978637695
2025-12-09 12:05:50.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0029772996817976696 Training loss: 6.622788906097412
2025-12-09 12:05:50.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.002977141065916781 Training loss: 6.862191200256348
2025-12-09 12:05:50.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0029769819020630597 Training loss: 6.291868686676025
2025-12-09 12:05:50.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0029768221902955485 Training loss: 6.594232082366943
2025-12-09 12:05:50.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.002976661930673497 Training loss: 6.357644557952881
2025-12-09 12:05:50.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.002976501123256355 Training loss: 6.541407585144043
2025-12-09 12:05:51.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0029763397681037787 Training loss: 6.670552730560303
2025-12-09 12:05:51.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0029761778652756246 Training loss: 7.7720866203308105
2025-12-09 12:05:51.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0029760154148319538 Training loss: 6.528280258178711
2025-12-09 12:05:51.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0029758524168330305 Training loss: 6.716691970825195
2025-12-09 12:05:51.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.0029756888713393216 Training loss: 6.676687240600586
2025-12-09 12:05:51.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.002975524778411498 Training loss: 7.010941028594971
2025-12-09 12:05:51.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0029753601381104318 Training loss: 6.745908260345459
2025-12-09 12:05:51.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0029751949504972 Training loss: 6.755494594573975
2025-12-09 12:05:52.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0029750292156330823 Training loss: 6.949006080627441
2025-12-09 12:05:52.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0029748629335795604 Training loss: 7.18541955947876
2025-12-09 12:05:52.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.002974696104398321 Training loss: 6.591330528259277
2025-12-09 12:05:52.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.002974528728151251 Training loss: 6.770793437957764
2025-12-09 12:05:52.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0029743608049004424 Training loss: 6.570029258728027
2025-12-09 12:05:52.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.002974192334708189 Training loss: 6.469422817230225
2025-12-09 12:05:52.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.002974023317636989 Training loss: 6.49569034576416
2025-12-09 12:05:52.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0029738537537495413 Training loss: 6.162042140960693
2025-12-09 12:05:52.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.0029736836431087494 Training loss: 6.575479984283447
2025-12-09 12:05:53.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0029735129857777188 Training loss: 6.488759994506836
2025-12-09 12:05:53.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.002973341781819758 Training loss: 6.601925373077393
2025-12-09 12:05:53.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.002973170031298378 Training loss: 6.40312385559082
2025-12-09 12:05:53.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0029729977342772937 Training loss: 6.450708866119385
2025-12-09 12:05:53.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.0029728248908204207 Training loss: 7.596249103546143
2025-12-09 12:05:53.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.0029726515009918793 Training loss: 7.53360652923584
2025-12-09 12:05:53.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0029724775648559913 Training loss: 6.8780107498168945
2025-12-09 12:05:53.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0029723030824772814 Training loss: 6.445672512054443
2025-12-09 12:05:54.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.002972128053920478 Training loss: 7.416211128234863
2025-12-09 12:05:54.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00297195247925051 Training loss: 6.566400527954102
2025-12-09 12:05:54.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.0029717763585325103 Training loss: 6.5160441398620605
2025-12-09 12:05:54.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.002971599691831815 Training loss: 6.800753593444824
2025-12-09 12:05:54.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.0029714224792139607 Training loss: 6.634922504425049
2025-12-09 12:05:54.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.002971244720744689 Training loss: 6.7791547775268555
2025-12-09 12:05:54.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.0029710664164899416 Training loss: 6.782408714294434
2025-12-09 12:05:54.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0029708875665158643 Training loss: 6.822163105010986
2025-12-09 12:05:55.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0029707081708888047 Training loss: 6.74821138381958
2025-12-09 12:05:55.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0029705282296753127 Training loss: 6.194578170776367
2025-12-09 12:05:55.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.002970347742942141 Training loss: 6.548239231109619
2025-12-09 12:05:55.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.002970166710756245 Training loss: 6.072834491729736
2025-12-09 12:05:55.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.002969985133184781 Training loss: 6.844629764556885
2025-12-09 12:05:55.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0029698030102951094 Training loss: 6.3774003982543945
2025-12-09 12:05:55.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0029696203421547916 Training loss: 6.292753219604492
2025-12-09 12:05:55.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0029694371288315913 Training loss: 6.436191558837891
2025-12-09 12:05:56.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.0029692533703934757 Training loss: 6.366152286529541
2025-12-09 12:05:56.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.002969069066908613 Training loss: 6.702482223510742
2025-12-09 12:05:56.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0029688842184453744 Training loss: 6.569467544555664
2025-12-09 12:05:56.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.002968698825072332 Training loss: 6.725257396697998
2025-12-09 12:05:56.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0029685128868582626 Training loss: 6.452209949493408
2025-12-09 12:05:56.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0029683264038721418 Training loss: 6.076386451721191
2025-12-09 12:05:56.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0029681393761831487 Training loss: 6.797595024108887
2025-12-09 12:05:56.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.002967951803860666 Training loss: 6.443729400634766
2025-12-09 12:05:57.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.002967763686974276 Training loss: 6.910253524780273
2025-12-09 12:05:57.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.002967575025593765 Training loss: 6.936112880706787
2025-12-09 12:05:57.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.00296738581978912 Training loss: 6.789383411407471
2025-12-09 12:05:57.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0029671960696305306 Training loss: 6.286722660064697
2025-12-09 12:05:57.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.002967005775188388 Training loss: 6.644083023071289
2025-12-09 12:05:57.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0029668149365332853 Training loss: 6.542397499084473
2025-12-09 12:05:57.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.002966623553736018 Training loss: 6.516792297363281
2025-12-09 12:05:57.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0029664316268675824 Training loss: 6.53737735748291
2025-12-09 12:05:58.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0029662391559991783 Training loss: 6.592297554016113
2025-12-09 12:05:58.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0029660461412022057 Training loss: 7.604376316070557
2025-12-09 12:05:58.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.002965852582548267 Training loss: 6.254014492034912
2025-12-09 12:05:58.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0029656584801091668 Training loss: 6.340149402618408
2025-12-09 12:05:58.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0029654638339569107 Training loss: 7.411369323730469
2025-12-09 12:05:58.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.002965268644163706 Training loss: 6.685967445373535
2025-12-09 12:05:58.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0029650729108019625 Training loss: 6.49324893951416
2025-12-09 12:05:58.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.002964876633944291 Training loss: 6.2864766120910645
2025-12-09 12:05:59.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0029646798136635038 Training loss: 7.112297534942627
2025-12-09 12:05:59.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.002964482450032615 Training loss: 6.6277594566345215
2025-12-09 12:05:59.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.002964284543124841 Training loss: 6.333153247833252
2025-12-09 12:05:59.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0029640860930135976 Training loss: 6.84635066986084
2025-12-09 12:05:59.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.002963887099772505 Training loss: 6.529354095458984
2025-12-09 12:05:59.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.0029636875634753827 Training loss: 6.497745037078857
2025-12-09 12:05:59.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.002963487484196253 Training loss: 6.736903667449951
2025-12-09 12:05:59.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.002963286862009338 Training loss: 6.9636616706848145
2025-12-09 12:06:00.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0029630856969890627 Training loss: 6.979200839996338
2025-12-09 12:06:00.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0029628839892100536 Training loss: 6.5458784103393555
2025-12-09 12:06:00.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.002962681738747137 Training loss: 6.587778568267822
2025-12-09 12:06:00.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.002962478945675342 Training loss: 6.74720573425293
2025-12-09 12:06:00.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.002962275610069898 Training loss: 6.341144561767578
2025-12-09 12:06:00.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.002962071732006237 Training loss: 6.232266426086426
2025-12-09 12:06:00.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00296186731155999 Training loss: 7.296854496002197
2025-12-09 12:06:00.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0029616623488069923 Training loss: 6.866327285766602
2025-12-09 12:06:01.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0029614568438232768 Training loss: 6.656506538391113
2025-12-09 12:06:01.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.0029612507966850807 Training loss: 6.867827892303467
2025-12-09 12:06:01.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0029610442074688398 Training loss: 6.49148416519165
2025-12-09 12:06:01.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0029608370762511937 Training loss: 6.465587615966797
2025-12-09 12:06:01.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0029606294031089804 Training loss: 6.068771839141846
2025-12-09 12:06:01.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.00296042118811924 Training loss: 6.681188106536865
2025-12-09 12:06:01.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.002960212431359215 Training loss: 6.268427848815918
2025-12-09 12:06:01.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0029600031329063466 Training loss: 6.794858932495117
2025-12-09 12:06:01.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.002959793292838278 Training loss: 6.239582061767578
2025-12-09 12:06:02.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.002959582911232853 Training loss: 6.586069583892822
2025-12-09 12:06:02.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0029593719881681173 Training loss: 6.824810981750488
2025-12-09 12:06:02.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.002959160523722316 Training loss: 6.707910537719727
2025-12-09 12:06:02.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0029589485179738963 Training loss: 6.8673624992370605
2025-12-09 12:06:02.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0029587359710015054 Training loss: 6.5138397216796875
2025-12-09 12:06:02.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0029585228828839915 Training loss: 6.838531494140625
2025-12-09 12:06:02.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.002958309253700404 Training loss: 6.917374134063721
2025-12-09 12:06:02.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.002958095083529992 Training loss: 6.793575286865234
2025-12-09 12:06:03.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0029578803724522058 Training loss: 5.655684471130371
2025-12-09 12:06:03.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.002957665120546697 Training loss: 6.541335582733154
2025-12-09 12:06:03.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0029574493278933175 Training loss: 6.607463359832764
2025-12-09 12:06:03.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.002957232994572119 Training loss: 6.734254360198975
2025-12-09 12:06:03.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0029570161206633546 Training loss: 6.510766983032227
2025-12-09 12:06:03.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0029567987062474772 Training loss: 6.709718227386475
2025-12-09 12:06:03.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.002956580751405141 Training loss: 6.504281997680664
2025-12-09 12:06:03.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.002956362256217201 Training loss: 6.557107448577881
2025-12-09 12:06:04.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0029561432207647113 Training loss: 6.65219259262085
2025-12-09 12:06:04.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.002955923645128927 Training loss: 6.471831798553467
2025-12-09 12:06:04.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0029557035293913047 Training loss: 6.287477493286133
2025-12-09 12:06:04.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.0029554828736334995 Training loss: 6.563843250274658
2025-12-09 12:06:04.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0029552616779373684 Training loss: 6.865727424621582
2025-12-09 12:06:04.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0029550399423849674 Training loss: 6.446978569030762
2025-12-09 12:06:04.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.002954817667058554 Training loss: 6.318028450012207
2025-12-09 12:06:04.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.002954594852040585 Training loss: 5.71954345703125
2025-12-09 12:06:05.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0029543714974137178 Training loss: 6.738978385925293
2025-12-09 12:06:05.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.00295414760326081 Training loss: 6.495001792907715
2025-12-09 12:06:05.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.002953923169664919 Training loss: 6.588277816772461
2025-12-09 12:06:05.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.002953698196709303 Training loss: 6.528335094451904
2025-12-09 12:06:05.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.00295347268447742 Training loss: 6.70581579208374
2025-12-09 12:06:05.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.002953246633052928 Training loss: 6.691713809967041
2025-12-09 12:06:05.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0029530200425196837 Training loss: 7.209948539733887
2025-12-09 12:06:05.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0029527929129617467 Training loss: 6.690213680267334
2025-12-09 12:06:06.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.002952565244463374 Training loss: 7.236590385437012
2025-12-09 12:06:06.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0029523370371090235 Training loss: 6.684006214141846
2025-12-09 12:06:06.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.002952108290983353 Training loss: 6.421080589294434
2025-12-09 12:06:06.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.002951879006171221 Training loss: 6.622962951660156
2025-12-09 12:06:06.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0029516491827576833 Training loss: 6.544199466705322
2025-12-09 12:06:06.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.0029514188208279984 Training loss: 6.922192096710205
2025-12-09 12:06:06.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0029511879204676223 Training loss: 6.914835453033447
2025-12-09 12:06:06.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0029509564817622133 Training loss: 6.756515026092529
2025-12-09 12:06:07.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0029507245047976265 Training loss: 6.704845905303955
2025-12-09 12:06:07.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.002950491989659918 Training loss: 6.601622581481934
2025-12-09 12:06:07.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0029502589364353454 Training loss: 6.527646064758301
2025-12-09 12:06:07.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0029500253452103622 Training loss: 6.629278182983398
2025-12-09 12:06:07.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.0029497912160716235 Training loss: 6.625006198883057
2025-12-09 12:06:07.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.002949556549105985 Training loss: 7.164630889892578
2025-12-09 12:06:07.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0029493213444005 Training loss: 6.582413196563721
2025-12-09 12:06:07.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.002949085602042422 Training loss: 7.090836524963379
2025-12-09 12:06:08.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0029488493221192045 Training loss: 6.221736431121826
2025-12-09 12:06:08.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.002948612504718499 Training loss: 6.576568126678467
2025-12-09 12:06:08.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0029483751499281585 Training loss: 6.178523540496826
2025-12-09 12:06:08.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0029481372578362332 Training loss: 6.442831039428711
2025-12-09 12:06:08.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.002947898828530974 Training loss: 6.502846717834473
2025-12-09 12:06:08.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.00294765986210083 Training loss: 6.322373867034912
2025-12-09 12:06:08.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0029474203586344516 Training loss: 6.050717353820801
2025-12-09 12:06:08.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.0029471803182206857 Training loss: 6.424219608306885
2025-12-09 12:06:09.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.0029469397409485807 Training loss: 6.62857723236084
2025-12-09 12:06:09.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0029466986269073825 Training loss: 6.508120059967041
2025-12-09 12:06:09.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.0029464569761865366 Training loss: 6.612873554229736
2025-12-09 12:06:09.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.002946214788875689 Training loss: 6.184014797210693
2025-12-09 12:06:09.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0029459720650646826 Training loss: 6.4313740730285645
2025-12-09 12:06:09.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0029457288048435606 Training loss: 6.021550178527832
2025-12-09 12:06:09.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.002945485008302565 Training loss: 5.214909076690674
2025-12-09 12:06:09.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0029452406755321363 Training loss: 6.838201999664307
2025-12-09 12:06:10.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0029449958066229145 Training loss: 6.202032566070557
2025-12-09 12:06:10.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0029447504016657383 Training loss: 6.4004011154174805
2025-12-09 12:06:10.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.002944504460751645 Training loss: 6.6075873374938965
2025-12-09 12:06:10.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.002944257983971871 Training loss: 6.8642730712890625
2025-12-09 12:06:10.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.002944010971417851 Training loss: 6.5561842918396
2025-12-09 12:06:10.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00294376342318122 Training loss: 6.447722911834717
2025-12-09 12:06:10.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.00294351533935381 Training loss: 6.399353504180908
2025-12-09 12:06:10.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.002943266720027652 Training loss: 6.589293003082275
2025-12-09 12:06:10.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0029430175652949762 Training loss: 6.7755446434021
2025-12-09 12:06:11.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0029427678752482114 Training loss: 6.503210544586182
2025-12-09 12:06:11.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0029425176499799843 Training loss: 6.8726301193237305
2025-12-09 12:06:11.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.002942266889583121 Training loss: 6.270790100097656
2025-12-09 12:06:11.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.0029420155941506454 Training loss: 6.648542881011963
2025-12-09 12:06:11.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00294176376377578 Training loss: 6.604698657989502
2025-12-09 12:06:11.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0029415113985519466 Training loss: 6.753939151763916
2025-12-09 12:06:11.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0029412584985727642 Training loss: 7.065680027008057
2025-12-09 12:06:11.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.002941005063932051 Training loss: 7.319888591766357
2025-12-09 12:06:12.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.002940751094723823 Training loss: 6.486825466156006
2025-12-09 12:06:12.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0029404965910422953 Training loss: 6.579789638519287
2025-12-09 12:06:12.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0029402415529818805 Training loss: 6.405035018920898
2025-12-09 12:06:12.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.0029399859806371895 Training loss: 6.709202766418457
2025-12-09 12:06:12.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.002939729874103032 Training loss: 6.760897159576416
2025-12-09 12:06:12.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.002939473233474415 Training loss: 6.436379432678223
2025-12-09 12:06:12.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.002939216058846544 Training loss: 7.561707973480225
2025-12-09 12:06:12.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0029389583503148234 Training loss: 6.544485092163086
2025-12-09 12:06:13.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.0029387001079748537 Training loss: 6.518718242645264
2025-12-09 12:06:13.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0029384413319224366 Training loss: 5.948535919189453
2025-12-09 12:06:13.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.002938182022253568 Training loss: 6.197293758392334
2025-12-09 12:06:13.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.002937922179064445 Training loss: 6.665858268737793
2025-12-09 12:06:13.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.00293766180245146 Training loss: 6.580999851226807
2025-12-09 12:06:13.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0029374008925112057 Training loss: 6.56744909286499
2025-12-09 12:06:13.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0029371394493404707 Training loss: 6.561537265777588
2025-12-09 12:06:13.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0029368774730362426 Training loss: 6.02355432510376
2025-12-09 12:06:14.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.002936614963695706 Training loss: 6.823266506195068
2025-12-09 12:06:14.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.002936351921416244 Training loss: 6.285183906555176
2025-12-09 12:06:14.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0029360883462954362 Training loss: 6.3579840660095215
2025-12-09 12:06:14.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.002935824238431062 Training loss: 6.423604965209961
2025-12-09 12:06:14.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0029355595979210962 Training loss: 6.4203410148620605
2025-12-09 12:06:14.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.002935294424863712 Training loss: 5.817826747894287
2025-12-09 12:06:14.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.002935028719357281 Training loss: 5.739675998687744
2025-12-09 12:06:14.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0029347624815003704 Training loss: 6.755683422088623
2025-12-09 12:06:15.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.0029344957113917472 Training loss: 6.686555862426758
2025-12-09 12:06:15.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.002934228409130374 Training loss: 6.905627250671387
2025-12-09 12:06:15.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0029339605748154125 Training loss: 6.464472770690918
2025-12-09 12:06:15.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0029336922085462193 Training loss: 6.782835006713867
2025-12-09 12:06:15.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.002933423310422351 Training loss: 6.549301624298096
2025-12-09 12:06:15.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00293315388054356 Training loss: 6.267641067504883
2025-12-09 12:06:15.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.002932883919009796 Training loss: 6.312150955200195
2025-12-09 12:06:15.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.002932613425921207 Training loss: 6.599656581878662
2025-12-09 12:06:16.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.002932342401378137 Training loss: 6.5836358070373535
2025-12-09 12:06:16.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0029320708454811267 Training loss: 6.503737449645996
2025-12-09 12:06:16.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.002931798758330916 Training loss: 6.755625247955322
2025-12-09 12:06:16.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.002931526140028441 Training loss: 6.328779697418213
2025-12-09 12:06:16.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0029312529906748326 Training loss: 6.5220489501953125
2025-12-09 12:06:16.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0029309793103714224 Training loss: 6.4843268394470215
2025-12-09 12:06:16.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.002930705099219736 Training loss: 6.772233486175537
2025-12-09 12:06:16.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0029304303573214983 Training loss: 6.428831100463867
2025-12-09 12:06:17.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0029301550847786293 Training loss: 6.781599998474121
2025-12-09 12:06:17.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0029298792816932462 Training loss: 6.574564456939697
2025-12-09 12:06:17.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0029296029481676636 Training loss: 6.625732421875
2025-12-09 12:06:17.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0029293260843043924 Training loss: 6.6084184646606445
2025-12-09 12:06:17.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0029290486902061397 Training loss: 6.419661045074463
2025-12-09 12:06:17.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0029287707659758117 Training loss: 5.985689163208008
2025-12-09 12:06:17.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0029284923117165076 Training loss: 6.254428386688232
2025-12-09 12:06:17.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0029282133275315265 Training loss: 6.6333794593811035
2025-12-09 12:06:17.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.0029279338135243626 Training loss: 6.140649795532227
2025-12-09 12:06:18.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0029276537697987062 Training loss: 6.57239294052124
2025-12-09 12:06:18.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0029273731964584446 Training loss: 6.270838737487793
2025-12-09 12:06:18.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0029270920936076625 Training loss: 6.352469444274902
2025-12-09 12:06:18.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0029268104613506397 Training loss: 6.354484558105469
2025-12-09 12:06:18.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0029265282997918535 Training loss: 6.302236080169678
2025-12-09 12:06:18.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.0029262456090359762 Training loss: 6.3806939125061035
2025-12-09 12:06:18.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.002925962389187877 Training loss: 6.580389976501465
2025-12-09 12:06:18.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0029256786403526226 Training loss: 6.569784641265869
2025-12-09 12:06:19.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0029253943626354737 Training loss: 6.580355167388916
2025-12-09 12:06:19.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0029251095561418894 Training loss: 6.36591911315918
2025-12-09 12:06:19.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0029248242209775235 Training loss: 5.67539119720459
2025-12-09 12:06:19.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.002924538357248226 Training loss: 6.477421760559082
2025-12-09 12:06:19.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0029242519650600437 Training loss: 6.490217685699463
2025-12-09 12:06:19.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.002923965044519219 Training loss: 6.582173824310303
2025-12-09 12:06:19.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.002923677595732191 Training loss: 6.364938735961914
2025-12-09 12:06:19.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0029233896188055933 Training loss: 6.517190933227539
2025-12-09 12:06:20.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0029231011138462566 Training loss: 6.4929280281066895
2025-12-09 12:06:20.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0029228120809612072 Training loss: 6.222652435302734
2025-12-09 12:06:20.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.002922522520257667 Training loss: 6.4005327224731445
2025-12-09 12:06:20.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0029222324318430542 Training loss: 6.500649452209473
2025-12-09 12:06:20.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0029219418158249826 Training loss: 6.5463643074035645
2025-12-09 12:06:20.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0029216506723112614 Training loss: 6.182040691375732
2025-12-09 12:06:20.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0029213590014098953 Training loss: 6.670246124267578
2025-12-09 12:06:20.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.002921066803229085 Training loss: 6.527848243713379
2025-12-09 12:06:21.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.002920774077877228 Training loss: 6.138282299041748
2025-12-09 12:06:21.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0029204808254629146 Training loss: 6.420607566833496
2025-12-09 12:06:21.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.002920187046094933 Training loss: 6.688811302185059
2025-12-09 12:06:21.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.002919892739882266 Training loss: 6.260159969329834
2025-12-09 12:06:21.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0029195979069340924 Training loss: 5.897103786468506
2025-12-09 12:06:21.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0029193025473597855 Training loss: 6.633561134338379
2025-12-09 12:06:21.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0029190066612689142 Training loss: 6.113748550415039
2025-12-09 12:06:21.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0029187102487712433 Training loss: 6.553441524505615
2025-12-09 12:06:22.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0029184133099767326 Training loss: 7.09044075012207
2025-12-09 12:06:22.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.0029181158449955364 Training loss: 6.109679222106934
2025-12-09 12:06:22.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0029178178539380054 Training loss: 6.647181034088135
2025-12-09 12:06:22.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0029175193369146844 Training loss: 6.053887367248535
2025-12-09 12:06:22.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.002917220294036315 Training loss: 7.125547885894775
2025-12-09 12:06:22.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0029169207254138314 Training loss: 6.571855068206787
2025-12-09 12:06:22.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0029166206311583647 Training loss: 6.518782138824463
2025-12-09 12:06:22.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.00291632001138124 Training loss: 6.4908857345581055
2025-12-09 12:06:23.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0029160188661939783 Training loss: 6.400137901306152
2025-12-09 12:06:23.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.002915717195708295 Training loss: 6.3501434326171875
2025-12-09 12:06:23.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0029154150000361 Training loss: 6.33779239654541
2025-12-09 12:06:23.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0029151122792894987 Training loss: 6.762510299682617
2025-12-09 12:06:23.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.00291480903358079 Training loss: 6.576660633087158
2025-12-09 12:06:23.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00291450526302247 Training loss: 6.591068267822266
2025-12-09 12:06:23.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0029142009677272274 Training loss: 6.274646759033203
2025-12-09 12:06:23.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0029138961478079456 Training loss: 5.932990550994873
2025-12-09 12:06:24.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.002913590803377704 Training loss: 6.439676284790039
2025-12-09 12:06:24.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0029132849345497756 Training loss: 6.329270362854004
2025-12-09 12:06:24.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0029129785414376275 Training loss: 6.703711032867432
2025-12-09 12:06:24.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0029126716241549225 Training loss: 6.507875919342041
2025-12-09 12:06:24.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0029123641828155173 Training loss: 6.356836795806885
2025-12-09 12:06:24.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0029120562175334627 Training loss: 6.335257053375244
2025-12-09 12:06:24.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0029117477284230043 Training loss: 6.642904758453369
2025-12-09 12:06:24.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0029114387155985814 Training loss: 6.370178699493408
2025-12-09 12:06:25.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0029111291791748283 Training loss: 6.915926933288574
2025-12-09 12:06:25.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.002910819119266574 Training loss: 6.338812828063965
2025-12-09 12:06:25.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0029105085359888397 Training loss: 6.148807048797607
2025-12-09 12:06:25.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.0029101974294568427 Training loss: 6.199697971343994
2025-12-09 12:06:25.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0029098857997859936 Training loss: 6.68057107925415
2025-12-09 12:06:25.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0029095736470918974 Training loss: 6.827271461486816
2025-12-09 12:06:25.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0029092609714903525 Training loss: 6.526318073272705
2025-12-09 12:06:25.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.002908947773097352 Training loss: 6.738820552825928
2025-12-09 12:06:26.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.002908634052029083 Training loss: 6.212721824645996
2025-12-09 12:06:26.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0029083198084019256 Training loss: 6.323048114776611
2025-12-09 12:06:26.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0029080050423324543 Training loss: 6.410024166107178
2025-12-09 12:06:26.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.002907689753937438 Training loss: 6.039742946624756
2025-12-09 12:06:26.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.0029073739433338377 Training loss: 6.425073146820068
2025-12-09 12:06:26.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0029070576106388106 Training loss: 6.164061069488525
2025-12-09 12:06:26.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.002906740755969705 Training loss: 6.155194282531738
2025-12-09 12:06:26.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.002906423379444064 Training loss: 6.36160135269165
2025-12-09 12:06:27.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0029061054811796248 Training loss: 5.5694260597229
2025-12-09 12:06:27.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0029057870612943177 Training loss: 6.374414443969727
2025-12-09 12:06:27.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0029054681199062664 Training loss: 6.260662078857422
2025-12-09 12:06:27.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.002905148657133788 Training loss: 6.149483680725098
2025-12-09 12:06:27.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0029048286730953927 Training loss: 6.4309983253479
2025-12-09 12:06:27.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.002904508167909785 Training loss: 6.940184116363525
2025-12-09 12:06:27.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.002904187141695863 Training loss: 6.20512580871582
2025-12-09 12:06:27.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.002903865594572716 Training loss: 6.61351203918457
2025-12-09 12:06:28.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.002903543526659628 Training loss: 6.683595657348633
2025-12-09 12:06:28.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.0029032209380760766 Training loss: 6.724071979522705
2025-12-09 12:06:28.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0029028978289417323 Training loss: 6.699657917022705
2025-12-09 12:06:28.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.002902574199376457 Training loss: 6.500405311584473
2025-12-09 12:06:28.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.002902250049500309 Training loss: 6.364421367645264
2025-12-09 12:06:28.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0029019253794335363 Training loss: 6.678765773773193
2025-12-09 12:06:28.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0029016001892965817 Training loss: 6.098539352416992
2025-12-09 12:06:28.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.00290127447921008 Training loss: 6.5119500160217285
2025-12-09 12:06:29.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.0029009482492948608 Training loss: 6.15609884262085
2025-12-09 12:06:29.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.002900621499671944 Training loss: 6.679952144622803
2025-12-09 12:06:29.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0029002942304625435 Training loss: 6.348617076873779
2025-12-09 12:06:29.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0028999664417880657 Training loss: 6.336254596710205
2025-12-09 12:06:29.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.0028996381337701104 Training loss: 6.015413761138916
2025-12-09 12:06:29.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0028993093065304695 Training loss: 6.440831184387207
2025-12-09 12:06:29.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.002898979960191127 Training loss: 6.234418869018555
2025-12-09 12:06:29.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.002898650094874261 Training loss: 6.042644500732422
2025-12-09 12:06:30.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00289831971070224 Training loss: 6.275795936584473
2025-12-09 12:06:30.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.002897988807797627 Training loss: 6.067231178283691
2025-12-09 12:06:30.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.002897657386283176 Training loss: 7.487936019897461
2025-12-09 12:06:30.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0028973254462818345 Training loss: 6.422121524810791
2025-12-09 12:06:30.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.002896992987916741 Training loss: 6.401440620422363
2025-12-09 12:06:30.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0028966600113112277 Training loss: 6.007553577423096
2025-12-09 12:06:30.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.002896326516588819 Training loss: 6.299907207489014
2025-12-09 12:06:30.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0028959925038732296 Training loss: 6.563823699951172
2025-12-09 12:06:31.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.0028956579732883686 Training loss: 6.310547351837158
2025-12-09 12:06:31.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.002895322924958336 Training loss: 6.556789398193359
2025-12-09 12:06:31.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.002894987359007424 Training loss: 6.165389537811279
2025-12-09 12:06:31.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.0028946512755601175 Training loss: 6.5900397300720215
2025-12-09 12:06:31.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0028943146747410927 Training loss: 6.432644367218018
2025-12-09 12:06:31.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.002893977556675218 Training loss: 6.723244667053223
2025-12-09 12:06:31.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.002893639921487553 Training loss: 6.725358486175537
2025-12-09 12:06:31.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.0028933017693033502 Training loss: 5.72571325302124
2025-12-09 12:06:32.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.002892963100248053 Training loss: 6.450347900390625
2025-12-09 12:06:32.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0028926239144472982 Training loss: 6.3730998039245605
2025-12-09 12:06:32.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.002892284212026912 Training loss: 5.979291915893555
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.32 GiB is free. Including non-PyTorch memory, this process has 90.85 GiB memory in use. Of the allocated memory 89.88 GiB is allocated by PyTorch, and 213.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:07, 147.03it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 176.32it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:21, 122.63it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:09, 141.92it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:56, 174.02it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 214.38it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:52, 186.13it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 185.25it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 198.18it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 203.13it/s]Tokenizing texts:   2%|▏         | 241/10000 [00:01<00:47, 205.81it/s]Tokenizing texts:   3%|▎         | 263/10000 [00:01<00:47, 205.02it/s]Tokenizing texts:   3%|▎         | 292/10000 [00:01<00:42, 228.83it/s]Tokenizing texts:   3%|▎         | 316/10000 [00:01<00:42, 229.15it/s]Tokenizing texts:   3%|▎         | 340/10000 [00:01<00:52, 183.78it/s]Tokenizing texts:   4%|▎         | 365/10000 [00:01<00:48, 199.28it/s]Tokenizing texts:   4%|▍         | 387/10000 [00:02<00:51, 185.26it/s]Tokenizing texts:   4%|▍         | 407/10000 [00:02<00:51, 186.69it/s]Tokenizing texts:   4%|▍         | 427/10000 [00:02<00:53, 180.21it/s]Tokenizing texts:   4%|▍         | 449/10000 [00:02<00:50, 190.56it/s]Tokenizing texts:   5%|▍         | 473/10000 [00:02<00:46, 203.41it/s]Tokenizing texts:   5%|▌         | 500/10000 [00:02<00:42, 220.94it/s]Tokenizing texts:   5%|▌         | 529/10000 [00:02<00:39, 239.21it/s]Tokenizing texts:   6%|▌         | 554/10000 [00:02<00:50, 187.73it/s]Tokenizing texts:   6%|▌         | 583/10000 [00:02<00:44, 212.09it/s]Tokenizing texts:   6%|▌         | 607/10000 [00:03<00:44, 209.21it/s]Tokenizing texts:   6%|▋         | 630/10000 [00:03<00:47, 195.49it/s]Tokenizing texts:   7%|▋         | 653/10000 [00:03<00:45, 203.90it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 200.19it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 191.59it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 201.43it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 201.03it/s]Tokenizing texts:   8%|▊         | 767/10000 [00:03<00:43, 214.06it/s]Tokenizing texts:   8%|▊         | 789/10000 [00:04<00:45, 204.14it/s]Tokenizing texts:   8%|▊         | 813/10000 [00:04<00:43, 212.43it/s]Tokenizing texts:   8%|▊         | 835/10000 [00:04<00:43, 212.87it/s]Tokenizing texts:   9%|▊         | 861/10000 [00:04<00:40, 226.30it/s]Tokenizing texts:   9%|▉         | 884/10000 [00:04<00:41, 218.59it/s]Tokenizing texts:   9%|▉         | 910/10000 [00:04<00:39, 229.13it/s]Tokenizing texts:   9%|▉         | 940/10000 [00:04<00:36, 248.70it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:04<00:33, 268.52it/s]Tokenizing texts:  10%|█         | 1001/10000 [00:04<00:32, 272.82it/s]Tokenizing texts:  10%|█         | 1033/10000 [00:04<00:31, 285.34it/s]Tokenizing texts:  11%|█         | 1062/10000 [00:05<00:39, 223.69it/s]Tokenizing texts:  11%|█         | 1087/10000 [00:05<00:39, 228.51it/s]Tokenizing texts:  11%|█         | 1112/10000 [00:05<00:38, 231.59it/s]Tokenizing texts:  11%|█▏        | 1137/10000 [00:05<00:44, 197.06it/s]Tokenizing texts:  12%|█▏        | 1160/10000 [00:05<00:43, 204.62it/s]Tokenizing texts:  12%|█▏        | 1186/10000 [00:05<00:40, 218.44it/s]Tokenizing texts:  12%|█▏        | 1209/10000 [00:05<00:42, 204.96it/s]Tokenizing texts:  12%|█▏        | 1235/10000 [00:05<00:41, 210.07it/s]Tokenizing texts:  13%|█▎        | 1257/10000 [00:06<00:42, 204.81it/s]Tokenizing texts:  13%|█▎        | 1292/10000 [00:06<00:36, 241.70it/s]Tokenizing texts:  13%|█▎        | 1317/10000 [00:06<00:37, 230.56it/s]Tokenizing texts:  13%|█▎        | 1341/10000 [00:06<00:41, 206.81it/s]Tokenizing texts:  14%|█▎        | 1368/10000 [00:06<00:38, 221.49it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:39, 216.68it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 234.39it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:33, 251.79it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:06<00:30, 275.18it/s]Tokenizing texts:  15%|█▌        | 1517/10000 [00:07<00:31, 268.88it/s]Tokenizing texts:  15%|█▌        | 1545/10000 [00:07<00:33, 253.31it/s]Tokenizing texts:  16%|█▌        | 1578/10000 [00:07<00:30, 273.92it/s]Tokenizing texts:  16%|█▌        | 1606/10000 [00:07<00:33, 251.43it/s]Tokenizing texts:  16%|█▋        | 1632/10000 [00:07<00:40, 207.81it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 216.05it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 217.90it/s]Tokenizing texts:  17%|█▋        | 1705/10000 [00:07<00:37, 222.55it/s]Tokenizing texts:  17%|█▋        | 1738/10000 [00:08<00:32, 251.54it/s]Tokenizing texts:  18%|█▊        | 1764/10000 [00:08<00:34, 241.32it/s]Tokenizing texts:  18%|█▊        | 1793/10000 [00:08<00:33, 243.96it/s]Tokenizing texts:  18%|█▊        | 1827/10000 [00:08<00:30, 267.88it/s]Tokenizing texts:  19%|█▊        | 1855/10000 [00:08<00:36, 222.85it/s]Tokenizing texts:  19%|█▉        | 1879/10000 [00:08<00:36, 223.17it/s]Tokenizing texts:  19%|█▉        | 1905/10000 [00:08<00:34, 231.90it/s]Tokenizing texts:  19%|█▉        | 1930/10000 [00:08<00:34, 231.65it/s]Tokenizing texts:  20%|█▉        | 1954/10000 [00:09<00:42, 188.01it/s]Tokenizing texts:  20%|█▉        | 1985/10000 [00:09<00:37, 216.42it/s]Tokenizing texts:  20%|██        | 2014/10000 [00:09<00:34, 233.28it/s]Tokenizing texts:  20%|██        | 2043/10000 [00:09<00:32, 248.19it/s]Tokenizing texts:  21%|██        | 2070/10000 [00:09<00:32, 247.38it/s]Tokenizing texts:  21%|██        | 2096/10000 [00:09<00:40, 196.78it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:37, 211.55it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:09<00:38, 203.98it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:35, 220.92it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:31, 248.21it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:29, 258.64it/s]Tokenizing texts:  23%|██▎       | 2274/10000 [00:10<00:28, 269.76it/s]Tokenizing texts:  23%|██▎       | 2302/10000 [00:10<00:29, 260.69it/s]Tokenizing texts:  23%|██▎       | 2329/10000 [00:10<00:36, 211.60it/s]Tokenizing texts:  24%|██▎       | 2366/10000 [00:10<00:30, 248.67it/s]Tokenizing texts:  24%|██▍       | 2399/10000 [00:10<00:28, 268.89it/s]Tokenizing texts:  24%|██▍       | 2428/10000 [00:11<00:30, 249.38it/s]Tokenizing texts:  25%|██▍       | 2455/10000 [00:11<00:29, 252.94it/s]Tokenizing texts:  25%|██▍       | 2482/10000 [00:11<00:30, 247.75it/s]Tokenizing texts:  25%|██▌       | 2508/10000 [00:11<00:30, 246.71it/s]Tokenizing texts:  25%|██▌       | 2538/10000 [00:11<00:28, 259.60it/s]Tokenizing texts:  26%|██▌       | 2565/10000 [00:11<00:33, 223.17it/s]Tokenizing texts:  26%|██▌       | 2589/10000 [00:11<00:33, 221.11it/s]Tokenizing texts:  26%|██▌       | 2612/10000 [00:11<00:36, 203.24it/s]Tokenizing texts:  26%|██▋       | 2639/10000 [00:11<00:33, 219.90it/s]Tokenizing texts:  27%|██▋       | 2666/10000 [00:12<00:31, 231.70it/s]Tokenizing texts:  27%|██▋       | 2690/10000 [00:12<00:33, 219.52it/s]Tokenizing texts:  27%|██▋       | 2713/10000 [00:12<00:36, 201.08it/s]Tokenizing texts:  27%|██▋       | 2748/10000 [00:12<00:30, 238.80it/s]Tokenizing texts:  28%|██▊       | 2774/10000 [00:12<00:29, 243.16it/s]Tokenizing texts:  28%|██▊       | 2800/10000 [00:12<00:40, 176.22it/s]Tokenizing texts:  28%|██▊       | 2836/10000 [00:12<00:33, 215.79it/s]Tokenizing texts:  29%|██▊       | 2862/10000 [00:13<00:35, 199.57it/s]Tokenizing texts:  29%|██▉       | 2890/10000 [00:13<00:32, 217.42it/s]Tokenizing texts:  29%|██▉       | 2923/10000 [00:13<00:28, 244.39it/s]Tokenizing texts:  30%|██▉       | 2950/10000 [00:13<00:31, 223.93it/s]Tokenizing texts:  30%|██▉       | 2975/10000 [00:13<00:30, 226.66it/s]Tokenizing texts:  30%|███       | 3013/10000 [00:13<00:26, 263.76it/s]Tokenizing texts:  30%|███       | 3041/10000 [00:13<00:30, 228.37it/s]Tokenizing texts:  31%|███       | 3074/10000 [00:13<00:27, 252.98it/s]Tokenizing texts:  31%|███       | 3101/10000 [00:13<00:28, 242.65it/s]Tokenizing texts:  31%|███▏      | 3135/10000 [00:14<00:25, 267.17it/s]Tokenizing texts:  32%|███▏      | 3163/10000 [00:14<00:26, 253.78it/s]Tokenizing texts:  32%|███▏      | 3190/10000 [00:14<00:26, 256.39it/s]Tokenizing texts:  32%|███▏      | 3219/10000 [00:14<00:25, 265.14it/s]Tokenizing texts:  32%|███▏      | 3247/10000 [00:14<00:26, 256.96it/s]Tokenizing texts:  33%|███▎      | 3278/10000 [00:14<00:25, 267.83it/s]Tokenizing texts:  33%|███▎      | 3306/10000 [00:14<00:25, 260.55it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:14<00:26, 249.11it/s]Tokenizing texts:  34%|███▎      | 3362/10000 [00:14<00:25, 259.68it/s]Tokenizing texts:  34%|███▍      | 3389/10000 [00:15<00:26, 250.55it/s]Tokenizing texts:  34%|███▍      | 3416/10000 [00:15<00:25, 255.14it/s]Tokenizing texts:  35%|███▍      | 3457/10000 [00:15<00:21, 297.74it/s]Tokenizing texts:  35%|███▍      | 3488/10000 [00:15<00:22, 291.88it/s]Tokenizing texts:  35%|███▌      | 3518/10000 [00:15<00:26, 248.02it/s]Tokenizing texts:  35%|███▌      | 3545/10000 [00:15<00:27, 232.53it/s]Tokenizing texts:  36%|███▌      | 3574/10000 [00:15<00:26, 244.99it/s]Tokenizing texts:  36%|███▌      | 3602/10000 [00:15<00:25, 249.30it/s]Tokenizing texts:  36%|███▋      | 3632/10000 [00:15<00:24, 262.32it/s]Tokenizing texts:  37%|███▋      | 3659/10000 [00:16<00:25, 252.04it/s]Tokenizing texts:  37%|███▋      | 3685/10000 [00:16<00:25, 250.70it/s]Tokenizing texts:  37%|███▋      | 3711/10000 [00:16<00:24, 252.16it/s]Tokenizing texts:  37%|███▋      | 3737/10000 [00:16<00:25, 249.44it/s]Tokenizing texts:  38%|███▊      | 3763/10000 [00:16<00:27, 224.39it/s]Tokenizing texts:  38%|███▊      | 3788/10000 [00:16<00:26, 231.13it/s]Tokenizing texts:  38%|███▊      | 3812/10000 [00:16<00:29, 208.46it/s]Tokenizing texts:  38%|███▊      | 3842/10000 [00:16<00:26, 231.10it/s]Tokenizing texts:  39%|███▊      | 3870/10000 [00:17<00:25, 241.78it/s]Tokenizing texts:  39%|███▉      | 3896/10000 [00:17<00:25, 239.62it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:25, 239.38it/s]Tokenizing texts:  39%|███▉      | 3946/10000 [00:17<00:26, 232.67it/s]Tokenizing texts:  40%|███▉      | 3979/10000 [00:17<00:23, 259.43it/s]Tokenizing texts:  40%|████      | 4006/10000 [00:17<00:27, 220.13it/s]Tokenizing texts:  40%|████      | 4037/10000 [00:17<00:25, 238.16it/s]Tokenizing texts:  41%|████      | 4067/10000 [00:17<00:23, 254.00it/s]Tokenizing texts:  41%|████      | 4099/10000 [00:17<00:21, 271.59it/s]Tokenizing texts:  41%|████▏     | 4132/10000 [00:18<00:20, 287.86it/s]Tokenizing texts:  42%|████▏     | 4162/10000 [00:18<00:21, 272.86it/s]Tokenizing texts:  42%|████▏     | 4195/10000 [00:18<00:20, 286.16it/s]Tokenizing texts:  42%|████▏     | 4225/10000 [00:18<00:22, 260.74it/s]Tokenizing texts:  43%|████▎     | 4252/10000 [00:18<00:23, 246.70it/s]Tokenizing texts:  43%|████▎     | 4278/10000 [00:18<00:24, 236.47it/s]Tokenizing texts:  43%|████▎     | 4310/10000 [00:18<00:22, 257.40it/s]Tokenizing texts:  43%|████▎     | 4345/10000 [00:18<00:20, 279.90it/s]Tokenizing texts:  44%|████▎     | 4374/10000 [00:18<00:21, 263.25it/s]Tokenizing texts:  44%|████▍     | 4412/10000 [00:19<00:19, 292.81it/s]Tokenizing texts:  44%|████▍     | 4442/10000 [00:19<00:19, 281.20it/s]Tokenizing texts:  45%|████▍     | 4471/10000 [00:19<00:21, 257.88it/s]Tokenizing texts:  45%|████▍     | 4498/10000 [00:19<00:22, 241.43it/s]Tokenizing texts:  45%|████▌     | 4525/10000 [00:19<00:22, 248.42it/s]Tokenizing texts:  46%|████▌     | 4552/10000 [00:19<00:21, 253.88it/s]Tokenizing texts:  46%|████▌     | 4578/10000 [00:19<00:21, 251.39it/s]Tokenizing texts:  46%|████▌     | 4604/10000 [00:19<00:22, 240.31it/s]Tokenizing texts:  46%|████▋     | 4637/10000 [00:19<00:20, 263.86it/s]Tokenizing texts:  47%|████▋     | 4671/10000 [00:20<00:18, 284.89it/s]Tokenizing texts:  47%|████▋     | 4708/10000 [00:20<00:17, 307.75it/s]Tokenizing texts:  47%|████▋     | 4740/10000 [00:20<00:18, 283.24it/s]Tokenizing texts:  48%|████▊     | 4771/10000 [00:20<00:18, 288.12it/s]Tokenizing texts:  48%|████▊     | 4801/10000 [00:20<00:21, 246.39it/s]Tokenizing texts:  48%|████▊     | 4835/10000 [00:20<00:19, 267.74it/s]Tokenizing texts:  49%|████▊     | 4865/10000 [00:20<00:18, 275.79it/s]Tokenizing texts:  49%|████▉     | 4896/10000 [00:20<00:17, 284.00it/s]Tokenizing texts:  49%|████▉     | 4926/10000 [00:21<00:19, 264.26it/s]Tokenizing texts:  50%|████▉     | 4956/10000 [00:21<00:18, 267.65it/s]Tokenizing texts:  50%|████▉     | 4990/10000 [00:21<00:17, 284.17it/s]Tokenizing texts:  50%|█████     | 5022/10000 [00:21<00:16, 293.41it/s]Tokenizing texts:  51%|█████     | 5052/10000 [00:21<00:19, 258.20it/s]Tokenizing texts:  51%|█████     | 5079/10000 [00:21<00:20, 235.15it/s]Tokenizing texts:  51%|█████     | 5109/10000 [00:21<00:19, 248.51it/s]Tokenizing texts:  51%|█████▏    | 5135/10000 [00:21<00:20, 232.80it/s]Tokenizing texts:  52%|█████▏    | 5166/10000 [00:21<00:19, 249.20it/s]Tokenizing texts:  52%|█████▏    | 5193/10000 [00:22<00:18, 254.29it/s]Tokenizing texts:  52%|█████▏    | 5219/10000 [00:22<00:19, 242.17it/s]Tokenizing texts:  53%|█████▎    | 5253/10000 [00:22<00:17, 267.18it/s]Tokenizing texts:  53%|█████▎    | 5281/10000 [00:22<00:18, 259.27it/s]Tokenizing texts:  53%|█████▎    | 5311/10000 [00:22<00:17, 269.32it/s]Tokenizing texts:  53%|█████▎    | 5346/10000 [00:22<00:15, 290.94it/s]Tokenizing texts:  54%|█████▍    | 5379/10000 [00:22<00:15, 301.10it/s]Tokenizing texts:  54%|█████▍    | 5410/10000 [00:22<00:15, 295.99it/s]Tokenizing texts:  54%|█████▍    | 5440/10000 [00:22<00:16, 277.84it/s]Tokenizing texts:  55%|█████▍    | 5470/10000 [00:23<00:16, 281.03it/s]Tokenizing texts:  55%|█████▍    | 5499/10000 [00:23<00:21, 211.97it/s]Tokenizing texts:  55%|█████▌    | 5536/10000 [00:23<00:18, 246.74it/s]Tokenizing texts:  56%|█████▌    | 5564/10000 [00:23<00:17, 248.18it/s]Tokenizing texts:  56%|█████▌    | 5591/10000 [00:23<00:17, 249.54it/s]Tokenizing texts:  56%|█████▋    | 5626/10000 [00:23<00:15, 274.04it/s]Tokenizing texts:  57%|█████▋    | 5665/10000 [00:23<00:14, 303.67it/s]Tokenizing texts:  57%|█████▋    | 5697/10000 [00:23<00:14, 305.64it/s]Tokenizing texts:  57%|█████▋    | 5729/10000 [00:24<00:14, 290.11it/s]Tokenizing texts:  58%|█████▊    | 5759/10000 [00:24<00:16, 259.82it/s]Tokenizing texts:  58%|█████▊    | 5786/10000 [00:24<00:19, 219.73it/s]Tokenizing texts:  58%|█████▊    | 5810/10000 [00:24<00:19, 219.09it/s]Tokenizing texts:  58%|█████▊    | 5833/10000 [00:24<00:18, 220.71it/s]Tokenizing texts:  59%|█████▊    | 5865/10000 [00:24<00:16, 244.86it/s]Tokenizing texts:  59%|█████▉    | 5892/10000 [00:24<00:16, 250.60it/s]Tokenizing texts:  59%|█████▉    | 5918/10000 [00:24<00:16, 247.91it/s]Tokenizing texts:  59%|█████▉    | 5944/10000 [00:24<00:16, 239.87it/s]Tokenizing texts:  60%|█████▉    | 5973/10000 [00:25<00:15, 253.47it/s]Tokenizing texts:  60%|█████▉    | 5999/10000 [00:25<00:15, 251.04it/s]Tokenizing texts:  60%|██████    | 6025/10000 [00:25<00:16, 246.86it/s]Tokenizing texts:  61%|██████    | 6056/10000 [00:25<00:14, 263.60it/s]Tokenizing texts:  61%|██████    | 6084/10000 [00:25<00:14, 264.12it/s]Tokenizing texts:  61%|██████    | 6111/10000 [00:25<00:14, 259.87it/s]Tokenizing texts:  61%|██████▏   | 6143/10000 [00:25<00:13, 276.99it/s]Tokenizing texts:  62%|██████▏   | 6171/10000 [00:25<00:13, 276.12it/s]Tokenizing texts:  62%|██████▏   | 6199/10000 [00:25<00:13, 271.92it/s]Tokenizing texts:  62%|██████▏   | 6227/10000 [00:26<00:15, 246.54it/s]Tokenizing texts:  63%|██████▎   | 6259/10000 [00:26<00:14, 265.11it/s]Tokenizing texts:  63%|██████▎   | 6287/10000 [00:26<00:14, 255.31it/s]Tokenizing texts:  63%|██████▎   | 6313/10000 [00:26<00:16, 230.04it/s]Tokenizing texts:  63%|██████▎   | 6337/10000 [00:26<00:15, 229.52it/s]Tokenizing texts:  64%|██████▎   | 6365/10000 [00:26<00:15, 242.33it/s]Tokenizing texts:  64%|██████▍   | 6390/10000 [00:26<00:14, 243.85it/s]Tokenizing texts:  64%|██████▍   | 6420/10000 [00:26<00:13, 257.02it/s]Tokenizing texts:  64%|██████▍   | 6448/10000 [00:26<00:13, 262.09it/s]Tokenizing texts:  65%|██████▍   | 6475/10000 [00:27<00:14, 249.77it/s]Tokenizing texts:  65%|██████▌   | 6501/10000 [00:27<00:13, 251.94it/s]Tokenizing texts:  65%|██████▌   | 6527/10000 [00:27<00:15, 225.87it/s]Tokenizing texts:  66%|██████▌   | 6567/10000 [00:27<00:12, 270.91it/s]Tokenizing texts:  66%|██████▌   | 6597/10000 [00:27<00:12, 278.41it/s]Tokenizing texts:  66%|██████▋   | 6626/10000 [00:27<00:12, 264.51it/s]Tokenizing texts:  67%|██████▋   | 6654/10000 [00:27<00:12, 258.49it/s]Tokenizing texts:  67%|██████▋   | 6683/10000 [00:27<00:12, 265.76it/s]Tokenizing texts:  67%|██████▋   | 6710/10000 [00:27<00:13, 248.95it/s]Tokenizing texts:  67%|██████▋   | 6736/10000 [00:28<00:13, 244.57it/s]Tokenizing texts:  68%|██████▊   | 6767/10000 [00:28<00:12, 260.79it/s]Tokenizing texts:  68%|██████▊   | 6803/10000 [00:28<00:11, 285.95it/s]Tokenizing texts:  68%|██████▊   | 6835/10000 [00:28<00:10, 295.57it/s]Tokenizing texts:  69%|██████▊   | 6868/10000 [00:28<00:10, 300.95it/s]Tokenizing texts:  69%|██████▉   | 6899/10000 [00:28<00:13, 235.98it/s]Tokenizing texts:  69%|██████▉   | 6925/10000 [00:28<00:13, 231.78it/s]Tokenizing texts:  70%|██████▉   | 6950/10000 [00:28<00:13, 232.95it/s]Tokenizing texts:  70%|██████▉   | 6978/10000 [00:29<00:12, 242.82it/s]Tokenizing texts:  70%|███████   | 7009/10000 [00:29<00:11, 259.00it/s]Tokenizing texts:  70%|███████   | 7036/10000 [00:29<00:11, 249.64it/s]Tokenizing texts:  71%|███████   | 7062/10000 [00:29<00:13, 211.69it/s]Tokenizing texts:  71%|███████   | 7086/10000 [00:29<00:13, 217.50it/s]Tokenizing texts:  71%|███████   | 7112/10000 [00:29<00:12, 227.11it/s]Tokenizing texts:  71%|███████▏  | 7142/10000 [00:29<00:11, 246.22it/s]Tokenizing texts:  72%|███████▏  | 7168/10000 [00:29<00:11, 236.42it/s]Tokenizing texts:  72%|███████▏  | 7195/10000 [00:29<00:11, 245.48it/s]Tokenizing texts:  72%|███████▏  | 7224/10000 [00:30<00:10, 255.97it/s]Tokenizing texts:  73%|███████▎  | 7251/10000 [00:30<00:11, 243.71it/s]Tokenizing texts:  73%|███████▎  | 7297/10000 [00:30<00:09, 296.60it/s]Tokenizing texts:  73%|███████▎  | 7331/10000 [00:30<00:08, 306.47it/s]Tokenizing texts:  74%|███████▎  | 7363/10000 [00:30<00:09, 265.85it/s]Tokenizing texts:  74%|███████▍  | 7391/10000 [00:30<00:09, 267.91it/s]Tokenizing texts:  74%|███████▍  | 7419/10000 [00:30<00:11, 224.00it/s]Tokenizing texts:  74%|███████▍  | 7444/10000 [00:30<00:12, 205.29it/s]Tokenizing texts:  75%|███████▍  | 7477/10000 [00:31<00:10, 234.43it/s]Tokenizing texts:  75%|███████▌  | 7513/10000 [00:31<00:09, 265.77it/s]Tokenizing texts:  75%|███████▌  | 7542/10000 [00:31<00:09, 266.82it/s]Tokenizing texts:  76%|███████▌  | 7570/10000 [00:31<00:09, 266.22it/s]Tokenizing texts:  76%|███████▌  | 7608/10000 [00:31<00:08, 294.17it/s]Tokenizing texts:  76%|███████▋  | 7639/10000 [00:31<00:08, 268.50it/s]Tokenizing texts:  77%|███████▋  | 7673/10000 [00:31<00:08, 284.84it/s]Tokenizing texts:  77%|███████▋  | 7703/10000 [00:31<00:08, 286.46it/s]Tokenizing texts:  77%|███████▋  | 7733/10000 [00:32<00:09, 248.19it/s]Tokenizing texts:  78%|███████▊  | 7760/10000 [00:32<00:09, 244.72it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 259.47it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 238.37it/s]Tokenizing texts:  78%|███████▊  | 7845/10000 [00:32<00:08, 246.29it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 258.20it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 255.51it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:32<00:07, 283.59it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:32<00:07, 281.31it/s]Tokenizing texts:  80%|███████▉  | 7999/10000 [00:32<00:06, 292.20it/s]Tokenizing texts:  80%|████████  | 8029/10000 [00:33<00:06, 288.38it/s]Tokenizing texts:  81%|████████  | 8059/10000 [00:33<00:11, 171.54it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 200.45it/s]Tokenizing texts:  81%|████████  | 8120/10000 [00:33<00:08, 209.18it/s]Tokenizing texts:  81%|████████▏ | 8149/10000 [00:33<00:08, 226.45it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:33<00:07, 238.82it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:33<00:07, 249.92it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 214.53it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 225.14it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 259.08it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 254.94it/s]Tokenizing texts:  84%|████████▎ | 8357/10000 [00:34<00:06, 258.20it/s]Tokenizing texts:  84%|████████▍ | 8384/10000 [00:34<00:06, 258.37it/s]Tokenizing texts:  84%|████████▍ | 8411/10000 [00:34<00:06, 235.55it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:34<00:05, 259.44it/s]Tokenizing texts:  85%|████████▍ | 8473/10000 [00:35<00:05, 265.38it/s]Tokenizing texts:  85%|████████▌ | 8507/10000 [00:35<00:05, 285.10it/s]Tokenizing texts:  85%|████████▌ | 8537/10000 [00:35<00:05, 264.94it/s]Tokenizing texts:  86%|████████▌ | 8565/10000 [00:35<00:05, 256.83it/s]Tokenizing texts:  86%|████████▌ | 8596/10000 [00:35<00:05, 270.67it/s]Tokenizing texts:  86%|████████▋ | 8629/10000 [00:35<00:04, 286.01it/s]Tokenizing texts:  87%|████████▋ | 8660/10000 [00:35<00:04, 292.15it/s]Tokenizing texts:  87%|████████▋ | 8690/10000 [00:35<00:04, 285.61it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:35<00:04, 285.19it/s]Tokenizing texts:  88%|████████▊ | 8757/10000 [00:36<00:04, 292.28it/s]Tokenizing texts:  88%|████████▊ | 8787/10000 [00:36<00:04, 294.17it/s]Tokenizing texts:  88%|████████▊ | 8817/10000 [00:36<00:05, 221.04it/s]Tokenizing texts:  88%|████████▊ | 8842/10000 [00:36<00:05, 222.84it/s]Tokenizing texts:  89%|████████▉ | 8876/10000 [00:36<00:04, 250.58it/s]Tokenizing texts:  89%|████████▉ | 8914/10000 [00:36<00:03, 274.94it/s]Tokenizing texts:  89%|████████▉ | 8946/10000 [00:36<00:03, 285.85it/s]Tokenizing texts:  90%|████████▉ | 8976/10000 [00:36<00:03, 283.20it/s]Tokenizing texts:  90%|█████████ | 9006/10000 [00:37<00:04, 246.02it/s]Tokenizing texts:  90%|█████████ | 9038/10000 [00:37<00:03, 262.41it/s]Tokenizing texts:  91%|█████████ | 9066/10000 [00:37<00:03, 252.60it/s]Tokenizing texts:  91%|█████████ | 9098/10000 [00:37<00:03, 269.06it/s]Tokenizing texts:  91%|█████████▏| 9126/10000 [00:37<00:03, 259.30it/s]Tokenizing texts:  92%|█████████▏| 9158/10000 [00:37<00:03, 271.13it/s]Tokenizing texts:  92%|█████████▏| 9191/10000 [00:37<00:02, 286.53it/s]Tokenizing texts:  92%|█████████▏| 9221/10000 [00:37<00:02, 285.62it/s]Tokenizing texts:  92%|█████████▎| 9250/10000 [00:37<00:02, 278.80it/s]Tokenizing texts:  93%|█████████▎| 9279/10000 [00:38<00:02, 248.32it/s]Tokenizing texts:  93%|█████████▎| 9310/10000 [00:38<00:02, 258.62it/s]Tokenizing texts:  93%|█████████▎| 9337/10000 [00:38<00:02, 249.78it/s]Tokenizing texts:  94%|█████████▎| 9363/10000 [00:38<00:02, 213.49it/s]Tokenizing texts:  94%|█████████▍| 9400/10000 [00:38<00:02, 251.36it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:38<00:02, 229.47it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:38<00:02, 233.99it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:38<00:02, 242.92it/s]Tokenizing texts:  95%|█████████▌| 9505/10000 [00:39<00:02, 186.17it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 222.69it/s]Tokenizing texts:  96%|█████████▌| 9566/10000 [00:39<00:01, 229.97it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 234.18it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 232.65it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 235.20it/s]Tokenizing texts:  97%|█████████▋| 9674/10000 [00:39<00:01, 241.63it/s]Tokenizing texts:  97%|█████████▋| 9699/10000 [00:39<00:01, 236.94it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:39<00:01, 239.39it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 207.94it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 216.37it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 262.16it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 254.78it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 260.08it/s]Tokenizing texts:  99%|█████████▉| 9907/10000 [00:40<00:00, 287.02it/s]Tokenizing texts:  99%|█████████▉| 9937/10000 [00:40<00:00, 284.10it/s]Tokenizing texts: 100%|█████████▉| 9969/10000 [00:40<00:00, 293.12it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:40<00:00, 244.08it/s]
2025-12-09 12:07:33.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 12.068840980529785
2025-12-09 12:07:33.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 12.02260684967041
2025-12-09 12:07:33.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 12.026599884033203
2025-12-09 12:07:33.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 12.004720687866211
2025-12-09 12:07:33.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 11.88783073425293
2025-12-09 12:07:33.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 11.86867904663086
2025-12-09 12:07:33.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 11.766304969787598
2025-12-09 12:07:34.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 11.51852798461914
2025-12-09 12:07:34.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 11.287763595581055
2025-12-09 12:07:34.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 11.013116836547852
2025-12-09 12:07:34.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 10.738701820373535
2025-12-09 12:07:34.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 10.292537689208984
2025-12-09 12:07:34.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 9.728489875793457
2025-12-09 12:07:34.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 9.249190330505371
2025-12-09 12:07:34.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 9.052129745483398
2025-12-09 12:07:35.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 8.963780403137207
2025-12-09 12:07:35.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 8.617135047912598
2025-12-09 12:07:35.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 8.00594711303711
2025-12-09 12:07:35.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 7.99178409576416
2025-12-09 12:07:35.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 8.255762100219727
2025-12-09 12:07:35.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 8.177886962890625
2025-12-09 12:07:35.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 8.573063850402832
2025-12-09 12:07:36.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 8.141733169555664
2025-12-09 12:07:36.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 8.074204444885254
2025-12-09 12:07:36.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 7.922895908355713
2025-12-09 12:07:36.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 8.313508033752441
2025-12-09 12:07:36.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 8.23602294921875
2025-12-09 12:07:36.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 8.101274490356445
2025-12-09 12:07:36.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 7.795276165008545
2025-12-09 12:07:36.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 8.094681739807129
2025-12-09 12:07:37.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 8.112197875976562
2025-12-09 12:07:37.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 7.788909912109375
2025-12-09 12:07:37.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 7.734545707702637
2025-12-09 12:07:37.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 7.904723644256592
2025-12-09 12:07:37.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 8.141336441040039
2025-12-09 12:07:37.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 7.900674343109131
2025-12-09 12:07:37.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 8.333357810974121
2025-12-09 12:07:37.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 7.748377799987793
2025-12-09 12:07:38.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 7.6838579177856445
2025-12-09 12:07:38.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 7.821983337402344
2025-12-09 12:07:38.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 7.82401180267334
2025-12-09 12:07:38.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 7.7646307945251465
2025-12-09 12:07:38.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 7.946351051330566
2025-12-09 12:07:38.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 8.469719886779785
2025-12-09 12:07:38.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 7.5404133796691895
2025-12-09 12:07:39.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 7.640711307525635
2025-12-09 12:07:39.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 7.739111423492432
2025-12-09 12:07:39.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 7.634212970733643
2025-12-09 12:07:39.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 7.528499603271484
2025-12-09 12:07:39.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 7.67531156539917
2025-12-09 12:07:39.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 7.800546169281006
2025-12-09 12:07:39.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 7.776494979858398
2025-12-09 12:07:39.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 7.610541343688965
2025-12-09 12:07:40.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 7.7797040939331055
2025-12-09 12:07:40.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 6.835300445556641
2025-12-09 12:07:40.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 7.320685386657715
2025-12-09 12:07:40.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 8.274541854858398
2025-12-09 12:07:40.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 7.443312644958496
2025-12-09 12:07:40.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 7.969785690307617
2025-12-09 12:07:40.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 7.639789581298828
2025-12-09 12:07:41.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 7.474161148071289
2025-12-09 12:07:41.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 7.884998321533203
2025-12-09 12:07:41.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 7.425140380859375
2025-12-09 12:07:41.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 7.69309139251709
2025-12-09 12:07:41.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 7.225432395935059
2025-12-09 12:07:41.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 6.72602653503418
2025-12-09 12:07:41.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 7.163479804992676
2025-12-09 12:07:42.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 7.487659454345703
2025-12-09 12:07:42.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 7.409610748291016
2025-12-09 12:07:42.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 7.688326358795166
2025-12-09 12:07:42.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 7.7820868492126465
2025-12-09 12:07:42.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 7.711095809936523
2025-12-09 12:07:42.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 7.455517292022705
2025-12-09 12:07:42.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 7.6629486083984375
2025-12-09 12:07:42.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 7.5943217277526855
2025-12-09 12:07:43.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 7.601009368896484
2025-12-09 12:07:43.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 8.897843360900879
2025-12-09 12:07:43.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 7.483178615570068
2025-12-09 12:07:43.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 7.707221984863281
2025-12-09 12:07:43.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 7.410747051239014
2025-12-09 12:07:43.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 8.559602737426758
2025-12-09 12:07:43.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 7.762932300567627
2025-12-09 12:07:44.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 7.817702770233154
2025-12-09 12:07:44.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 7.659909248352051
2025-12-09 12:07:44.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 7.594141483306885
2025-12-09 12:07:44.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 8.687628746032715
2025-12-09 12:07:44.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 7.5960493087768555
2025-12-09 12:07:44.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 7.510204315185547
2025-12-09 12:07:44.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 7.095714092254639
2025-12-09 12:07:44.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 6.660184383392334
2025-12-09 12:07:45.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 7.4977569580078125
2025-12-09 12:07:45.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 7.382529258728027
2025-12-09 12:07:45.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 7.625558376312256
2025-12-09 12:07:45.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 6.860884666442871
2025-12-09 12:07:45.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 7.274074077606201
2025-12-09 12:07:45.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 7.371702671051025
2025-12-09 12:07:45.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 7.528745174407959
2025-12-09 12:07:46.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 7.5378642082214355
2025-12-09 12:07:46.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 7.2867937088012695
2025-12-09 12:07:46.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 7.272467136383057
2025-12-09 12:07:46.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999999072578702 Training loss: 6.572204113006592
2025-12-09 12:07:46.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.009999996290315153 Training loss: 6.760834693908691
2025-12-09 12:07:46.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009999991653210385 Training loss: 7.008298397064209
2025-12-09 12:07:46.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009999985161266116 Training loss: 7.420165538787842
2025-12-09 12:07:46.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009999976814484758 Training loss: 6.557229995727539
2025-12-09 12:07:47.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009999966612869405 Training loss: 7.22451639175415
2025-12-09 12:07:47.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009999954556423843 Training loss: 7.309723377227783
2025-12-09 12:07:47.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.009999940645152541 Training loss: 7.19513463973999
2025-12-09 12:07:47.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.009999924879060665 Training loss: 7.662733554840088
2025-12-09 12:07:47.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.00999990725815406 Training loss: 7.355587005615234
2025-12-09 12:07:47.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009999887782439263 Training loss: 7.448657035827637
2025-12-09 12:07:47.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0099998664519235 Training loss: 7.410080432891846
2025-12-09 12:07:48.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009999843266614685 Training loss: 7.501899242401123
2025-12-09 12:07:48.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009999818226521415 Training loss: 7.165841102600098
2025-12-09 12:07:48.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.009999791331652984 Training loss: 7.106850624084473
2025-12-09 12:07:48.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009999762582019366 Training loss: 7.002358913421631
2025-12-09 12:07:48.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.009999731977631227 Training loss: 7.3983283042907715
2025-12-09 12:07:48.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.00999969951849992 Training loss: 7.245190143585205
2025-12-09 12:07:48.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.009999665204637487 Training loss: 7.340131759643555
2025-12-09 12:07:48.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009999629036056657 Training loss: 7.302378177642822
2025-12-09 12:07:49.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.009999591012770847 Training loss: 7.169332981109619
2025-12-09 12:07:49.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.009999551134794164 Training loss: 7.2034525871276855
2025-12-09 12:07:49.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0099995094021414 Training loss: 7.109748363494873
2025-12-09 12:07:49.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.009999465814828036 Training loss: 7.138673305511475
2025-12-09 12:07:49.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.009999420372870242 Training loss: 7.06169319152832
2025-12-09 12:07:49.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.009999373076284877 Training loss: 7.2944512367248535
2025-12-09 12:07:49.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009999323925089485 Training loss: 7.11116886138916
2025-12-09 12:07:50.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009999272919302301 Training loss: 7.545513153076172
2025-12-09 12:07:50.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009999220058942245 Training loss: 7.217156887054443
2025-12-09 12:07:50.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009999165344028926 Training loss: 7.084878444671631
2025-12-09 12:07:50.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.009999108774582644 Training loss: 7.222248554229736
2025-12-09 12:07:50.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009999050350624381 Training loss: 6.913064002990723
2025-12-09 12:07:50.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.009998990072175813 Training loss: 7.469227313995361
2025-12-09 12:07:50.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009998927939259302 Training loss: 7.2413859367370605
2025-12-09 12:07:50.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009998863951897896 Training loss: 7.29779577255249
2025-12-09 12:07:51.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009998798110115333 Training loss: 6.9896087646484375
2025-12-09 12:07:51.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.009998730413936037 Training loss: 6.977590560913086
2025-12-09 12:07:51.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.009998660863385123 Training loss: 6.816384792327881
2025-12-09 12:07:51.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00999858945848839 Training loss: 7.253718376159668
2025-12-09 12:07:51.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009998516199272327 Training loss: 6.762037754058838
2025-12-09 12:07:51.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009998441085764113 Training loss: 6.791608810424805
2025-12-09 12:07:51.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.009998364117991612 Training loss: 7.514488220214844
2025-12-09 12:07:52.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.009998285295983376 Training loss: 7.3405914306640625
2025-12-09 12:07:52.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009998204619768645 Training loss: 7.113119125366211
2025-12-09 12:07:52.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.00999812208937735 Training loss: 7.178669452667236
2025-12-09 12:07:52.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.009998037704840102 Training loss: 7.142597675323486
2025-12-09 12:07:52.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00999795146618821 Training loss: 7.262045860290527
2025-12-09 12:07:52.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009997863373453663 Training loss: 7.244924068450928
2025-12-09 12:07:52.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.00999777342666914 Training loss: 7.167050361633301
2025-12-09 12:07:52.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009997681625868013 Training loss: 7.032390594482422
2025-12-09 12:07:53.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009997587971084335 Training loss: 7.048050403594971
2025-12-09 12:07:53.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009997492462352845 Training loss: 7.082368850708008
2025-12-09 12:07:53.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009997395099708982 Training loss: 6.95436429977417
2025-12-09 12:07:53.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.009997295883188855 Training loss: 6.782889366149902
2025-12-09 12:07:53.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009997194812829277 Training loss: 7.279543399810791
2025-12-09 12:07:53.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009997091888667738 Training loss: 6.9621992111206055
2025-12-09 12:07:53.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009996987110742421 Training loss: 6.819182872772217
2025-12-09 12:07:54.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009996880479092198 Training loss: 6.774692535400391
2025-12-09 12:07:54.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.009996771993756622 Training loss: 7.047635555267334
2025-12-09 12:07:54.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.009996661654775939 Training loss: 7.071662902832031
2025-12-09 12:07:54.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.009996549462191081 Training loss: 6.884219169616699
2025-12-09 12:07:54.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00999643541604367 Training loss: 6.885129451751709
2025-12-09 12:07:54.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.00999631951637601 Training loss: 7.00908088684082
2025-12-09 12:07:54.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.009996201763231098 Training loss: 7.444464206695557
2025-12-09 12:07:54.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.009996082156652618 Training loss: 6.799189567565918
2025-12-09 12:07:55.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.009995960696684939 Training loss: 7.476241111755371
2025-12-09 12:07:55.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.009995837383373118 Training loss: 7.048361778259277
2025-12-09 12:07:55.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.009995712216762901 Training loss: 6.862912178039551
2025-12-09 12:07:55.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.009995585196900723 Training loss: 7.046309471130371
2025-12-09 12:07:55.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.009995456323833701 Training loss: 7.146768093109131
2025-12-09 12:07:55.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.009995325597609645 Training loss: 7.1847076416015625
2025-12-09 12:07:55.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00999519301827705 Training loss: 7.102998733520508
2025-12-09 12:07:56.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.009995058585885095 Training loss: 7.6591477394104
2025-12-09 12:07:56.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.009994922300483657 Training loss: 7.084669589996338
2025-12-09 12:07:56.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.00999478416212329 Training loss: 7.2329607009887695
2025-12-09 12:07:56.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.009994644170855237 Training loss: 6.903466701507568
2025-12-09 12:07:56.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.009994502326731434 Training loss: 6.671261310577393
2025-12-09 12:07:56.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.009994358629804499 Training loss: 7.0196709632873535
2025-12-09 12:07:56.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.009994213080127738 Training loss: 6.843265056610107
2025-12-09 12:07:56.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.009994065677755147 Training loss: 7.047752857208252
2025-12-09 12:07:57.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00999391642274141 Training loss: 6.861800670623779
2025-12-09 12:07:57.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00999376531514189 Training loss: 6.939489841461182
2025-12-09 12:07:57.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.009993612355012647 Training loss: 7.17892599105835
2025-12-09 12:07:57.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.009993457542410423 Training loss: 7.004901885986328
2025-12-09 12:07:57.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00999330087739265 Training loss: 6.904743194580078
2025-12-09 12:07:57.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.009993142360017445 Training loss: 6.731201171875
2025-12-09 12:07:57.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.009992981990343614 Training loss: 6.959482192993164
2025-12-09 12:07:58.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.009992819768430647 Training loss: 7.115710735321045
2025-12-09 12:07:58.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.009992655694338725 Training loss: 7.127700328826904
2025-12-09 12:07:58.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.009992489768128714 Training loss: 6.997066497802734
2025-12-09 12:07:58.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.009992321989862165 Training loss: 6.689940929412842
2025-12-09 12:07:58.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.009992152359601322 Training loss: 6.980851650238037
2025-12-09 12:07:58.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.00999198087740911 Training loss: 7.064992427825928
2025-12-09 12:07:58.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.009991807543349147 Training loss: 6.865890979766846
2025-12-09 12:07:58.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.009991632357485729 Training loss: 6.967036724090576
2025-12-09 12:07:59.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.009991455319883848 Training loss: 6.950840950012207
2025-12-09 12:07:59.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.00999127643060918 Training loss: 7.034242630004883
2025-12-09 12:07:59.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.009991095689728087 Training loss: 6.694693565368652
2025-12-09 12:07:59.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.009990913097307614 Training loss: 6.664931774139404
2025-12-09 12:07:59.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.009990728653415505 Training loss: 7.149901390075684
2025-12-09 12:07:59.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.009990542358120174 Training loss: 7.019759178161621
2025-12-09 12:07:59.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.009990354211490735 Training loss: 6.950501441955566
2025-12-09 12:08:00.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.009990164213596987 Training loss: 6.8844218254089355
2025-12-09 12:08:00.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.009989972364509407 Training loss: 6.801263809204102
2025-12-09 12:08:00.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.009989778664299172 Training loss: 6.561446189880371
2025-12-09 12:08:00.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.009989583113038134 Training loss: 6.851664066314697
2025-12-09 12:08:00.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.009989385710798838 Training loss: 7.557477951049805
2025-12-09 12:08:00.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.009989186457654514 Training loss: 6.8050408363342285
2025-12-09 12:08:00.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.009988985353679076 Training loss: 6.8399271965026855
2025-12-09 12:08:01.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.009988782398947132 Training loss: 6.626336574554443
2025-12-09 12:08:01.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.009988577593533967 Training loss: 6.478717803955078
2025-12-09 12:08:01.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00998837093751556 Training loss: 5.89055871963501
2025-12-09 12:08:01.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.009988162430968575 Training loss: 6.7928786277771
2025-12-09 12:08:01.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.009987952073970359 Training loss: 7.168625831604004
2025-12-09 12:08:01.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00998773986659895 Training loss: 7.275319576263428
2025-12-09 12:08:01.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.009987525808933069 Training loss: 6.994012355804443
2025-12-09 12:08:01.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.009987309901052122 Training loss: 7.041020393371582
2025-12-09 12:08:02.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.009987092143036209 Training loss: 6.723902225494385
2025-12-09 12:08:02.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.009986872534966109 Training loss: 6.988259315490723
2025-12-09 12:08:02.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.009986651076923288 Training loss: 6.694704532623291
2025-12-09 12:08:02.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.009986427768989904 Training loss: 7.187310695648193
2025-12-09 12:08:02.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.009986202611248794 Training loss: 6.57732629776001
2025-12-09 12:08:02.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.009985975603783484 Training loss: 6.667487621307373
2025-12-09 12:08:02.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.00998574674667819 Training loss: 6.950439453125
2025-12-09 12:08:03.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.009985516040017807 Training loss: 6.904449462890625
2025-12-09 12:08:03.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.009985283483887922 Training loss: 6.578742027282715
2025-12-09 12:08:03.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.009985049078374806 Training loss: 7.190900802612305
2025-12-09 12:08:03.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.009984812823565416 Training loss: 6.769802570343018
2025-12-09 12:08:03.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.009984574719547395 Training loss: 7.206991672515869
2025-12-09 12:08:03.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.00998433476640907 Training loss: 7.0008416175842285
2025-12-09 12:08:03.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00998409296423946 Training loss: 6.909497261047363
2025-12-09 12:08:03.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.009983849313128264 Training loss: 7.4279303550720215
2025-12-09 12:08:04.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.009983603813165869 Training loss: 6.962609767913818
2025-12-09 12:08:04.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.009983356464443347 Training loss: 6.8223795890808105
2025-12-09 12:08:04.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.009983107267052456 Training loss: 6.7518486976623535
2025-12-09 12:08:04.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.009982856221085643 Training loss: 6.5444135665893555
2025-12-09 12:08:04.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.009982603326636037 Training loss: 7.348174095153809
2025-12-09 12:08:04.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.009982348583797453 Training loss: 7.006933689117432
2025-12-09 12:08:04.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.009982091992664392 Training loss: 6.655892848968506
2025-12-09 12:08:05.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.009981833553332044 Training loss: 6.883561611175537
2025-12-09 12:08:05.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.009981573265896281 Training loss: 7.180989742279053
2025-12-09 12:08:05.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00998131113045366 Training loss: 6.777850151062012
2025-12-09 12:08:05.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.009981047147101425 Training loss: 6.207882404327393
2025-12-09 12:08:05.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.009980781315937506 Training loss: 6.5651774406433105
2025-12-09 12:08:05.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.00998051363706052 Training loss: 7.130690574645996
2025-12-09 12:08:05.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.009980244110569764 Training loss: 6.7978692054748535
2025-12-09 12:08:05.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.009979972736565226 Training loss: 7.322526931762695
2025-12-09 12:08:06.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.009979699515147577 Training loss: 6.687015533447266
2025-12-09 12:08:06.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.009979424446418172 Training loss: 7.044014930725098
2025-12-09 12:08:06.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.009979147530479055 Training loss: 6.94110631942749
2025-12-09 12:08:06.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.009978868767432954 Training loss: 6.781683444976807
2025-12-09 12:08:06.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.009978588157383277 Training loss: 7.111745834350586
2025-12-09 12:08:06.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.009978305700434125 Training loss: 6.6356706619262695
2025-12-09 12:08:06.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00997802139669028 Training loss: 6.642664432525635
2025-12-09 12:08:07.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.009977735246257209 Training loss: 7.345240592956543
2025-12-09 12:08:07.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.009977447249241066 Training loss: 7.15781831741333
2025-12-09 12:08:07.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.009977157405748687 Training loss: 6.935877323150635
2025-12-09 12:08:07.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.009976865715887595 Training loss: 6.799764633178711
2025-12-09 12:08:07.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.009976572179765998 Training loss: 6.811060428619385
2025-12-09 12:08:07.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.009976276797492793 Training loss: 7.739269256591797
2025-12-09 12:08:07.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.009975979569177552 Training loss: 6.677379131317139
2025-12-09 12:08:07.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.009975680494930538 Training loss: 7.4922871589660645
2025-12-09 12:08:08.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.0099753795748627 Training loss: 7.454557418823242
2025-12-09 12:08:08.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.00997507680908567 Training loss: 6.879276752471924
2025-12-09 12:08:08.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.009974772197711762 Training loss: 6.374277114868164
2025-12-09 12:08:08.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.009974465740853979 Training loss: 7.156674861907959
2025-12-09 12:08:08.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.009974157438626008 Training loss: 7.0208353996276855
2025-12-09 12:08:08.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.009973847291142217 Training loss: 6.957695960998535
2025-12-09 12:08:08.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.009973535298517662 Training loss: 6.725693702697754
2025-12-09 12:08:09.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.009973221460868084 Training loss: 6.961019039154053
2025-12-09 12:08:09.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.009972905778309905 Training loss: 6.858501434326172
2025-12-09 12:08:09.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.009972588250960234 Training loss: 5.924248695373535
2025-12-09 12:08:09.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.009972268878936864 Training loss: 6.887411594390869
2025-12-09 12:08:09.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.009971947662358269 Training loss: 6.764411449432373
2025-12-09 12:08:09.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.009971624601343614 Training loss: 6.968844890594482
2025-12-09 12:08:09.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.009971299696012743 Training loss: 6.781720161437988
2025-12-09 12:08:09.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.009970972946486186 Training loss: 7.039689540863037
2025-12-09 12:08:10.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.009970644352885156 Training loss: 6.693514823913574
2025-12-09 12:08:10.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.009970313915331553 Training loss: 6.856912136077881
2025-12-09 12:08:10.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.009969981633947956 Training loss: 7.030015468597412
2025-12-09 12:08:10.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.009969647508857631 Training loss: 6.853391170501709
2025-12-09 12:08:10.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.00996931154018453 Training loss: 7.045505523681641
2025-12-09 12:08:10.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.009968973728053289 Training loss: 6.574999809265137
2025-12-09 12:08:10.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.009968634072589218 Training loss: 7.343296527862549
2025-12-09 12:08:11.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.009968292573918324 Training loss: 6.882440090179443
2025-12-09 12:08:11.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.009967949232167294 Training loss: 6.7375311851501465
2025-12-09 12:08:11.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.009967604047463493 Training loss: 6.861779689788818
2025-12-09 12:08:11.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.009967257019934974 Training loss: 7.012475490570068
2025-12-09 12:08:11.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.009966908149710475 Training loss: 6.454311370849609
2025-12-09 12:08:11.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.009966557436919416 Training loss: 6.7954277992248535
2025-12-09 12:08:11.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.009966204881691898 Training loss: 6.758236408233643
2025-12-09 12:08:12.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.00996585048415871 Training loss: 6.630969047546387
2025-12-09 12:08:12.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.009965494244451324 Training loss: 6.829540729522705
2025-12-09 12:08:12.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00996513616270189 Training loss: 6.6797966957092285
2025-12-09 12:08:12.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.009964776239043245 Training loss: 6.9043450355529785
2025-12-09 12:08:12.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.009964414473608912 Training loss: 6.518424987792969
2025-12-09 12:08:12.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.009964050866533094 Training loss: 6.32871150970459
2025-12-09 12:08:12.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.009963685417950676 Training loss: 6.529360294342041
2025-12-09 12:08:12.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00996331812799723 Training loss: 7.706552505493164
2025-12-09 12:08:13.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.009962948996809008 Training loss: 6.8440775871276855
2025-12-09 12:08:13.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.009962578024522948 Training loss: 6.663963317871094
2025-12-09 12:08:13.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.009962205211276665 Training loss: 7.38213586807251
2025-12-09 12:08:13.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.009961830557208463 Training loss: 6.665764808654785
2025-12-09 12:08:13.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.009961454062457329 Training loss: 7.3640851974487305
2025-12-09 12:08:13.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.009961075727162927 Training loss: 6.447863578796387
2025-12-09 12:08:13.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.009960695551465611 Training loss: 6.902159214019775
2025-12-09 12:08:14.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.009960313535506412 Training loss: 6.732865810394287
2025-12-09 12:08:14.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.009959929679427047 Training loss: 6.826725482940674
2025-12-09 12:08:14.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.009959543983369913 Training loss: 7.037285327911377
2025-12-09 12:08:14.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.009959156447478091 Training loss: 6.80013370513916
2025-12-09 12:08:14.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.009958767071895348 Training loss: 6.8972249031066895
2025-12-09 12:08:14.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.009958375856766127 Training loss: 6.7561774253845215
2025-12-09 12:08:14.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.009957982802235556 Training loss: 6.771286487579346
2025-12-09 12:08:14.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.009957587908449448 Training loss: 6.732707500457764
2025-12-09 12:08:15.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.009957191175554294 Training loss: 7.07874059677124
2025-12-09 12:08:15.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.009956792603697273 Training loss: 6.776334762573242
2025-12-09 12:08:15.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.009956392193026239 Training loss: 7.612469673156738
2025-12-09 12:08:15.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.009955989943689734 Training loss: 6.795159339904785
2025-12-09 12:08:15.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.009955585855836977 Training loss: 6.499397277832031
2025-12-09 12:08:15.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.009955179929617875 Training loss: 7.129858493804932
2025-12-09 12:08:15.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.009954772165183012 Training loss: 6.849942207336426
2025-12-09 12:08:16.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.009954362562683658 Training loss: 6.603510856628418
2025-12-09 12:08:16.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00995395112227176 Training loss: 6.3835015296936035
2025-12-09 12:08:16.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00995353784409995 Training loss: 6.774927616119385
2025-12-09 12:08:16.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.009953122728321542 Training loss: 7.023837089538574
2025-12-09 12:08:16.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00995270577509053 Training loss: 6.852781295776367
2025-12-09 12:08:16.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.009952286984561591 Training loss: 6.611172199249268
2025-12-09 12:08:16.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.009951866356890084 Training loss: 7.284759998321533
2025-12-09 12:08:16.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.009951443892232048 Training loss: 7.192677021026611
2025-12-09 12:08:17.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.009951019590744202 Training loss: 6.58676815032959
2025-12-09 12:08:17.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.009950593452583952 Training loss: 6.270585536956787
2025-12-09 12:08:17.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00995016547790938 Training loss: 6.765294551849365
2025-12-09 12:08:17.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.009949735666879251 Training loss: 7.042377471923828
2025-12-09 12:08:17.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.009949304019653011 Training loss: 6.481563568115234
2025-12-09 12:08:17.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00994887053639079 Training loss: 6.781949520111084
2025-12-09 12:08:17.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.009948435217253393 Training loss: 7.036929607391357
2025-12-09 12:08:18.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.009947998062402312 Training loss: 6.666540622711182
2025-12-09 12:08:18.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.009947559071999719 Training loss: 6.750131607055664
2025-12-09 12:08:18.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.009947118246208461 Training loss: 6.950690746307373
2025-12-09 12:08:18.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.009946675585192076 Training loss: 7.11681604385376
2025-12-09 12:08:18.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.009946231089114773 Training loss: 6.969967365264893
2025-12-09 12:08:18.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.009945784758141448 Training loss: 6.729570388793945
2025-12-09 12:08:18.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.009945336592437678 Training loss: 6.726468086242676
2025-12-09 12:08:18.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.009944886592169713 Training loss: 6.627646446228027
2025-12-09 12:08:19.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.009944434757504492 Training loss: 6.969018459320068
2025-12-09 12:08:19.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.009943981088609631 Training loss: 7.571821689605713
2025-12-09 12:08:19.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.009943525585653428 Training loss: 7.174222469329834
2025-12-09 12:08:19.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.009943068248804858 Training loss: 6.9564032554626465
2025-12-09 12:08:19.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.009942609078233581 Training loss: 6.8608479499816895
2025-12-09 12:08:19.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.009942148074109933 Training loss: 6.473161697387695
2025-12-09 12:08:19.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.009941685236604934 Training loss: 6.774484634399414
2025-12-09 12:08:20.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.009941220565890278 Training loss: 6.598858833312988
2025-12-09 12:08:20.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00994075406213835 Training loss: 6.60795783996582
2025-12-09 12:08:20.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.009940285725522203 Training loss: 6.859521389007568
2025-12-09 12:08:20.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.009939815556215575 Training loss: 7.081836223602295
2025-12-09 12:08:20.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.009939343554392886 Training loss: 6.7040205001831055
2025-12-09 12:08:20.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.009938869720229233 Training loss: 6.948998928070068
2025-12-09 12:08:20.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.009938394053900394 Training loss: 6.988515853881836
2025-12-09 12:08:20.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.009937916555582828 Training loss: 6.865485191345215
2025-12-09 12:08:21.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.009937437225453668 Training loss: 6.589911460876465
2025-12-09 12:08:21.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.009936956063690734 Training loss: 7.359358310699463
2025-12-09 12:08:21.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.009936473070472518 Training loss: 7.180293083190918
2025-12-09 12:08:21.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.009935988245978198 Training loss: 6.800931453704834
2025-12-09 12:08:21.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.009935501590387627 Training loss: 6.842100143432617
2025-12-09 12:08:21.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.009935013103881342 Training loss: 6.473936557769775
2025-12-09 12:08:21.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.009934522786640554 Training loss: 6.8957414627075195
2025-12-09 12:08:22.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.009934030638847154 Training loss: 6.725762367248535
2025-12-09 12:08:22.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.009933536660683716 Training loss: 6.730216979980469
2025-12-09 12:08:22.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.009933040852333487 Training loss: 6.511056423187256
2025-12-09 12:08:22.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.009932543213980401 Training loss: 6.499095916748047
2025-12-09 12:08:22.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.009932043745809064 Training loss: 6.504152297973633
2025-12-09 12:08:22.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.009931542448004758 Training loss: 7.478404998779297
2025-12-09 12:08:22.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.009931039320753456 Training loss: 6.693082332611084
2025-12-09 12:08:22.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0099305343642418 Training loss: 6.86342716217041
2025-12-09 12:08:23.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.009930027578657113 Training loss: 6.451340675354004
2025-12-09 12:08:23.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.009929518964187393 Training loss: 7.126231670379639
2025-12-09 12:08:23.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.009929008521021325 Training loss: 7.516251087188721
2025-12-09 12:08:23.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.009928496249348266 Training loss: 6.5705084800720215
2025-12-09 12:08:23.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.00992798214935825 Training loss: 6.877208709716797
2025-12-09 12:08:23.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.009927466221241995 Training loss: 7.244980812072754
2025-12-09 12:08:23.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.009926948465190892 Training loss: 6.896451473236084
2025-12-09 12:08:24.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.009926428881397015 Training loss: 7.153913497924805
2025-12-09 12:08:24.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.00992590747005311 Training loss: 7.009307861328125
2025-12-09 12:08:24.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.009925384231352606 Training loss: 6.662200927734375
2025-12-09 12:08:24.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.009924859165489608 Training loss: 6.514883041381836
2025-12-09 12:08:24.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.009924332272658898 Training loss: 6.48444938659668
2025-12-09 12:08:24.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.009923803553055936 Training loss: 6.7733283042907715
2025-12-09 12:08:24.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.009923273006876865 Training loss: 6.505600452423096
2025-12-09 12:08:24.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.009922740634318495 Training loss: 6.702679634094238
2025-12-09 12:08:25.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.009922206435578323 Training loss: 6.781561374664307
2025-12-09 12:08:25.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.009921670410854518 Training loss: 6.689470291137695
2025-12-09 12:08:25.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.009921132560345929 Training loss: 6.739403247833252
2025-12-09 12:08:25.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.009920592884252082 Training loss: 6.849154949188232
2025-12-09 12:08:25.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.009920051382773179 Training loss: 6.836354732513428
2025-12-09 12:08:25.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.009919508056110101 Training loss: 6.77110481262207
2025-12-09 12:08:25.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.009918962904464406 Training loss: 6.694324970245361
2025-12-09 12:08:26.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.009918415928038325 Training loss: 6.79633092880249
2025-12-09 12:08:26.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.009917867127034772 Training loss: 7.3155951499938965
2025-12-09 12:08:26.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.009917316501657334 Training loss: 6.98588752746582
2025-12-09 12:08:26.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.009916764052110274 Training loss: 7.012304306030273
2025-12-09 12:08:26.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.009916209778598535 Training loss: 6.288936138153076
2025-12-09 12:08:26.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.009915653681327737 Training loss: 6.866219520568848
2025-12-09 12:08:26.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.009915095760504169 Training loss: 6.958688735961914
2025-12-09 12:08:27.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.009914536016334808 Training loss: 7.041754722595215
2025-12-09 12:08:27.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.009913974449027297 Training loss: 6.477917671203613
2025-12-09 12:08:27.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.009913411058789963 Training loss: 6.746083736419678
2025-12-09 12:08:27.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.009912845845831806 Training loss: 6.106548309326172
2025-12-09 12:08:27.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.009912278810362498 Training loss: 6.348630428314209
2025-12-09 12:08:27.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.009911709952592397 Training loss: 6.758655548095703
2025-12-09 12:08:27.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.009911139272732527 Training loss: 5.559207439422607
2025-12-09 12:08:27.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.009910566770994594 Training loss: 6.7939558029174805
2025-12-09 12:08:28.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.00990999244759098 Training loss: 6.515869140625
2025-12-09 12:08:28.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.009909416302734736 Training loss: 6.972872257232666
2025-12-09 12:08:28.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.009908838336639598 Training loss: 7.781882286071777
2025-12-09 12:08:28.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.00990825854951997 Training loss: 6.553752899169922
2025-12-09 12:08:28.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.009907676941590938 Training loss: 6.991635799407959
2025-12-09 12:08:28.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.009907093513068259 Training loss: 7.088854789733887
2025-12-09 12:08:28.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.009906508264168366 Training loss: 7.085658550262451
2025-12-09 12:08:29.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.009905921195108367 Training loss: 6.584487438201904
2025-12-09 12:08:29.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.00990533230610605 Training loss: 6.90223503112793
2025-12-09 12:08:29.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00990474159737987 Training loss: 6.5904459953308105
2025-12-09 12:08:29.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.009904149069148962 Training loss: 6.771214962005615
2025-12-09 12:08:29.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.009903554721633139 Training loss: 6.68637752532959
2025-12-09 12:08:29.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.00990295855505288 Training loss: 6.648433208465576
2025-12-09 12:08:29.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.009902360569629348 Training loss: 6.724135398864746
2025-12-09 12:08:29.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.009901760765584376 Training loss: 6.524781227111816
2025-12-09 12:08:30.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.00990115914314047 Training loss: 6.581480503082275
2025-12-09 12:08:30.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.009900555702520816 Training loss: 6.807405948638916
2025-12-09 12:08:30.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.00989995044394927 Training loss: 7.112631797790527
2025-12-09 12:08:30.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.009899343367650365 Training loss: 6.628777027130127
2025-12-09 12:08:30.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.009898734473849305 Training loss: 6.590308666229248
2025-12-09 12:08:30.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.00989812376277197 Training loss: 6.567853927612305
2025-12-09 12:08:30.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00989751123464492 Training loss: 6.5913190841674805
2025-12-09 12:08:31.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.009896896889695377 Training loss: 6.728925704956055
2025-12-09 12:08:31.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.009896280728151248 Training loss: 6.560935974121094
2025-12-09 12:08:31.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.009895662750241108 Training loss: 6.693058013916016
2025-12-09 12:08:31.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.009895042956194209 Training loss: 6.9183855056762695
2025-12-09 12:08:31.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.009894421346240473 Training loss: 6.9643449783325195
2025-12-09 12:08:31.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.009893797920610495 Training loss: 7.120804309844971
2025-12-09 12:08:31.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.009893172679535553 Training loss: 6.797580718994141
2025-12-09 12:08:31.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.009892545623247586 Training loss: 6.460376739501953
2025-12-09 12:08:32.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.009891916751979217 Training loss: 7.106161117553711
2025-12-09 12:08:32.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.009891286065963734 Training loss: 7.238894462585449
2025-12-09 12:08:32.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0098906535654351 Training loss: 6.854647636413574
2025-12-09 12:08:32.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.00989001925062796 Training loss: 6.8708319664001465
2025-12-09 12:08:32.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.009889383121777617 Training loss: 6.549482345581055
2025-12-09 12:08:32.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.00988874517912006 Training loss: 6.797246932983398
2025-12-09 12:08:32.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.009888105422891941 Training loss: 6.4989118576049805
2025-12-09 12:08:33.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.009887463853330595 Training loss: 6.496575832366943
2025-12-09 12:08:33.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.009886820470674018 Training loss: 6.5009236335754395
2025-12-09 12:08:33.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.00988617527516089 Training loss: 6.780055999755859
2025-12-09 12:08:33.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.009885528267030555 Training loss: 6.660881042480469
2025-12-09 12:08:33.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.009884879446523035 Training loss: 6.606099605560303
2025-12-09 12:08:33.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00988422881387902 Training loss: 6.8248772621154785
2025-12-09 12:08:33.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.009883576369339874 Training loss: 6.904320240020752
2025-12-09 12:08:33.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.009882922113147636 Training loss: 7.341379642486572
2025-12-09 12:08:34.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.009882266045545011 Training loss: 6.4415364265441895
2025-12-09 12:08:34.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.009881608166775383 Training loss: 6.523916721343994
2025-12-09 12:08:34.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.009880948477082803 Training loss: 6.831442356109619
2025-12-09 12:08:34.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.009880286976711991 Training loss: 6.5638298988342285
2025-12-09 12:08:34.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.00987962366590835 Training loss: 6.211339473724365
2025-12-09 12:08:34.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.009878958544917943 Training loss: 6.521543502807617
2025-12-09 12:08:34.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00987829161398751 Training loss: 6.740270137786865
2025-12-09 12:08:35.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00987762287336446 Training loss: 6.630337715148926
2025-12-09 12:08:35.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.009876952323296877 Training loss: 6.910219192504883
2025-12-09 12:08:35.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.009876279964033513 Training loss: 6.794439792633057
2025-12-09 12:08:35.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00987560579582379 Training loss: 6.472777366638184
2025-12-09 12:08:35.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.009874929818917806 Training loss: 6.799317359924316
2025-12-09 12:08:35.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.009874252033566327 Training loss: 7.209147930145264
2025-12-09 12:08:35.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.009873572440020791 Training loss: 6.987101078033447
2025-12-09 12:08:36.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.0098728910385333 Training loss: 6.496164798736572
2025-12-09 12:08:36.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.009872207829356642 Training loss: 6.583715915679932
2025-12-09 12:08:36.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.009871522812744256 Training loss: 6.557833194732666
2025-12-09 12:08:36.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.009870835988950269 Training loss: 6.575080394744873
2025-12-09 12:08:36.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.009870147358229466 Training loss: 6.749377250671387
2025-12-09 12:08:36.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.009869456920837311 Training loss: 6.612781047821045
2025-12-09 12:08:36.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.009868764677029934 Training loss: 6.824190616607666
2025-12-09 12:08:36.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.009868070627064135 Training loss: 6.510495662689209
2025-12-09 12:08:37.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.009867374771197384 Training loss: 7.783288478851318
2025-12-09 12:08:37.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.009866677109687822 Training loss: 6.549577713012695
2025-12-09 12:08:37.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.009865977642794259 Training loss: 6.55304479598999
2025-12-09 12:08:37.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.009865276370776178 Training loss: 6.830275535583496
2025-12-09 12:08:37.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.009864573293893723 Training loss: 6.665699005126953
2025-12-09 12:08:37.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00986386841240772 Training loss: 6.940836429595947
2025-12-09 12:08:37.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.009863161726579655 Training loss: 6.720981121063232
2025-12-09 12:08:38.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.009862453236671684 Training loss: 6.477381229400635
2025-12-09 12:08:38.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.009861742942946639 Training loss: 7.046326160430908
2025-12-09 12:08:38.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.009861030845668013 Training loss: 6.458734035491943
2025-12-09 12:08:38.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.009860316945099973 Training loss: 6.306370258331299
2025-12-09 12:08:38.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.009859601241507353 Training loss: 6.572041034698486
2025-12-09 12:08:38.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.009858883735155657 Training loss: 6.546146869659424
2025-12-09 12:08:38.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.009858164426311058 Training loss: 6.761453151702881
2025-12-09 12:08:38.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.009857443315240397 Training loss: 6.871712684631348
2025-12-09 12:08:39.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.00985672040221118 Training loss: 6.702463626861572
2025-12-09 12:08:39.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.00985599568749159 Training loss: 6.717737674713135
2025-12-09 12:08:39.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00985526917135047 Training loss: 6.7900190353393555
2025-12-09 12:08:39.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.009854540854057337 Training loss: 6.3940887451171875
2025-12-09 12:08:39.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00985381073588237 Training loss: 7.13010311126709
2025-12-09 12:08:39.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.009853078817096423 Training loss: 6.416250228881836
2025-12-09 12:08:39.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.009852345097971017 Training loss: 7.98431396484375
2025-12-09 12:08:40.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.009851609578778332 Training loss: 6.7832865715026855
2025-12-09 12:08:40.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.009850872259791229 Training loss: 6.851373195648193
2025-12-09 12:08:40.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.009850133141283225 Training loss: 7.177769660949707
2025-12-09 12:08:40.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.009849392223528514 Training loss: 7.701266765594482
2025-12-09 12:08:40.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00984864950680195 Training loss: 6.761897563934326
2025-12-09 12:08:40.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00984790499137906 Training loss: 7.005186080932617
2025-12-09 12:08:40.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.009847158677536034 Training loss: 6.882162570953369
2025-12-09 12:08:40.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00984641056554973 Training loss: 6.689735412597656
2025-12-09 12:08:41.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.009845660655697678 Training loss: 6.517639636993408
2025-12-09 12:08:41.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.009844908948258067 Training loss: 6.509895324707031
2025-12-09 12:08:41.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.009844155443509759 Training loss: 7.030264854431152
2025-12-09 12:08:41.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.009843400141732279 Training loss: 6.414584636688232
2025-12-09 12:08:41.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.009842643043205822 Training loss: 6.285444736480713
2025-12-09 12:08:41.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.009841884148211246 Training loss: 6.729576110839844
2025-12-09 12:08:41.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.009841123457030079 Training loss: 6.643771648406982
2025-12-09 12:08:42.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.009840360969944511 Training loss: 7.695133209228516
2025-12-09 12:08:42.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.009839596687237401 Training loss: 6.528184413909912
2025-12-09 12:08:42.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.009838830609192278 Training loss: 6.729249477386475
2025-12-09 12:08:42.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.009838062736093327 Training loss: 6.581391334533691
2025-12-09 12:08:42.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.009837293068225408 Training loss: 6.357097148895264
2025-12-09 12:08:42.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.009836521605874044 Training loss: 6.77377462387085
2025-12-09 12:08:42.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.009835748349325421 Training loss: 7.195592403411865
2025-12-09 12:08:42.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.009834973298866394 Training loss: 6.322962284088135
2025-12-09 12:08:43.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.009834196454784484 Training loss: 6.914031982421875
2025-12-09 12:08:43.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.009833417817367874 Training loss: 6.6714186668396
2025-12-09 12:08:43.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.009832637386905412 Training loss: 7.762450218200684
2025-12-09 12:08:43.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.009831855163686617 Training loss: 6.235714435577393
2025-12-09 12:08:43.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.009831071148001667 Training loss: 6.915584087371826
2025-12-09 12:08:43.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.009830285340141407 Training loss: 6.988616943359375
2025-12-09 12:08:43.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.009829497740397349 Training loss: 6.954751968383789
2025-12-09 12:08:44.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.009828708349061663 Training loss: 7.926614761352539
2025-12-09 12:08:44.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.009827917166427195 Training loss: 6.56548547744751
2025-12-09 12:08:44.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.009827124192787444 Training loss: 6.432253360748291
2025-12-09 12:08:44.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.00982632942843658 Training loss: 6.414690017700195
2025-12-09 12:08:44.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.009825532873669433 Training loss: 6.734241008758545
2025-12-09 12:08:44.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.009824734528781505 Training loss: 6.759487628936768
2025-12-09 12:08:44.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.009823934394068952 Training loss: 6.330987453460693
2025-12-09 12:08:44.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.009823132469828601 Training loss: 6.620509624481201
2025-12-09 12:08:45.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.009822328756357942 Training loss: 7.395278453826904
2025-12-09 12:08:45.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.009821523253955123 Training loss: 6.644076347351074
2025-12-09 12:08:45.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.009820715962918964 Training loss: 6.850991249084473
2025-12-09 12:08:45.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.009819906883548942 Training loss: 6.569371700286865
2025-12-09 12:08:45.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.009819096016145203 Training loss: 6.719439506530762
2025-12-09 12:08:45.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.00981828336100855 Training loss: 6.767709732055664
2025-12-09 12:08:45.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.009817468918440455 Training loss: 6.357865333557129
2025-12-09 12:08:46.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.009816652688743049 Training loss: 6.855771541595459
2025-12-09 12:08:46.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.009815834672219127 Training loss: 6.813036918640137
2025-12-09 12:08:46.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00981501486917215 Training loss: 6.611138343811035
2025-12-09 12:08:46.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.009814193279906236 Training loss: 6.894766807556152
2025-12-09 12:08:46.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.00981336990472617 Training loss: 5.887566566467285
2025-12-09 12:08:46.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.0098125447439374 Training loss: 6.229372978210449
2025-12-09 12:08:46.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.009811717797846033 Training loss: 6.685842514038086
2025-12-09 12:08:47.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.00981088906675884 Training loss: 6.751376152038574
2025-12-09 12:08:47.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.009810058550983254 Training loss: 6.366332054138184
2025-12-09 12:08:47.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.009809226250827372 Training loss: 6.824065208435059
2025-12-09 12:08:47.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.009808392166599948 Training loss: 6.687029838562012
2025-12-09 12:08:47.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.009807556298610402 Training loss: 6.598841190338135
2025-12-09 12:08:47.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.009806718647168818 Training loss: 6.625784397125244
2025-12-09 12:08:47.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.009805879212585933 Training loss: 6.861649990081787
2025-12-09 12:08:47.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.009805037995173155 Training loss: 6.685739040374756
2025-12-09 12:08:48.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.009804194995242549 Training loss: 6.905912399291992
2025-12-09 12:08:48.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.009803350213106837 Training loss: 6.766468524932861
2025-12-09 12:08:48.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.00980250364907941 Training loss: 6.574820041656494
2025-12-09 12:08:48.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.009801655303474318 Training loss: 6.381176948547363
2025-12-09 12:08:48.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.009800805176606269 Training loss: 6.664783477783203
2025-12-09 12:08:48.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.009799953268790632 Training loss: 6.537549018859863
2025-12-09 12:08:48.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.00979909958034344 Training loss: 6.432698726654053
2025-12-09 12:08:49.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.009798244111581382 Training loss: 6.922760963439941
2025-12-09 12:08:49.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.009797386862821814 Training loss: 6.3246002197265625
2025-12-09 12:08:49.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.009796527834382745 Training loss: 6.4424662590026855
2025-12-09 12:08:49.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.009795667026582846 Training loss: 6.450649738311768
2025-12-09 12:08:49.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.009794804439741454 Training loss: 6.539015769958496
2025-12-09 12:08:49.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.00979394007417856 Training loss: 7.049798011779785
2025-12-09 12:08:49.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.009793073930214816 Training loss: 6.57327127456665
2025-12-09 12:08:49.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.009792206008171534 Training loss: 6.456254959106445
2025-12-09 12:08:50.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.009791336308370686 Training loss: 6.508340358734131
2025-12-09 12:08:50.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.009790464831134903 Training loss: 7.004909038543701
2025-12-09 12:08:50.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.009789591576787476 Training loss: 6.635045051574707
2025-12-09 12:08:50.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.009788716545652353 Training loss: 6.387712001800537
2025-12-09 12:08:50.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.009787839738054147 Training loss: 6.730361461639404
2025-12-09 12:08:50.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.009786961154318121 Training loss: 6.5340704917907715
2025-12-09 12:08:50.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.009786080794770207 Training loss: 6.941072463989258
2025-12-09 12:08:51.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.009785198659736987 Training loss: 6.351048946380615
2025-12-09 12:08:51.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.009784314749545706 Training loss: 6.270993232727051
2025-12-09 12:08:51.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00978342906452427 Training loss: 6.622349262237549
2025-12-09 12:08:51.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.009782541605001235 Training loss: 6.541226863861084
2025-12-09 12:08:51.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.009781652371305825 Training loss: 6.3982110023498535
2025-12-09 12:08:51.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.009780761363767914 Training loss: 6.393101692199707
2025-12-09 12:08:51.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.009779868582718041 Training loss: 6.582632064819336
2025-12-09 12:08:51.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.009778974028487398 Training loss: 6.089515686035156
2025-12-09 12:08:52.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.009778077701407838 Training loss: 6.4507341384887695
2025-12-09 12:08:52.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.009777179601811866 Training loss: 6.476016044616699
2025-12-09 12:08:52.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.009776279730032653 Training loss: 6.573533535003662
2025-12-09 12:08:52.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.009775378086404022 Training loss: 7.024589538574219
2025-12-09 12:08:52.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.009774474671260455 Training loss: 6.818474292755127
2025-12-09 12:08:52.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.00977356948493709 Training loss: 6.530781269073486
2025-12-09 12:08:52.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.00977266252776972 Training loss: 6.843957901000977
2025-12-09 12:08:53.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.009771753800094802 Training loss: 6.493797779083252
2025-12-09 12:08:53.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.009770843302249442 Training loss: 6.397099494934082
2025-12-09 12:08:53.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.009769931034571407 Training loss: 6.583032131195068
2025-12-09 12:08:53.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.00976901699739912 Training loss: 7.582807540893555
2025-12-09 12:08:53.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.009768101191071661 Training loss: 6.5025763511657715
2025-12-09 12:08:53.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.009767183615928763 Training loss: 6.8968424797058105
2025-12-09 12:08:53.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.009766264272310822 Training loss: 6.426103115081787
2025-12-09 12:08:53.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.009765343160558878 Training loss: 7.293953895568848
2025-12-09 12:08:54.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.009764420281014641 Training loss: 6.955187797546387
2025-12-09 12:08:54.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.009763495634020467 Training loss: 6.332810401916504
2025-12-09 12:08:54.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.009762569219919371 Training loss: 6.554468631744385
2025-12-09 12:08:54.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.009761641039055026 Training loss: 6.46536922454834
2025-12-09 12:08:54.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.009760711091771755 Training loss: 6.432880878448486
2025-12-09 12:08:54.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.009759779378414542 Training loss: 6.799166679382324
2025-12-09 12:08:54.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.009758845899329021 Training loss: 6.775013446807861
2025-12-09 12:08:55.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.009757910654861483 Training loss: 6.617191791534424
2025-12-09 12:08:55.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.009756973645358876 Training loss: 6.559113025665283
2025-12-09 12:08:55.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.009756034871168799 Training loss: 6.717047691345215
2025-12-09 12:08:55.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.009755094332639512 Training loss: 6.660333156585693
2025-12-09 12:08:55.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00975415203011992 Training loss: 6.475316524505615
2025-12-09 12:08:55.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.00975320796395959 Training loss: 6.67305326461792
2025-12-09 12:08:55.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.009752262134508742 Training loss: 6.677375316619873
2025-12-09 12:08:55.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.009751314542118247 Training loss: 6.6840596199035645
2025-12-09 12:08:56.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.009750365187139632 Training loss: 6.592565536499023
2025-12-09 12:08:56.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.009749414069925078 Training loss: 6.8610124588012695
2025-12-09 12:08:56.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.00974846119082742 Training loss: 6.7560648918151855
2025-12-09 12:08:56.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.009747506550200145 Training loss: 7.043131351470947
2025-12-09 12:08:56.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.009746550148397396 Training loss: 6.430453777313232
2025-12-09 12:08:56.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.00974559198577397 Training loss: 7.014657974243164
2025-12-09 12:08:56.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.009744632062685311 Training loss: 6.608686923980713
2025-12-09 12:08:57.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.009743670379487522 Training loss: 6.7747578620910645
2025-12-09 12:08:57.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.009742706936537358 Training loss: 6.774240016937256
2025-12-09 12:08:57.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.009741741734192224 Training loss: 6.4782938957214355
2025-12-09 12:08:57.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.009740774772810181 Training loss: 6.509241580963135
2025-12-09 12:08:57.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.009739806052749942 Training loss: 6.721220016479492
2025-12-09 12:08:57.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.009738835574370872 Training loss: 6.717033863067627
2025-12-09 12:08:57.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.009737863338032985 Training loss: 6.712897300720215
2025-12-09 12:08:57.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.009736889344096951 Training loss: 6.62851095199585
2025-12-09 12:08:58.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.009735913592924092 Training loss: 6.7002105712890625
2025-12-09 12:08:58.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.009734936084876382 Training loss: 6.730133533477783
2025-12-09 12:08:58.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.009733956820316443 Training loss: 6.620143890380859
2025-12-09 12:08:58.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.009732975799607553 Training loss: 6.6481451988220215
2025-12-09 12:08:58.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.00973199302311364 Training loss: 6.750279426574707
2025-12-09 12:08:58.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.009731008491199285 Training loss: 6.698187828063965
2025-12-09 12:08:58.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.009730022204229714 Training loss: 6.376865386962891
2025-12-09 12:08:59.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.009729034162570812 Training loss: 6.470648765563965
2025-12-09 12:08:59.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.009728044366589108 Training loss: 6.897624969482422
2025-12-09 12:08:59.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.009727052816651788 Training loss: 6.612435340881348
2025-12-09 12:08:59.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.009726059513126686 Training loss: 6.607030391693115
2025-12-09 12:08:59.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.009725064456382283 Training loss: 6.513270854949951
2025-12-09 12:08:59.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.009724067646787717 Training loss: 6.790168762207031
2025-12-09 12:08:59.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.00972306908471277 Training loss: 6.264499664306641
2025-12-09 12:08:59.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.009722068770527881 Training loss: 6.592294692993164
2025-12-09 12:09:00.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.009721066704604134 Training loss: 6.519721031188965
2025-12-09 12:09:00.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.00972006288731326 Training loss: 6.237661838531494
2025-12-09 12:09:00.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.00971905731902765 Training loss: 6.794955253601074
2025-12-09 12:09:00.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.009718050000120333 Training loss: 6.536647796630859
2025-12-09 12:09:00.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.009717040930964996 Training loss: 6.733771324157715
2025-12-09 12:09:00.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.009716030111935968 Training loss: 6.520160675048828
2025-12-09 12:09:00.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.009715017543408233 Training loss: 6.717585563659668
2025-12-09 12:09:01.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.009714003225757424 Training loss: 6.4487786293029785
2025-12-09 12:09:01.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.009712987159359818 Training loss: 6.836394786834717
2025-12-09 12:09:01.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.009711969344592347 Training loss: 6.657700061798096
2025-12-09 12:09:01.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.009710949781832585 Training loss: 6.626388072967529
2025-12-09 12:09:01.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.009709928471458759 Training loss: 6.42011833190918
2025-12-09 12:09:01.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.009708905413849743 Training loss: 6.267456531524658
2025-12-09 12:09:01.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.009707880609385058 Training loss: 6.347795486450195
2025-12-09 12:09:01.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.009706854058444877 Training loss: 6.186948299407959
2025-12-09 12:09:02.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.009705825761410014 Training loss: 6.868524551391602
2025-12-09 12:09:02.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.009704795718661938 Training loss: 6.740872859954834
2025-12-09 12:09:02.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.00970376393058276 Training loss: 6.5814127922058105
2025-12-09 12:09:02.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.009702730397555245 Training loss: 6.843767166137695
2025-12-09 12:09:02.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.009701695119962798 Training loss: 6.690139293670654
2025-12-09 12:09:02.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.009700658098189475 Training loss: 6.1951165199279785
2025-12-09 12:09:02.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.009699619332619978 Training loss: 6.429620265960693
2025-12-09 12:09:03.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.009698578823639658 Training loss: 6.761087417602539
2025-12-09 12:09:03.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.009697536571634508 Training loss: 6.550556182861328
2025-12-09 12:09:03.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.009696492576991175 Training loss: 6.422069549560547
2025-12-09 12:09:03.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.009695446840096945 Training loss: 6.485523223876953
2025-12-09 12:09:03.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.009694399361339753 Training loss: 6.369103908538818
2025-12-09 12:09:03.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.009693350141108182 Training loss: 6.725902557373047
2025-12-09 12:09:03.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00969229917979146 Training loss: 6.947174072265625
2025-12-09 12:09:04.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.009691246477779459 Training loss: 6.451821327209473
2025-12-09 12:09:04.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0096901920354627 Training loss: 6.709645748138428
2025-12-09 12:09:04.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.009689135853232349 Training loss: 6.8299689292907715
2025-12-09 12:09:04.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.009688077931480212 Training loss: 6.941962242126465
2025-12-09 12:09:04.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.009687018270598749 Training loss: 6.429923057556152
2025-12-09 12:09:04.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.009685956870981059 Training loss: 6.640658378601074
2025-12-09 12:09:04.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.009684893733020887 Training loss: 6.4688334465026855
2025-12-09 12:09:04.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.009683828857112626 Training loss: 6.7475080490112305
2025-12-09 12:09:05.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.009682762243651308 Training loss: 5.867447376251221
2025-12-09 12:09:05.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.009681693893032617 Training loss: 6.129164218902588
2025-12-09 12:09:05.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.009680623805652875 Training loss: 6.940500736236572
2025-12-09 12:09:05.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.009679551981909052 Training loss: 6.750192165374756
2025-12-09 12:09:05.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.00967847842219876 Training loss: 5.952862739562988
2025-12-09 12:09:05.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.009677403126920255 Training loss: 6.6299824714660645
2025-12-09 12:09:05.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.00967632609647244 Training loss: 6.820544719696045
2025-12-09 12:09:06.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.009675247331254857 Training loss: 6.690580368041992
2025-12-09 12:09:06.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.009674166831667696 Training loss: 6.40915584564209
2025-12-09 12:09:06.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.009673084598111788 Training loss: 6.873575687408447
2025-12-09 12:09:06.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.009672000630988605 Training loss: 6.376134872436523
2025-12-09 12:09:06.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.009670914930700268 Training loss: 7.093797206878662
2025-12-09 12:09:06.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.009669827497649537 Training loss: 6.361751079559326
2025-12-09 12:09:06.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.009668738332239813 Training loss: 6.747757911682129
2025-12-09 12:09:06.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.009667647434875144 Training loss: 6.855175495147705
2025-12-09 12:09:07.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.00966655480596022 Training loss: 6.620063781738281
2025-12-09 12:09:07.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.009665460445900368 Training loss: 6.472957611083984
2025-12-09 12:09:07.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.009664364355101564 Training loss: 6.656523704528809
2025-12-09 12:09:07.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.009663266533970424 Training loss: 6.645740509033203
2025-12-09 12:09:07.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.009662166982914203 Training loss: 6.545040607452393
2025-12-09 12:09:07.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.0096610657023408 Training loss: 6.4658918380737305
2025-12-09 12:09:07.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.009659962692658756 Training loss: 7.291782379150391
2025-12-09 12:09:08.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.009658857954277254 Training loss: 6.852299213409424
2025-12-09 12:09:08.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.009657751487606114 Training loss: 5.509487628936768
2025-12-09 12:09:08.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.009656643293055805 Training loss: 6.753030776977539
2025-12-09 12:09:08.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.009655533371037426 Training loss: 6.526644229888916
2025-12-09 12:09:08.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.009654421721962729 Training loss: 6.220489025115967
2025-12-09 12:09:08.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.009653308346244099 Training loss: 6.460282802581787
2025-12-09 12:09:08.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.009652193244294562 Training loss: 6.053178310394287
2025-12-09 12:09:09.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.009651076416527786 Training loss: 6.833230972290039
2025-12-09 12:09:09.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.00964995786335808 Training loss: 6.566738128662109
2025-12-09 12:09:09.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.009648837585200392 Training loss: 6.476998329162598
2025-12-09 12:09:09.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.00964771558247031 Training loss: 6.470883369445801
2025-12-09 12:09:09.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.00964659185558406 Training loss: 7.117226600646973
2025-12-09 12:09:09.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.00964546640495851 Training loss: 7.232943058013916
2025-12-09 12:09:09.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.009644339231011169 Training loss: 6.451460838317871
2025-12-09 12:09:09.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.009643210334160178 Training loss: 6.526371955871582
2025-12-09 12:09:10.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.009642079714824328 Training loss: 6.312585830688477
2025-12-09 12:09:10.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.00964094737342304 Training loss: 6.919478416442871
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.32 GiB is free. Including non-PyTorch memory, this process has 90.85 GiB memory in use. Of the allocated memory 89.88 GiB is allocated by PyTorch, and 213.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:07, 147.05it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 176.46it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:21, 122.49it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:10, 141.74it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:56, 173.77it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 214.17it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:53, 185.78it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 184.96it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 197.81it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 202.74it/s]Tokenizing texts:   2%|▏         | 241/10000 [00:01<00:47, 205.44it/s]Tokenizing texts:   3%|▎         | 263/10000 [00:01<00:47, 204.47it/s]Tokenizing texts:   3%|▎         | 292/10000 [00:01<00:42, 228.29it/s]Tokenizing texts:   3%|▎         | 316/10000 [00:01<00:42, 228.66it/s]Tokenizing texts:   3%|▎         | 340/10000 [00:01<00:52, 183.65it/s]Tokenizing texts:   4%|▎         | 364/10000 [00:01<00:48, 197.37it/s]Tokenizing texts:   4%|▍         | 386/10000 [00:02<00:52, 183.93it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 186.61it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:51, 185.97it/s]Tokenizing texts:   4%|▍         | 446/10000 [00:02<00:51, 187.30it/s]Tokenizing texts:   5%|▍         | 468/10000 [00:02<00:48, 195.93it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 218.12it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:39, 237.34it/s]Tokenizing texts:   6%|▌         | 551/10000 [00:02<00:50, 187.34it/s]Tokenizing texts:   6%|▌         | 581/10000 [00:02<00:44, 211.46it/s]Tokenizing texts:   6%|▌         | 604/10000 [00:03<00:44, 213.53it/s]Tokenizing texts:   6%|▋         | 627/10000 [00:03<00:46, 200.11it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 202.10it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 199.82it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 190.95it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 200.59it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 200.43it/s]Tokenizing texts:   8%|▊         | 766/10000 [00:03<00:43, 211.22it/s]Tokenizing texts:   8%|▊         | 788/10000 [00:04<00:45, 202.91it/s]Tokenizing texts:   8%|▊         | 812/10000 [00:04<00:43, 212.33it/s]Tokenizing texts:   8%|▊         | 834/10000 [00:04<00:43, 210.44it/s]Tokenizing texts:   9%|▊         | 860/10000 [00:04<00:40, 224.18it/s]Tokenizing texts:   9%|▉         | 883/10000 [00:04<00:42, 216.72it/s]Tokenizing texts:   9%|▉         | 910/10000 [00:04<00:39, 229.79it/s]Tokenizing texts:   9%|▉         | 940/10000 [00:04<00:36, 249.14it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:04<00:33, 268.59it/s]Tokenizing texts:  10%|█         | 1001/10000 [00:04<00:32, 272.95it/s]Tokenizing texts:  10%|█         | 1033/10000 [00:04<00:31, 285.18it/s]Tokenizing texts:  11%|█         | 1062/10000 [00:05<00:40, 223.38it/s]Tokenizing texts:  11%|█         | 1087/10000 [00:05<00:39, 228.16it/s]Tokenizing texts:  11%|█         | 1112/10000 [00:05<00:38, 231.41it/s]Tokenizing texts:  11%|█▏        | 1137/10000 [00:05<00:44, 196.97it/s]Tokenizing texts:  12%|█▏        | 1160/10000 [00:05<00:43, 204.57it/s]Tokenizing texts:  12%|█▏        | 1185/10000 [00:05<00:40, 216.19it/s]Tokenizing texts:  12%|█▏        | 1208/10000 [00:05<00:43, 203.49it/s]Tokenizing texts:  12%|█▏        | 1235/10000 [00:05<00:41, 210.25it/s]Tokenizing texts:  13%|█▎        | 1257/10000 [00:06<00:42, 204.75it/s]Tokenizing texts:  13%|█▎        | 1292/10000 [00:06<00:36, 241.34it/s]Tokenizing texts:  13%|█▎        | 1317/10000 [00:06<00:37, 229.24it/s]Tokenizing texts:  13%|█▎        | 1341/10000 [00:06<00:42, 205.59it/s]Tokenizing texts:  14%|█▎        | 1368/10000 [00:06<00:39, 220.27it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:39, 215.41it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 232.82it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:34, 250.32it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:07<00:31, 273.52it/s]Tokenizing texts:  15%|█▌        | 1516/10000 [00:07<00:31, 267.51it/s]Tokenizing texts:  15%|█▌        | 1544/10000 [00:07<00:33, 250.89it/s]Tokenizing texts:  16%|█▌        | 1577/10000 [00:07<00:30, 272.11it/s]Tokenizing texts:  16%|█▌        | 1605/10000 [00:07<00:33, 253.13it/s]Tokenizing texts:  16%|█▋        | 1631/10000 [00:07<00:40, 208.78it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 216.34it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 217.91it/s]Tokenizing texts:  17%|█▋        | 1704/10000 [00:07<00:37, 223.48it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:33, 245.20it/s]Tokenizing texts:  18%|█▊        | 1761/10000 [00:08<00:33, 245.91it/s]Tokenizing texts:  18%|█▊        | 1787/10000 [00:08<00:32, 249.12it/s]Tokenizing texts:  18%|█▊        | 1821/10000 [00:08<00:30, 268.65it/s]Tokenizing texts:  18%|█▊        | 1849/10000 [00:08<00:36, 220.49it/s]Tokenizing texts:  19%|█▊        | 1873/10000 [00:08<00:36, 221.29it/s]Tokenizing texts:  19%|█▉        | 1898/10000 [00:08<00:35, 227.89it/s]Tokenizing texts:  19%|█▉        | 1922/10000 [00:08<00:35, 229.28it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 195.02it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:38, 210.91it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:34, 228.76it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 234.50it/s]Tokenizing texts:  21%|██        | 2055/10000 [00:09<00:31, 248.90it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:37, 209.69it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:37, 210.44it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:36, 215.58it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:09<00:38, 206.02it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:34, 223.55it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:30, 251.41it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:29, 261.45it/s]Tokenizing texts:  23%|██▎       | 2275/10000 [00:10<00:28, 270.78it/s]Tokenizing texts:  23%|██▎       | 2303/10000 [00:10<00:29, 258.94it/s]Tokenizing texts:  23%|██▎       | 2330/10000 [00:10<00:36, 210.50it/s]Tokenizing texts:  24%|██▎       | 2370/10000 [00:10<00:29, 254.94it/s]Tokenizing texts:  24%|██▍       | 2401/10000 [00:10<00:28, 268.68it/s]Tokenizing texts:  24%|██▍       | 2430/10000 [00:11<00:31, 243.56it/s]Tokenizing texts:  25%|██▍       | 2461/10000 [00:11<00:28, 260.32it/s]Tokenizing texts:  25%|██▍       | 2489/10000 [00:11<00:30, 249.82it/s]Tokenizing texts:  25%|██▌       | 2515/10000 [00:11<00:29, 251.29it/s]Tokenizing texts:  25%|██▌       | 2544/10000 [00:11<00:28, 261.02it/s]Tokenizing texts:  26%|██▌       | 2571/10000 [00:11<00:32, 228.75it/s]Tokenizing texts:  26%|██▌       | 2595/10000 [00:11<00:34, 217.37it/s]Tokenizing texts:  26%|██▌       | 2618/10000 [00:11<00:35, 207.04it/s]Tokenizing texts:  26%|██▋       | 2647/10000 [00:11<00:32, 227.19it/s]Tokenizing texts:  27%|██▋       | 2671/10000 [00:12<00:32, 228.47it/s]Tokenizing texts:  27%|██▋       | 2695/10000 [00:12<00:37, 196.68it/s]Tokenizing texts:  27%|██▋       | 2722/10000 [00:12<00:33, 214.73it/s]Tokenizing texts:  28%|██▊       | 2758/10000 [00:12<00:29, 245.30it/s]Tokenizing texts:  28%|██▊       | 2784/10000 [00:12<00:29, 240.97it/s]Tokenizing texts:  28%|██▊       | 2809/10000 [00:12<00:37, 191.66it/s]Tokenizing texts:  28%|██▊       | 2841/10000 [00:12<00:32, 220.04it/s]Tokenizing texts:  29%|██▊       | 2866/10000 [00:13<00:34, 203.96it/s]Tokenizing texts:  29%|██▉       | 2894/10000 [00:13<00:32, 220.85it/s]Tokenizing texts:  29%|██▉       | 2924/10000 [00:13<00:29, 238.58it/s]Tokenizing texts:  30%|██▉       | 2950/10000 [00:13<00:31, 226.67it/s]Tokenizing texts:  30%|██▉       | 2974/10000 [00:13<00:30, 228.52it/s]Tokenizing texts:  30%|███       | 3013/10000 [00:13<00:25, 268.87it/s]Tokenizing texts:  30%|███       | 3041/10000 [00:13<00:29, 232.25it/s]Tokenizing texts:  31%|███       | 3075/10000 [00:13<00:26, 258.71it/s]Tokenizing texts:  31%|███       | 3103/10000 [00:13<00:27, 246.92it/s]Tokenizing texts:  31%|███▏      | 3137/10000 [00:14<00:25, 269.66it/s]Tokenizing texts:  32%|███▏      | 3166/10000 [00:14<00:26, 253.60it/s]Tokenizing texts:  32%|███▏      | 3197/10000 [00:14<00:25, 263.48it/s]Tokenizing texts:  32%|███▏      | 3226/10000 [00:14<00:25, 267.84it/s]Tokenizing texts:  33%|███▎      | 3254/10000 [00:14<00:25, 260.84it/s]Tokenizing texts:  33%|███▎      | 3283/10000 [00:14<00:25, 266.94it/s]Tokenizing texts:  33%|███▎      | 3310/10000 [00:14<00:25, 266.99it/s]Tokenizing texts:  33%|███▎      | 3337/10000 [00:14<00:26, 250.33it/s]Tokenizing texts:  34%|███▎      | 3367/10000 [00:14<00:25, 263.36it/s]Tokenizing texts:  34%|███▍      | 3394/10000 [00:15<00:26, 251.28it/s]Tokenizing texts:  34%|███▍      | 3423/10000 [00:15<00:25, 261.81it/s]Tokenizing texts:  35%|███▍      | 3464/10000 [00:15<00:21, 303.22it/s]Tokenizing texts:  35%|███▍      | 3495/10000 [00:15<00:24, 268.15it/s]Tokenizing texts:  35%|███▌      | 3523/10000 [00:15<00:25, 254.37it/s]Tokenizing texts:  36%|███▌      | 3550/10000 [00:15<00:27, 237.33it/s]Tokenizing texts:  36%|███▌      | 3579/10000 [00:15<00:25, 249.18it/s]Tokenizing texts:  36%|███▌      | 3606/10000 [00:15<00:26, 242.55it/s]Tokenizing texts:  36%|███▋      | 3637/10000 [00:15<00:24, 259.42it/s]Tokenizing texts:  37%|███▋      | 3664/10000 [00:16<00:24, 255.84it/s]Tokenizing texts:  37%|███▋      | 3690/10000 [00:16<00:25, 251.63it/s]Tokenizing texts:  37%|███▋      | 3717/10000 [00:16<00:24, 255.09it/s]Tokenizing texts:  37%|███▋      | 3743/10000 [00:16<00:24, 253.94it/s]Tokenizing texts:  38%|███▊      | 3769/10000 [00:16<00:26, 233.76it/s]Tokenizing texts:  38%|███▊      | 3793/10000 [00:16<00:26, 230.15it/s]Tokenizing texts:  38%|███▊      | 3817/10000 [00:16<00:28, 214.73it/s]Tokenizing texts:  38%|███▊      | 3845/10000 [00:16<00:26, 231.43it/s]Tokenizing texts:  39%|███▊      | 3872/10000 [00:16<00:25, 239.16it/s]Tokenizing texts:  39%|███▉      | 3897/10000 [00:17<00:26, 231.62it/s]Tokenizing texts:  39%|███▉      | 3924/10000 [00:17<00:25, 240.68it/s]Tokenizing texts:  39%|███▉      | 3949/10000 [00:17<00:25, 237.79it/s]Tokenizing texts:  40%|███▉      | 3981/10000 [00:17<00:23, 259.23it/s]Tokenizing texts:  40%|████      | 4008/10000 [00:17<00:26, 222.67it/s]Tokenizing texts:  40%|████      | 4037/10000 [00:17<00:25, 238.52it/s]Tokenizing texts:  41%|████      | 4067/10000 [00:17<00:23, 254.79it/s]Tokenizing texts:  41%|████      | 4100/10000 [00:17<00:21, 275.30it/s]Tokenizing texts:  41%|████▏     | 4133/10000 [00:17<00:20, 290.58it/s]Tokenizing texts:  42%|████▏     | 4163/10000 [00:18<00:21, 275.58it/s]Tokenizing texts:  42%|████▏     | 4195/10000 [00:18<00:20, 287.59it/s]Tokenizing texts:  42%|████▏     | 4225/10000 [00:18<00:21, 262.54it/s]Tokenizing texts:  43%|████▎     | 4252/10000 [00:18<00:23, 248.20it/s]Tokenizing texts:  43%|████▎     | 4278/10000 [00:18<00:24, 238.08it/s]Tokenizing texts:  43%|████▎     | 4310/10000 [00:18<00:21, 259.59it/s]Tokenizing texts:  43%|████▎     | 4345/10000 [00:18<00:20, 282.48it/s]Tokenizing texts:  44%|████▎     | 4374/10000 [00:18<00:21, 265.35it/s]Tokenizing texts:  44%|████▍     | 4412/10000 [00:19<00:18, 294.23it/s]Tokenizing texts:  44%|████▍     | 4443/10000 [00:19<00:19, 284.67it/s]Tokenizing texts:  45%|████▍     | 4472/10000 [00:19<00:21, 259.63it/s]Tokenizing texts:  45%|████▍     | 4499/10000 [00:19<00:22, 243.37it/s]Tokenizing texts:  45%|████▌     | 4527/10000 [00:19<00:21, 249.56it/s]Tokenizing texts:  46%|████▌     | 4553/10000 [00:19<00:21, 251.16it/s]Tokenizing texts:  46%|████▌     | 4581/10000 [00:19<00:20, 258.55it/s]Tokenizing texts:  46%|████▌     | 4608/10000 [00:19<00:21, 246.14it/s]Tokenizing texts:  46%|████▋     | 4642/10000 [00:19<00:19, 269.40it/s]Tokenizing texts:  47%|████▋     | 4678/10000 [00:20<00:18, 293.64it/s]Tokenizing texts:  47%|████▋     | 4714/10000 [00:20<00:17, 306.67it/s]Tokenizing texts:  47%|████▋     | 4745/10000 [00:20<00:19, 275.84it/s]Tokenizing texts:  48%|████▊     | 4776/10000 [00:20<00:18, 275.92it/s]Tokenizing texts:  48%|████▊     | 4805/10000 [00:20<00:20, 254.97it/s]Tokenizing texts:  48%|████▊     | 4837/10000 [00:20<00:19, 266.46it/s]Tokenizing texts:  49%|████▊     | 4867/10000 [00:20<00:18, 274.80it/s]Tokenizing texts:  49%|████▉     | 4899/10000 [00:20<00:17, 285.50it/s]Tokenizing texts:  49%|████▉     | 4928/10000 [00:20<00:18, 268.18it/s]Tokenizing texts:  50%|████▉     | 4956/10000 [00:21<00:18, 268.81it/s]Tokenizing texts:  50%|████▉     | 4990/10000 [00:21<00:17, 286.60it/s]Tokenizing texts:  50%|█████     | 5022/10000 [00:21<00:16, 295.63it/s]Tokenizing texts:  51%|█████     | 5052/10000 [00:21<00:19, 259.21it/s]Tokenizing texts:  51%|█████     | 5079/10000 [00:21<00:20, 235.63it/s]Tokenizing texts:  51%|█████     | 5109/10000 [00:21<00:19, 249.11it/s]Tokenizing texts:  51%|█████▏    | 5135/10000 [00:21<00:20, 233.11it/s]Tokenizing texts:  52%|█████▏    | 5166/10000 [00:21<00:19, 249.38it/s]Tokenizing texts:  52%|█████▏    | 5193/10000 [00:22<00:18, 254.13it/s]Tokenizing texts:  52%|█████▏    | 5219/10000 [00:22<00:19, 241.99it/s]Tokenizing texts:  53%|█████▎    | 5253/10000 [00:22<00:17, 267.57it/s]Tokenizing texts:  53%|█████▎    | 5281/10000 [00:22<00:18, 259.52it/s]Tokenizing texts:  53%|█████▎    | 5311/10000 [00:22<00:17, 269.76it/s]Tokenizing texts:  53%|█████▎    | 5346/10000 [00:22<00:15, 291.83it/s]Tokenizing texts:  54%|█████▍    | 5379/10000 [00:22<00:15, 302.29it/s]Tokenizing texts:  54%|█████▍    | 5410/10000 [00:22<00:15, 296.76it/s]Tokenizing texts:  54%|█████▍    | 5440/10000 [00:22<00:16, 278.74it/s]Tokenizing texts:  55%|█████▍    | 5470/10000 [00:22<00:16, 281.50it/s]Tokenizing texts:  55%|█████▍    | 5499/10000 [00:23<00:21, 212.61it/s]Tokenizing texts:  55%|█████▌    | 5536/10000 [00:23<00:18, 247.62it/s]Tokenizing texts:  56%|█████▌    | 5564/10000 [00:23<00:17, 248.76it/s]Tokenizing texts:  56%|█████▌    | 5591/10000 [00:23<00:17, 250.25it/s]Tokenizing texts:  56%|█████▋    | 5626/10000 [00:23<00:15, 274.46it/s]Tokenizing texts:  57%|█████▋    | 5665/10000 [00:23<00:14, 304.16it/s]Tokenizing texts:  57%|█████▋    | 5697/10000 [00:23<00:14, 306.42it/s]Tokenizing texts:  57%|█████▋    | 5729/10000 [00:23<00:14, 290.81it/s]Tokenizing texts:  58%|█████▊    | 5759/10000 [00:24<00:16, 260.56it/s]Tokenizing texts:  58%|█████▊    | 5787/10000 [00:24<00:19, 220.15it/s]Tokenizing texts:  58%|█████▊    | 5811/10000 [00:24<00:19, 219.59it/s]Tokenizing texts:  58%|█████▊    | 5835/10000 [00:24<00:19, 216.64it/s]Tokenizing texts:  59%|█████▊    | 5871/10000 [00:24<00:16, 252.90it/s]Tokenizing texts:  59%|█████▉    | 5898/10000 [00:24<00:16, 250.80it/s]Tokenizing texts:  59%|█████▉    | 5924/10000 [00:24<00:17, 236.35it/s]Tokenizing texts:  60%|█████▉    | 5952/10000 [00:24<00:16, 247.27it/s]Tokenizing texts:  60%|█████▉    | 5980/10000 [00:25<00:15, 253.79it/s]Tokenizing texts:  60%|██████    | 6008/10000 [00:25<00:15, 251.07it/s]Tokenizing texts:  60%|██████    | 6034/10000 [00:25<00:15, 250.79it/s]Tokenizing texts:  61%|██████    | 6066/10000 [00:25<00:14, 263.29it/s]Tokenizing texts:  61%|██████    | 6093/10000 [00:25<00:15, 260.15it/s]Tokenizing texts:  61%|██████    | 6120/10000 [00:25<00:14, 261.58it/s]Tokenizing texts:  62%|██████▏   | 6154/10000 [00:25<00:13, 279.73it/s]Tokenizing texts:  62%|██████▏   | 6183/10000 [00:25<00:13, 278.80it/s]Tokenizing texts:  62%|██████▏   | 6211/10000 [00:25<00:15, 251.96it/s]Tokenizing texts:  62%|██████▏   | 6239/10000 [00:26<00:14, 257.21it/s]Tokenizing texts:  63%|██████▎   | 6266/10000 [00:26<00:14, 252.44it/s]Tokenizing texts:  63%|██████▎   | 6292/10000 [00:26<00:15, 240.56it/s]Tokenizing texts:  63%|██████▎   | 6318/10000 [00:26<00:15, 244.62it/s]Tokenizing texts:  63%|██████▎   | 6343/10000 [00:26<00:15, 230.53it/s]Tokenizing texts:  64%|██████▍   | 6376/10000 [00:26<00:14, 254.93it/s]Tokenizing texts:  64%|██████▍   | 6402/10000 [00:26<00:14, 248.82it/s]Tokenizing texts:  64%|██████▍   | 6428/10000 [00:26<00:14, 248.28it/s]Tokenizing texts:  65%|██████▍   | 6455/10000 [00:26<00:14, 252.92it/s]Tokenizing texts:  65%|██████▍   | 6481/10000 [00:27<00:14, 247.82it/s]Tokenizing texts:  65%|██████▌   | 6506/10000 [00:27<00:14, 236.87it/s]Tokenizing texts:  65%|██████▌   | 6531/10000 [00:27<00:14, 239.98it/s]Tokenizing texts:  66%|██████▌   | 6572/10000 [00:27<00:11, 287.66it/s]Tokenizing texts:  66%|██████▌   | 6602/10000 [00:27<00:12, 274.09it/s]Tokenizing texts:  66%|██████▋   | 6630/10000 [00:27<00:12, 266.10it/s]Tokenizing texts:  67%|██████▋   | 6658/10000 [00:27<00:12, 268.77it/s]Tokenizing texts:  67%|██████▋   | 6686/10000 [00:27<00:12, 266.22it/s]Tokenizing texts:  67%|██████▋   | 6713/10000 [00:27<00:12, 254.79it/s]Tokenizing texts:  67%|██████▋   | 6739/10000 [00:28<00:13, 246.57it/s]Tokenizing texts:  68%|██████▊   | 6770/10000 [00:28<00:12, 263.53it/s]Tokenizing texts:  68%|██████▊   | 6804/10000 [00:28<00:11, 284.53it/s]Tokenizing texts:  68%|██████▊   | 6837/10000 [00:28<00:10, 292.98it/s]Tokenizing texts:  69%|██████▊   | 6869/10000 [00:28<00:10, 289.41it/s]Tokenizing texts:  69%|██████▉   | 6899/10000 [00:28<00:13, 237.04it/s]Tokenizing texts:  69%|██████▉   | 6925/10000 [00:28<00:13, 232.75it/s]Tokenizing texts:  70%|██████▉   | 6950/10000 [00:28<00:13, 233.97it/s]Tokenizing texts:  70%|██████▉   | 6978/10000 [00:28<00:12, 243.72it/s]Tokenizing texts:  70%|███████   | 7009/10000 [00:29<00:11, 259.18it/s]Tokenizing texts:  70%|███████   | 7036/10000 [00:29<00:11, 249.90it/s]Tokenizing texts:  71%|███████   | 7062/10000 [00:29<00:13, 211.60it/s]Tokenizing texts:  71%|███████   | 7086/10000 [00:29<00:13, 217.23it/s]Tokenizing texts:  71%|███████   | 7112/10000 [00:29<00:12, 226.84it/s]Tokenizing texts:  71%|███████▏  | 7142/10000 [00:29<00:11, 245.77it/s]Tokenizing texts:  72%|███████▏  | 7168/10000 [00:29<00:11, 236.14it/s]Tokenizing texts:  72%|███████▏  | 7195/10000 [00:29<00:11, 245.05it/s]Tokenizing texts:  72%|███████▏  | 7224/10000 [00:29<00:10, 255.59it/s]Tokenizing texts:  72%|███████▎  | 7250/10000 [00:30<00:11, 243.76it/s]Tokenizing texts:  73%|███████▎  | 7295/10000 [00:30<00:09, 299.25it/s]Tokenizing texts:  73%|███████▎  | 7328/10000 [00:30<00:08, 307.11it/s]Tokenizing texts:  74%|███████▎  | 7360/10000 [00:30<00:09, 274.92it/s]Tokenizing texts:  74%|███████▍  | 7389/10000 [00:30<00:09, 263.87it/s]Tokenizing texts:  74%|███████▍  | 7417/10000 [00:30<00:11, 222.75it/s]Tokenizing texts:  74%|███████▍  | 7442/10000 [00:30<00:12, 206.89it/s]Tokenizing texts:  75%|███████▍  | 7474/10000 [00:30<00:10, 233.33it/s]Tokenizing texts:  75%|███████▌  | 7509/10000 [00:31<00:09, 262.62it/s]Tokenizing texts:  75%|███████▌  | 7538/10000 [00:31<00:09, 269.46it/s]Tokenizing texts:  76%|███████▌  | 7567/10000 [00:31<00:09, 259.33it/s]Tokenizing texts:  76%|███████▌  | 7605/10000 [00:31<00:08, 291.81it/s]Tokenizing texts:  76%|███████▋  | 7636/10000 [00:31<00:08, 276.73it/s]Tokenizing texts:  77%|███████▋  | 7665/10000 [00:31<00:08, 279.08it/s]Tokenizing texts:  77%|███████▋  | 7699/10000 [00:31<00:07, 295.38it/s]Tokenizing texts:  77%|███████▋  | 7730/10000 [00:31<00:08, 267.09it/s]Tokenizing texts:  78%|███████▊  | 7758/10000 [00:32<00:09, 239.04it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 257.42it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 238.08it/s]Tokenizing texts:  78%|███████▊  | 7846/10000 [00:32<00:08, 248.39it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 257.71it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 255.49it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:32<00:07, 284.02it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:32<00:07, 282.26it/s]Tokenizing texts:  80%|████████  | 8000/10000 [00:32<00:06, 290.09it/s]Tokenizing texts:  80%|████████  | 8030/10000 [00:33<00:06, 291.81it/s]Tokenizing texts:  81%|████████  | 8060/10000 [00:33<00:11, 172.39it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 198.61it/s]Tokenizing texts:  81%|████████  | 8119/10000 [00:33<00:09, 205.72it/s]Tokenizing texts:  81%|████████▏ | 8148/10000 [00:33<00:08, 223.09it/s]Tokenizing texts:  82%|████████▏ | 8176/10000 [00:33<00:07, 236.45it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:33<00:07, 246.65it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 212.10it/s]Tokenizing texts:  83%|████████▎ | 8264/10000 [00:34<00:07, 220.84it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 256.38it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 252.39it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:34<00:06, 256.27it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:34<00:06, 255.63it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:34<00:06, 231.34it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:34<00:06, 257.12it/s]Tokenizing texts:  85%|████████▍ | 8473/10000 [00:34<00:05, 263.58it/s]Tokenizing texts:  85%|████████▌ | 8507/10000 [00:35<00:05, 283.75it/s]Tokenizing texts:  85%|████████▌ | 8536/10000 [00:35<00:05, 262.27it/s]Tokenizing texts:  86%|████████▌ | 8564/10000 [00:35<00:05, 253.18it/s]Tokenizing texts:  86%|████████▌ | 8595/10000 [00:35<00:05, 268.18it/s]Tokenizing texts:  86%|████████▋ | 8626/10000 [00:35<00:04, 279.03it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:35<00:04, 280.03it/s]Tokenizing texts:  87%|████████▋ | 8686/10000 [00:35<00:04, 287.51it/s]Tokenizing texts:  87%|████████▋ | 8721/10000 [00:35<00:04, 305.03it/s]Tokenizing texts:  88%|████████▊ | 8752/10000 [00:35<00:04, 274.36it/s]Tokenizing texts:  88%|████████▊ | 8784/10000 [00:36<00:04, 285.43it/s]Tokenizing texts:  88%|████████▊ | 8814/10000 [00:36<00:05, 214.49it/s]Tokenizing texts:  88%|████████▊ | 8839/10000 [00:36<00:05, 215.93it/s]Tokenizing texts:  89%|████████▊ | 8871/10000 [00:36<00:04, 239.93it/s]Tokenizing texts:  89%|████████▉ | 8910/10000 [00:36<00:03, 276.58it/s]Tokenizing texts:  89%|████████▉ | 8940/10000 [00:36<00:03, 282.18it/s]Tokenizing texts:  90%|████████▉ | 8970/10000 [00:36<00:03, 268.68it/s]Tokenizing texts:  90%|████████▉ | 8998/10000 [00:36<00:04, 237.07it/s]Tokenizing texts:  90%|█████████ | 9027/10000 [00:37<00:03, 247.54it/s]Tokenizing texts:  91%|█████████ | 9053/10000 [00:37<00:03, 238.34it/s]Tokenizing texts:  91%|█████████ | 9080/10000 [00:37<00:03, 244.31it/s]Tokenizing texts:  91%|█████████ | 9109/10000 [00:37<00:03, 251.90it/s]Tokenizing texts:  91%|█████████▏| 9140/10000 [00:37<00:03, 266.97it/s]Tokenizing texts:  92%|█████████▏| 9168/10000 [00:37<00:03, 270.46it/s]Tokenizing texts:  92%|█████████▏| 9199/10000 [00:37<00:02, 281.75it/s]Tokenizing texts:  92%|█████████▏| 9228/10000 [00:37<00:02, 263.94it/s]Tokenizing texts:  93%|█████████▎| 9256/10000 [00:37<00:02, 266.57it/s]Tokenizing texts:  93%|█████████▎| 9283/10000 [00:38<00:02, 244.69it/s]Tokenizing texts:  93%|█████████▎| 9313/10000 [00:38<00:02, 259.35it/s]Tokenizing texts:  93%|█████████▎| 9340/10000 [00:38<00:02, 237.16it/s]Tokenizing texts:  94%|█████████▎| 9365/10000 [00:38<00:02, 212.49it/s]Tokenizing texts:  94%|█████████▍| 9401/10000 [00:38<00:02, 248.75it/s]Tokenizing texts:  94%|█████████▍| 9428/10000 [00:38<00:02, 226.01it/s]Tokenizing texts:  95%|█████████▍| 9453/10000 [00:38<00:02, 231.95it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:38<00:02, 239.05it/s]Tokenizing texts:  95%|█████████▌| 9504/10000 [00:39<00:02, 181.52it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 220.73it/s]Tokenizing texts:  96%|█████████▌| 9565/10000 [00:39<00:01, 227.00it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 233.03it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 231.04it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 233.56it/s]Tokenizing texts:  97%|█████████▋| 9673/10000 [00:39<00:01, 237.87it/s]Tokenizing texts:  97%|█████████▋| 9698/10000 [00:39<00:01, 235.49it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:40<00:01, 239.11it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 207.34it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 215.28it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 260.81it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 253.61it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 258.89it/s]Tokenizing texts:  99%|█████████▉| 9907/10000 [00:40<00:00, 285.90it/s]Tokenizing texts:  99%|█████████▉| 9937/10000 [00:40<00:00, 283.30it/s]Tokenizing texts: 100%|█████████▉| 9968/10000 [00:40<00:00, 290.67it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 243.82it/s]
2025-12-09 12:10:11.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1.0000000000000002e-06 Training loss: 12.022470474243164
2025-12-09 12:10:11.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2.0000000000000003e-06 Training loss: 12.024420738220215
2025-12-09 12:10:11.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-06 Training loss: 12.017550468444824
2025-12-09 12:10:11.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4.000000000000001e-06 Training loss: 12.058804512023926
2025-12-09 12:10:11.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-06 Training loss: 12.057635307312012
2025-12-09 12:10:11.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-06 Training loss: 12.053874015808105
2025-12-09 12:10:11.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-06 Training loss: 12.048155784606934
2025-12-09 12:10:11.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8.000000000000001e-06 Training loss: 12.033495903015137
2025-12-09 12:10:12.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 9e-06 Training loss: 12.020997047424316
2025-12-09 12:10:12.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 1e-05 Training loss: 12.054067611694336
2025-12-09 12:10:12.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 1.1000000000000001e-05 Training loss: 12.018692970275879
2025-12-09 12:10:12.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 1.2e-05 Training loss: 12.044845581054688
2025-12-09 12:10:12.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 1.3000000000000001e-05 Training loss: 12.035110473632812
2025-12-09 12:10:12.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 1.4000000000000001e-05 Training loss: 12.03701400756836
2025-12-09 12:10:12.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 1.5e-05 Training loss: 12.011531829833984
2025-12-09 12:10:12.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 1.6000000000000003e-05 Training loss: 12.032957077026367
2025-12-09 12:10:12.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 1.7000000000000003e-05 Training loss: 12.04356861114502
2025-12-09 12:10:12.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 1.8e-05 Training loss: 12.025691986083984
2025-12-09 12:10:12.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 1.9e-05 Training loss: 12.00849437713623
2025-12-09 12:10:12.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 2e-05 Training loss: 12.067388534545898
2025-12-09 12:10:12.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 2.1e-05 Training loss: 12.028495788574219
2025-12-09 12:10:13.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 2.2000000000000003e-05 Training loss: 11.984912872314453
2025-12-09 12:10:13.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 2.3000000000000003e-05 Training loss: 12.025358200073242
2025-12-09 12:10:13.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 2.4e-05 Training loss: 12.04796028137207
2025-12-09 12:10:13.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 2.5e-05 Training loss: 12.053857803344727
2025-12-09 12:10:13.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 2.6000000000000002e-05 Training loss: 12.000388145446777
2025-12-09 12:10:13.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 2.7000000000000002e-05 Training loss: 12.06914234161377
2025-12-09 12:10:13.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 2.8000000000000003e-05 Training loss: 12.055463790893555
2025-12-09 12:10:13.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 2.9e-05 Training loss: 12.05499267578125
2025-12-09 12:10:13.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 3e-05 Training loss: 12.059199333190918
2025-12-09 12:10:13.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 3.1e-05 Training loss: 12.013541221618652
2025-12-09 12:10:13.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 3.2000000000000005e-05 Training loss: 12.038371086120605
2025-12-09 12:10:13.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 3.3e-05 Training loss: 12.036580085754395
2025-12-09 12:10:13.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 3.4000000000000007e-05 Training loss: 12.032655715942383
2025-12-09 12:10:14.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 3.5e-05 Training loss: 11.99233341217041
2025-12-09 12:10:14.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 3.6e-05 Training loss: 12.044118881225586
2025-12-09 12:10:14.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 3.7e-05 Training loss: 12.049951553344727
2025-12-09 12:10:14.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 3.8e-05 Training loss: 12.03101634979248
2025-12-09 12:10:14.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 3.9000000000000006e-05 Training loss: 12.0649995803833
2025-12-09 12:10:14.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 4e-05 Training loss: 12.047680854797363
2025-12-09 12:10:14.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 4.1e-05 Training loss: 12.019879341125488
2025-12-09 12:10:14.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 4.2e-05 Training loss: 12.009567260742188
2025-12-09 12:10:14.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 4.3e-05 Training loss: 12.005314826965332
2025-12-09 12:10:14.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 4.4000000000000006e-05 Training loss: 12.026283264160156
2025-12-09 12:10:14.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 4.5e-05 Training loss: 12.054891586303711
2025-12-09 12:10:14.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 4.600000000000001e-05 Training loss: 12.057329177856445
2025-12-09 12:10:14.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 4.7e-05 Training loss: 12.029622077941895
2025-12-09 12:10:15.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 4.8e-05 Training loss: 12.037625312805176
2025-12-09 12:10:15.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 4.9e-05 Training loss: 12.031719207763672
2025-12-09 12:10:15.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 5e-05 Training loss: 12.028987884521484
2025-12-09 12:10:15.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 5.1000000000000006e-05 Training loss: 12.059114456176758
2025-12-09 12:10:15.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 5.2000000000000004e-05 Training loss: 12.007692337036133
2025-12-09 12:10:15.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 5.300000000000001e-05 Training loss: 11.997940063476562
2025-12-09 12:10:15.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 5.4000000000000005e-05 Training loss: 12.023649215698242
2025-12-09 12:10:15.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 5.500000000000001e-05 Training loss: 12.004486083984375
2025-12-09 12:10:15.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 5.6000000000000006e-05 Training loss: 12.028648376464844
2025-12-09 12:10:15.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 5.6999999999999996e-05 Training loss: 12.015265464782715
2025-12-09 12:10:15.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 5.8e-05 Training loss: 12.030165672302246
2025-12-09 12:10:15.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 5.9e-05 Training loss: 12.018102645874023
2025-12-09 12:10:15.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 6e-05 Training loss: 12.011122703552246
2025-12-09 12:10:16.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 6.1e-05 Training loss: 12.002437591552734
2025-12-09 12:10:16.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 6.2e-05 Training loss: 12.02529525756836
2025-12-09 12:10:16.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 6.3e-05 Training loss: 11.991121292114258
2025-12-09 12:10:16.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 6.400000000000001e-05 Training loss: 12.019968032836914
2025-12-09 12:10:16.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 6.500000000000001e-05 Training loss: 12.004965782165527
2025-12-09 12:10:16.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 6.6e-05 Training loss: 12.00721549987793
2025-12-09 12:10:16.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 6.7e-05 Training loss: 11.97265338897705
2025-12-09 12:10:16.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 6.800000000000001e-05 Training loss: 12.014328002929688
2025-12-09 12:10:16.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 6.9e-05 Training loss: 12.0042724609375
2025-12-09 12:10:16.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 7e-05 Training loss: 11.981664657592773
2025-12-09 12:10:16.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 7.1e-05 Training loss: 12.002824783325195
2025-12-09 12:10:16.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 7.2e-05 Training loss: 11.997543334960938
2025-12-09 12:10:16.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 7.3e-05 Training loss: 12.02537727355957
2025-12-09 12:10:17.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 7.4e-05 Training loss: 11.982466697692871
2025-12-09 12:10:17.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 7.500000000000001e-05 Training loss: 11.978203773498535
2025-12-09 12:10:17.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 7.6e-05 Training loss: 11.98340129852295
2025-12-09 12:10:17.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 7.7e-05 Training loss: 12.010320663452148
2025-12-09 12:10:17.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 7.800000000000001e-05 Training loss: 11.984570503234863
2025-12-09 12:10:17.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 7.900000000000001e-05 Training loss: 11.951943397521973
2025-12-09 12:10:17.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 8e-05 Training loss: 12.023876190185547
2025-12-09 12:10:17.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 8.1e-05 Training loss: 11.956451416015625
2025-12-09 12:10:17.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 8.2e-05 Training loss: 12.006675720214844
2025-12-09 12:10:17.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 8.3e-05 Training loss: 11.993728637695312
2025-12-09 12:10:17.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 8.4e-05 Training loss: 11.977484703063965
2025-12-09 12:10:17.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 8.5e-05 Training loss: 11.960177421569824
2025-12-09 12:10:17.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 8.6e-05 Training loss: 11.994820594787598
2025-12-09 12:10:17.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 8.7e-05 Training loss: 11.989387512207031
2025-12-09 12:10:18.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 8.800000000000001e-05 Training loss: 11.954575538635254
2025-12-09 12:10:18.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 8.900000000000001e-05 Training loss: 11.95726490020752
2025-12-09 12:10:18.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 9e-05 Training loss: 11.953269958496094
2025-12-09 12:10:18.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 9.1e-05 Training loss: 11.956336975097656
2025-12-09 12:10:18.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 9.200000000000001e-05 Training loss: 11.956291198730469
2025-12-09 12:10:18.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 9.300000000000001e-05 Training loss: 11.99058723449707
2025-12-09 12:10:18.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 9.4e-05 Training loss: 11.948129653930664
2025-12-09 12:10:18.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 9.5e-05 Training loss: 11.987183570861816
2025-12-09 12:10:18.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 9.6e-05 Training loss: 11.969348907470703
2025-12-09 12:10:18.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 9.7e-05 Training loss: 11.966141700744629
2025-12-09 12:10:18.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 9.8e-05 Training loss: 11.969522476196289
2025-12-09 12:10:18.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 9.900000000000001e-05 Training loss: 11.935869216918945
2025-12-09 12:10:18.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0001 Training loss: 11.950881958007812
2025-12-09 12:10:19.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 9.999999072578703e-05 Training loss: 11.916707992553711
2025-12-09 12:10:19.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 9.999996290315153e-05 Training loss: 11.949678421020508
2025-12-09 12:10:19.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 9.999991653210385e-05 Training loss: 11.917821884155273
2025-12-09 12:10:19.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 9.999985161266117e-05 Training loss: 11.923672676086426
2025-12-09 12:10:19.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 9.999976814484758e-05 Training loss: 11.868951797485352
2025-12-09 12:10:19.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 9.999966612869405e-05 Training loss: 11.942273139953613
2025-12-09 12:10:19.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 9.999954556423843e-05 Training loss: 11.951508522033691
2025-12-09 12:10:19.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.999940645152541e-05 Training loss: 11.92813491821289
2025-12-09 12:10:19.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 9.999924879060665e-05 Training loss: 11.938146591186523
2025-12-09 12:10:19.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 9.999907258154059e-05 Training loss: 11.912071228027344
2025-12-09 12:10:19.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 9.999887782439263e-05 Training loss: 11.937402725219727
2025-12-09 12:10:19.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 9.999866451923501e-05 Training loss: 11.917580604553223
2025-12-09 12:10:19.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 9.999843266614685e-05 Training loss: 11.873620986938477
2025-12-09 12:10:20.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 9.999818226521415e-05 Training loss: 11.897353172302246
2025-12-09 12:10:20.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 9.999791331652984e-05 Training loss: 11.931138038635254
2025-12-09 12:10:20.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 9.999762582019365e-05 Training loss: 11.845487594604492
2025-12-09 12:10:20.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 9.999731977631227e-05 Training loss: 11.897744178771973
2025-12-09 12:10:20.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 9.999699518499921e-05 Training loss: 11.9109468460083
2025-12-09 12:10:20.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 9.999665204637487e-05 Training loss: 11.89481258392334
2025-12-09 12:10:20.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 9.999629036056657e-05 Training loss: 11.916727066040039
2025-12-09 12:10:20.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 9.999591012770848e-05 Training loss: 11.891340255737305
2025-12-09 12:10:20.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 9.999551134794164e-05 Training loss: 11.869600296020508
2025-12-09 12:10:20.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 9.999509402141401e-05 Training loss: 11.836764335632324
2025-12-09 12:10:20.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 9.999465814828036e-05 Training loss: 11.849803924560547
2025-12-09 12:10:20.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 9.999420372870242e-05 Training loss: 11.909147262573242
2025-12-09 12:10:21.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 9.999373076284877e-05 Training loss: 11.816278457641602
2025-12-09 12:10:21.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 9.999323925089486e-05 Training loss: 11.858147621154785
2025-12-09 12:10:21.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 9.999272919302301e-05 Training loss: 11.861292839050293
2025-12-09 12:10:21.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 9.999220058942245e-05 Training loss: 11.8413724899292
2025-12-09 12:10:21.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 9.999165344028926e-05 Training loss: 11.814852714538574
2025-12-09 12:10:21.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 9.999108774582645e-05 Training loss: 11.911050796508789
2025-12-09 12:10:21.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 9.999050350624382e-05 Training loss: 11.883712768554688
2025-12-09 12:10:21.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 9.998990072175813e-05 Training loss: 11.875548362731934
2025-12-09 12:10:21.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 9.998927939259303e-05 Training loss: 11.917344093322754
2025-12-09 12:10:21.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 9.998863951897897e-05 Training loss: 11.82629680633545
2025-12-09 12:10:21.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 9.998798110115333e-05 Training loss: 11.798593521118164
2025-12-09 12:10:21.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 9.998730413936037e-05 Training loss: 11.862112998962402
2025-12-09 12:10:21.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 9.998660863385123e-05 Training loss: 11.767922401428223
2025-12-09 12:10:22.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 9.99858945848839e-05 Training loss: 11.795578956604004
2025-12-09 12:10:22.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 9.998516199272327e-05 Training loss: 11.809335708618164
2025-12-09 12:10:22.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 9.998441085764113e-05 Training loss: 11.809372901916504
2025-12-09 12:10:22.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 9.998364117991612e-05 Training loss: 11.809435844421387
2025-12-09 12:10:22.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 9.998285295983376e-05 Training loss: 11.760931015014648
2025-12-09 12:10:22.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 9.998204619768646e-05 Training loss: 11.769194602966309
2025-12-09 12:10:22.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 9.998122089377349e-05 Training loss: 11.788714408874512
2025-12-09 12:10:22.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 9.998037704840102e-05 Training loss: 11.807096481323242
2025-12-09 12:10:22.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 9.99795146618821e-05 Training loss: 11.8131685256958
2025-12-09 12:10:22.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 9.997863373453663e-05 Training loss: 11.819453239440918
2025-12-09 12:10:22.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 9.997773426669142e-05 Training loss: 11.72299575805664
2025-12-09 12:10:22.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 9.997681625868013e-05 Training loss: 11.865621566772461
2025-12-09 12:10:22.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 9.997587971084335e-05 Training loss: 11.630027770996094
2025-12-09 12:10:23.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 9.997492462352846e-05 Training loss: 11.762415885925293
2025-12-09 12:10:23.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 9.997395099708982e-05 Training loss: 11.833487510681152
2025-12-09 12:10:23.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 9.997295883188856e-05 Training loss: 11.775360107421875
2025-12-09 12:10:23.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 9.997194812829276e-05 Training loss: 11.645721435546875
2025-12-09 12:10:23.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 9.997091888667738e-05 Training loss: 11.809316635131836
2025-12-09 12:10:23.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 9.996987110742422e-05 Training loss: 11.683401107788086
2025-12-09 12:10:23.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 9.996880479092198e-05 Training loss: 11.738852500915527
2025-12-09 12:10:23.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 9.996771993756621e-05 Training loss: 11.685229301452637
2025-12-09 12:10:23.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 9.996661654775938e-05 Training loss: 11.78377628326416
2025-12-09 12:10:23.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 9.996549462191082e-05 Training loss: 11.644624710083008
2025-12-09 12:10:23.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 9.99643541604367e-05 Training loss: 11.806143760681152
2025-12-09 12:10:23.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 9.99631951637601e-05 Training loss: 11.667323112487793
2025-12-09 12:10:23.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 9.996201763231099e-05 Training loss: 11.750166893005371
2025-12-09 12:10:24.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 9.996082156652618e-05 Training loss: 11.704590797424316
2025-12-09 12:10:24.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 9.995960696684939e-05 Training loss: 11.638922691345215
2025-12-09 12:10:24.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 9.995837383373119e-05 Training loss: 11.656618118286133
2025-12-09 12:10:24.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 9.995712216762902e-05 Training loss: 11.650559425354004
2025-12-09 12:10:24.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 9.995585196900723e-05 Training loss: 11.727387428283691
2025-12-09 12:10:24.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 9.9954563238337e-05 Training loss: 11.66995620727539
2025-12-09 12:10:24.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 9.995325597609645e-05 Training loss: 11.653281211853027
2025-12-09 12:10:24.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 9.99519301827705e-05 Training loss: 11.669978141784668
2025-12-09 12:10:24.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 9.995058585885095e-05 Training loss: 11.735312461853027
2025-12-09 12:10:24.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 9.994922300483656e-05 Training loss: 11.662168502807617
2025-12-09 12:10:24.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 9.99478416212329e-05 Training loss: 11.683852195739746
2025-12-09 12:10:24.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 9.994644170855237e-05 Training loss: 11.573963165283203
2025-12-09 12:10:24.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 9.994502326731434e-05 Training loss: 11.580606460571289
2025-12-09 12:10:25.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 9.994358629804499e-05 Training loss: 11.54646110534668
2025-12-09 12:10:25.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 9.994213080127739e-05 Training loss: 11.555031776428223
2025-12-09 12:10:25.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 9.994065677755147e-05 Training loss: 11.613447189331055
2025-12-09 12:10:25.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 9.993916422741409e-05 Training loss: 11.556954383850098
2025-12-09 12:10:25.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 9.99376531514189e-05 Training loss: 11.637733459472656
2025-12-09 12:10:25.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 9.993612355012647e-05 Training loss: 11.526470184326172
2025-12-09 12:10:25.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 9.993457542410424e-05 Training loss: 11.518278121948242
2025-12-09 12:10:25.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 9.993300877392651e-05 Training loss: 11.643256187438965
2025-12-09 12:10:25.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 9.993142360017446e-05 Training loss: 11.698448181152344
2025-12-09 12:10:25.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 9.992981990343614e-05 Training loss: 11.610673904418945
2025-12-09 12:10:25.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 9.992819768430647e-05 Training loss: 11.63674259185791
2025-12-09 12:10:25.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 9.992655694338725e-05 Training loss: 11.565428733825684
2025-12-09 12:10:25.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 9.992489768128713e-05 Training loss: 11.612225532531738
2025-12-09 12:10:26.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 9.992321989862166e-05 Training loss: 11.53821086883545
2025-12-09 12:10:26.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 9.992152359601322e-05 Training loss: 11.550450325012207
2025-12-09 12:10:26.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 9.99198087740911e-05 Training loss: 11.58596134185791
2025-12-09 12:10:26.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 9.991807543349146e-05 Training loss: 11.541131973266602
2025-12-09 12:10:26.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 9.99163235748573e-05 Training loss: 11.513006210327148
2025-12-09 12:10:26.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 9.99145531988385e-05 Training loss: 11.383098602294922
2025-12-09 12:10:26.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 9.99127643060918e-05 Training loss: 11.37060832977295
2025-12-09 12:10:26.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 9.991095689728087e-05 Training loss: 11.34981632232666
2025-12-09 12:10:26.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 9.990913097307614e-05 Training loss: 11.513809204101562
2025-12-09 12:10:26.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 9.990728653415504e-05 Training loss: 11.553871154785156
2025-12-09 12:10:26.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 9.990542358120174e-05 Training loss: 11.520008087158203
2025-12-09 12:10:26.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 9.990354211490735e-05 Training loss: 11.503327369689941
2025-12-09 12:10:26.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 9.990164213596986e-05 Training loss: 11.470121383666992
2025-12-09 12:10:27.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 9.989972364509408e-05 Training loss: 11.372611045837402
2025-12-09 12:10:27.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 9.989778664299172e-05 Training loss: 11.560148239135742
2025-12-09 12:10:27.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 9.989583113038135e-05 Training loss: 11.484755516052246
2025-12-09 12:10:27.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 9.989385710798837e-05 Training loss: 11.529316902160645
2025-12-09 12:10:27.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 9.989186457654513e-05 Training loss: 11.375419616699219
2025-12-09 12:10:27.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 9.988985353679077e-05 Training loss: 11.513303756713867
2025-12-09 12:10:27.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 9.988782398947131e-05 Training loss: 11.4281587600708
2025-12-09 12:10:27.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 9.988577593533967e-05 Training loss: 11.331336975097656
2025-12-09 12:10:27.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 9.988370937515561e-05 Training loss: 11.374975204467773
2025-12-09 12:10:27.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 9.988162430968575e-05 Training loss: 11.358022689819336
2025-12-09 12:10:27.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 9.987952073970359e-05 Training loss: 11.421931266784668
2025-12-09 12:10:27.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 9.98773986659895e-05 Training loss: 11.411975860595703
2025-12-09 12:10:27.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 9.987525808933068e-05 Training loss: 11.356632232666016
2025-12-09 12:10:28.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 9.987309901052121e-05 Training loss: 11.405474662780762
2025-12-09 12:10:28.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 9.98709214303621e-05 Training loss: 11.396510124206543
2025-12-09 12:10:28.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 9.986872534966109e-05 Training loss: 11.364960670471191
2025-12-09 12:10:28.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 9.986651076923288e-05 Training loss: 11.525280952453613
2025-12-09 12:10:28.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 9.986427768989903e-05 Training loss: 11.398951530456543
2025-12-09 12:10:28.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 9.986202611248793e-05 Training loss: 11.401939392089844
2025-12-09 12:10:28.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 9.985975603783484e-05 Training loss: 11.478934288024902
2025-12-09 12:10:28.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 9.98574674667819e-05 Training loss: 11.30064582824707
2025-12-09 12:10:28.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 9.985516040017807e-05 Training loss: 11.176070213317871
2025-12-09 12:10:28.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 9.985283483887923e-05 Training loss: 11.374584197998047
2025-12-09 12:10:28.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 9.985049078374806e-05 Training loss: 11.536046028137207
2025-12-09 12:10:28.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 9.984812823565417e-05 Training loss: 11.363007545471191
2025-12-09 12:10:28.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 9.984574719547395e-05 Training loss: 11.402167320251465
2025-12-09 12:10:29.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 9.984334766409071e-05 Training loss: 11.395577430725098
2025-12-09 12:10:29.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 9.98409296423946e-05 Training loss: 11.289190292358398
2025-12-09 12:10:29.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 9.983849313128264e-05 Training loss: 11.300948143005371
2025-12-09 12:10:29.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 9.983603813165869e-05 Training loss: 11.367548942565918
2025-12-09 12:10:29.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 9.983356464443347e-05 Training loss: 11.242788314819336
2025-12-09 12:10:29.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 9.983107267052457e-05 Training loss: 11.332818031311035
2025-12-09 12:10:29.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 9.982856221085644e-05 Training loss: 11.232608795166016
2025-12-09 12:10:29.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 9.982603326636037e-05 Training loss: 11.265122413635254
2025-12-09 12:10:29.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 9.982348583797454e-05 Training loss: 11.147777557373047
2025-12-09 12:10:29.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 9.982091992664392e-05 Training loss: 11.142679214477539
2025-12-09 12:10:29.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 9.981833553332045e-05 Training loss: 11.270607948303223
2025-12-09 12:10:29.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 9.981573265896281e-05 Training loss: 11.440084457397461
2025-12-09 12:10:29.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 9.981311130453659e-05 Training loss: 11.252274513244629
2025-12-09 12:10:30.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 9.981047147101426e-05 Training loss: 11.31112289428711
2025-12-09 12:10:30.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 9.980781315937507e-05 Training loss: 11.29645824432373
2025-12-09 12:10:30.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 9.98051363706052e-05 Training loss: 11.19350814819336
2025-12-09 12:10:30.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 9.980244110569765e-05 Training loss: 11.262394905090332
2025-12-09 12:10:30.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 9.979972736565226e-05 Training loss: 11.09550666809082
2025-12-09 12:10:30.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 9.979699515147578e-05 Training loss: 11.291608810424805
2025-12-09 12:10:30.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 9.979424446418173e-05 Training loss: 11.351851463317871
2025-12-09 12:10:30.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 9.979147530479056e-05 Training loss: 11.198613166809082
2025-12-09 12:10:30.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 9.978868767432954e-05 Training loss: 11.310866355895996
2025-12-09 12:10:30.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 9.978588157383277e-05 Training loss: 11.209403991699219
2025-12-09 12:10:30.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 9.978305700434125e-05 Training loss: 11.162301063537598
2025-12-09 12:10:30.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 9.97802139669028e-05 Training loss: 11.298272132873535
2025-12-09 12:10:30.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 9.97773524625721e-05 Training loss: 11.239218711853027
2025-12-09 12:10:30.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 9.977447249241066e-05 Training loss: 11.12024974822998
2025-12-09 12:10:31.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 9.977157405748687e-05 Training loss: 11.242238998413086
2025-12-09 12:10:31.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 9.976865715887595e-05 Training loss: 11.279422760009766
2025-12-09 12:10:31.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 9.976572179765999e-05 Training loss: 11.119996070861816
2025-12-09 12:10:31.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 9.976276797492793e-05 Training loss: 11.138323783874512
2025-12-09 12:10:31.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 9.975979569177552e-05 Training loss: 11.103217124938965
2025-12-09 12:10:31.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 9.975680494930538e-05 Training loss: 11.212929725646973
2025-12-09 12:10:31.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 9.9753795748627e-05 Training loss: 11.1092529296875
2025-12-09 12:10:31.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 9.975076809085669e-05 Training loss: 11.355557441711426
2025-12-09 12:10:31.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 9.974772197711761e-05 Training loss: 11.144861221313477
2025-12-09 12:10:31.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 9.97446574085398e-05 Training loss: 11.359477043151855
2025-12-09 12:10:31.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 9.974157438626008e-05 Training loss: 11.17542839050293
2025-12-09 12:10:31.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 9.973847291142218e-05 Training loss: 11.12736701965332
2025-12-09 12:10:31.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 9.973535298517663e-05 Training loss: 11.142688751220703
2025-12-09 12:10:32.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 9.973221460868086e-05 Training loss: 11.140735626220703
2025-12-09 12:10:32.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 9.972905778309906e-05 Training loss: 11.089943885803223
2025-12-09 12:10:32.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 9.972588250960234e-05 Training loss: 11.083270072937012
2025-12-09 12:10:32.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 9.972268878936863e-05 Training loss: 11.05602741241455
2025-12-09 12:10:32.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 9.97194766235827e-05 Training loss: 11.120074272155762
2025-12-09 12:10:32.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 9.971624601343615e-05 Training loss: 11.088461875915527
2025-12-09 12:10:32.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 9.971299696012743e-05 Training loss: 11.071653366088867
2025-12-09 12:10:32.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 9.970972946486185e-05 Training loss: 11.272936820983887
2025-12-09 12:10:32.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 9.970644352885157e-05 Training loss: 11.068013191223145
2025-12-09 12:10:32.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 9.970313915331553e-05 Training loss: 11.186118125915527
2025-12-09 12:10:32.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 9.969981633947956e-05 Training loss: 11.127511978149414
2025-12-09 12:10:32.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 9.969647508857631e-05 Training loss: 11.053886413574219
2025-12-09 12:10:32.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 9.969311540184532e-05 Training loss: 11.14184856414795
2025-12-09 12:10:33.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 9.968973728053288e-05 Training loss: 11.02656364440918
2025-12-09 12:10:33.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 9.968634072589218e-05 Training loss: 11.19555950164795
2025-12-09 12:10:33.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 9.968292573918325e-05 Training loss: 11.000077247619629
2025-12-09 12:10:33.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 9.967949232167294e-05 Training loss: 11.187084197998047
2025-12-09 12:10:33.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 9.967604047463493e-05 Training loss: 11.028182029724121
2025-12-09 12:10:33.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 9.967257019934975e-05 Training loss: 11.003657341003418
2025-12-09 12:10:33.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 9.966908149710476e-05 Training loss: 11.09396743774414
2025-12-09 12:10:33.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 9.966557436919416e-05 Training loss: 11.137202262878418
2025-12-09 12:10:33.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 9.966204881691898e-05 Training loss: 11.113310813903809
2025-12-09 12:10:33.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 9.965850484158711e-05 Training loss: 11.060324668884277
2025-12-09 12:10:33.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 9.965494244451324e-05 Training loss: 11.218955039978027
2025-12-09 12:10:33.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 9.96513616270189e-05 Training loss: 11.107355117797852
2025-12-09 12:10:33.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 9.964776239043246e-05 Training loss: 11.036310195922852
2025-12-09 12:10:34.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 9.964414473608912e-05 Training loss: 10.919681549072266
2025-12-09 12:10:34.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 9.964050866533094e-05 Training loss: 11.16622257232666
2025-12-09 12:10:34.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 9.963685417950677e-05 Training loss: 11.062568664550781
2025-12-09 12:10:34.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 9.96331812799723e-05 Training loss: 11.043081283569336
2025-12-09 12:10:34.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 9.962948996809008e-05 Training loss: 11.103498458862305
2025-12-09 12:10:34.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 9.962578024522948e-05 Training loss: 11.008072853088379
2025-12-09 12:10:34.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 9.962205211276665e-05 Training loss: 10.938817977905273
2025-12-09 12:10:34.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 9.961830557208464e-05 Training loss: 11.037747383117676
2025-12-09 12:10:34.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 9.961454062457329e-05 Training loss: 11.133991241455078
2025-12-09 12:10:34.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 9.961075727162928e-05 Training loss: 11.068883895874023
2025-12-09 12:10:34.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 9.960695551465611e-05 Training loss: 11.027877807617188
2025-12-09 12:10:34.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 9.960313535506411e-05 Training loss: 11.040704727172852
2025-12-09 12:10:34.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 9.959929679427047e-05 Training loss: 11.077591896057129
2025-12-09 12:10:35.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 9.959543983369912e-05 Training loss: 11.123663902282715
2025-12-09 12:10:35.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 9.959156447478091e-05 Training loss: 10.962389945983887
2025-12-09 12:10:35.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 9.958767071895347e-05 Training loss: 11.102049827575684
2025-12-09 12:10:35.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 9.958375856766127e-05 Training loss: 10.918949127197266
2025-12-09 12:10:35.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 9.957982802235556e-05 Training loss: 11.038817405700684
2025-12-09 12:10:35.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 9.957587908449448e-05 Training loss: 11.06484317779541
2025-12-09 12:10:35.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 9.957191175554294e-05 Training loss: 10.935270309448242
2025-12-09 12:10:35.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 9.956792603697273e-05 Training loss: 11.035805702209473
2025-12-09 12:10:35.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 9.956392193026239e-05 Training loss: 11.13508415222168
2025-12-09 12:10:35.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 9.955989943689734e-05 Training loss: 11.0183687210083
2025-12-09 12:10:35.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 9.955585855836978e-05 Training loss: 10.826730728149414
2025-12-09 12:10:35.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 9.955179929617875e-05 Training loss: 11.103569030761719
2025-12-09 12:10:35.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 9.954772165183013e-05 Training loss: 10.9910306930542
2025-12-09 12:10:36.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 9.954362562683658e-05 Training loss: 11.020405769348145
2025-12-09 12:10:36.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 9.95395112227176e-05 Training loss: 10.855096817016602
2025-12-09 12:10:36.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 9.95353784409995e-05 Training loss: 11.044817924499512
2025-12-09 12:10:36.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 9.953122728321542e-05 Training loss: 10.938458442687988
2025-12-09 12:10:36.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 9.952705775090529e-05 Training loss: 10.86618709564209
2025-12-09 12:10:36.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 9.952286984561592e-05 Training loss: 10.939502716064453
2025-12-09 12:10:36.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 9.951866356890084e-05 Training loss: 10.878260612487793
2025-12-09 12:10:36.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 9.951443892232047e-05 Training loss: 10.922262191772461
2025-12-09 12:10:36.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 9.951019590744203e-05 Training loss: 10.888561248779297
2025-12-09 12:10:36.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 9.950593452583952e-05 Training loss: 11.01523208618164
2025-12-09 12:10:36.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 9.95016547790938e-05 Training loss: 11.042248725891113
2025-12-09 12:10:36.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 9.949735666879252e-05 Training loss: 10.894701957702637
2025-12-09 12:10:36.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 9.949304019653012e-05 Training loss: 10.906238555908203
2025-12-09 12:10:37.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 9.94887053639079e-05 Training loss: 10.980313301086426
2025-12-09 12:10:37.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 9.948435217253393e-05 Training loss: 10.879924774169922
2025-12-09 12:10:37.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 9.947998062402313e-05 Training loss: 11.076577186584473
2025-12-09 12:10:37.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 9.947559071999719e-05 Training loss: 10.982869148254395
2025-12-09 12:10:37.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 9.947118246208462e-05 Training loss: 11.00661849975586
2025-12-09 12:10:37.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 9.946675585192075e-05 Training loss: 10.968931198120117
2025-12-09 12:10:37.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 9.946231089114774e-05 Training loss: 10.925580978393555
2025-12-09 12:10:37.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 9.945784758141448e-05 Training loss: 10.834830284118652
2025-12-09 12:10:37.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 9.945336592437678e-05 Training loss: 11.07140827178955
2025-12-09 12:10:37.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 9.944886592169713e-05 Training loss: 10.754196166992188
2025-12-09 12:10:37.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 9.944434757504492e-05 Training loss: 10.803857803344727
2025-12-09 12:10:37.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 9.94398108860963e-05 Training loss: 10.809263229370117
2025-12-09 12:10:37.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 9.943525585653428e-05 Training loss: 10.87425708770752
2025-12-09 12:10:38.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 9.94306824880486e-05 Training loss: 10.966246604919434
2025-12-09 12:10:38.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 9.942609078233581e-05 Training loss: 10.893988609313965
2025-12-09 12:10:38.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 9.942148074109934e-05 Training loss: 10.939480781555176
2025-12-09 12:10:38.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 9.941685236604934e-05 Training loss: 10.833643913269043
2025-12-09 12:10:38.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 9.941220565890279e-05 Training loss: 10.915162086486816
2025-12-09 12:10:38.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 9.94075406213835e-05 Training loss: 10.820796966552734
2025-12-09 12:10:38.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 9.940285725522203e-05 Training loss: 10.900782585144043
2025-12-09 12:10:38.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 9.939815556215575e-05 Training loss: 11.002467155456543
2025-12-09 12:10:38.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 9.939343554392886e-05 Training loss: 11.014647483825684
2025-12-09 12:10:38.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 9.938869720229234e-05 Training loss: 10.901679039001465
2025-12-09 12:10:38.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 9.938394053900395e-05 Training loss: 10.788182258605957
2025-12-09 12:10:38.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 9.937916555582828e-05 Training loss: 10.761419296264648
2025-12-09 12:10:38.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 9.937437225453669e-05 Training loss: 10.846794128417969
2025-12-09 12:10:39.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 9.936956063690733e-05 Training loss: 10.890034675598145
2025-12-09 12:10:39.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 9.936473070472518e-05 Training loss: 10.812575340270996
2025-12-09 12:10:39.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 9.935988245978199e-05 Training loss: 10.912626266479492
2025-12-09 12:10:39.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 9.935501590387628e-05 Training loss: 10.756498336791992
2025-12-09 12:10:39.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 9.935013103881343e-05 Training loss: 10.963181495666504
2025-12-09 12:10:39.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 9.934522786640555e-05 Training loss: 10.865066528320312
2025-12-09 12:10:39.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 9.934030638847155e-05 Training loss: 10.946229934692383
2025-12-09 12:10:39.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 9.933536660683717e-05 Training loss: 10.824027061462402
2025-12-09 12:10:39.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 9.933040852333488e-05 Training loss: 10.953389167785645
2025-12-09 12:10:39.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 9.9325432139804e-05 Training loss: 10.78428840637207
2025-12-09 12:10:39.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 9.932043745809063e-05 Training loss: 10.737635612487793
2025-12-09 12:10:39.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 9.93154244800476e-05 Training loss: 10.807642936706543
2025-12-09 12:10:39.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.931039320753456e-05 Training loss: 10.91079330444336
2025-12-09 12:10:40.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 9.9305343642418e-05 Training loss: 10.717241287231445
2025-12-09 12:10:40.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 9.930027578657113e-05 Training loss: 10.779099464416504
2025-12-09 12:10:40.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 9.929518964187395e-05 Training loss: 10.97531509399414
2025-12-09 12:10:40.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 9.929008521021325e-05 Training loss: 11.152636528015137
2025-12-09 12:10:40.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 9.928496249348265e-05 Training loss: 10.845479011535645
2025-12-09 12:10:40.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 9.92798214935825e-05 Training loss: 10.836627960205078
2025-12-09 12:10:40.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 9.927466221241996e-05 Training loss: 10.867136001586914
2025-12-09 12:10:40.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 9.926948465190892e-05 Training loss: 10.928436279296875
2025-12-09 12:10:40.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 9.926428881397015e-05 Training loss: 10.671375274658203
2025-12-09 12:10:40.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 9.925907470053111e-05 Training loss: 10.7754545211792
2025-12-09 12:10:40.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 9.925384231352606e-05 Training loss: 10.863398551940918
2025-12-09 12:10:40.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 9.924859165489608e-05 Training loss: 11.032941818237305
2025-12-09 12:10:40.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 9.924332272658898e-05 Training loss: 10.61700439453125
2025-12-09 12:10:41.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 9.923803553055937e-05 Training loss: 10.718684196472168
2025-12-09 12:10:41.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 9.923273006876865e-05 Training loss: 10.716558456420898
2025-12-09 12:10:41.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 9.922740634318495e-05 Training loss: 10.969812393188477
2025-12-09 12:10:41.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 9.922206435578323e-05 Training loss: 10.675753593444824
2025-12-09 12:10:41.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 9.921670410854518e-05 Training loss: 10.86523151397705
2025-12-09 12:10:41.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 9.92113256034593e-05 Training loss: 11.008240699768066
2025-12-09 12:10:41.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 9.920592884252082e-05 Training loss: 10.746679306030273
2025-12-09 12:10:41.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 9.920051382773179e-05 Training loss: 10.746978759765625
2025-12-09 12:10:41.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 9.919508056110102e-05 Training loss: 10.644325256347656
2025-12-09 12:10:41.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 9.918962904464407e-05 Training loss: 10.681894302368164
2025-12-09 12:10:41.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 9.918415928038325e-05 Training loss: 10.938432693481445
2025-12-09 12:10:41.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 9.917867127034772e-05 Training loss: 10.961153984069824
2025-12-09 12:10:41.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 9.917316501657334e-05 Training loss: 10.712449073791504
2025-12-09 12:10:42.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 9.916764052110274e-05 Training loss: 10.612203598022461
2025-12-09 12:10:42.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 9.916209778598535e-05 Training loss: 10.59600830078125
2025-12-09 12:10:42.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 9.915653681327737e-05 Training loss: 10.72482681274414
2025-12-09 12:10:42.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 9.915095760504169e-05 Training loss: 10.931723594665527
2025-12-09 12:10:42.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 9.914536016334808e-05 Training loss: 10.877005577087402
2025-12-09 12:10:42.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 9.913974449027298e-05 Training loss: 10.7151517868042
2025-12-09 12:10:42.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 9.913411058789963e-05 Training loss: 10.839138984680176
2025-12-09 12:10:42.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 9.912845845831805e-05 Training loss: 10.686741828918457
2025-12-09 12:10:42.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 9.912278810362498e-05 Training loss: 11.037922859191895
2025-12-09 12:10:42.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 9.911709952592397e-05 Training loss: 10.69286060333252
2025-12-09 12:10:42.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 9.911139272732526e-05 Training loss: 10.716821670532227
2025-12-09 12:10:42.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 9.910566770994594e-05 Training loss: 10.701249122619629
2025-12-09 12:10:42.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 9.909992447590979e-05 Training loss: 10.8328857421875
2025-12-09 12:10:43.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 9.909416302734736e-05 Training loss: 10.854757308959961
2025-12-09 12:10:43.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 9.908838336639597e-05 Training loss: 10.731467247009277
2025-12-09 12:10:43.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 9.908258549519971e-05 Training loss: 10.690872192382812
2025-12-09 12:10:43.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 9.907676941590939e-05 Training loss: 10.647581100463867
2025-12-09 12:10:43.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 9.907093513068259e-05 Training loss: 10.698710441589355
2025-12-09 12:10:43.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 9.906508264168366e-05 Training loss: 11.018040657043457
2025-12-09 12:10:43.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 9.905921195108368e-05 Training loss: 10.607071876525879
2025-12-09 12:10:43.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 9.90533230610605e-05 Training loss: 10.77277660369873
2025-12-09 12:10:43.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 9.90474159737987e-05 Training loss: 10.643946647644043
2025-12-09 12:10:43.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 9.904149069148963e-05 Training loss: 10.592546463012695
2025-12-09 12:10:43.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 9.903554721633139e-05 Training loss: 10.698202133178711
2025-12-09 12:10:43.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 9.902958555052882e-05 Training loss: 10.641928672790527
2025-12-09 12:10:43.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 9.902360569629349e-05 Training loss: 10.731019973754883
2025-12-09 12:10:44.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 9.901760765584375e-05 Training loss: 10.634678840637207
2025-12-09 12:10:44.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 9.901159143140471e-05 Training loss: 10.79497241973877
2025-12-09 12:10:44.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 9.900555702520816e-05 Training loss: 10.892112731933594
2025-12-09 12:10:44.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 9.89995044394927e-05 Training loss: 10.769485473632812
2025-12-09 12:10:44.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 9.899343367650365e-05 Training loss: 10.720817565917969
2025-12-09 12:10:44.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 9.898734473849305e-05 Training loss: 10.93765640258789
2025-12-09 12:10:44.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 9.898123762771971e-05 Training loss: 10.533571243286133
2025-12-09 12:10:44.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 9.89751123464492e-05 Training loss: 10.885892868041992
2025-12-09 12:10:44.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 9.896896889695378e-05 Training loss: 11.071900367736816
2025-12-09 12:10:44.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 9.896280728151248e-05 Training loss: 10.765266418457031
2025-12-09 12:10:44.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 9.895662750241108e-05 Training loss: 10.652480125427246
2025-12-09 12:10:44.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 9.89504295619421e-05 Training loss: 10.898207664489746
2025-12-09 12:10:44.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 9.894421346240473e-05 Training loss: 10.876635551452637
2025-12-09 12:10:45.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 9.893797920610496e-05 Training loss: 10.668200492858887
2025-12-09 12:10:45.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 9.893172679535553e-05 Training loss: 10.70363712310791
2025-12-09 12:10:45.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 9.892545623247586e-05 Training loss: 10.599995613098145
2025-12-09 12:10:45.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 9.891916751979218e-05 Training loss: 10.660184860229492
2025-12-09 12:10:45.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 9.891286065963734e-05 Training loss: 10.841784477233887
2025-12-09 12:10:45.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 9.890653565435101e-05 Training loss: 10.982141494750977
2025-12-09 12:10:45.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 9.89001925062796e-05 Training loss: 10.643482208251953
2025-12-09 12:10:45.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 9.889383121777617e-05 Training loss: 10.513687133789062
2025-12-09 12:10:45.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 9.88874517912006e-05 Training loss: 10.7587308883667
2025-12-09 12:10:45.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 9.888105422891943e-05 Training loss: 10.633163452148438
2025-12-09 12:10:45.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 9.887463853330594e-05 Training loss: 10.62476921081543
2025-12-09 12:10:45.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 9.886820470674018e-05 Training loss: 10.685291290283203
2025-12-09 12:10:45.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 9.88617527516089e-05 Training loss: 10.79131031036377
2025-12-09 12:10:46.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 9.885528267030557e-05 Training loss: 10.67408561706543
2025-12-09 12:10:46.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 9.884879446523035e-05 Training loss: 10.67605209350586
2025-12-09 12:10:46.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 9.88422881387902e-05 Training loss: 10.662304878234863
2025-12-09 12:10:46.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 9.883576369339875e-05 Training loss: 10.596434593200684
2025-12-09 12:10:46.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 9.882922113147637e-05 Training loss: 10.654742240905762
2025-12-09 12:10:46.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 9.882266045545012e-05 Training loss: 10.681097030639648
2025-12-09 12:10:46.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 9.881608166775383e-05 Training loss: 10.647310256958008
2025-12-09 12:10:46.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 9.880948477082804e-05 Training loss: 10.513432502746582
2025-12-09 12:10:46.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 9.880286976711992e-05 Training loss: 10.685638427734375
2025-12-09 12:10:46.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 9.87962366590835e-05 Training loss: 10.81798267364502
2025-12-09 12:10:46.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 9.878958544917942e-05 Training loss: 10.629335403442383
2025-12-09 12:10:46.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 9.87829161398751e-05 Training loss: 10.516854286193848
2025-12-09 12:10:46.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 9.87762287336446e-05 Training loss: 10.565435409545898
2025-12-09 12:10:47.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 9.876952323296877e-05 Training loss: 10.64048957824707
2025-12-09 12:10:47.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 9.876279964033512e-05 Training loss: 10.359559059143066
2025-12-09 12:10:47.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 9.87560579582379e-05 Training loss: 10.970967292785645
2025-12-09 12:10:47.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 9.874929818917806e-05 Training loss: 10.682259559631348
2025-12-09 12:10:47.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 9.874252033566327e-05 Training loss: 10.762001991271973
2025-12-09 12:10:47.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 9.87357244002079e-05 Training loss: 10.919625282287598
2025-12-09 12:10:47.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 9.8728910385333e-05 Training loss: 10.701066017150879
2025-12-09 12:10:47.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 9.872207829356641e-05 Training loss: 10.54670524597168
2025-12-09 12:10:47.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 9.871522812744256e-05 Training loss: 10.787574768066406
2025-12-09 12:10:47.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 9.870835988950268e-05 Training loss: 10.48807430267334
2025-12-09 12:10:47.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 9.870147358229467e-05 Training loss: 10.586912155151367
2025-12-09 12:10:47.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 9.869456920837312e-05 Training loss: 10.422399520874023
2025-12-09 12:10:47.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 9.868764677029934e-05 Training loss: 10.604862213134766
2025-12-09 12:10:48.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 9.868070627064135e-05 Training loss: 10.536519050598145
2025-12-09 12:10:48.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 9.867374771197383e-05 Training loss: 10.462255477905273
2025-12-09 12:10:48.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 9.866677109687822e-05 Training loss: 10.673965454101562
2025-12-09 12:10:48.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 9.86597764279426e-05 Training loss: 10.586030960083008
2025-12-09 12:10:48.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 9.865276370776177e-05 Training loss: 10.40990924835205
2025-12-09 12:10:48.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 9.864573293893725e-05 Training loss: 10.605765342712402
2025-12-09 12:10:48.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 9.863868412407721e-05 Training loss: 10.533113479614258
2025-12-09 12:10:48.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 9.863161726579655e-05 Training loss: 10.399259567260742
2025-12-09 12:10:48.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 9.862453236671685e-05 Training loss: 10.410653114318848
2025-12-09 12:10:48.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 9.861742942946639e-05 Training loss: 10.448796272277832
2025-12-09 12:10:48.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 9.861030845668014e-05 Training loss: 10.565292358398438
2025-12-09 12:10:48.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 9.860316945099973e-05 Training loss: 10.519129753112793
2025-12-09 12:10:48.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 9.859601241507353e-05 Training loss: 10.933095932006836
2025-12-09 12:10:48.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 9.858883735155657e-05 Training loss: 10.519013404846191
2025-12-09 12:10:49.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 9.858164426311059e-05 Training loss: 10.72181510925293
2025-12-09 12:10:49.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 9.857443315240397e-05 Training loss: 10.547866821289062
2025-12-09 12:10:49.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 9.856720402211182e-05 Training loss: 10.759135246276855
2025-12-09 12:10:49.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 9.855995687491591e-05 Training loss: 10.529008865356445
2025-12-09 12:10:49.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 9.855269171350471e-05 Training loss: 10.704930305480957
2025-12-09 12:10:49.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 9.854540854057337e-05 Training loss: 10.540860176086426
2025-12-09 12:10:49.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 9.85381073588237e-05 Training loss: 10.706048011779785
2025-12-09 12:10:49.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 9.853078817096424e-05 Training loss: 10.448266983032227
2025-12-09 12:10:49.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 9.852345097971016e-05 Training loss: 10.446516990661621
2025-12-09 12:10:49.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 9.851609578778332e-05 Training loss: 10.51284122467041
2025-12-09 12:10:49.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 9.850872259791228e-05 Training loss: 10.493452072143555
2025-12-09 12:10:49.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 9.850133141283226e-05 Training loss: 10.335041046142578
2025-12-09 12:10:49.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 9.849392223528514e-05 Training loss: 10.580329895019531
2025-12-09 12:10:50.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 9.84864950680195e-05 Training loss: 10.392882347106934
2025-12-09 12:10:50.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 9.84790499137906e-05 Training loss: 10.6460599899292
2025-12-09 12:10:50.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 9.847158677536034e-05 Training loss: 10.397265434265137
2025-12-09 12:10:50.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 9.84641056554973e-05 Training loss: 10.625651359558105
2025-12-09 12:10:50.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 9.845660655697679e-05 Training loss: 10.504347801208496
2025-12-09 12:10:50.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 9.844908948258067e-05 Training loss: 10.620368003845215
2025-12-09 12:10:50.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 9.844155443509759e-05 Training loss: 10.607305526733398
2025-12-09 12:10:50.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 9.84340014173228e-05 Training loss: 10.641826629638672
2025-12-09 12:10:50.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 9.842643043205822e-05 Training loss: 10.473735809326172
2025-12-09 12:10:50.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 9.841884148211247e-05 Training loss: 10.53507137298584
2025-12-09 12:10:50.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 9.84112345703008e-05 Training loss: 10.493447303771973
2025-12-09 12:10:50.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 9.84036096994451e-05 Training loss: 10.571264266967773
2025-12-09 12:10:50.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 9.839596687237403e-05 Training loss: 10.416379928588867
2025-12-09 12:10:51.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 9.838830609192277e-05 Training loss: 10.618063926696777
2025-12-09 12:10:51.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 9.838062736093328e-05 Training loss: 10.518926620483398
2025-12-09 12:10:51.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 9.837293068225408e-05 Training loss: 10.531153678894043
2025-12-09 12:10:51.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 9.836521605874044e-05 Training loss: 10.541139602661133
2025-12-09 12:10:51.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 9.835748349325422e-05 Training loss: 10.187667846679688
2025-12-09 12:10:51.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 9.834973298866395e-05 Training loss: 10.324583053588867
2025-12-09 12:10:51.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 9.834196454784485e-05 Training loss: 10.45665454864502
2025-12-09 12:10:51.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 9.833417817367874e-05 Training loss: 10.592376708984375
2025-12-09 12:10:51.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 9.832637386905412e-05 Training loss: 10.58388614654541
2025-12-09 12:10:51.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 9.831855163686618e-05 Training loss: 10.64742374420166
2025-12-09 12:10:51.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 9.831071148001667e-05 Training loss: 10.61267375946045
2025-12-09 12:10:51.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 9.830285340141408e-05 Training loss: 10.54119873046875
2025-12-09 12:10:51.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 9.829497740397349e-05 Training loss: 10.654678344726562
2025-12-09 12:10:52.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 9.828708349061664e-05 Training loss: 10.445222854614258
2025-12-09 12:10:52.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 9.827917166427195e-05 Training loss: 10.732365608215332
2025-12-09 12:10:52.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 9.827124192787444e-05 Training loss: 10.461564064025879
2025-12-09 12:10:52.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 9.82632942843658e-05 Training loss: 10.409892082214355
2025-12-09 12:10:52.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 9.825532873669435e-05 Training loss: 10.455578804016113
2025-12-09 12:10:52.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 9.824734528781505e-05 Training loss: 10.322160720825195
2025-12-09 12:10:52.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 9.823934394068952e-05 Training loss: 10.544295310974121
2025-12-09 12:10:52.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 9.823132469828601e-05 Training loss: 10.37078857421875
2025-12-09 12:10:52.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 9.822328756357942e-05 Training loss: 10.437601089477539
2025-12-09 12:10:52.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 9.821523253955122e-05 Training loss: 10.531698226928711
2025-12-09 12:10:52.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 9.820715962918964e-05 Training loss: 10.465985298156738
2025-12-09 12:10:52.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 9.819906883548943e-05 Training loss: 10.587518692016602
2025-12-09 12:10:52.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 9.819096016145203e-05 Training loss: 10.435622215270996
2025-12-09 12:10:53.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 9.81828336100855e-05 Training loss: 10.400256156921387
2025-12-09 12:10:53.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 9.817468918440454e-05 Training loss: 10.476333618164062
2025-12-09 12:10:53.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 9.816652688743049e-05 Training loss: 10.407844543457031
2025-12-09 12:10:53.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 9.815834672219127e-05 Training loss: 10.681536674499512
2025-12-09 12:10:53.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 9.815014869172149e-05 Training loss: 10.464223861694336
2025-12-09 12:10:53.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 9.814193279906237e-05 Training loss: 10.487334251403809
2025-12-09 12:10:53.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 9.81336990472617e-05 Training loss: 10.370026588439941
2025-12-09 12:10:53.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 9.8125447439374e-05 Training loss: 10.390451431274414
2025-12-09 12:10:53.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 9.811717797846033e-05 Training loss: 10.378984451293945
2025-12-09 12:10:53.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 9.81088906675884e-05 Training loss: 10.54079532623291
2025-12-09 12:10:53.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 9.810058550983254e-05 Training loss: 10.476152420043945
2025-12-09 12:10:53.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 9.809226250827371e-05 Training loss: 10.440242767333984
2025-12-09 12:10:53.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 9.808392166599948e-05 Training loss: 10.513203620910645
2025-12-09 12:10:54.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 9.807556298610404e-05 Training loss: 10.611153602600098
2025-12-09 12:10:54.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 9.806718647168818e-05 Training loss: 10.529951095581055
2025-12-09 12:10:54.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 9.805879212585933e-05 Training loss: 10.468125343322754
2025-12-09 12:10:54.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 9.805037995173155e-05 Training loss: 10.306828498840332
2025-12-09 12:10:54.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 9.804194995242548e-05 Training loss: 10.45527172088623
2025-12-09 12:10:54.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 9.803350213106836e-05 Training loss: 10.291545867919922
2025-12-09 12:10:54.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 9.802503649079411e-05 Training loss: 10.456341743469238
2025-12-09 12:10:54.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 9.801655303474318e-05 Training loss: 10.48349666595459
2025-12-09 12:10:54.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 9.80080517660627e-05 Training loss: 10.484519958496094
2025-12-09 12:10:54.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 9.799953268790633e-05 Training loss: 10.399151802062988
2025-12-09 12:10:54.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 9.799099580343441e-05 Training loss: 10.457603454589844
2025-12-09 12:10:54.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 9.798244111581382e-05 Training loss: 10.120800018310547
2025-12-09 12:10:54.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 9.797386862821813e-05 Training loss: 10.331511497497559
2025-12-09 12:10:55.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 9.796527834382745e-05 Training loss: 10.41930103302002
2025-12-09 12:10:55.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 9.795667026582847e-05 Training loss: 10.357555389404297
2025-12-09 12:10:55.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 9.794804439741456e-05 Training loss: 10.536484718322754
2025-12-09 12:10:55.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 9.79394007417856e-05 Training loss: 10.340365409851074
2025-12-09 12:10:55.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 9.793073930214817e-05 Training loss: 10.481019020080566
2025-12-09 12:10:55.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 9.792206008171533e-05 Training loss: 10.37424373626709
2025-12-09 12:10:55.521 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 9.791336308370687e-05 Training loss: 10.383172988891602
2025-12-09 12:10:55.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 9.790464831134903e-05 Training loss: 10.386346817016602
2025-12-09 12:10:55.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 9.789591576787476e-05 Training loss: 10.589258193969727
2025-12-09 12:10:55.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 9.788716545652353e-05 Training loss: 10.452025413513184
2025-12-09 12:10:55.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 9.787839738054146e-05 Training loss: 10.441094398498535
2025-12-09 12:10:55.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 9.786961154318121e-05 Training loss: 10.381579399108887
2025-12-09 12:10:55.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 9.786080794770207e-05 Training loss: 10.509641647338867
2025-12-09 12:10:56.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 9.785198659736988e-05 Training loss: 10.290582656860352
2025-12-09 12:10:56.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 9.784314749545707e-05 Training loss: 10.440030097961426
2025-12-09 12:10:56.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 9.78342906452427e-05 Training loss: 10.181086540222168
2025-12-09 12:10:56.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 9.782541605001235e-05 Training loss: 10.798103332519531
2025-12-09 12:10:56.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 9.781652371305824e-05 Training loss: 10.399683952331543
2025-12-09 12:10:56.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 9.780761363767914e-05 Training loss: 10.612390518188477
2025-12-09 12:10:56.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 9.779868582718041e-05 Training loss: 10.565367698669434
2025-12-09 12:10:56.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 9.778974028487398e-05 Training loss: 10.312777519226074
2025-12-09 12:10:56.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 9.778077701407837e-05 Training loss: 10.436214447021484
2025-12-09 12:10:56.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 9.777179601811867e-05 Training loss: 10.251147270202637
2025-12-09 12:10:56.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 9.776279730032654e-05 Training loss: 10.279513359069824
2025-12-09 12:10:56.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 9.775378086404023e-05 Training loss: 10.313570022583008
2025-12-09 12:10:56.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 9.774474671260457e-05 Training loss: 10.479825019836426
2025-12-09 12:10:57.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 9.77356948493709e-05 Training loss: 10.468968391418457
2025-12-09 12:10:57.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 9.77266252776972e-05 Training loss: 10.196012496948242
2025-12-09 12:10:57.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 9.771753800094803e-05 Training loss: 10.55333423614502
2025-12-09 12:10:57.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 9.770843302249443e-05 Training loss: 10.31752872467041
2025-12-09 12:10:57.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 9.769931034571408e-05 Training loss: 10.103849411010742
2025-12-09 12:10:57.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 9.76901699739912e-05 Training loss: 10.468789100646973
2025-12-09 12:10:57.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 9.768101191071661e-05 Training loss: 10.364760398864746
2025-12-09 12:10:57.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 9.767183615928765e-05 Training loss: 10.733831405639648
2025-12-09 12:10:57.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 9.766264272310822e-05 Training loss: 10.294078826904297
2025-12-09 12:10:57.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 9.765343160558879e-05 Training loss: 10.346559524536133
2025-12-09 12:10:57.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 9.764420281014642e-05 Training loss: 10.32013988494873
2025-12-09 12:10:57.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 9.763495634020467e-05 Training loss: 10.368712425231934
2025-12-09 12:10:57.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 9.762569219919372e-05 Training loss: 10.351436614990234
2025-12-09 12:10:58.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 9.761641039055026e-05 Training loss: 10.479459762573242
2025-12-09 12:10:58.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 9.760711091771755e-05 Training loss: 10.49051284790039
2025-12-09 12:10:58.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 9.759779378414542e-05 Training loss: 10.348984718322754
2025-12-09 12:10:58.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 9.758845899329021e-05 Training loss: 10.301118850708008
2025-12-09 12:10:58.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 9.757910654861483e-05 Training loss: 10.276748657226562
2025-12-09 12:10:58.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 9.756973645358876e-05 Training loss: 10.366838455200195
2025-12-09 12:10:58.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 9.7560348711688e-05 Training loss: 10.299324989318848
2025-12-09 12:10:58.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 9.755094332639512e-05 Training loss: 10.248188972473145
2025-12-09 12:10:58.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 9.754152030119921e-05 Training loss: 10.37624454498291
2025-12-09 12:10:58.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 9.75320796395959e-05 Training loss: 10.254812240600586
2025-12-09 12:10:58.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 9.752262134508742e-05 Training loss: 10.3710355758667
2025-12-09 12:10:58.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 9.751314542118246e-05 Training loss: 10.549720764160156
2025-12-09 12:10:58.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 9.750365187139632e-05 Training loss: 10.240046501159668
2025-12-09 12:10:59.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 9.749414069925078e-05 Training loss: 10.194406509399414
2025-12-09 12:10:59.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 9.74846119082742e-05 Training loss: 10.234050750732422
2025-12-09 12:10:59.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 9.747506550200146e-05 Training loss: 10.20343017578125
2025-12-09 12:10:59.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 9.746550148397398e-05 Training loss: 10.505426406860352
2025-12-09 12:10:59.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 9.745591985773971e-05 Training loss: 10.47496509552002
2025-12-09 12:10:59.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 9.744632062685311e-05 Training loss: 10.477557182312012
2025-12-09 12:10:59.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 9.743670379487522e-05 Training loss: 10.32265853881836
2025-12-09 12:10:59.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 9.742706936537358e-05 Training loss: 10.346272468566895
2025-12-09 12:10:59.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 9.741741734192224e-05 Training loss: 10.072893142700195
2025-12-09 12:10:59.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 9.740774772810182e-05 Training loss: 10.445380210876465
2025-12-09 12:10:59.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 9.739806052749943e-05 Training loss: 10.34713363647461
2025-12-09 12:10:59.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 9.738835574370871e-05 Training loss: 10.29249382019043
2025-12-09 12:10:59.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 9.737863338032985e-05 Training loss: 10.634635925292969
2025-12-09 12:11:00.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 9.736889344096952e-05 Training loss: 10.414458274841309
2025-12-09 12:11:00.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 9.735913592924093e-05 Training loss: 10.463173866271973
2025-12-09 12:11:00.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 9.734936084876383e-05 Training loss: 10.23762321472168
2025-12-09 12:11:00.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 9.733956820316444e-05 Training loss: 10.445034980773926
2025-12-09 12:11:00.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 9.732975799607555e-05 Training loss: 10.276115417480469
2025-12-09 12:11:00.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 9.731993023113642e-05 Training loss: 10.412034034729004
2025-12-09 12:11:00.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 9.731008491199284e-05 Training loss: 10.111591339111328
2025-12-09 12:11:00.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 9.730022204229714e-05 Training loss: 10.264484405517578
2025-12-09 12:11:00.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 9.729034162570811e-05 Training loss: 10.159788131713867
2025-12-09 12:11:00.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 9.728044366589108e-05 Training loss: 10.261483192443848
2025-12-09 12:11:00.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 9.727052816651788e-05 Training loss: 10.410380363464355
2025-12-09 12:11:00.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 9.726059513126685e-05 Training loss: 10.18440055847168
2025-12-09 12:11:00.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 9.725064456382283e-05 Training loss: 10.410931587219238
2025-12-09 12:11:01.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 9.724067646787717e-05 Training loss: 10.19076919555664
2025-12-09 12:11:01.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 9.723069084712772e-05 Training loss: 10.305022239685059
2025-12-09 12:11:01.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 9.722068770527883e-05 Training loss: 10.343817710876465
2025-12-09 12:11:01.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 9.721066704604134e-05 Training loss: 10.469804763793945
2025-12-09 12:11:01.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 9.720062887313261e-05 Training loss: 10.355177879333496
2025-12-09 12:11:01.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 9.71905731902765e-05 Training loss: 10.423357009887695
2025-12-09 12:11:01.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 9.718050000120334e-05 Training loss: 10.438175201416016
2025-12-09 12:11:01.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 9.717040930964995e-05 Training loss: 10.276130676269531
2025-12-09 12:11:01.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 9.716030111935967e-05 Training loss: 10.155543327331543
2025-12-09 12:11:01.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 9.715017543408233e-05 Training loss: 10.348359107971191
2025-12-09 12:11:01.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 9.714003225757424e-05 Training loss: 10.459002494812012
2025-12-09 12:11:01.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 9.712987159359818e-05 Training loss: 10.307426452636719
2025-12-09 12:11:01.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 9.711969344592346e-05 Training loss: 10.38453483581543
2025-12-09 12:11:02.054 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 9.710949781832585e-05 Training loss: 10.19151496887207
2025-12-09 12:11:02.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 9.709928471458759e-05 Training loss: 10.349971771240234
2025-12-09 12:11:02.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 9.708905413849743e-05 Training loss: 10.56131649017334
2025-12-09 12:11:02.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 9.707880609385059e-05 Training loss: 10.34528923034668
2025-12-09 12:11:02.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 9.706854058444876e-05 Training loss: 10.32632827758789
2025-12-09 12:11:02.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 9.705825761410014e-05 Training loss: 10.34040641784668
2025-12-09 12:11:02.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 9.704795718661939e-05 Training loss: 10.084909439086914
2025-12-09 12:11:02.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 9.703763930582761e-05 Training loss: 10.556686401367188
2025-12-09 12:11:02.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 9.702730397555247e-05 Training loss: 10.57402515411377
2025-12-09 12:11:02.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 9.7016951199628e-05 Training loss: 10.246817588806152
2025-12-09 12:11:02.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 9.700658098189475e-05 Training loss: 10.242149353027344
2025-12-09 12:11:02.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 9.69961933261998e-05 Training loss: 10.255813598632812
2025-12-09 12:11:02.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 9.698578823639659e-05 Training loss: 10.242137908935547
2025-12-09 12:11:03.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 9.697536571634509e-05 Training loss: 10.31680679321289
2025-12-09 12:11:03.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 9.696492576991174e-05 Training loss: 10.33475112915039
2025-12-09 12:11:03.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 9.695446840096944e-05 Training loss: 10.201888084411621
2025-12-09 12:11:03.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 9.694399361339752e-05 Training loss: 10.245059967041016
2025-12-09 12:11:03.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 9.693350141108182e-05 Training loss: 10.260161399841309
2025-12-09 12:11:03.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 9.692299179791459e-05 Training loss: 10.560257911682129
2025-12-09 12:11:03.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 9.69124647777946e-05 Training loss: 10.135507583618164
2025-12-09 12:11:03.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 9.690192035462702e-05 Training loss: 10.486656188964844
2025-12-09 12:11:03.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 9.689135853232349e-05 Training loss: 10.301101684570312
2025-12-09 12:11:03.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 9.688077931480212e-05 Training loss: 10.251380920410156
2025-12-09 12:11:03.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 9.687018270598749e-05 Training loss: 10.237093925476074
2025-12-09 12:11:03.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 9.685956870981058e-05 Training loss: 10.214325904846191
2025-12-09 12:11:03.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 9.684893733020888e-05 Training loss: 10.142095565795898
2025-12-09 12:11:04.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 9.683828857112627e-05 Training loss: 10.279361724853516
2025-12-09 12:11:04.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 9.682762243651308e-05 Training loss: 10.289223670959473
2025-12-09 12:11:04.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 9.681693893032618e-05 Training loss: 10.195623397827148
2025-12-09 12:11:04.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 9.680623805652876e-05 Training loss: 10.541671752929688
2025-12-09 12:11:04.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 9.679551981909053e-05 Training loss: 10.174699783325195
2025-12-09 12:11:04.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 9.67847842219876e-05 Training loss: 10.141902923583984
2025-12-09 12:11:04.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 9.677403126920256e-05 Training loss: 10.308605194091797
2025-12-09 12:11:04.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 9.676326096472441e-05 Training loss: 10.312880516052246
2025-12-09 12:11:04.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 9.675247331254858e-05 Training loss: 10.274697303771973
2025-12-09 12:11:04.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 9.674166831667697e-05 Training loss: 10.376041412353516
2025-12-09 12:11:04.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 9.673084598111789e-05 Training loss: 10.309718132019043
2025-12-09 12:11:04.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 9.672000630988605e-05 Training loss: 10.322115898132324
2025-12-09 12:11:04.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 9.670914930700267e-05 Training loss: 10.219110488891602
2025-12-09 12:11:05.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 9.669827497649536e-05 Training loss: 10.355547904968262
2025-12-09 12:11:05.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 9.668738332239813e-05 Training loss: 10.111917495727539
2025-12-09 12:11:05.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 9.667647434875145e-05 Training loss: 10.2658052444458
2025-12-09 12:11:05.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 9.66655480596022e-05 Training loss: 10.208574295043945
2025-12-09 12:11:05.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 9.665460445900368e-05 Training loss: 10.221205711364746
2025-12-09 12:11:05.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 9.664364355101565e-05 Training loss: 10.21484375
2025-12-09 12:11:05.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 9.663266533970424e-05 Training loss: 10.186376571655273
2025-12-09 12:11:05.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 9.662166982914203e-05 Training loss: 10.31956672668457
2025-12-09 12:11:05.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 9.661065702340801e-05 Training loss: 10.176342010498047
2025-12-09 12:11:05.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 9.659962692658758e-05 Training loss: 10.092905044555664
2025-12-09 12:11:05.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 9.658857954277254e-05 Training loss: 10.034130096435547
2025-12-09 12:11:05.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 9.657751487606115e-05 Training loss: 10.253608703613281
2025-12-09 12:11:05.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 9.656643293055804e-05 Training loss: 10.210362434387207
2025-12-09 12:11:06.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 9.655533371037426e-05 Training loss: 10.389552116394043
2025-12-09 12:11:06.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 9.65442172196273e-05 Training loss: 10.28951644897461
2025-12-09 12:11:06.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 9.653308346244098e-05 Training loss: 10.439753532409668
2025-12-09 12:11:06.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 9.652193244294562e-05 Training loss: 10.508787155151367
2025-12-09 12:11:06.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 9.651076416527787e-05 Training loss: 10.082972526550293
2025-12-09 12:11:06.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 9.64995786335808e-05 Training loss: 10.295344352722168
2025-12-09 12:11:06.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 9.648837585200393e-05 Training loss: 10.349627494812012
2025-12-09 12:11:06.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 9.64771558247031e-05 Training loss: 10.583346366882324
2025-12-09 12:11:06.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 9.64659185558406e-05 Training loss: 10.138968467712402
2025-12-09 12:11:06.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 9.64546640495851e-05 Training loss: 10.312275886535645
2025-12-09 12:11:06.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 9.644339231011168e-05 Training loss: 10.214496612548828
2025-12-09 12:11:06.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 9.643210334160177e-05 Training loss: 10.172677993774414
2025-12-09 12:11:06.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 9.642079714824328e-05 Training loss: 10.452247619628906
2025-12-09 12:11:07.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 9.64094737342304e-05 Training loss: 10.316688537597656
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 90.83 GiB memory in use. Of the allocated memory 89.58 GiB is allocated by PyTorch, and 500.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:07, 147.22it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 176.86it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:20, 123.32it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:09, 142.97it/s]Tokenizing texts:   1%|          | 99/10000 [00:00<00:55, 177.21it/s]Tokenizing texts:   1%|▏         | 130/10000 [00:00<00:45, 214.92it/s]Tokenizing texts:   2%|▏         | 153/10000 [00:00<00:52, 188.78it/s]Tokenizing texts:   2%|▏         | 174/10000 [00:00<00:52, 188.47it/s]Tokenizing texts:   2%|▏         | 198/10000 [00:01<00:49, 198.16it/s]Tokenizing texts:   2%|▏         | 222/10000 [00:01<00:47, 204.68it/s]Tokenizing texts:   2%|▏         | 244/10000 [00:01<00:46, 207.72it/s]Tokenizing texts:   3%|▎         | 266/10000 [00:01<00:46, 209.33it/s]Tokenizing texts:   3%|▎         | 293/10000 [00:01<00:42, 226.22it/s]Tokenizing texts:   3%|▎         | 319/10000 [00:01<00:42, 225.42it/s]Tokenizing texts:   3%|▎         | 342/10000 [00:01<00:50, 189.45it/s]Tokenizing texts:   4%|▎         | 368/10000 [00:01<00:46, 205.82it/s]Tokenizing texts:   4%|▍         | 390/10000 [00:02<00:50, 188.98it/s]Tokenizing texts:   4%|▍         | 412/10000 [00:02<00:49, 194.41it/s]Tokenizing texts:   4%|▍         | 433/10000 [00:02<00:50, 188.60it/s]Tokenizing texts:   5%|▍         | 453/10000 [00:02<00:51, 186.96it/s]Tokenizing texts:   5%|▍         | 481/10000 [00:02<00:45, 211.49it/s]Tokenizing texts:   5%|▌         | 507/10000 [00:02<00:42, 224.69it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:02<00:49, 189.46it/s]Tokenizing texts:   6%|▌         | 552/10000 [00:02<00:48, 196.33it/s]Tokenizing texts:   6%|▌         | 581/10000 [00:02<00:42, 219.63it/s]Tokenizing texts:   6%|▌         | 604/10000 [00:03<00:42, 220.30it/s]Tokenizing texts:   6%|▋         | 627/10000 [00:03<00:45, 205.06it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:45, 206.54it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:45, 203.46it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:47, 193.95it/s]Tokenizing texts:   7%|▋         | 722/10000 [00:03<00:45, 204.70it/s]Tokenizing texts:   7%|▋         | 743/10000 [00:03<00:45, 204.57it/s]Tokenizing texts:   8%|▊         | 768/10000 [00:03<00:42, 216.29it/s]Tokenizing texts:   8%|▊         | 790/10000 [00:03<00:45, 203.85it/s]Tokenizing texts:   8%|▊         | 814/10000 [00:04<00:43, 211.48it/s]Tokenizing texts:   8%|▊         | 837/10000 [00:04<00:42, 214.93it/s]Tokenizing texts:   9%|▊         | 864/10000 [00:04<00:39, 229.71it/s]Tokenizing texts:   9%|▉         | 888/10000 [00:04<00:42, 213.92it/s]Tokenizing texts:   9%|▉         | 918/10000 [00:04<00:38, 235.19it/s]Tokenizing texts:  10%|▉         | 952/10000 [00:04<00:34, 259.73it/s]Tokenizing texts:  10%|▉         | 981/10000 [00:04<00:33, 268.05it/s]Tokenizing texts:  10%|█         | 1014/10000 [00:04<00:31, 285.62it/s]Tokenizing texts:  10%|█         | 1043/10000 [00:04<00:31, 284.54it/s]Tokenizing texts:  11%|█         | 1072/10000 [00:05<00:39, 224.87it/s]Tokenizing texts:  11%|█         | 1097/10000 [00:05<00:39, 222.73it/s]Tokenizing texts:  11%|█         | 1121/10000 [00:05<00:39, 224.34it/s]Tokenizing texts:  11%|█▏        | 1145/10000 [00:05<00:44, 198.59it/s]Tokenizing texts:  12%|█▏        | 1167/10000 [00:05<00:44, 199.35it/s]Tokenizing texts:  12%|█▏        | 1193/10000 [00:05<00:41, 213.32it/s]Tokenizing texts:  12%|█▏        | 1216/10000 [00:05<00:40, 216.92it/s]Tokenizing texts:  12%|█▏        | 1239/10000 [00:05<00:41, 213.45it/s]Tokenizing texts:  13%|█▎        | 1261/10000 [00:06<00:41, 210.40it/s]Tokenizing texts:  13%|█▎        | 1296/10000 [00:06<00:34, 248.82it/s]Tokenizing texts:  13%|█▎        | 1322/10000 [00:06<00:39, 217.82it/s]Tokenizing texts:  13%|█▎        | 1345/10000 [00:06<00:42, 205.55it/s]Tokenizing texts:  14%|█▎        | 1370/10000 [00:06<00:39, 216.23it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:39, 220.21it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 237.96it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:33, 254.88it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:06<00:30, 278.00it/s]Tokenizing texts:  15%|█▌        | 1517/10000 [00:07<00:31, 271.85it/s]Tokenizing texts:  15%|█▌        | 1545/10000 [00:07<00:33, 255.88it/s]Tokenizing texts:  16%|█▌        | 1578/10000 [00:07<00:30, 276.13it/s]Tokenizing texts:  16%|█▌        | 1607/10000 [00:07<00:40, 209.45it/s]Tokenizing texts:  16%|█▋        | 1635/10000 [00:07<00:37, 225.39it/s]Tokenizing texts:  17%|█▋        | 1660/10000 [00:07<00:36, 231.23it/s]Tokenizing texts:  17%|█▋        | 1685/10000 [00:07<00:37, 222.40it/s]Tokenizing texts:  17%|█▋        | 1709/10000 [00:07<00:36, 226.46it/s]Tokenizing texts:  17%|█▋        | 1744/10000 [00:08<00:32, 256.63it/s]Tokenizing texts:  18%|█▊        | 1771/10000 [00:08<00:32, 254.82it/s]Tokenizing texts:  18%|█▊        | 1798/10000 [00:08<00:32, 250.21it/s]Tokenizing texts:  18%|█▊        | 1831/10000 [00:08<00:30, 267.90it/s]Tokenizing texts:  19%|█▊        | 1859/10000 [00:08<00:36, 225.22it/s]Tokenizing texts:  19%|█▉        | 1883/10000 [00:08<00:36, 222.69it/s]Tokenizing texts:  19%|█▉        | 1911/10000 [00:08<00:34, 236.53it/s]Tokenizing texts:  19%|█▉        | 1936/10000 [00:08<00:35, 224.66it/s]Tokenizing texts:  20%|█▉        | 1960/10000 [00:09<00:41, 192.50it/s]Tokenizing texts:  20%|█▉        | 1991/10000 [00:09<00:36, 217.59it/s]Tokenizing texts:  20%|██        | 2017/10000 [00:09<00:35, 227.92it/s]Tokenizing texts:  21%|██        | 2051/10000 [00:09<00:30, 257.49it/s]Tokenizing texts:  21%|██        | 2078/10000 [00:09<00:31, 247.79it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:39, 201.42it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:37, 208.46it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:09<00:38, 201.88it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:09<00:35, 220.11it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:31, 248.81it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:29, 259.78it/s]Tokenizing texts:  23%|██▎       | 2275/10000 [00:10<00:28, 269.78it/s]Tokenizing texts:  23%|██▎       | 2303/10000 [00:10<00:29, 258.31it/s]Tokenizing texts:  23%|██▎       | 2330/10000 [00:10<00:36, 210.78it/s]Tokenizing texts:  24%|██▎       | 2370/10000 [00:10<00:29, 255.06it/s]Tokenizing texts:  24%|██▍       | 2401/10000 [00:10<00:28, 268.48it/s]Tokenizing texts:  24%|██▍       | 2430/10000 [00:10<00:31, 243.08it/s]Tokenizing texts:  25%|██▍       | 2462/10000 [00:11<00:28, 261.65it/s]Tokenizing texts:  25%|██▍       | 2490/10000 [00:11<00:30, 248.82it/s]Tokenizing texts:  25%|██▌       | 2518/10000 [00:11<00:29, 255.94it/s]Tokenizing texts:  25%|██▌       | 2545/10000 [00:11<00:29, 256.73it/s]Tokenizing texts:  26%|██▌       | 2572/10000 [00:11<00:32, 230.04it/s]Tokenizing texts:  26%|██▌       | 2596/10000 [00:11<00:34, 217.13it/s]Tokenizing texts:  26%|██▌       | 2619/10000 [00:11<00:35, 207.37it/s]Tokenizing texts:  26%|██▋       | 2647/10000 [00:11<00:32, 225.98it/s]Tokenizing texts:  27%|██▋       | 2671/10000 [00:11<00:32, 227.26it/s]Tokenizing texts:  27%|██▋       | 2695/10000 [00:12<00:37, 195.90it/s]Tokenizing texts:  27%|██▋       | 2722/10000 [00:12<00:33, 214.16it/s]Tokenizing texts:  28%|██▊       | 2758/10000 [00:12<00:29, 245.01it/s]Tokenizing texts:  28%|██▊       | 2784/10000 [00:12<00:29, 241.36it/s]Tokenizing texts:  28%|██▊       | 2809/10000 [00:12<00:37, 193.00it/s]Tokenizing texts:  28%|██▊       | 2841/10000 [00:12<00:32, 221.01it/s]Tokenizing texts:  29%|██▊       | 2866/10000 [00:12<00:34, 204.49it/s]Tokenizing texts:  29%|██▉       | 2894/10000 [00:13<00:32, 221.59it/s]Tokenizing texts:  29%|██▉       | 2924/10000 [00:13<00:29, 239.26it/s]Tokenizing texts:  30%|██▉       | 2950/10000 [00:13<00:31, 227.33it/s]Tokenizing texts:  30%|██▉       | 2974/10000 [00:13<00:30, 228.87it/s]Tokenizing texts:  30%|███       | 3012/10000 [00:13<00:25, 269.52it/s]Tokenizing texts:  30%|███       | 3040/10000 [00:13<00:30, 230.40it/s]Tokenizing texts:  31%|███       | 3074/10000 [00:13<00:26, 256.89it/s]Tokenizing texts:  31%|███       | 3102/10000 [00:13<00:27, 246.69it/s]Tokenizing texts:  31%|███▏      | 3136/10000 [00:13<00:25, 270.71it/s]Tokenizing texts:  32%|███▏      | 3165/10000 [00:14<00:26, 257.86it/s]Tokenizing texts:  32%|███▏      | 3194/10000 [00:14<00:25, 264.64it/s]Tokenizing texts:  32%|███▏      | 3223/10000 [00:14<00:25, 269.10it/s]Tokenizing texts:  33%|███▎      | 3251/10000 [00:14<00:26, 257.29it/s]Tokenizing texts:  33%|███▎      | 3281/10000 [00:14<00:25, 267.90it/s]Tokenizing texts:  33%|███▎      | 3309/10000 [00:14<00:25, 264.99it/s]Tokenizing texts:  33%|███▎      | 3336/10000 [00:14<00:26, 249.48it/s]Tokenizing texts:  34%|███▎      | 3366/10000 [00:14<00:25, 263.27it/s]Tokenizing texts:  34%|███▍      | 3393/10000 [00:14<00:25, 257.32it/s]Tokenizing texts:  34%|███▍      | 3421/10000 [00:15<00:25, 259.79it/s]Tokenizing texts:  35%|███▍      | 3462/10000 [00:15<00:21, 302.24it/s]Tokenizing texts:  35%|███▍      | 3493/10000 [00:15<00:24, 267.42it/s]Tokenizing texts:  35%|███▌      | 3521/10000 [00:15<00:25, 254.64it/s]Tokenizing texts:  35%|███▌      | 3548/10000 [00:15<00:27, 238.46it/s]Tokenizing texts:  36%|███▌      | 3578/10000 [00:15<00:25, 254.22it/s]Tokenizing texts:  36%|███▌      | 3605/10000 [00:15<00:24, 256.08it/s]Tokenizing texts:  36%|███▋      | 3635/10000 [00:15<00:24, 264.74it/s]Tokenizing texts:  37%|███▋      | 3662/10000 [00:15<00:24, 253.54it/s]Tokenizing texts:  37%|███▋      | 3688/10000 [00:16<00:25, 252.40it/s]Tokenizing texts:  37%|███▋      | 3714/10000 [00:16<00:25, 250.66it/s]Tokenizing texts:  37%|███▋      | 3740/10000 [00:16<00:24, 250.99it/s]Tokenizing texts:  38%|███▊      | 3766/10000 [00:16<00:26, 232.35it/s]Tokenizing texts:  38%|███▊      | 3790/10000 [00:16<00:26, 234.28it/s]Tokenizing texts:  38%|███▊      | 3814/10000 [00:16<00:29, 212.14it/s]Tokenizing texts:  38%|███▊      | 3842/10000 [00:16<00:26, 229.86it/s]Tokenizing texts:  39%|███▊      | 3870/10000 [00:16<00:25, 241.01it/s]Tokenizing texts:  39%|███▉      | 3896/10000 [00:16<00:25, 238.81it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:25, 238.92it/s]Tokenizing texts:  39%|███▉      | 3946/10000 [00:17<00:25, 233.49it/s]Tokenizing texts:  40%|███▉      | 3980/10000 [00:17<00:23, 260.03it/s]Tokenizing texts:  40%|████      | 4007/10000 [00:17<00:26, 224.26it/s]Tokenizing texts:  40%|████      | 4037/10000 [00:17<00:24, 240.38it/s]Tokenizing texts:  41%|████      | 4067/10000 [00:17<00:23, 256.07it/s]Tokenizing texts:  41%|████      | 4100/10000 [00:17<00:21, 276.04it/s]Tokenizing texts:  41%|████▏     | 4134/10000 [00:17<00:19, 293.45it/s]Tokenizing texts:  42%|████▏     | 4164/10000 [00:18<00:21, 277.06it/s]Tokenizing texts:  42%|████▏     | 4196/10000 [00:18<00:20, 288.65it/s]Tokenizing texts:  42%|████▏     | 4226/10000 [00:18<00:22, 262.27it/s]Tokenizing texts:  43%|████▎     | 4253/10000 [00:18<00:23, 248.42it/s]Tokenizing texts:  43%|████▎     | 4279/10000 [00:18<00:23, 239.77it/s]Tokenizing texts:  43%|████▎     | 4311/10000 [00:18<00:22, 256.40it/s]Tokenizing texts:  43%|████▎     | 4348/10000 [00:18<00:19, 286.97it/s]Tokenizing texts:  44%|████▍     | 4378/10000 [00:18<00:20, 272.28it/s]Tokenizing texts:  44%|████▍     | 4413/10000 [00:18<00:19, 291.59it/s]Tokenizing texts:  44%|████▍     | 4443/10000 [00:19<00:19, 284.78it/s]Tokenizing texts:  45%|████▍     | 4472/10000 [00:19<00:21, 258.99it/s]Tokenizing texts:  45%|████▍     | 4499/10000 [00:19<00:22, 243.22it/s]Tokenizing texts:  45%|████▌     | 4527/10000 [00:19<00:21, 249.88it/s]Tokenizing texts:  46%|████▌     | 4553/10000 [00:19<00:21, 251.65it/s]Tokenizing texts:  46%|████▌     | 4581/10000 [00:19<00:20, 259.05it/s]Tokenizing texts:  46%|████▌     | 4608/10000 [00:19<00:21, 245.77it/s]Tokenizing texts:  46%|████▋     | 4641/10000 [00:19<00:19, 268.81it/s]Tokenizing texts:  47%|████▋     | 4676/10000 [00:19<00:18, 291.43it/s]Tokenizing texts:  47%|████▋     | 4712/10000 [00:20<00:17, 309.67it/s]Tokenizing texts:  47%|████▋     | 4744/10000 [00:20<00:19, 275.96it/s]Tokenizing texts:  48%|████▊     | 4774/10000 [00:20<00:18, 281.90it/s]Tokenizing texts:  48%|████▊     | 4803/10000 [00:20<00:20, 251.45it/s]Tokenizing texts:  48%|████▊     | 4835/10000 [00:20<00:19, 268.35it/s]Tokenizing texts:  49%|████▊     | 4865/10000 [00:20<00:18, 276.58it/s]Tokenizing texts:  49%|████▉     | 4895/10000 [00:20<00:18, 282.91it/s]Tokenizing texts:  49%|████▉     | 4924/10000 [00:20<00:18, 268.05it/s]Tokenizing texts:  50%|████▉     | 4953/10000 [00:20<00:18, 272.18it/s]Tokenizing texts:  50%|████▉     | 4982/10000 [00:21<00:18, 277.10it/s]Tokenizing texts:  50%|█████     | 5013/10000 [00:21<00:17, 284.56it/s]Tokenizing texts:  50%|█████     | 5042/10000 [00:21<00:17, 283.58it/s]Tokenizing texts:  51%|█████     | 5071/10000 [00:21<00:19, 248.54it/s]Tokenizing texts:  51%|█████     | 5097/10000 [00:21<00:20, 244.12it/s]Tokenizing texts:  51%|█████     | 5123/10000 [00:21<00:20, 235.79it/s]Tokenizing texts:  52%|█████▏    | 5151/10000 [00:21<00:19, 245.35it/s]Tokenizing texts:  52%|█████▏    | 5180/10000 [00:21<00:18, 257.51it/s]Tokenizing texts:  52%|█████▏    | 5207/10000 [00:21<00:20, 236.52it/s]Tokenizing texts:  52%|█████▏    | 5235/10000 [00:22<00:19, 247.50it/s]Tokenizing texts:  53%|█████▎    | 5269/10000 [00:22<00:18, 255.54it/s]Tokenizing texts:  53%|█████▎    | 5299/10000 [00:22<00:17, 267.01it/s]Tokenizing texts:  53%|█████▎    | 5336/10000 [00:22<00:16, 287.11it/s]Tokenizing texts:  54%|█████▎    | 5372/10000 [00:22<00:15, 306.09it/s]Tokenizing texts:  54%|█████▍    | 5403/10000 [00:22<00:15, 297.54it/s]Tokenizing texts:  54%|█████▍    | 5433/10000 [00:22<00:16, 283.07it/s]Tokenizing texts:  55%|█████▍    | 5463/10000 [00:22<00:15, 283.73it/s]Tokenizing texts:  55%|█████▍    | 5492/10000 [00:23<00:21, 214.40it/s]Tokenizing texts:  55%|█████▌    | 5526/10000 [00:23<00:18, 242.75it/s]Tokenizing texts:  56%|█████▌    | 5553/10000 [00:23<00:18, 246.07it/s]Tokenizing texts:  56%|█████▌    | 5580/10000 [00:23<00:17, 246.97it/s]Tokenizing texts:  56%|█████▌    | 5615/10000 [00:23<00:15, 274.34it/s]Tokenizing texts:  57%|█████▋    | 5652/10000 [00:23<00:14, 299.80it/s]Tokenizing texts:  57%|█████▋    | 5684/10000 [00:23<00:14, 304.03it/s]Tokenizing texts:  57%|█████▋    | 5716/10000 [00:23<00:14, 303.18it/s]Tokenizing texts:  57%|█████▋    | 5747/10000 [00:23<00:16, 263.16it/s]Tokenizing texts:  58%|█████▊    | 5775/10000 [00:24<00:15, 267.36it/s]Tokenizing texts:  58%|█████▊    | 5803/10000 [00:24<00:18, 223.19it/s]Tokenizing texts:  58%|█████▊    | 5828/10000 [00:24<00:19, 211.16it/s]Tokenizing texts:  59%|█████▊    | 5859/10000 [00:24<00:17, 234.00it/s]Tokenizing texts:  59%|█████▉    | 5889/10000 [00:24<00:16, 250.49it/s]Tokenizing texts:  59%|█████▉    | 5916/10000 [00:24<00:16, 245.71it/s]Tokenizing texts:  59%|█████▉    | 5942/10000 [00:24<00:16, 245.93it/s]Tokenizing texts:  60%|█████▉    | 5968/10000 [00:24<00:16, 247.68it/s]Tokenizing texts:  60%|█████▉    | 5994/10000 [00:24<00:16, 249.75it/s]Tokenizing texts:  60%|██████    | 6020/10000 [00:25<00:16, 241.34it/s]Tokenizing texts:  61%|██████    | 6054/10000 [00:25<00:14, 264.96it/s]Tokenizing texts:  61%|██████    | 6083/10000 [00:25<00:14, 269.52it/s]Tokenizing texts:  61%|██████    | 6111/10000 [00:25<00:14, 261.70it/s]Tokenizing texts:  61%|██████▏   | 6144/10000 [00:25<00:13, 279.92it/s]Tokenizing texts:  62%|██████▏   | 6173/10000 [00:25<00:13, 277.87it/s]Tokenizing texts:  62%|██████▏   | 6201/10000 [00:25<00:14, 265.90it/s]Tokenizing texts:  62%|██████▏   | 6228/10000 [00:25<00:15, 249.50it/s]Tokenizing texts:  63%|██████▎   | 6260/10000 [00:25<00:13, 267.72it/s]Tokenizing texts:  63%|██████▎   | 6288/10000 [00:26<00:14, 254.67it/s]Tokenizing texts:  63%|██████▎   | 6314/10000 [00:26<00:15, 232.04it/s]Tokenizing texts:  63%|██████▎   | 6338/10000 [00:26<00:15, 230.76it/s]Tokenizing texts:  64%|██████▎   | 6366/10000 [00:26<00:15, 241.11it/s]Tokenizing texts:  64%|██████▍   | 6393/10000 [00:26<00:14, 248.76it/s]Tokenizing texts:  64%|██████▍   | 6422/10000 [00:26<00:13, 257.93it/s]Tokenizing texts:  64%|██████▍   | 6450/10000 [00:26<00:13, 262.70it/s]Tokenizing texts:  65%|██████▍   | 6477/10000 [00:26<00:14, 236.56it/s]Tokenizing texts:  65%|██████▌   | 6506/10000 [00:27<00:14, 233.61it/s]Tokenizing texts:  65%|██████▌   | 6532/10000 [00:27<00:14, 238.31it/s]Tokenizing texts:  66%|██████▌   | 6573/10000 [00:27<00:12, 280.34it/s]Tokenizing texts:  66%|██████▌   | 6602/10000 [00:27<00:12, 274.21it/s]Tokenizing texts:  66%|██████▋   | 6630/10000 [00:27<00:12, 267.13it/s]Tokenizing texts:  67%|██████▋   | 6658/10000 [00:27<00:12, 270.04it/s]Tokenizing texts:  67%|██████▋   | 6686/10000 [00:27<00:12, 267.93it/s]Tokenizing texts:  67%|██████▋   | 6713/10000 [00:27<00:12, 256.76it/s]Tokenizing texts:  67%|██████▋   | 6739/10000 [00:27<00:13, 248.72it/s]Tokenizing texts:  68%|██████▊   | 6771/10000 [00:28<00:12, 267.59it/s]Tokenizing texts:  68%|██████▊   | 6805/10000 [00:28<00:11, 286.76it/s]Tokenizing texts:  68%|██████▊   | 6837/10000 [00:28<00:10, 294.66it/s]Tokenizing texts:  69%|██████▊   | 6869/10000 [00:28<00:10, 290.86it/s]Tokenizing texts:  69%|██████▉   | 6899/10000 [00:28<00:12, 238.56it/s]Tokenizing texts:  69%|██████▉   | 6925/10000 [00:28<00:13, 234.20it/s]Tokenizing texts:  70%|██████▉   | 6950/10000 [00:28<00:12, 234.98it/s]Tokenizing texts:  70%|██████▉   | 6978/10000 [00:28<00:12, 244.87it/s]Tokenizing texts:  70%|███████   | 7009/10000 [00:28<00:11, 261.15it/s]Tokenizing texts:  70%|███████   | 7036/10000 [00:29<00:11, 251.64it/s]Tokenizing texts:  71%|███████   | 7062/10000 [00:29<00:13, 213.09it/s]Tokenizing texts:  71%|███████   | 7086/10000 [00:29<00:13, 218.82it/s]Tokenizing texts:  71%|███████   | 7112/10000 [00:29<00:12, 228.65it/s]Tokenizing texts:  71%|███████▏  | 7142/10000 [00:29<00:11, 247.91it/s]Tokenizing texts:  72%|███████▏  | 7168/10000 [00:29<00:11, 238.22it/s]Tokenizing texts:  72%|███████▏  | 7196/10000 [00:29<00:11, 247.98it/s]Tokenizing texts:  72%|███████▏  | 7225/10000 [00:29<00:11, 250.64it/s]Tokenizing texts:  73%|███████▎  | 7251/10000 [00:29<00:11, 246.96it/s]Tokenizing texts:  73%|███████▎  | 7297/10000 [00:30<00:09, 300.06it/s]Tokenizing texts:  73%|███████▎  | 7331/10000 [00:30<00:08, 309.10it/s]Tokenizing texts:  74%|███████▎  | 7363/10000 [00:30<00:09, 266.99it/s]Tokenizing texts:  74%|███████▍  | 7391/10000 [00:30<00:09, 268.96it/s]Tokenizing texts:  74%|███████▍  | 7419/10000 [00:30<00:11, 224.73it/s]Tokenizing texts:  74%|███████▍  | 7444/10000 [00:30<00:12, 205.70it/s]Tokenizing texts:  75%|███████▍  | 7477/10000 [00:30<00:10, 234.76it/s]Tokenizing texts:  75%|███████▌  | 7513/10000 [00:30<00:09, 266.18it/s]Tokenizing texts:  75%|███████▌  | 7542/10000 [00:31<00:09, 267.45it/s]Tokenizing texts:  76%|███████▌  | 7571/10000 [00:31<00:09, 263.15it/s]Tokenizing texts:  76%|███████▌  | 7611/10000 [00:31<00:08, 291.62it/s]Tokenizing texts:  76%|███████▋  | 7641/10000 [00:31<00:08, 273.99it/s]Tokenizing texts:  77%|███████▋  | 7674/10000 [00:31<00:08, 288.06it/s]Tokenizing texts:  77%|███████▋  | 7704/10000 [00:31<00:07, 291.05it/s]Tokenizing texts:  77%|███████▋  | 7734/10000 [00:31<00:09, 250.75it/s]Tokenizing texts:  78%|███████▊  | 7761/10000 [00:31<00:09, 246.64it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:31<00:08, 260.11it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 239.22it/s]Tokenizing texts:  78%|███████▊  | 7846/10000 [00:32<00:08, 249.46it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 259.10it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 256.33it/s]Tokenizing texts:  79%|███████▉  | 7939/10000 [00:32<00:07, 287.53it/s]Tokenizing texts:  80%|███████▉  | 7969/10000 [00:32<00:07, 280.73it/s]Tokenizing texts:  80%|████████  | 8002/10000 [00:32<00:06, 293.27it/s]Tokenizing texts:  80%|████████  | 8032/10000 [00:32<00:06, 293.29it/s]Tokenizing texts:  81%|████████  | 8062/10000 [00:33<00:11, 175.33it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 199.38it/s]Tokenizing texts:  81%|████████  | 8119/10000 [00:33<00:09, 207.81it/s]Tokenizing texts:  81%|████████▏ | 8148/10000 [00:33<00:08, 226.68it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:33<00:07, 240.33it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:33<00:07, 251.50it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:33<00:08, 215.67it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 225.97it/s]Tokenizing texts:  83%|████████▎ | 8302/10000 [00:34<00:06, 261.24it/s]Tokenizing texts:  83%|████████▎ | 8330/10000 [00:34<00:06, 257.37it/s]Tokenizing texts:  84%|████████▎ | 8357/10000 [00:34<00:06, 258.27it/s]Tokenizing texts:  84%|████████▍ | 8384/10000 [00:34<00:06, 258.53it/s]Tokenizing texts:  84%|████████▍ | 8411/10000 [00:34<00:06, 236.01it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:34<00:05, 260.19it/s]Tokenizing texts:  85%|████████▍ | 8473/10000 [00:34<00:05, 266.61it/s]Tokenizing texts:  85%|████████▌ | 8507/10000 [00:34<00:05, 286.81it/s]Tokenizing texts:  85%|████████▌ | 8537/10000 [00:35<00:05, 266.50it/s]Tokenizing texts:  86%|████████▌ | 8565/10000 [00:35<00:05, 258.07it/s]Tokenizing texts:  86%|████████▌ | 8597/10000 [00:35<00:05, 274.04it/s]Tokenizing texts:  86%|████████▋ | 8630/10000 [00:35<00:04, 285.69it/s]Tokenizing texts:  87%|████████▋ | 8662/10000 [00:35<00:04, 288.85it/s]Tokenizing texts:  87%|████████▋ | 8692/10000 [00:35<00:04, 289.85it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:35<00:04, 287.68it/s]Tokenizing texts:  88%|████████▊ | 8757/10000 [00:35<00:04, 294.48it/s]Tokenizing texts:  88%|████████▊ | 8787/10000 [00:35<00:04, 294.95it/s]Tokenizing texts:  88%|████████▊ | 8817/10000 [00:36<00:05, 220.71it/s]Tokenizing texts:  88%|████████▊ | 8842/10000 [00:36<00:05, 222.79it/s]Tokenizing texts:  89%|████████▉ | 8876/10000 [00:36<00:04, 250.73it/s]Tokenizing texts:  89%|████████▉ | 8914/10000 [00:36<00:03, 275.94it/s]Tokenizing texts:  89%|████████▉ | 8946/10000 [00:36<00:03, 286.67it/s]Tokenizing texts:  90%|████████▉ | 8976/10000 [00:36<00:03, 283.73it/s]Tokenizing texts:  90%|█████████ | 9006/10000 [00:36<00:04, 246.54it/s]Tokenizing texts:  90%|█████████ | 9038/10000 [00:36<00:03, 263.05it/s]Tokenizing texts:  91%|█████████ | 9066/10000 [00:37<00:03, 252.57it/s]Tokenizing texts:  91%|█████████ | 9098/10000 [00:37<00:03, 269.39it/s]Tokenizing texts:  91%|█████████▏| 9126/10000 [00:37<00:03, 259.68it/s]Tokenizing texts:  92%|█████████▏| 9158/10000 [00:37<00:03, 271.53it/s]Tokenizing texts:  92%|█████████▏| 9191/10000 [00:37<00:02, 286.86it/s]Tokenizing texts:  92%|█████████▏| 9221/10000 [00:37<00:02, 285.66it/s]Tokenizing texts:  92%|█████████▎| 9250/10000 [00:37<00:02, 278.67it/s]Tokenizing texts:  93%|█████████▎| 9279/10000 [00:37<00:02, 248.21it/s]Tokenizing texts:  93%|█████████▎| 9310/10000 [00:37<00:02, 258.45it/s]Tokenizing texts:  93%|█████████▎| 9337/10000 [00:38<00:02, 249.31it/s]Tokenizing texts:  94%|█████████▎| 9363/10000 [00:38<00:02, 212.74it/s]Tokenizing texts:  94%|█████████▍| 9400/10000 [00:38<00:02, 250.20it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:38<00:02, 228.38it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:38<00:02, 233.23it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:38<00:02, 242.65it/s]Tokenizing texts:  95%|█████████▌| 9505/10000 [00:38<00:02, 186.12it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:38<00:02, 222.89it/s]Tokenizing texts:  96%|█████████▌| 9566/10000 [00:39<00:01, 230.40it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 235.08it/s]Tokenizing texts:  96%|█████████▌| 9622/10000 [00:39<00:01, 232.32it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 236.64it/s]Tokenizing texts:  97%|█████████▋| 9674/10000 [00:39<00:01, 242.94it/s]Tokenizing texts:  97%|█████████▋| 9699/10000 [00:39<00:01, 238.48it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:39<00:01, 240.78it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:39<00:01, 208.73it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:39<00:01, 217.19it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 262.86it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 255.65it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 261.00it/s]Tokenizing texts:  99%|█████████▉| 9907/10000 [00:40<00:00, 288.05it/s]Tokenizing texts:  99%|█████████▉| 9937/10000 [00:40<00:00, 285.23it/s]Tokenizing texts: 100%|█████████▉| 9969/10000 [00:40<00:00, 293.95it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:40<00:00, 245.62it/s]
2025-12-09 12:12:08.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 11.988485336303711
2025-12-09 12:12:08.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 12.019883155822754
2025-12-09 12:12:08.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 12.016643524169922
2025-12-09 12:12:08.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 12.012618064880371
2025-12-09 12:12:08.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 12.033417701721191
2025-12-09 12:12:08.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 12.010530471801758
2025-12-09 12:12:08.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 11.999041557312012
2025-12-09 12:12:09.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 12.024602890014648
2025-12-09 12:12:09.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 12.023719787597656
2025-12-09 12:12:09.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 12.025039672851562
2025-12-09 12:12:09.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 12.032837867736816
2025-12-09 12:12:09.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 11.977789878845215
2025-12-09 12:12:09.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 12.015585899353027
2025-12-09 12:12:09.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 12.038607597351074
2025-12-09 12:12:09.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 12.042963027954102
2025-12-09 12:12:09.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 12.00180721282959
2025-12-09 12:12:09.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 12.003508567810059
2025-12-09 12:12:09.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 12.028188705444336
2025-12-09 12:12:09.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 12.053606033325195
2025-12-09 12:12:09.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 11.997822761535645
2025-12-09 12:12:10.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 12.014863967895508
2025-12-09 12:12:10.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 12.030811309814453
2025-12-09 12:12:10.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 12.01469612121582
2025-12-09 12:12:10.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 12.011981010437012
2025-12-09 12:12:10.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 11.997891426086426
2025-12-09 12:12:10.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 12.021570205688477
2025-12-09 12:12:10.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 12.051602363586426
2025-12-09 12:12:10.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 11.982552528381348
2025-12-09 12:12:10.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 12.014087677001953
2025-12-09 12:12:10.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 12.015222549438477
2025-12-09 12:12:10.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 11.958949089050293
2025-12-09 12:12:10.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 11.996261596679688
2025-12-09 12:12:10.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 12.016069412231445
2025-12-09 12:12:11.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 12.067584991455078
2025-12-09 12:12:11.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 12.017204284667969
2025-12-09 12:12:11.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 11.924118995666504
2025-12-09 12:12:11.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 12.038602828979492
2025-12-09 12:12:11.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 11.981672286987305
2025-12-09 12:12:11.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 11.98815631866455
2025-12-09 12:12:11.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 11.963424682617188
2025-12-09 12:12:11.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 11.921338081359863
2025-12-09 12:12:11.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 11.929407119750977
2025-12-09 12:12:11.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 12.022296905517578
2025-12-09 12:12:11.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 11.876920700073242
2025-12-09 12:12:11.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 11.935954093933105
2025-12-09 12:12:11.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 11.95319938659668
2025-12-09 12:12:12.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 11.997225761413574
2025-12-09 12:12:12.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 11.935653686523438
2025-12-09 12:12:12.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 11.95724105834961
2025-12-09 12:12:12.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 11.979856491088867
2025-12-09 12:12:12.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 12.002161979675293
2025-12-09 12:12:12.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 11.962347984313965
2025-12-09 12:12:12.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 11.982053756713867
2025-12-09 12:12:12.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 11.955131530761719
2025-12-09 12:12:12.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 11.879355430603027
2025-12-09 12:12:12.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 11.963486671447754
2025-12-09 12:12:12.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 11.91454792022705
2025-12-09 12:12:12.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 11.948946952819824
2025-12-09 12:12:12.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 11.849044799804688
2025-12-09 12:12:12.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 11.9718599319458
2025-12-09 12:12:13.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 11.906638145446777
2025-12-09 12:12:13.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 11.903031349182129
2025-12-09 12:12:13.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 11.905155181884766
2025-12-09 12:12:13.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 11.894660949707031
2025-12-09 12:12:13.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 11.913886070251465
2025-12-09 12:12:13.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 11.869075775146484
2025-12-09 12:12:13.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 11.864295959472656
2025-12-09 12:12:13.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 11.92031192779541
2025-12-09 12:12:13.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 11.872305870056152
2025-12-09 12:12:13.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 11.869165420532227
2025-12-09 12:12:13.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 11.831799507141113
2025-12-09 12:12:13.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 11.851286888122559
2025-12-09 12:12:13.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 11.910656929016113
2025-12-09 12:12:14.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 11.811100006103516
2025-12-09 12:12:14.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 11.83397102355957
2025-12-09 12:12:14.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 11.755773544311523
2025-12-09 12:12:14.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 11.77124309539795
2025-12-09 12:12:14.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 11.895484924316406
2025-12-09 12:12:14.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 11.836806297302246
2025-12-09 12:12:14.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 11.790895462036133
2025-12-09 12:12:14.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 11.69411849975586
2025-12-09 12:12:14.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 11.746362686157227
2025-12-09 12:12:14.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 11.646007537841797
2025-12-09 12:12:14.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 11.681938171386719
2025-12-09 12:12:14.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 11.65904426574707
2025-12-09 12:12:14.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 11.750018119812012
2025-12-09 12:12:15.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 11.736917495727539
2025-12-09 12:12:15.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 11.830230712890625
2025-12-09 12:12:15.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 11.675528526306152
2025-12-09 12:12:15.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 11.635801315307617
2025-12-09 12:12:15.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 11.643543243408203
2025-12-09 12:12:15.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 11.617947578430176
2025-12-09 12:12:15.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 11.600214958190918
2025-12-09 12:12:15.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 11.562980651855469
2025-12-09 12:12:15.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 11.592996597290039
2025-12-09 12:12:15.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 11.488302230834961
2025-12-09 12:12:15.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 11.633698463439941
2025-12-09 12:12:15.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 11.47545051574707
2025-12-09 12:12:15.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 11.408560752868652
2025-12-09 12:12:16.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 11.572155952453613
2025-12-09 12:12:16.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999997217736103 Training loss: 11.50780200958252
2025-12-09 12:12:16.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029999988870945456 Training loss: 11.574600219726562
2025-12-09 12:12:16.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0002999997495963115 Training loss: 11.477407455444336
2025-12-09 12:12:16.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00029999955483798346 Training loss: 11.397965431213379
2025-12-09 12:12:16.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0002999993044345427 Training loss: 11.510024070739746
2025-12-09 12:12:16.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002999989983860821 Training loss: 11.403233528137207
2025-12-09 12:12:16.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.00029999863669271526 Training loss: 11.354389190673828
2025-12-09 12:12:16.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0002999982193545762 Training loss: 11.343214988708496
2025-12-09 12:12:16.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0002999977463718199 Training loss: 11.4358549118042
2025-12-09 12:12:16.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.00029999721774462174 Training loss: 11.318583488464355
2025-12-09 12:12:16.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029999663347317785 Training loss: 11.256096839904785
2025-12-09 12:12:16.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.00029999599355770497 Training loss: 11.286069869995117
2025-12-09 12:12:17.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0002999952979984405 Training loss: 11.23662281036377
2025-12-09 12:12:17.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.00029999454679564244 Training loss: 11.299992561340332
2025-12-09 12:12:17.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0002999937399495895 Training loss: 11.087885856628418
2025-12-09 12:12:17.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00029999287746058093 Training loss: 11.281717300415039
2025-12-09 12:12:17.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029999195932893676 Training loss: 11.258094787597656
2025-12-09 12:12:17.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.00029999098555499756 Training loss: 11.217456817626953
2025-12-09 12:12:17.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002999899561391246 Training loss: 11.245796203613281
2025-12-09 12:12:17.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.00029998887108169967 Training loss: 11.22295093536377
2025-12-09 12:12:17.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0002999877303831254 Training loss: 11.118993759155273
2025-12-09 12:12:17.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00029998653404382487 Training loss: 11.085098266601562
2025-12-09 12:12:17.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.000299985282064242 Training loss: 11.147326469421387
2025-12-09 12:12:17.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00029998397444484104 Training loss: 10.926178932189941
2025-12-09 12:12:17.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002999826111861073 Training loss: 11.189926147460938
2025-12-09 12:12:18.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00029998119228854625 Training loss: 11.065835952758789
2025-12-09 12:12:18.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0002999797177526845 Training loss: 11.195711135864258
2025-12-09 12:12:18.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.000299978187579069 Training loss: 11.193941116333008
2025-12-09 12:12:18.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0002999766017682673 Training loss: 11.386784553527832
2025-12-09 12:12:18.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029997496032086775 Training loss: 11.292831420898438
2025-12-09 12:12:18.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00029997326323747927 Training loss: 10.907846450805664
2025-12-09 12:12:18.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0002999715105187314 Training loss: 11.049338340759277
2025-12-09 12:12:18.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.00029996970216527436 Training loss: 11.130353927612305
2025-12-09 12:12:18.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.000299967838177779 Training loss: 11.11014175415039
2025-12-09 12:12:18.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.00029996591855693686 Training loss: 10.939108848571777
2025-12-09 12:12:18.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.00029996394330345996 Training loss: 11.052485466003418
2025-12-09 12:12:18.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002999619124180811 Training loss: 10.897549629211426
2025-12-09 12:12:19.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00029995982590155367 Training loss: 10.999734878540039
2025-12-09 12:12:19.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00029995768375465164 Training loss: 11.184443473815918
2025-12-09 12:12:19.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0002999554859781698 Training loss: 11.201723098754883
2025-12-09 12:12:19.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.00029995323257292337 Training loss: 10.986186027526855
2025-12-09 12:12:19.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0002999509235397483 Training loss: 11.050108909606934
2025-12-09 12:12:19.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.00029994855887950124 Training loss: 10.881783485412598
2025-12-09 12:12:19.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.00029994613859305933 Training loss: 10.845906257629395
2025-12-09 12:12:19.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0002999436626813204 Training loss: 10.854023933410645
2025-12-09 12:12:19.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.000299941131145203 Training loss: 10.909793853759766
2025-12-09 12:12:19.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0002999385439856462 Training loss: 11.03523063659668
2025-12-09 12:12:19.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0002999359012036099 Training loss: 10.92375659942627
2025-12-09 12:12:19.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0002999332028000742 Training loss: 10.859685897827148
2025-12-09 12:12:19.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002999304487760404 Training loss: 10.895059585571289
2025-12-09 12:12:19.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.00029992763913253 Training loss: 10.930769920349121
2025-12-09 12:12:20.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00029992477387058537 Training loss: 10.947020530700684
2025-12-09 12:12:20.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0002999218529912694 Training loss: 10.8137788772583
2025-12-09 12:12:20.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00029991887649566564 Training loss: 10.848978042602539
2025-12-09 12:12:20.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.00029991584438487825 Training loss: 10.988801002502441
2025-12-09 12:12:20.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0002999127566600321 Training loss: 10.887813568115234
2025-12-09 12:12:20.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.00029990961332227264 Training loss: 10.871634483337402
2025-12-09 12:12:20.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002999064143727659 Training loss: 10.69404411315918
2025-12-09 12:12:20.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.00029990315981269863 Training loss: 10.740984916687012
2025-12-09 12:12:20.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0002998998496432781 Training loss: 10.675546646118164
2025-12-09 12:12:20.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0002998964838657324 Training loss: 10.884519577026367
2025-12-09 12:12:20.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0002998930624813101 Training loss: 10.918778419494629
2025-12-09 12:12:20.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.00029988958549128026 Training loss: 10.759039878845215
2025-12-09 12:12:20.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00029988605289693295 Training loss: 10.796222686767578
2025-12-09 12:12:21.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0002998824646995785 Training loss: 10.64499282836914
2025-12-09 12:12:21.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.00029987882090054817 Training loss: 10.710432052612305
2025-12-09 12:12:21.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0002998751215011935 Training loss: 10.7997407913208
2025-12-09 12:12:21.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.000299871366502887 Training loss: 10.889202117919922
2025-12-09 12:12:21.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00029986755590702164 Training loss: 10.75041675567627
2025-12-09 12:12:21.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.000299863689715011 Training loss: 10.558945655822754
2025-12-09 12:12:21.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0002998597679282893 Training loss: 10.674521446228027
2025-12-09 12:12:21.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00029985579054831146 Training loss: 10.646223068237305
2025-12-09 12:12:21.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0002998517575765528 Training loss: 10.642849922180176
2025-12-09 12:12:21.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00029984766901450965 Training loss: 10.690605163574219
2025-12-09 12:12:21.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.00029984352486369867 Training loss: 10.733278274536133
2025-12-09 12:12:21.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00029983932512565707 Training loss: 10.639750480651855
2025-12-09 12:12:21.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.00029983506980194296 Training loss: 10.741083145141602
2025-12-09 12:12:22.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00029983075889413493 Training loss: 10.731307983398438
2025-12-09 12:12:22.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00029982639240383214 Training loss: 10.6353178024292
2025-12-09 12:12:22.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.00029982197033265437 Training loss: 10.756240844726562
2025-12-09 12:12:22.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00029981749268224225 Training loss: 10.690491676330566
2025-12-09 12:12:22.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00029981295945425665 Training loss: 10.763368606567383
2025-12-09 12:12:22.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.00029980837065037935 Training loss: 10.591133117675781
2025-12-09 12:12:22.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.00029980372627231265 Training loss: 10.66025447845459
2025-12-09 12:12:22.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00029979902632177945 Training loss: 10.568802833557129
2025-12-09 12:12:22.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0002997942708005233 Training loss: 10.555302619934082
2025-12-09 12:12:22.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.00029978945971030835 Training loss: 10.575874328613281
2025-12-09 12:12:22.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0002997845930529194 Training loss: 10.393263816833496
2025-12-09 12:12:22.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.00029977967083016173 Training loss: 10.612215995788574
2025-12-09 12:12:22.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00029977469304386133 Training loss: 10.616180419921875
2025-12-09 12:12:23.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0002997696596958649 Training loss: 10.584135055541992
2025-12-09 12:12:23.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0002997645707880396 Training loss: 10.563016891479492
2025-12-09 12:12:23.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0002997594263222733 Training loss: 10.597294807434082
2025-12-09 12:12:23.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.00029975422630047435 Training loss: 10.400252342224121
2025-12-09 12:12:23.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.00029974897072457187 Training loss: 10.537753105163574
2025-12-09 12:12:23.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0002997436595965154 Training loss: 10.414833068847656
2025-12-09 12:12:23.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0002997382929182754 Training loss: 10.52497386932373
2025-12-09 12:12:23.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00029973287069184255 Training loss: 10.704992294311523
2025-12-09 12:12:23.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0002997273929192284 Training loss: 10.511930465698242
2025-12-09 12:12:23.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0002997218596024651 Training loss: 10.465620040893555
2025-12-09 12:12:23.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.00029971627074360516 Training loss: 10.624897003173828
2025-12-09 12:12:23.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00029971062634472203 Training loss: 10.503138542175293
2025-12-09 12:12:23.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00029970492640790956 Training loss: 10.516456604003906
2025-12-09 12:12:24.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0002996991709352822 Training loss: 10.519794464111328
2025-12-09 12:12:24.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0002996933599289751 Training loss: 10.436507225036621
2025-12-09 12:12:24.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.000299687493391144 Training loss: 10.560504913330078
2025-12-09 12:12:24.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00029968157132396507 Training loss: 10.587303161621094
2025-12-09 12:12:24.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00029967559372963534 Training loss: 10.498185157775879
2025-12-09 12:12:24.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00029966956061037227 Training loss: 10.473407745361328
2025-12-09 12:12:24.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.00029966347196841393 Training loss: 10.493012428283691
2025-12-09 12:12:24.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.000299657327806019 Training loss: 10.427425384521484
2025-12-09 12:12:24.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0002996511281254668 Training loss: 10.51142692565918
2025-12-09 12:12:24.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0002996448729290572 Training loss: 10.602248191833496
2025-12-09 12:12:24.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.00029963856221911075 Training loss: 10.661251068115234
2025-12-09 12:12:24.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00029963219599796843 Training loss: 10.451111793518066
2025-12-09 12:12:24.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.000299625774267992 Training loss: 10.5235595703125
2025-12-09 12:12:25.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0002996192970315636 Training loss: 10.297707557678223
2025-12-09 12:12:25.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00029961276429108625 Training loss: 10.476105690002441
2025-12-09 12:12:25.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00029960617604898323 Training loss: 10.514236450195312
2025-12-09 12:12:25.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0002995995323076986 Training loss: 10.462250709533691
2025-12-09 12:12:25.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.00029959283306969705 Training loss: 10.481025695800781
2025-12-09 12:12:25.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.00029958607833746375 Training loss: 10.427534103393555
2025-12-09 12:12:25.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0002995792681135045 Training loss: 10.417417526245117
2025-12-09 12:12:25.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.00029957240240034564 Training loss: 10.237824440002441
2025-12-09 12:12:25.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0002995654812005342 Training loss: 10.467877388000488
2025-12-09 12:12:25.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0002995585045166376 Training loss: 10.47498893737793
2025-12-09 12:12:25.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00029955147235124417 Training loss: 10.499985694885254
2025-12-09 12:12:25.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00029954438470696247 Training loss: 10.322284698486328
2025-12-09 12:12:25.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0002995372415864218 Training loss: 10.376261711120605
2025-12-09 12:12:26.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0002995300429922721 Training loss: 10.371529579162598
2025-12-09 12:12:26.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00029952278892718376 Training loss: 10.185587882995605
2025-12-09 12:12:26.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0002995154793938479 Training loss: 10.19900131225586
2025-12-09 12:12:26.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.00029950811439497606 Training loss: 10.306455612182617
2025-12-09 12:12:26.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0002995006939333004 Training loss: 10.31859302520752
2025-12-09 12:12:26.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.00029949321801157365 Training loss: 10.259644508361816
2025-12-09 12:12:26.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.00029948568663256927 Training loss: 10.445984840393066
2025-12-09 12:12:26.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0002994780997990811 Training loss: 10.184627532958984
2025-12-09 12:12:26.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0002994704575139236 Training loss: 10.365453720092773
2025-12-09 12:12:26.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00029946275977993175 Training loss: 10.350852966308594
2025-12-09 12:12:26.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0002994550065999613 Training loss: 10.315569877624512
2025-12-09 12:12:26.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0002994471979768884 Training loss: 10.25295639038086
2025-12-09 12:12:26.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00029943933391360974 Training loss: 10.281044006347656
2025-12-09 12:12:26.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00029943141441304274 Training loss: 10.13183879852295
2025-12-09 12:12:27.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.00029942343947812517 Training loss: 10.409323692321777
2025-12-09 12:12:27.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0002994154091118156 Training loss: 10.453907012939453
2025-12-09 12:12:27.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0002994073233170929 Training loss: 10.18792724609375
2025-12-09 12:12:27.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00029939918209695676 Training loss: 10.653705596923828
2025-12-09 12:12:27.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0002993909854544273 Training loss: 10.31118106842041
2025-12-09 12:12:27.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00029938273339254515 Training loss: 10.420845985412598
2025-12-09 12:12:27.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0002993744259143716 Training loss: 10.498724937438965
2025-12-09 12:12:27.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0002993660630229886 Training loss: 10.012089729309082
2025-12-09 12:12:27.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0002993576447214983 Training loss: 10.066705703735352
2025-12-09 12:12:27.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0002993491710130237 Training loss: 10.244161605834961
2025-12-09 12:12:27.835 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00029934064190070836 Training loss: 10.438501358032227
2025-12-09 12:12:27.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00029933205738771624 Training loss: 10.298535346984863
2025-12-09 12:12:27.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0002993234174772319 Training loss: 10.29333209991455
2025-12-09 12:12:28.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00029931472217246057 Training loss: 10.203672409057617
2025-12-09 12:12:28.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0002993059714766278 Training loss: 10.339699745178223
2025-12-09 12:12:28.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00029929716539297993 Training loss: 10.350663185119629
2025-12-09 12:12:28.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00029928830392478376 Training loss: 10.234733581542969
2025-12-09 12:12:28.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0002992793870753265 Training loss: 10.250096321105957
2025-12-09 12:12:28.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0002992704148479161 Training loss: 10.266512870788574
2025-12-09 12:12:28.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00029926138724588097 Training loss: 10.347145080566406
2025-12-09 12:12:28.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.00029925230427257004 Training loss: 10.302846908569336
2025-12-09 12:12:28.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0002992431659313528 Training loss: 10.316390037536621
2025-12-09 12:12:28.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.00029923397222561933 Training loss: 10.239485740661621
2025-12-09 12:12:28.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0002992247231587802 Training loss: 10.418526649475098
2025-12-09 12:12:28.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00029921541873426647 Training loss: 10.287590026855469
2025-12-09 12:12:28.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00029920605895552985 Training loss: 10.355920791625977
2025-12-09 12:12:29.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0002991966438260425 Training loss: 10.22227668762207
2025-12-09 12:12:29.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0002991871733492971 Training loss: 10.113396644592285
2025-12-09 12:12:29.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00029917764752880697 Training loss: 10.228078842163086
2025-12-09 12:12:29.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0002991680663681059 Training loss: 10.266761779785156
2025-12-09 12:12:29.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00029915842987074804 Training loss: 10.128660202026367
2025-12-09 12:12:29.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0002991487380403084 Training loss: 10.129972457885742
2025-12-09 12:12:29.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00029913899088038226 Training loss: 10.027502059936523
2025-12-09 12:12:29.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.00029912918839458555 Training loss: 10.312114715576172
2025-12-09 12:12:29.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00029911933058655464 Training loss: 10.211996078491211
2025-12-09 12:12:29.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.00029910941745994653 Training loss: 10.081042289733887
2025-12-09 12:12:29.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.00029909944901843863 Training loss: 10.312597274780273
2025-12-09 12:12:29.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0002990894252657289 Training loss: 10.43770980834961
2025-12-09 12:12:29.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0002990793462055359 Training loss: 10.140006065368652
2025-12-09 12:12:30.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0002990692118415986 Training loss: 10.010988235473633
2025-12-09 12:12:30.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002990590221776765 Training loss: 10.123008728027344
2025-12-09 12:12:30.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0002990487772175497 Training loss: 10.364123344421387
2025-12-09 12:12:30.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00029903847696501876 Training loss: 10.09558391571045
2025-12-09 12:12:30.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.00029902812142390474 Training loss: 10.15112018585205
2025-12-09 12:12:30.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0002990177105980492 Training loss: 10.392627716064453
2025-12-09 12:12:30.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.00029900724449131424 Training loss: 10.374015808105469
2025-12-09 12:12:30.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.00029899672310758243 Training loss: 10.128114700317383
2025-12-09 12:12:30.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0002989861464507569 Training loss: 10.19479751586914
2025-12-09 12:12:30.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0002989755145247613 Training loss: 10.092523574829102
2025-12-09 12:12:30.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.00029896482733353965 Training loss: 10.220696449279785
2025-12-09 12:12:30.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00029895408488105665 Training loss: 10.186626434326172
2025-12-09 12:12:30.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0002989432871712973 Training loss: 10.002731323242188
2025-12-09 12:12:31.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0002989324342082673 Training loss: 10.087414741516113
2025-12-09 12:12:31.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00029892152599599275 Training loss: 10.085741996765137
2025-12-09 12:12:31.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.00029891056253852026 Training loss: 10.335282325744629
2025-12-09 12:12:31.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0002988995438399169 Training loss: 9.97880744934082
2025-12-09 12:12:31.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0002988884699042702 Training loss: 10.162528991699219
2025-12-09 12:12:31.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002988773407356884 Training loss: 10.124772071838379
2025-12-09 12:12:31.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0002988661563382999 Training loss: 10.199373245239258
2025-12-09 12:12:31.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0002988549167162539 Training loss: 10.070443153381348
2025-12-09 12:12:31.645 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.00029884362187371986 Training loss: 10.056851387023926
2025-12-09 12:12:31.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0002988322718148878 Training loss: 10.41404914855957
2025-12-09 12:12:31.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0002988208665439683 Training loss: 10.091659545898438
2025-12-09 12:12:31.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0002988094060651923 Training loss: 10.053655624389648
2025-12-09 12:12:31.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0002987978903828114 Training loss: 10.519944190979004
2025-12-09 12:12:32.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.00029878631950109734 Training loss: 10.071942329406738
2025-12-09 12:12:32.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0002987746934243427 Training loss: 10.089301109313965
2025-12-09 12:12:32.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0002987630121568604 Training loss: 10.569732666015625
2025-12-09 12:12:32.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.00029875127570298376 Training loss: 10.064719200134277
2025-12-09 12:12:32.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0002987394840670666 Training loss: 10.22564697265625
2025-12-09 12:12:32.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0002987276372534834 Training loss: 10.127655029296875
2025-12-09 12:12:32.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0002987157352666288 Training loss: 10.191116333007812
2025-12-09 12:12:32.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0002987037781109182 Training loss: 10.253289222717285
2025-12-09 12:12:32.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00029869176579078714 Training loss: 10.065923690795898
2025-12-09 12:12:32.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.000298679698310692 Training loss: 10.13968563079834
2025-12-09 12:12:32.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00029866757567510927 Training loss: 10.406425476074219
2025-12-09 12:12:32.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0002986553978885362 Training loss: 10.073702812194824
2025-12-09 12:12:32.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00029864316495549037 Training loss: 10.137594223022461
2025-12-09 12:12:33.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0002986308768805097 Training loss: 10.142056465148926
2025-12-09 12:12:33.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00029861853366815275 Training loss: 10.282981872558594
2025-12-09 12:12:33.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00029860613532299845 Training loss: 10.221277236938477
2025-12-09 12:12:33.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00029859368184964624 Training loss: 10.363560676574707
2025-12-09 12:12:33.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00029858117325271585 Training loss: 10.196810722351074
2025-12-09 12:12:33.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.00029856860953684773 Training loss: 10.001590728759766
2025-12-09 12:12:33.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0002985559907067025 Training loss: 9.998418807983398
2025-12-09 12:12:33.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00029854331676696137 Training loss: 10.184745788574219
2025-12-09 12:12:33.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.000298530587722326 Training loss: 9.832910537719727
2025-12-09 12:12:33.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.00029851780357751853 Training loss: 10.1554536819458
2025-12-09 12:12:33.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00029850496433728136 Training loss: 10.126254081726074
2025-12-09 12:12:33.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0002984920700063775 Training loss: 10.014954566955566
2025-12-09 12:12:33.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.00029847912058959033 Training loss: 10.058953285217285
2025-12-09 12:12:34.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00029846611609172363 Training loss: 10.312658309936523
2025-12-09 12:12:34.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.00029845305651760175 Training loss: 9.693077087402344
2025-12-09 12:12:34.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00029843994187206933 Training loss: 10.272215843200684
2025-12-09 12:12:34.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0002984267721599915 Training loss: 10.254924774169922
2025-12-09 12:12:34.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0002984135473862538 Training loss: 9.990497589111328
2025-12-09 12:12:34.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0002984002675557622 Training loss: 10.104690551757812
2025-12-09 12:12:34.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0002983869326734432 Training loss: 10.012210845947266
2025-12-09 12:12:34.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0002983735427442434 Training loss: 9.90909194946289
2025-12-09 12:12:34.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.00029836009777313026 Training loss: 10.10096549987793
2025-12-09 12:12:34.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.00029834659776509134 Training loss: 10.039717674255371
2025-12-09 12:12:34.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0002983330427251347 Training loss: 9.980509757995605
2025-12-09 12:12:34.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0002983194326582889 Training loss: 10.009613037109375
2025-12-09 12:12:34.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0002983057675696028 Training loss: 10.258275985717773
2025-12-09 12:12:34.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0002982920474641457 Training loss: 10.014641761779785
2025-12-09 12:12:35.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0002982782723470074 Training loss: 10.181526184082031
2025-12-09 12:12:35.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.000298264442223298 Training loss: 9.843101501464844
2025-12-09 12:12:35.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00029825055709814795 Training loss: 9.840873718261719
2025-12-09 12:12:35.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00029823661697670834 Training loss: 10.035858154296875
2025-12-09 12:12:35.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00029822262186415046 Training loss: 9.899924278259277
2025-12-09 12:12:35.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00029820857176566606 Training loss: 10.096449851989746
2025-12-09 12:12:35.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0002981944666864672 Training loss: 10.167098999023438
2025-12-09 12:12:35.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0002981803066317865 Training loss: 9.953062057495117
2025-12-09 12:12:35.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00029816609160687697 Training loss: 10.209781646728516
2025-12-09 12:12:35.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0002981518216170118 Training loss: 9.977590560913086
2025-12-09 12:12:35.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002981374966674848 Training loss: 10.03300666809082
2025-12-09 12:12:35.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.00029812311676361003 Training loss: 9.98095417022705
2025-12-09 12:12:35.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00029810868191072195 Training loss: 10.115490913391113
2025-12-09 12:12:36.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.00029809419211417553 Training loss: 10.125734329223633
2025-12-09 12:12:36.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0002980796473793459 Training loss: 10.07544231414795
2025-12-09 12:12:36.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0002980650477116288 Training loss: 10.363219261169434
2025-12-09 12:12:36.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00029805039311644023 Training loss: 10.005763053894043
2025-12-09 12:12:36.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0002980356835992166 Training loss: 9.868036270141602
2025-12-09 12:12:36.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0002980209191654146 Training loss: 9.805890083312988
2025-12-09 12:12:36.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.00029800609982051147 Training loss: 10.297268867492676
2025-12-09 12:12:36.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0002979912255700046 Training loss: 10.076053619384766
2025-12-09 12:12:36.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.000297976296419412 Training loss: 10.030438423156738
2025-12-09 12:12:36.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00029796131237427186 Training loss: 9.951580047607422
2025-12-09 12:12:36.829 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00029794627344014276 Training loss: 9.992178916931152
2025-12-09 12:12:36.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.00029793117962260366 Training loss: 9.989237785339355
2025-12-09 12:12:36.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.000297916030927254 Training loss: 10.054617881774902
2025-12-09 12:12:37.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0002979008273597133 Training loss: 9.871161460876465
2025-12-09 12:12:37.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0002978855689256218 Training loss: 9.92967700958252
2025-12-09 12:12:37.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.00029787025563063975 Training loss: 9.8855619430542
2025-12-09 12:12:37.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.0002978548874804479 Training loss: 9.945246696472168
2025-12-09 12:12:37.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.0002978394644807475 Training loss: 10.086456298828125
2025-12-09 12:12:37.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0002978239866372598 Training loss: 10.188660621643066
2025-12-09 12:12:37.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.00029780845395572673 Training loss: 10.00741958618164
2025-12-09 12:12:37.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0002977928664419104 Training loss: 9.870856285095215
2025-12-09 12:12:37.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0002977772241015933 Training loss: 9.979585647583008
2025-12-09 12:12:37.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.00029776152694057815 Training loss: 9.759254455566406
2025-12-09 12:12:37.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0002977457749646882 Training loss: 10.191426277160645
2025-12-09 12:12:37.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.00029772996817976693 Training loss: 9.966717720031738
2025-12-09 12:12:37.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.00029771410659167806 Training loss: 10.116772651672363
2025-12-09 12:12:38.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.00029769819020630594 Training loss: 10.331995010375977
2025-12-09 12:12:38.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0002976822190295548 Training loss: 9.995214462280273
2025-12-09 12:12:38.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.00029766619306734963 Training loss: 9.823144912719727
2025-12-09 12:12:38.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0002976501123256355 Training loss: 10.073469161987305
2025-12-09 12:12:38.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.00029763397681037787 Training loss: 9.941010475158691
2025-12-09 12:12:38.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.00029761778652756245 Training loss: 9.906798362731934
2025-12-09 12:12:38.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.00029760154148319534 Training loss: 9.9848051071167
2025-12-09 12:12:38.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.000297585241683303 Training loss: 9.978148460388184
2025-12-09 12:12:38.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.00029756888713393213 Training loss: 10.149748802185059
2025-12-09 12:12:38.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.00029755247784114976 Training loss: 9.861799240112305
2025-12-09 12:12:38.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0002975360138110431 Training loss: 10.014798164367676
2025-12-09 12:12:38.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.00029751949504972 Training loss: 9.92918586730957
2025-12-09 12:12:38.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0002975029215633082 Training loss: 9.853584289550781
2025-12-09 12:12:39.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.000297486293357956 Training loss: 9.923810958862305
2025-12-09 12:12:39.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.00029746961043983206 Training loss: 10.318513870239258
2025-12-09 12:12:39.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00029745287281512505 Training loss: 10.069185256958008
2025-12-09 12:12:39.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0002974360804900442 Training loss: 10.094100952148438
2025-12-09 12:12:39.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0002974192334708189 Training loss: 9.943717002868652
2025-12-09 12:12:39.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.00029740233176369887 Training loss: 10.030463218688965
2025-12-09 12:12:39.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0002973853753749541 Training loss: 10.12269115447998
2025-12-09 12:12:39.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00029736836431087493 Training loss: 10.3635892868042
2025-12-09 12:12:39.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.00029735129857777183 Training loss: 10.018526077270508
2025-12-09 12:12:39.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.00029733417818197575 Training loss: 10.199953079223633
2025-12-09 12:12:39.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.00029731700312983776 Training loss: 9.83073616027832
2025-12-09 12:12:39.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0002972997734277293 Training loss: 9.901604652404785
2025-12-09 12:12:39.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.000297282489082042 Training loss: 9.795276641845703
2025-12-09 12:12:40.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00029726515009918786 Training loss: 9.964009284973145
2025-12-09 12:12:40.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0002972477564855991 Training loss: 9.914778709411621
2025-12-09 12:12:40.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0002972303082477281 Training loss: 9.893509864807129
2025-12-09 12:12:40.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.00029721280539204774 Training loss: 10.049592971801758
2025-12-09 12:12:40.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.0002971952479250509 Training loss: 9.93450927734375
2025-12-09 12:12:40.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.000297177635853251 Training loss: 9.93813705444336
2025-12-09 12:12:40.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0002971599691831815 Training loss: 9.88200855255127
2025-12-09 12:12:40.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00029714224792139605 Training loss: 10.116305351257324
2025-12-09 12:12:40.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0002971244720744688 Training loss: 9.770888328552246
2025-12-09 12:12:40.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00029710664164899413 Training loss: 10.063304901123047
2025-12-09 12:12:40.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0002970887566515864 Training loss: 9.988494873046875
2025-12-09 12:12:40.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0002970708170888804 Training loss: 10.046853065490723
2025-12-09 12:12:40.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0002970528229675312 Training loss: 9.766763687133789
2025-12-09 12:12:41.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0002970347742942141 Training loss: 9.468742370605469
2025-12-09 12:12:41.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0002970166710756244 Training loss: 9.677972793579102
2025-12-09 12:12:41.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0002969985133184781 Training loss: 9.823941230773926
2025-12-09 12:12:41.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0002969803010295109 Training loss: 10.363134384155273
2025-12-09 12:12:41.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0002969620342154791 Training loss: 10.021533966064453
2025-12-09 12:12:41.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0002969437128831591 Training loss: 10.266617774963379
2025-12-09 12:12:41.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00029692533703934757 Training loss: 9.945967674255371
2025-12-09 12:12:41.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.00029690690669086127 Training loss: 10.068029403686523
2025-12-09 12:12:41.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0002968884218445374 Training loss: 9.920591354370117
2025-12-09 12:12:41.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.0002968698825072332 Training loss: 9.987085342407227
2025-12-09 12:12:41.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0002968512886858262 Training loss: 10.015768051147461
2025-12-09 12:12:41.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.00029683264038721414 Training loss: 9.93222713470459
2025-12-09 12:12:41.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.00029681393761831485 Training loss: 10.135669708251953
2025-12-09 12:12:42.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0002967951803860665 Training loss: 10.057148933410645
2025-12-09 12:12:42.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0002967763686974276 Training loss: 10.234457015991211
2025-12-09 12:12:42.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.00029675750255937647 Training loss: 9.891582489013672
2025-12-09 12:12:42.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.000296738581978912 Training loss: 10.289884567260742
2025-12-09 12:12:42.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.00029671960696305304 Training loss: 9.81190013885498
2025-12-09 12:12:42.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.00029670057751883874 Training loss: 9.884832382202148
2025-12-09 12:12:42.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0002966814936533285 Training loss: 9.538235664367676
2025-12-09 12:12:42.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.00029666235537360175 Training loss: 9.729692459106445
2025-12-09 12:12:42.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.00029664316268675824 Training loss: 9.927912712097168
2025-12-09 12:12:42.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0002966239155999178 Training loss: 10.242748260498047
2025-12-09 12:12:42.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0002966046141202205 Training loss: 9.847628593444824
2025-12-09 12:12:42.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0002965852582548267 Training loss: 9.995469093322754
2025-12-09 12:12:42.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.00029656584801091663 Training loss: 10.101978302001953
2025-12-09 12:12:43.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.000296546383395691 Training loss: 9.898446083068848
2025-12-09 12:12:43.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00029652686441637054 Training loss: 9.984335899353027
2025-12-09 12:12:43.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.00029650729108019624 Training loss: 9.77021312713623
2025-12-09 12:12:43.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0002964876633944291 Training loss: 9.860706329345703
2025-12-09 12:12:43.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.00029646798136635034 Training loss: 9.695486068725586
2025-12-09 12:12:43.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.0002964482450032615 Training loss: 9.816991806030273
2025-12-09 12:12:43.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.00029642845431248406 Training loss: 9.984134674072266
2025-12-09 12:12:43.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0002964086093013597 Training loss: 9.827773094177246
2025-12-09 12:12:43.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.00029638870997725046 Training loss: 9.91956901550293
2025-12-09 12:12:43.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00029636875634753824 Training loss: 9.801152229309082
2025-12-09 12:12:43.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00029634874841962525 Training loss: 10.236054420471191
2025-12-09 12:12:43.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00029632868620093375 Training loss: 9.938858032226562
2025-12-09 12:12:43.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0002963085696989063 Training loss: 10.299359321594238
2025-12-09 12:12:44.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.00029628839892100535 Training loss: 9.836743354797363
2025-12-09 12:12:44.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00029626817387471365 Training loss: 9.88934326171875
2025-12-09 12:12:44.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.00029624789456753417 Training loss: 9.767144203186035
2025-12-09 12:12:44.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.00029622756100698976 Training loss: 9.736184120178223
2025-12-09 12:12:44.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.0002962071732006237 Training loss: 9.951154708862305
2025-12-09 12:12:44.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00029618673115599896 Training loss: 9.948554039001465
2025-12-09 12:12:44.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0002961662348806992 Training loss: 10.267561912536621
2025-12-09 12:12:44.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.00029614568438232766 Training loss: 9.946659088134766
2025-12-09 12:12:44.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.000296125079668508 Training loss: 9.903874397277832
2025-12-09 12:12:44.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.00029610442074688394 Training loss: 10.161697387695312
2025-12-09 12:12:44.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.00029608370762511935 Training loss: 9.820652961730957
2025-12-09 12:12:44.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.000296062940310898 Training loss: 9.987106323242188
2025-12-09 12:12:44.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.000296042118811924 Training loss: 10.04458999633789
2025-12-09 12:12:44.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0002960212431359215 Training loss: 10.10774040222168
2025-12-09 12:12:45.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00029600031329063463 Training loss: 9.849552154541016
2025-12-09 12:12:45.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0002959793292838277 Training loss: 10.078547477722168
2025-12-09 12:12:45.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0002959582911232853 Training loss: 10.314032554626465
2025-12-09 12:12:45.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.00029593719881681167 Training loss: 9.797734260559082
2025-12-09 12:12:45.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00029591605237223157 Training loss: 10.049421310424805
2025-12-09 12:12:45.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0002958948517973896 Training loss: 9.827144622802734
2025-12-09 12:12:45.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0002958735971001505 Training loss: 9.969112396240234
2025-12-09 12:12:45.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0002958522882883991 Training loss: 10.009438514709473
2025-12-09 12:12:45.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0002958309253700404 Training loss: 10.098771095275879
2025-12-09 12:12:45.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.00029580950835299914 Training loss: 10.135249137878418
2025-12-09 12:12:45.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0002957880372452206 Training loss: 9.835610389709473
2025-12-09 12:12:45.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0002957665120546697 Training loss: 9.854670524597168
2025-12-09 12:12:45.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0002957449327893317 Training loss: 9.565245628356934
2025-12-09 12:12:46.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.00029572329945721186 Training loss: 9.858476638793945
2025-12-09 12:12:46.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0002957016120663354 Training loss: 9.804207801818848
2025-12-09 12:12:46.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.00029567987062474767 Training loss: 9.80837631225586
2025-12-09 12:12:46.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00029565807514051406 Training loss: 9.8768892288208
2025-12-09 12:12:46.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0002956362256217201 Training loss: 9.888572692871094
2025-12-09 12:12:46.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0002956143220764711 Training loss: 10.090165138244629
2025-12-09 12:12:46.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.0002955923645128927 Training loss: 9.966164588928223
2025-12-09 12:12:46.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.00029557035293913044 Training loss: 9.788026809692383
2025-12-09 12:12:46.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.00029554828736334994 Training loss: 9.982218742370605
2025-12-09 12:12:46.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0002955261677937368 Training loss: 9.505640029907227
2025-12-09 12:12:46.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.00029550399423849673 Training loss: 10.004497528076172
2025-12-09 12:12:46.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0002954817667058554 Training loss: 9.748290061950684
2025-12-09 12:12:46.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00029545948520405844 Training loss: 9.883732795715332
2025-12-09 12:12:47.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00029543714974137177 Training loss: 9.696683883666992
2025-12-09 12:12:47.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.000295414760326081 Training loss: 9.841897964477539
2025-12-09 12:12:47.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.0002953923169664919 Training loss: 9.821382522583008
2025-12-09 12:12:47.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00029536981967093033 Training loss: 9.81634521484375
2025-12-09 12:12:47.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.00029534726844774196 Training loss: 10.38624382019043
2025-12-09 12:12:47.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.00029532466330529277 Training loss: 9.8976469039917
2025-12-09 12:12:47.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.00029530200425196835 Training loss: 9.84871768951416
2025-12-09 12:12:47.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.00029527929129617464 Training loss: 9.748302459716797
2025-12-09 12:12:47.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.00029525652444633736 Training loss: 9.9818754196167
2025-12-09 12:12:47.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0002952337037109023 Training loss: 10.00219440460205
2025-12-09 12:12:47.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.0002952108290983353 Training loss: 9.85032844543457
2025-12-09 12:12:47.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.00029518790061712204 Training loss: 9.651104927062988
2025-12-09 12:12:47.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0002951649182757683 Training loss: 9.948134422302246
2025-12-09 12:12:48.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.00029514188208279977 Training loss: 10.191503524780273
2025-12-09 12:12:48.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0002951187920467622 Training loss: 9.857748031616211
2025-12-09 12:12:48.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0002950956481762213 Training loss: 9.952203750610352
2025-12-09 12:12:48.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0002950724504797626 Training loss: 9.853635787963867
2025-12-09 12:12:48.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0002950491989659918 Training loss: 9.920557975769043
2025-12-09 12:12:48.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.00029502589364353447 Training loss: 9.877180099487305
2025-12-09 12:12:48.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.00029500253452103615 Training loss: 9.935100555419922
2025-12-09 12:12:48.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.00029497912160716234 Training loss: 9.898710250854492
2025-12-09 12:12:48.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0002949556549105985 Training loss: 9.926079750061035
2025-12-09 12:12:48.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.00029493213444005 Training loss: 9.792200088500977
2025-12-09 12:12:48.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0002949085602042422 Training loss: 9.881258010864258
2025-12-09 12:12:48.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.00029488493221192043 Training loss: 9.807999610900879
2025-12-09 12:12:48.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.00029486125047184985 Training loss: 9.941967010498047
2025-12-09 12:12:49.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0002948375149928158 Training loss: 9.583479881286621
2025-12-09 12:12:49.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0002948137257836233 Training loss: 9.707866668701172
2025-12-09 12:12:49.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0002947898828530974 Training loss: 9.867273330688477
2025-12-09 12:12:49.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.000294765986210083 Training loss: 9.869650840759277
2025-12-09 12:12:49.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0002947420358634451 Training loss: 9.806166648864746
2025-12-09 12:12:49.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.00029471803182206855 Training loss: 9.685612678527832
2025-12-09 12:12:49.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.000294693974094858 Training loss: 10.006243705749512
2025-12-09 12:12:49.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0002946698626907382 Training loss: 9.90804672241211
2025-12-09 12:12:49.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.00029464569761865366 Training loss: 9.563207626342773
2025-12-09 12:12:49.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.0002946214788875689 Training loss: 9.993609428405762
2025-12-09 12:12:49.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.00029459720650646824 Training loss: 9.710834503173828
2025-12-09 12:12:49.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.00029457288048435605 Training loss: 9.792154312133789
2025-12-09 12:12:49.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.00029454850083025644 Training loss: 9.924492835998535
2025-12-09 12:12:50.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0002945240675532136 Training loss: 9.689571380615234
2025-12-09 12:12:50.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0002944995806622914 Training loss: 9.779256820678711
2025-12-09 12:12:50.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0002944750401665738 Training loss: 9.86080551147461
2025-12-09 12:12:50.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00029445044607516447 Training loss: 9.224464416503906
2025-12-09 12:12:50.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.00029442579839718703 Training loss: 9.947527885437012
2025-12-09 12:12:50.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.0002944010971417851 Training loss: 9.575003623962402
2025-12-09 12:12:50.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.000294376342318122 Training loss: 9.543170928955078
2025-12-09 12:12:50.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.000294351533935381 Training loss: 9.789997100830078
2025-12-09 12:12:50.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.00029432667200276515 Training loss: 9.838831901550293
2025-12-09 12:12:50.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0002943017565294976 Training loss: 9.757540702819824
2025-12-09 12:12:50.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0002942767875248211 Training loss: 9.690091133117676
2025-12-09 12:12:50.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0002942517649979984 Training loss: 9.818951606750488
2025-12-09 12:12:50.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.00029422668895831203 Training loss: 9.934748649597168
2025-12-09 12:12:51.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00029420155941506447 Training loss: 9.890661239624023
2025-12-09 12:12:51.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00029417637637757797 Training loss: 9.758182525634766
2025-12-09 12:12:51.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.00029415113985519463 Training loss: 9.829696655273438
2025-12-09 12:12:51.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0002941258498572764 Training loss: 9.785337448120117
2025-12-09 12:12:51.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.0002941005063932051 Training loss: 9.910271644592285
2025-12-09 12:12:51.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0002940751094723823 Training loss: 9.80529499053955
2025-12-09 12:12:51.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.00029404965910422953 Training loss: 10.300970077514648
2025-12-09 12:12:51.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.00029402415529818804 Training loss: 9.089458465576172
2025-12-09 12:12:51.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.00029399859806371895 Training loss: 9.783394813537598
2025-12-09 12:12:51.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.0002939729874103032 Training loss: 9.731389999389648
2025-12-09 12:12:51.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.00029394732334744146 Training loss: 9.594527244567871
2025-12-09 12:12:51.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.00029392160588465434 Training loss: 9.653002738952637
2025-12-09 12:12:51.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0002938958350314823 Training loss: 9.690028190612793
2025-12-09 12:12:52.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.00029387001079748536 Training loss: 9.651650428771973
2025-12-09 12:12:52.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0002938441331922436 Training loss: 9.812296867370605
2025-12-09 12:12:52.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0002938182022253568 Training loss: 9.892922401428223
2025-12-09 12:12:52.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0002937922179064445 Training loss: 10.049694061279297
2025-12-09 12:12:52.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.000293766180245146 Training loss: 9.760680198669434
2025-12-09 12:12:52.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.00029374008925112056 Training loss: 9.901992797851562
2025-12-09 12:12:52.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.00029371394493404705 Training loss: 9.853615760803223
2025-12-09 12:12:52.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.00029368774730362425 Training loss: 9.653948783874512
2025-12-09 12:12:52.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0002936614963695706 Training loss: 10.02747917175293
2025-12-09 12:12:52.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0002936351921416244 Training loss: 10.05627727508545
2025-12-09 12:12:52.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0002936088346295436 Training loss: 9.689678192138672
2025-12-09 12:12:52.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0002935824238431062 Training loss: 10.062873840332031
2025-12-09 12:12:52.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0002935559597921096 Training loss: 9.825339317321777
2025-12-09 12:12:53.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.00029352944248637117 Training loss: 9.80571460723877
2025-12-09 12:12:53.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00029350287193572806 Training loss: 10.104342460632324
2025-12-09 12:12:53.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.000293476248150037 Training loss: 9.651091575622559
2025-12-09 12:12:53.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.00029344957113917473 Training loss: 9.656389236450195
2025-12-09 12:12:53.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0002934228409130374 Training loss: 9.684293746948242
2025-12-09 12:12:53.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0002933960574815412 Training loss: 10.036836624145508
2025-12-09 12:12:53.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0002933692208546219 Training loss: 9.849530220031738
2025-12-09 12:12:53.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.00029334233104223506 Training loss: 9.7698974609375
2025-12-09 12:12:53.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00029331538805435595 Training loss: 9.75167465209961
2025-12-09 12:12:53.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.00029328839190097955 Training loss: 9.707232475280762
2025-12-09 12:12:53.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.00029326134259212064 Training loss: 9.660272598266602
2025-12-09 12:12:53.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0002932342401378136 Training loss: 9.883474349975586
2025-12-09 12:12:53.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0002932070845481126 Training loss: 9.94295597076416
2025-12-09 12:12:54.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.00029317987583309156 Training loss: 10.057422637939453
2025-12-09 12:12:54.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.00029315261400284404 Training loss: 9.874070167541504
2025-12-09 12:12:54.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0002931252990674832 Training loss: 9.974655151367188
2025-12-09 12:12:54.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0002930979310371422 Training loss: 9.771224021911621
2025-12-09 12:12:54.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0002930705099219736 Training loss: 9.780450820922852
2025-12-09 12:12:54.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0002930430357321498 Training loss: 10.080830574035645
2025-12-09 12:12:54.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0002930155084778629 Training loss: 9.665655136108398
2025-12-09 12:12:54.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0002929879281693246 Training loss: 9.81897258758545
2025-12-09 12:12:54.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0002929602948167663 Training loss: 9.739415168762207
2025-12-09 12:12:54.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0002929326084304392 Training loss: 9.997560501098633
2025-12-09 12:12:54.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.00029290486902061396 Training loss: 9.633095741271973
2025-12-09 12:12:54.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0002928770765975811 Training loss: 9.498291969299316
2025-12-09 12:12:54.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.00029284923117165075 Training loss: 9.908316612243652
2025-12-09 12:12:55.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0002928213327531526 Training loss: 9.756298065185547
2025-12-09 12:12:55.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.00029279338135243624 Training loss: 9.812602043151855
2025-12-09 12:12:55.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0002927653769798706 Training loss: 10.01568603515625
2025-12-09 12:12:55.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.00029273731964584446 Training loss: 9.655289649963379
2025-12-09 12:12:55.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.00029270920936076624 Training loss: 9.578282356262207
2025-12-09 12:12:55.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.00029268104613506396 Training loss: 10.113606452941895
2025-12-09 12:12:55.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.00029265282997918533 Training loss: 9.981468200683594
2025-12-09 12:12:55.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00029262456090359756 Training loss: 9.714166641235352
2025-12-09 12:12:55.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0002925962389187877 Training loss: 9.97215747833252
2025-12-09 12:12:55.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0002925678640352622 Training loss: 9.930206298828125
2025-12-09 12:12:55.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.00029253943626354734 Training loss: 9.56727123260498
2025-12-09 12:12:55.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0002925109556141889 Training loss: 9.744378089904785
2025-12-09 12:12:55.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0002924824220977523 Training loss: 9.88378620147705
2025-12-09 12:12:55.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0002924538357248226 Training loss: 9.926321029663086
2025-12-09 12:12:56.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.00029242519650600436 Training loss: 9.984091758728027
2025-12-09 12:12:56.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.0002923965044519219 Training loss: 9.861323356628418
2025-12-09 12:12:56.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0002923677595732191 Training loss: 9.760300636291504
2025-12-09 12:12:56.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0002923389618805593 Training loss: 9.273086547851562
2025-12-09 12:12:56.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.00029231011138462564 Training loss: 9.985840797424316
2025-12-09 12:12:56.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0002922812080961207 Training loss: 9.662694931030273
2025-12-09 12:12:56.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0002922522520257667 Training loss: 10.052870750427246
2025-12-09 12:12:56.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0002922232431843054 Training loss: 9.62685775756836
2025-12-09 12:12:56.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.00029219418158249824 Training loss: 9.771490097045898
2025-12-09 12:12:56.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0002921650672311261 Training loss: 9.81940746307373
2025-12-09 12:12:56.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0002921359001409895 Training loss: 9.599472999572754
2025-12-09 12:12:56.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0002921066803229085 Training loss: 9.570069313049316
2025-12-09 12:12:56.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.00029207740778772277 Training loss: 9.819734573364258
2025-12-09 12:12:57.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.00029204808254629146 Training loss: 9.657517433166504
2025-12-09 12:12:57.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.00029201870460949326 Training loss: 9.659159660339355
2025-12-09 12:12:57.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.00029198927398822657 Training loss: 9.623983383178711
2025-12-09 12:12:57.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0002919597906934092 Training loss: 9.488609313964844
2025-12-09 12:12:57.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0002919302547359785 Training loss: 9.979863166809082
2025-12-09 12:12:57.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0002919006661268914 Training loss: 9.868131637573242
2025-12-09 12:12:57.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0002918710248771243 Training loss: 9.82279109954834
2025-12-09 12:12:57.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0002918413309976732 Training loss: 9.986673355102539
2025-12-09 12:12:57.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00029181158449955363 Training loss: 10.037049293518066
2025-12-09 12:12:57.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0002917817853938005 Training loss: 9.907958030700684
2025-12-09 12:12:57.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0002917519336914684 Training loss: 9.83513355255127
2025-12-09 12:12:57.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.00029172202940363145 Training loss: 10.07021713256836
2025-12-09 12:12:57.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0002916920725413831 Training loss: 10.268266677856445
2025-12-09 12:12:58.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.00029166206311583644 Training loss: 9.4661865234375
2025-12-09 12:12:58.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.000291632001138124 Training loss: 9.446307182312012
2025-12-09 12:12:58.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0002916018866193978 Training loss: 9.500348091125488
2025-12-09 12:12:58.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0002915717195708295 Training loss: 9.741212844848633
2025-12-09 12:12:58.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.00029154150000360995 Training loss: 9.790919303894043
2025-12-09 12:12:58.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.00029151122792894985 Training loss: 9.956823348999023
2025-12-09 12:12:58.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.000291480903358079 Training loss: 9.693615913391113
2025-12-09 12:12:58.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00029145052630224696 Training loss: 9.861384391784668
2025-12-09 12:12:58.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0002914200967727227 Training loss: 9.684662818908691
2025-12-09 12:12:58.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00029138961478079455 Training loss: 10.058381080627441
2025-12-09 12:12:58.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.00029135908033777033 Training loss: 9.797011375427246
2025-12-09 12:12:58.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.00029132849345497755 Training loss: 9.522604942321777
2025-12-09 12:12:58.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.00029129785414376276 Training loss: 9.818205833435059
2025-12-09 12:12:59.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.00029126716241549224 Training loss: 10.06199836730957
2025-12-09 12:12:59.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0002912364182815517 Training loss: 9.782411575317383
2025-12-09 12:12:59.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.00029120562175334624 Training loss: 9.577971458435059
2025-12-09 12:12:59.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0002911747728423004 Training loss: 9.799596786499023
2025-12-09 12:12:59.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.00029114387155985814 Training loss: 9.767776489257812
2025-12-09 12:12:59.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0002911129179174828 Training loss: 9.701967239379883
2025-12-09 12:12:59.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.00029108191192665734 Training loss: 9.80141830444336
2025-12-09 12:12:59.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.00029105085359888396 Training loss: 9.669544219970703
2025-12-09 12:12:59.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.00029101974294568425 Training loss: 9.750006675720215
2025-12-09 12:12:59.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0002909885799785993 Training loss: 9.937758445739746
2025-12-09 12:12:59.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0002909573647091897 Training loss: 9.842612266540527
2025-12-09 12:12:59.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.00029092609714903523 Training loss: 9.728053092956543
2025-12-09 12:12:59.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.00029089477730973517 Training loss: 9.488540649414062
2025-12-09 12:13:00.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.0002908634052029083 Training loss: 9.722572326660156
2025-12-09 12:13:00.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0002908319808401925 Training loss: 9.859066009521484
2025-12-09 12:13:00.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0002908005042332454 Training loss: 9.803285598754883
2025-12-09 12:13:00.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00029076897539374375 Training loss: 9.846803665161133
2025-12-09 12:13:00.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.00029073739433338377 Training loss: 9.69784927368164
2025-12-09 12:13:00.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.000290705761063881 Training loss: 9.978041648864746
2025-12-09 12:13:00.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.00029067407559697046 Training loss: 9.738272666931152
2025-12-09 12:13:00.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0002906423379444063 Training loss: 9.781319618225098
2025-12-09 12:13:00.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.00029061054811796243 Training loss: 9.56656551361084
2025-12-09 12:13:00.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0002905787061294317 Training loss: 9.937498092651367
2025-12-09 12:13:00.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.00029054681199062657 Training loss: 9.617608070373535
2025-12-09 12:13:00.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.00029051486571337877 Training loss: 9.897047996520996
2025-12-09 12:13:00.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.00029048286730953924 Training loss: 9.715890884399414
2025-12-09 12:13:01.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0002904508167909785 Training loss: 9.724297523498535
2025-12-09 12:13:01.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.00029041871416958623 Training loss: 9.602107048034668
2025-12-09 12:13:01.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00029038655945727153 Training loss: 9.809417724609375
2025-12-09 12:13:01.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0002903543526659628 Training loss: 9.62078857421875
2025-12-09 12:13:01.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00029032209380760765 Training loss: 9.774953842163086
2025-12-09 12:13:01.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0002902897828941732 Training loss: 9.62644100189209
2025-12-09 12:13:01.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0002902574199376457 Training loss: 9.671622276306152
2025-12-09 12:13:01.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.00029022500495003086 Training loss: 9.720191955566406
2025-12-09 12:13:01.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0002901925379433536 Training loss: 9.794181823730469
2025-12-09 12:13:01.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0002901600189296581 Training loss: 9.782747268676758
2025-12-09 12:13:01.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.000290127447921008 Training loss: 10.346933364868164
2025-12-09 12:13:01.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.00029009482492948607 Training loss: 9.684854507446289
2025-12-09 12:13:01.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.00029006214996719437 Training loss: 9.732986450195312
2025-12-09 12:13:02.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0002900294230462543 Training loss: 9.544829368591309
2025-12-09 12:13:02.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.00028999664417880654 Training loss: 9.867680549621582
2025-12-09 12:13:02.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.000289963813377011 Training loss: 9.834282875061035
2025-12-09 12:13:02.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0002899309306530469 Training loss: 9.519745826721191
2025-12-09 12:13:02.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.0002898979960191127 Training loss: 9.617898941040039
2025-12-09 12:13:02.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.000289865009487426 Training loss: 9.513800621032715
2025-12-09 12:13:02.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00028983197107022396 Training loss: 9.693925857543945
2025-12-09 12:13:02.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0002897988807797627 Training loss: 9.574214935302734
2025-12-09 12:13:02.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.00028976573862831757 Training loss: 9.774344444274902
2025-12-09 12:13:02.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0002897325446281834 Training loss: 9.882477760314941
2025-12-09 12:13:02.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0002896992987916741 Training loss: 9.730926513671875
2025-12-09 12:13:02.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.00028966600113112276 Training loss: 9.746660232543945
2025-12-09 12:13:02.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.00028963265165888187 Training loss: 9.818414688110352
2025-12-09 12:13:03.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.00028959925038732294 Training loss: 9.677484512329102
2025-12-09 12:13:03.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.00028956579732883684 Training loss: 9.705652236938477
2025-12-09 12:13:03.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.00028953229249583355 Training loss: 9.65112018585205
2025-12-09 12:13:03.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.0002894987359007424 Training loss: 9.88802433013916
2025-12-09 12:13:03.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.00028946512755601174 Training loss: 9.226844787597656
2025-12-09 12:13:03.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0002894314674741092 Training loss: 9.774798393249512
2025-12-09 12:13:03.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.00028939775566752177 Training loss: 9.191145896911621
2025-12-09 12:13:03.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.00028936399214875524 Training loss: 9.785577774047852
2025-12-09 12:13:03.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.000289330176930335 Training loss: 9.335333824157715
2025-12-09 12:13:03.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.0002892963100248053 Training loss: 9.829625129699707
2025-12-09 12:13:03.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0002892623914447298 Training loss: 9.566803932189941
2025-12-09 12:13:03.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.00028922842120269115 Training loss: 9.647436141967773
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.36 GiB is free. Including non-PyTorch memory, this process has 90.83 GiB memory in use. Of the allocated memory 89.58 GiB is allocated by PyTorch, and 500.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:08, 146.29it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 175.68it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:21, 121.90it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:10, 141.27it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:57, 173.47it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 213.95it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:53, 185.79it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 184.86it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 197.86it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 203.06it/s]Tokenizing texts:   2%|▏         | 241/10000 [00:01<00:47, 205.80it/s]Tokenizing texts:   3%|▎         | 263/10000 [00:01<00:47, 204.95it/s]Tokenizing texts:   3%|▎         | 292/10000 [00:01<00:42, 228.65it/s]Tokenizing texts:   3%|▎         | 316/10000 [00:01<00:42, 228.98it/s]Tokenizing texts:   3%|▎         | 340/10000 [00:01<00:52, 183.94it/s]Tokenizing texts:   4%|▎         | 364/10000 [00:01<00:48, 197.70it/s]Tokenizing texts:   4%|▍         | 386/10000 [00:02<00:52, 184.22it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 186.71it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:51, 186.06it/s]Tokenizing texts:   4%|▍         | 446/10000 [00:02<00:50, 187.68it/s]Tokenizing texts:   5%|▍         | 468/10000 [00:02<00:48, 196.12it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 218.15it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:39, 237.02it/s]Tokenizing texts:   6%|▌         | 550/10000 [00:02<00:50, 186.51it/s]Tokenizing texts:   6%|▌         | 580/10000 [00:02<00:44, 212.31it/s]Tokenizing texts:   6%|▌         | 604/10000 [00:03<00:44, 213.33it/s]Tokenizing texts:   6%|▋         | 627/10000 [00:03<00:46, 200.36it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 202.91it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 200.56it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 191.97it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 201.54it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:45, 201.39it/s]Tokenizing texts:   8%|▊         | 767/10000 [00:03<00:43, 214.32it/s]Tokenizing texts:   8%|▊         | 789/10000 [00:04<00:45, 204.29it/s]Tokenizing texts:   8%|▊         | 813/10000 [00:04<00:43, 212.61it/s]Tokenizing texts:   8%|▊         | 835/10000 [00:04<00:42, 213.16it/s]Tokenizing texts:   9%|▊         | 862/10000 [00:04<00:40, 225.65it/s]Tokenizing texts:   9%|▉         | 885/10000 [00:04<00:41, 219.05it/s]Tokenizing texts:   9%|▉         | 911/10000 [00:04<00:39, 227.77it/s]Tokenizing texts:   9%|▉         | 943/10000 [00:04<00:35, 253.91it/s]Tokenizing texts:  10%|▉         | 974/10000 [00:04<00:34, 264.47it/s]Tokenizing texts:  10%|█         | 1004/10000 [00:04<00:32, 272.77it/s]Tokenizing texts:  10%|█         | 1036/10000 [00:04<00:31, 285.71it/s]Tokenizing texts:  11%|█         | 1065/10000 [00:05<00:39, 223.78it/s]Tokenizing texts:  11%|█         | 1090/10000 [00:05<00:40, 219.75it/s]Tokenizing texts:  11%|█         | 1116/10000 [00:05<00:38, 229.15it/s]Tokenizing texts:  11%|█▏        | 1141/10000 [00:05<00:44, 197.46it/s]Tokenizing texts:  12%|█▏        | 1165/10000 [00:05<00:43, 201.97it/s]Tokenizing texts:  12%|█▏        | 1193/10000 [00:05<00:41, 214.61it/s]Tokenizing texts:  12%|█▏        | 1216/10000 [00:05<00:40, 217.36it/s]Tokenizing texts:  12%|█▏        | 1239/10000 [00:05<00:41, 213.27it/s]Tokenizing texts:  13%|█▎        | 1261/10000 [00:06<00:41, 209.98it/s]Tokenizing texts:  13%|█▎        | 1296/10000 [00:06<00:35, 247.60it/s]Tokenizing texts:  13%|█▎        | 1322/10000 [00:06<00:39, 217.12it/s]Tokenizing texts:  13%|█▎        | 1345/10000 [00:06<00:42, 205.05it/s]Tokenizing texts:  14%|█▎        | 1370/10000 [00:06<00:39, 215.76it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:39, 219.48it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 237.04it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:33, 254.20it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:06<00:30, 277.30it/s]Tokenizing texts:  15%|█▌        | 1517/10000 [00:07<00:31, 270.67it/s]Tokenizing texts:  15%|█▌        | 1545/10000 [00:07<00:33, 254.59it/s]Tokenizing texts:  16%|█▌        | 1578/10000 [00:07<00:30, 274.93it/s]Tokenizing texts:  16%|█▌        | 1606/10000 [00:07<00:33, 251.98it/s]Tokenizing texts:  16%|█▋        | 1632/10000 [00:07<00:40, 207.48it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 215.80it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 217.30it/s]Tokenizing texts:  17%|█▋        | 1704/10000 [00:07<00:37, 222.82it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:33, 244.79it/s]Tokenizing texts:  18%|█▊        | 1761/10000 [00:08<00:33, 245.30it/s]Tokenizing texts:  18%|█▊        | 1787/10000 [00:08<00:33, 248.49it/s]Tokenizing texts:  18%|█▊        | 1821/10000 [00:08<00:30, 268.37it/s]Tokenizing texts:  18%|█▊        | 1849/10000 [00:08<00:37, 220.12it/s]Tokenizing texts:  19%|█▊        | 1873/10000 [00:08<00:36, 220.92it/s]Tokenizing texts:  19%|█▉        | 1898/10000 [00:08<00:35, 227.69it/s]Tokenizing texts:  19%|█▉        | 1922/10000 [00:08<00:35, 228.94it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 194.32it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:38, 210.37it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:35, 228.09it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 233.60it/s]Tokenizing texts:  21%|██        | 2055/10000 [00:09<00:32, 247.84it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:38, 208.39it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:37, 209.07it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:36, 214.09it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:09<00:38, 204.72it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:35, 222.21it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:31, 250.47it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:29, 260.53it/s]Tokenizing texts:  23%|██▎       | 2275/10000 [00:10<00:28, 270.03it/s]Tokenizing texts:  23%|██▎       | 2303/10000 [00:10<00:29, 257.83it/s]Tokenizing texts:  23%|██▎       | 2330/10000 [00:10<00:36, 209.57it/s]Tokenizing texts:  24%|██▎       | 2369/10000 [00:10<00:30, 252.71it/s]Tokenizing texts:  24%|██▍       | 2401/10000 [00:10<00:28, 266.83it/s]Tokenizing texts:  24%|██▍       | 2430/10000 [00:11<00:31, 241.56it/s]Tokenizing texts:  25%|██▍       | 2461/10000 [00:11<00:29, 258.46it/s]Tokenizing texts:  25%|██▍       | 2489/10000 [00:11<00:30, 249.19it/s]Tokenizing texts:  25%|██▌       | 2515/10000 [00:11<00:29, 250.46it/s]Tokenizing texts:  25%|██▌       | 2544/10000 [00:11<00:28, 259.84it/s]Tokenizing texts:  26%|██▌       | 2571/10000 [00:11<00:32, 227.19it/s]Tokenizing texts:  26%|██▌       | 2595/10000 [00:11<00:34, 215.56it/s]Tokenizing texts:  26%|██▌       | 2618/10000 [00:11<00:35, 206.17it/s]Tokenizing texts:  26%|██▋       | 2646/10000 [00:11<00:32, 224.66it/s]Tokenizing texts:  27%|██▋       | 2670/10000 [00:12<00:32, 227.04it/s]Tokenizing texts:  27%|██▋       | 2694/10000 [00:12<00:36, 197.78it/s]Tokenizing texts:  27%|██▋       | 2717/10000 [00:12<00:35, 205.71it/s]Tokenizing texts:  28%|██▊       | 2757/10000 [00:12<00:28, 252.87it/s]Tokenizing texts:  28%|██▊       | 2784/10000 [00:12<00:30, 238.99it/s]Tokenizing texts:  28%|██▊       | 2809/10000 [00:12<00:37, 190.94it/s]Tokenizing texts:  28%|██▊       | 2841/10000 [00:12<00:32, 218.61it/s]Tokenizing texts:  29%|██▊       | 2865/10000 [00:13<00:35, 201.88it/s]Tokenizing texts:  29%|██▉       | 2893/10000 [00:13<00:32, 220.62it/s]Tokenizing texts:  29%|██▉       | 2924/10000 [00:13<00:29, 237.10it/s]Tokenizing texts:  29%|██▉       | 2949/10000 [00:13<00:31, 226.68it/s]Tokenizing texts:  30%|██▉       | 2973/10000 [00:13<00:31, 224.67it/s]Tokenizing texts:  30%|███       | 3012/10000 [00:13<00:26, 267.53it/s]Tokenizing texts:  30%|███       | 3040/10000 [00:13<00:30, 229.11it/s]Tokenizing texts:  31%|███       | 3074/10000 [00:13<00:27, 254.98it/s]Tokenizing texts:  31%|███       | 3102/10000 [00:13<00:28, 244.70it/s]Tokenizing texts:  31%|███▏      | 3136/10000 [00:14<00:25, 268.31it/s]Tokenizing texts:  32%|███▏      | 3164/10000 [00:14<00:26, 254.17it/s]Tokenizing texts:  32%|███▏      | 3192/10000 [00:14<00:26, 260.34it/s]Tokenizing texts:  32%|███▏      | 3220/10000 [00:14<00:25, 264.27it/s]Tokenizing texts:  32%|███▏      | 3247/10000 [00:14<00:26, 257.00it/s]Tokenizing texts:  33%|███▎      | 3278/10000 [00:14<00:25, 268.30it/s]Tokenizing texts:  33%|███▎      | 3306/10000 [00:14<00:25, 261.40it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:14<00:26, 249.56it/s]Tokenizing texts:  34%|███▎      | 3362/10000 [00:14<00:25, 259.84it/s]Tokenizing texts:  34%|███▍      | 3389/10000 [00:15<00:26, 250.60it/s]Tokenizing texts:  34%|███▍      | 3416/10000 [00:15<00:25, 255.79it/s]Tokenizing texts:  35%|███▍      | 3457/10000 [00:15<00:21, 298.84it/s]Tokenizing texts:  35%|███▍      | 3488/10000 [00:15<00:22, 293.22it/s]Tokenizing texts:  35%|███▌      | 3518/10000 [00:15<00:26, 248.41it/s]Tokenizing texts:  35%|███▌      | 3545/10000 [00:15<00:27, 232.74it/s]Tokenizing texts:  36%|███▌      | 3574/10000 [00:15<00:26, 245.47it/s]Tokenizing texts:  36%|███▌      | 3602/10000 [00:15<00:25, 249.80it/s]Tokenizing texts:  36%|███▋      | 3632/10000 [00:15<00:24, 262.75it/s]Tokenizing texts:  37%|███▋      | 3659/10000 [00:16<00:25, 252.69it/s]Tokenizing texts:  37%|███▋      | 3685/10000 [00:16<00:25, 250.38it/s]Tokenizing texts:  37%|███▋      | 3711/10000 [00:16<00:24, 251.82it/s]Tokenizing texts:  37%|███▋      | 3737/10000 [00:16<00:25, 249.32it/s]Tokenizing texts:  38%|███▊      | 3763/10000 [00:16<00:27, 224.42it/s]Tokenizing texts:  38%|███▊      | 3789/10000 [00:16<00:26, 232.00it/s]Tokenizing texts:  38%|███▊      | 3813/10000 [00:16<00:29, 210.09it/s]Tokenizing texts:  38%|███▊      | 3842/10000 [00:16<00:26, 230.24it/s]Tokenizing texts:  39%|███▊      | 3870/10000 [00:17<00:25, 241.23it/s]Tokenizing texts:  39%|███▉      | 3896/10000 [00:17<00:25, 238.92it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:25, 238.02it/s]Tokenizing texts:  39%|███▉      | 3946/10000 [00:17<00:26, 231.78it/s]Tokenizing texts:  40%|███▉      | 3979/10000 [00:17<00:23, 258.71it/s]Tokenizing texts:  40%|████      | 4006/10000 [00:17<00:27, 220.35it/s]Tokenizing texts:  40%|████      | 4037/10000 [00:17<00:24, 238.54it/s]Tokenizing texts:  41%|████      | 4067/10000 [00:17<00:23, 254.52it/s]Tokenizing texts:  41%|████      | 4099/10000 [00:17<00:21, 272.28it/s]Tokenizing texts:  41%|████▏     | 4132/10000 [00:18<00:20, 287.85it/s]Tokenizing texts:  42%|████▏     | 4162/10000 [00:18<00:21, 272.25it/s]Tokenizing texts:  42%|████▏     | 4195/10000 [00:18<00:20, 285.79it/s]Tokenizing texts:  42%|████▏     | 4225/10000 [00:18<00:22, 260.85it/s]Tokenizing texts:  43%|████▎     | 4252/10000 [00:18<00:23, 246.92it/s]Tokenizing texts:  43%|████▎     | 4278/10000 [00:18<00:24, 236.89it/s]Tokenizing texts:  43%|████▎     | 4310/10000 [00:18<00:22, 258.02it/s]Tokenizing texts:  43%|████▎     | 4345/10000 [00:18<00:20, 280.72it/s]Tokenizing texts:  44%|████▎     | 4374/10000 [00:18<00:21, 263.48it/s]Tokenizing texts:  44%|████▍     | 4412/10000 [00:19<00:19, 292.33it/s]Tokenizing texts:  44%|████▍     | 4442/10000 [00:19<00:19, 281.21it/s]Tokenizing texts:  45%|████▍     | 4471/10000 [00:19<00:21, 257.44it/s]Tokenizing texts:  45%|████▍     | 4498/10000 [00:19<00:22, 241.94it/s]Tokenizing texts:  45%|████▌     | 4525/10000 [00:19<00:22, 248.79it/s]Tokenizing texts:  46%|████▌     | 4552/10000 [00:19<00:21, 253.78it/s]Tokenizing texts:  46%|████▌     | 4578/10000 [00:19<00:21, 251.11it/s]Tokenizing texts:  46%|████▌     | 4604/10000 [00:19<00:22, 239.71it/s]Tokenizing texts:  46%|████▋     | 4637/10000 [00:19<00:20, 263.08it/s]Tokenizing texts:  47%|████▋     | 4671/10000 [00:20<00:18, 283.56it/s]Tokenizing texts:  47%|████▋     | 4708/10000 [00:20<00:17, 306.22it/s]Tokenizing texts:  47%|████▋     | 4739/10000 [00:20<00:17, 292.83it/s]Tokenizing texts:  48%|████▊     | 4769/10000 [00:20<00:18, 282.45it/s]Tokenizing texts:  48%|████▊     | 4798/10000 [00:20<00:21, 245.04it/s]Tokenizing texts:  48%|████▊     | 4830/10000 [00:20<00:19, 261.76it/s]Tokenizing texts:  49%|████▊     | 4859/10000 [00:20<00:19, 267.00it/s]Tokenizing texts:  49%|████▉     | 4887/10000 [00:20<00:19, 268.50it/s]Tokenizing texts:  49%|████▉     | 4916/10000 [00:20<00:18, 273.09it/s]Tokenizing texts:  49%|████▉     | 4944/10000 [00:21<00:18, 271.24it/s]Tokenizing texts:  50%|████▉     | 4972/10000 [00:21<00:19, 261.63it/s]Tokenizing texts:  50%|█████     | 5011/10000 [00:21<00:17, 283.43it/s]Tokenizing texts:  50%|█████     | 5040/10000 [00:21<00:17, 280.87it/s]Tokenizing texts:  51%|█████     | 5069/10000 [00:21<00:19, 246.82it/s]Tokenizing texts:  51%|█████     | 5095/10000 [00:21<00:20, 243.55it/s]Tokenizing texts:  51%|█████     | 5122/10000 [00:21<00:20, 234.62it/s]Tokenizing texts:  52%|█████▏    | 5150/10000 [00:21<00:19, 244.56it/s]Tokenizing texts:  52%|█████▏    | 5178/10000 [00:22<00:19, 253.67it/s]Tokenizing texts:  52%|█████▏    | 5204/10000 [00:22<00:20, 238.62it/s]Tokenizing texts:  52%|█████▏    | 5231/10000 [00:22<00:19, 242.33it/s]Tokenizing texts:  53%|█████▎    | 5265/10000 [00:22<00:17, 266.19it/s]Tokenizing texts:  53%|█████▎    | 5292/10000 [00:22<00:18, 253.77it/s]Tokenizing texts:  53%|█████▎    | 5328/10000 [00:22<00:16, 280.31it/s]Tokenizing texts:  54%|█████▎    | 5361/10000 [00:22<00:15, 291.84it/s]Tokenizing texts:  54%|█████▍    | 5391/10000 [00:22<00:15, 293.04it/s]Tokenizing texts:  54%|█████▍    | 5424/10000 [00:22<00:15, 303.34it/s]Tokenizing texts:  55%|█████▍    | 5455/10000 [00:23<00:16, 274.47it/s]Tokenizing texts:  55%|█████▍    | 5484/10000 [00:23<00:17, 260.50it/s]Tokenizing texts:  55%|█████▌    | 5511/10000 [00:23<00:20, 218.06it/s]Tokenizing texts:  55%|█████▌    | 5542/10000 [00:23<00:18, 238.60it/s]Tokenizing texts:  56%|█████▌    | 5570/10000 [00:23<00:18, 241.09it/s]Tokenizing texts:  56%|█████▌    | 5600/10000 [00:23<00:17, 256.37it/s]Tokenizing texts:  56%|█████▋    | 5636/10000 [00:23<00:15, 282.86it/s]Tokenizing texts:  57%|█████▋    | 5672/10000 [00:23<00:14, 298.24it/s]Tokenizing texts:  57%|█████▋    | 5703/10000 [00:23<00:14, 298.56it/s]Tokenizing texts:  57%|█████▋    | 5734/10000 [00:24<00:14, 285.42it/s]Tokenizing texts:  58%|█████▊    | 5763/10000 [00:24<00:16, 259.44it/s]Tokenizing texts:  58%|█████▊    | 5790/10000 [00:24<00:19, 220.64it/s]Tokenizing texts:  58%|█████▊    | 5814/10000 [00:24<00:19, 218.46it/s]Tokenizing texts:  58%|█████▊    | 5837/10000 [00:24<00:19, 214.90it/s]Tokenizing texts:  59%|█████▊    | 5874/10000 [00:24<00:17, 239.68it/s]Tokenizing texts:  59%|█████▉    | 5899/10000 [00:24<00:16, 242.18it/s]Tokenizing texts:  59%|█████▉    | 5924/10000 [00:24<00:17, 239.58it/s]Tokenizing texts:  60%|█████▉    | 5952/10000 [00:25<00:16, 249.59it/s]Tokenizing texts:  60%|█████▉    | 5980/10000 [00:25<00:15, 255.08it/s]Tokenizing texts:  60%|██████    | 6008/10000 [00:25<00:15, 252.21it/s]Tokenizing texts:  60%|██████    | 6034/10000 [00:25<00:15, 251.95it/s]Tokenizing texts:  61%|██████    | 6066/10000 [00:25<00:14, 264.46it/s]Tokenizing texts:  61%|██████    | 6093/10000 [00:25<00:14, 261.75it/s]Tokenizing texts:  61%|██████    | 6120/10000 [00:25<00:14, 262.05it/s]Tokenizing texts:  62%|██████▏   | 6154/10000 [00:25<00:13, 279.47it/s]Tokenizing texts:  62%|██████▏   | 6182/10000 [00:25<00:13, 277.59it/s]Tokenizing texts:  62%|██████▏   | 6210/10000 [00:26<00:13, 276.84it/s]Tokenizing texts:  62%|██████▏   | 6238/10000 [00:26<00:15, 249.68it/s]Tokenizing texts:  63%|██████▎   | 6264/10000 [00:26<00:15, 245.80it/s]Tokenizing texts:  63%|██████▎   | 6290/10000 [00:26<00:14, 248.75it/s]Tokenizing texts:  63%|██████▎   | 6316/10000 [00:26<00:15, 234.66it/s]Tokenizing texts:  63%|██████▎   | 6340/10000 [00:26<00:15, 229.70it/s]Tokenizing texts:  64%|██████▎   | 6368/10000 [00:26<00:14, 242.20it/s]Tokenizing texts:  64%|██████▍   | 6395/10000 [00:26<00:14, 242.20it/s]Tokenizing texts:  64%|██████▍   | 6423/10000 [00:26<00:14, 252.39it/s]Tokenizing texts:  65%|██████▍   | 6453/10000 [00:27<00:13, 263.60it/s]Tokenizing texts:  65%|██████▍   | 6480/10000 [00:27<00:14, 240.52it/s]Tokenizing texts:  65%|██████▌   | 6506/10000 [00:27<00:15, 231.60it/s]Tokenizing texts:  65%|██████▌   | 6531/10000 [00:27<00:14, 235.80it/s]Tokenizing texts:  66%|██████▌   | 6572/10000 [00:27<00:12, 283.12it/s]Tokenizing texts:  66%|██████▌   | 6601/10000 [00:27<00:12, 269.60it/s]Tokenizing texts:  66%|██████▋   | 6630/10000 [00:27<00:12, 264.88it/s]Tokenizing texts:  67%|██████▋   | 6658/10000 [00:27<00:12, 267.53it/s]Tokenizing texts:  67%|██████▋   | 6686/10000 [00:27<00:12, 264.98it/s]Tokenizing texts:  67%|██████▋   | 6713/10000 [00:28<00:12, 253.87it/s]Tokenizing texts:  67%|██████▋   | 6739/10000 [00:28<00:13, 245.70it/s]Tokenizing texts:  68%|██████▊   | 6770/10000 [00:28<00:12, 262.85it/s]Tokenizing texts:  68%|██████▊   | 6804/10000 [00:28<00:11, 284.15it/s]Tokenizing texts:  68%|██████▊   | 6837/10000 [00:28<00:10, 292.42it/s]Tokenizing texts:  69%|██████▊   | 6869/10000 [00:28<00:10, 288.27it/s]Tokenizing texts:  69%|██████▉   | 6898/10000 [00:28<00:13, 235.47it/s]Tokenizing texts:  69%|██████▉   | 6924/10000 [00:28<00:13, 230.03it/s]Tokenizing texts:  69%|██████▉   | 6949/10000 [00:28<00:13, 233.45it/s]Tokenizing texts:  70%|██████▉   | 6977/10000 [00:29<00:12, 244.92it/s]Tokenizing texts:  70%|███████   | 7005/10000 [00:29<00:11, 253.29it/s]Tokenizing texts:  70%|███████   | 7031/10000 [00:29<00:11, 248.02it/s]Tokenizing texts:  71%|███████   | 7057/10000 [00:29<00:12, 235.02it/s]Tokenizing texts:  71%|███████   | 7081/10000 [00:29<00:13, 215.08it/s]Tokenizing texts:  71%|███████   | 7103/10000 [00:29<00:13, 212.87it/s]Tokenizing texts:  71%|███████▏  | 7134/10000 [00:29<00:12, 238.62it/s]Tokenizing texts:  72%|███████▏  | 7159/10000 [00:29<00:12, 234.76it/s]Tokenizing texts:  72%|███████▏  | 7189/10000 [00:29<00:11, 250.13it/s]Tokenizing texts:  72%|███████▏  | 7215/10000 [00:30<00:11, 245.19it/s]Tokenizing texts:  72%|███████▏  | 7244/10000 [00:30<00:10, 251.40it/s]Tokenizing texts:  73%|███████▎  | 7277/10000 [00:30<00:09, 273.33it/s]Tokenizing texts:  73%|███████▎  | 7311/10000 [00:30<00:09, 290.33it/s]Tokenizing texts:  73%|███████▎  | 7341/10000 [00:30<00:09, 280.11it/s]Tokenizing texts:  74%|███████▎  | 7370/10000 [00:30<00:09, 271.13it/s]Tokenizing texts:  74%|███████▍  | 7398/10000 [00:30<00:10, 251.46it/s]Tokenizing texts:  74%|███████▍  | 7424/10000 [00:30<00:11, 226.06it/s]Tokenizing texts:  74%|███████▍  | 7448/10000 [00:31<00:12, 203.96it/s]Tokenizing texts:  75%|███████▍  | 7481/10000 [00:31<00:10, 229.68it/s]Tokenizing texts:  75%|███████▌  | 7518/10000 [00:31<00:09, 261.74it/s]Tokenizing texts:  75%|███████▌  | 7546/10000 [00:31<00:09, 265.01it/s]Tokenizing texts:  76%|███████▌  | 7574/10000 [00:31<00:09, 264.31it/s]Tokenizing texts:  76%|███████▌  | 7611/10000 [00:31<00:08, 291.91it/s]Tokenizing texts:  76%|███████▋  | 7641/10000 [00:31<00:08, 273.32it/s]Tokenizing texts:  77%|███████▋  | 7674/10000 [00:31<00:08, 287.29it/s]Tokenizing texts:  77%|███████▋  | 7704/10000 [00:31<00:07, 289.96it/s]Tokenizing texts:  77%|███████▋  | 7734/10000 [00:32<00:09, 249.24it/s]Tokenizing texts:  78%|███████▊  | 7761/10000 [00:32<00:09, 244.82it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 258.08it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 236.85it/s]Tokenizing texts:  78%|███████▊  | 7845/10000 [00:32<00:08, 245.15it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 257.23it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 254.65it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:32<00:07, 283.00it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:32<00:07, 280.86it/s]Tokenizing texts:  80%|███████▉  | 7999/10000 [00:33<00:06, 291.83it/s]Tokenizing texts:  80%|████████  | 8029/10000 [00:33<00:06, 288.16it/s]Tokenizing texts:  81%|████████  | 8059/10000 [00:33<00:11, 170.52it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 199.25it/s]Tokenizing texts:  81%|████████  | 8120/10000 [00:33<00:09, 208.17it/s]Tokenizing texts:  81%|████████▏ | 8149/10000 [00:33<00:08, 225.61it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:33<00:07, 236.62it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:34<00:07, 248.24it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 213.24it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 223.64it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 257.20it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 253.03it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:34<00:06, 257.08it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:34<00:06, 256.67it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:34<00:06, 232.60it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:34<00:06, 258.66it/s]Tokenizing texts:  85%|████████▍ | 8473/10000 [00:35<00:05, 264.76it/s]Tokenizing texts:  85%|████████▌ | 8506/10000 [00:35<00:05, 282.45it/s]Tokenizing texts:  85%|████████▌ | 8535/10000 [00:35<00:05, 261.77it/s]Tokenizing texts:  86%|████████▌ | 8564/10000 [00:35<00:05, 255.37it/s]Tokenizing texts:  86%|████████▌ | 8596/10000 [00:35<00:05, 270.72it/s]Tokenizing texts:  86%|████████▋ | 8629/10000 [00:35<00:04, 285.21it/s]Tokenizing texts:  87%|████████▋ | 8660/10000 [00:35<00:04, 291.29it/s]Tokenizing texts:  87%|████████▋ | 8690/10000 [00:35<00:04, 284.62it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:35<00:04, 284.46it/s]Tokenizing texts:  88%|████████▊ | 8757/10000 [00:36<00:04, 291.78it/s]Tokenizing texts:  88%|████████▊ | 8787/10000 [00:36<00:04, 293.07it/s]Tokenizing texts:  88%|████████▊ | 8817/10000 [00:36<00:05, 220.22it/s]Tokenizing texts:  88%|████████▊ | 8842/10000 [00:36<00:05, 221.75it/s]Tokenizing texts:  89%|████████▉ | 8876/10000 [00:36<00:04, 249.12it/s]Tokenizing texts:  89%|████████▉ | 8914/10000 [00:36<00:03, 273.70it/s]Tokenizing texts:  89%|████████▉ | 8946/10000 [00:36<00:03, 284.55it/s]Tokenizing texts:  90%|████████▉ | 8976/10000 [00:36<00:03, 282.15it/s]Tokenizing texts:  90%|█████████ | 9006/10000 [00:37<00:04, 245.60it/s]Tokenizing texts:  90%|█████████ | 9038/10000 [00:37<00:03, 262.09it/s]Tokenizing texts:  91%|█████████ | 9066/10000 [00:37<00:03, 252.43it/s]Tokenizing texts:  91%|█████████ | 9098/10000 [00:37<00:03, 269.33it/s]Tokenizing texts:  91%|█████████▏| 9126/10000 [00:37<00:03, 259.20it/s]Tokenizing texts:  92%|█████████▏| 9158/10000 [00:37<00:03, 271.09it/s]Tokenizing texts:  92%|█████████▏| 9191/10000 [00:37<00:02, 286.27it/s]Tokenizing texts:  92%|█████████▏| 9221/10000 [00:37<00:02, 284.96it/s]Tokenizing texts:  92%|█████████▎| 9250/10000 [00:37<00:02, 277.97it/s]Tokenizing texts:  93%|█████████▎| 9279/10000 [00:38<00:02, 247.41it/s]Tokenizing texts:  93%|█████████▎| 9310/10000 [00:38<00:02, 257.61it/s]Tokenizing texts:  93%|█████████▎| 9337/10000 [00:38<00:02, 248.71it/s]Tokenizing texts:  94%|█████████▎| 9363/10000 [00:38<00:03, 212.26it/s]Tokenizing texts:  94%|█████████▍| 9400/10000 [00:38<00:02, 249.56it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:38<00:02, 228.03it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:38<00:02, 232.74it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:38<00:02, 241.82it/s]Tokenizing texts:  95%|█████████▌| 9505/10000 [00:39<00:02, 185.55it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 222.20it/s]Tokenizing texts:  96%|█████████▌| 9566/10000 [00:39<00:01, 229.43it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 233.51it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 231.81it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 234.83it/s]Tokenizing texts:  97%|█████████▋| 9674/10000 [00:39<00:01, 241.42it/s]Tokenizing texts:  97%|█████████▋| 9699/10000 [00:39<00:01, 237.34it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:40<00:01, 239.39it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 207.49it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 215.82it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 261.67it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 252.90it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 256.83it/s]Tokenizing texts:  99%|█████████▉| 9906/10000 [00:40<00:00, 281.17it/s]Tokenizing texts:  99%|█████████▉| 9935/10000 [00:40<00:00, 278.47it/s]Tokenizing texts: 100%|█████████▉| 9965/10000 [00:40<00:00, 283.56it/s]Tokenizing texts: 100%|█████████▉| 9998/10000 [00:41<00:00, 295.67it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 243.48it/s]
2025-12-09 12:14:04.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 12.026198387145996
2025-12-09 12:14:04.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 12.043404579162598
2025-12-09 12:14:04.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 11.992862701416016
2025-12-09 12:14:04.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 12.044525146484375
2025-12-09 12:14:04.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 12.03893756866455
2025-12-09 12:14:04.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 12.010963439941406
2025-12-09 12:14:04.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 12.014394760131836
2025-12-09 12:14:04.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 12.034148216247559
2025-12-09 12:14:04.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 12.071709632873535
2025-12-09 12:14:04.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 12.03562068939209
2025-12-09 12:14:04.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 11.976212501525879
2025-12-09 12:14:05.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 12.01708698272705
2025-12-09 12:14:05.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 12.033672332763672
2025-12-09 12:14:05.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 12.011858940124512
2025-12-09 12:14:05.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 12.009909629821777
2025-12-09 12:14:05.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 12.01820182800293
2025-12-09 12:14:05.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 12.001166343688965
2025-12-09 12:14:05.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 12.057205200195312
2025-12-09 12:14:05.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 12.019166946411133
2025-12-09 12:14:05.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 11.976936340332031
2025-12-09 12:14:05.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 11.994487762451172
2025-12-09 12:14:05.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 11.98105239868164
2025-12-09 12:14:05.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 12.00467586517334
2025-12-09 12:14:05.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 12.011951446533203
2025-12-09 12:14:06.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 11.999629020690918
2025-12-09 12:14:06.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 11.96753215789795
2025-12-09 12:14:06.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 12.011751174926758
2025-12-09 12:14:06.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 11.954078674316406
2025-12-09 12:14:06.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 11.928852081298828
2025-12-09 12:14:06.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 11.919164657592773
2025-12-09 12:14:06.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 11.95722484588623
2025-12-09 12:14:06.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 11.956334114074707
2025-12-09 12:14:06.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 11.959586143493652
2025-12-09 12:14:06.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 11.899168014526367
2025-12-09 12:14:06.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 11.953441619873047
2025-12-09 12:14:06.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 11.913326263427734
2025-12-09 12:14:07.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 11.903626441955566
2025-12-09 12:14:07.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 11.942782402038574
2025-12-09 12:14:07.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 11.87434196472168
2025-12-09 12:14:07.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 11.906645774841309
2025-12-09 12:14:07.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 11.910993576049805
2025-12-09 12:14:07.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 11.839768409729004
2025-12-09 12:14:07.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 11.844996452331543
2025-12-09 12:14:07.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 11.772462844848633
2025-12-09 12:14:07.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 11.87278938293457
2025-12-09 12:14:07.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 11.740086555480957
2025-12-09 12:14:07.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 11.839200019836426
2025-12-09 12:14:07.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 11.755148887634277
2025-12-09 12:14:07.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 11.643583297729492
2025-12-09 12:14:08.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 11.656426429748535
2025-12-09 12:14:08.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 11.621028900146484
2025-12-09 12:14:08.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 11.631108283996582
2025-12-09 12:14:08.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 11.607160568237305
2025-12-09 12:14:08.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 11.561397552490234
2025-12-09 12:14:08.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 11.587746620178223
2025-12-09 12:14:08.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 11.512386322021484
2025-12-09 12:14:08.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 11.75621223449707
2025-12-09 12:14:08.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 11.461410522460938
2025-12-09 12:14:08.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 11.485840797424316
2025-12-09 12:14:08.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 11.3569917678833
2025-12-09 12:14:08.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 11.383764266967773
2025-12-09 12:14:08.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 11.34099006652832
2025-12-09 12:14:09.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 11.37010383605957
2025-12-09 12:14:09.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 11.34264850616455
2025-12-09 12:14:09.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 11.277132034301758
2025-12-09 12:14:09.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 11.429131507873535
2025-12-09 12:14:09.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 11.279635429382324
2025-12-09 12:14:09.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 11.282888412475586
2025-12-09 12:14:09.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 11.1802339553833
2025-12-09 12:14:09.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 11.268327713012695
2025-12-09 12:14:09.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 11.217259407043457
2025-12-09 12:14:09.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 11.069029808044434
2025-12-09 12:14:09.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 11.105815887451172
2025-12-09 12:14:09.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 11.14590835571289
2025-12-09 12:14:09.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 11.02363109588623
2025-12-09 12:14:10.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 11.002938270568848
2025-12-09 12:14:10.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 11.015957832336426
2025-12-09 12:14:10.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 10.91630744934082
2025-12-09 12:14:10.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 11.182198524475098
2025-12-09 12:14:10.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0008 Training loss: 10.91679859161377
2025-12-09 12:14:10.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0008100000000000001 Training loss: 10.906307220458984
2025-12-09 12:14:10.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00082 Training loss: 10.878861427307129
2025-12-09 12:14:10.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00083 Training loss: 10.723920822143555
2025-12-09 12:14:10.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00084 Training loss: 10.873257637023926
2025-12-09 12:14:10.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00085 Training loss: 10.767251014709473
2025-12-09 12:14:10.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00086 Training loss: 10.609009742736816
2025-12-09 12:14:10.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00087 Training loss: 10.724340438842773
2025-12-09 12:14:10.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00088 Training loss: 10.680809020996094
2025-12-09 12:14:10.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0008900000000000001 Training loss: 10.660667419433594
2025-12-09 12:14:11.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0009000000000000001 Training loss: 10.725526809692383
2025-12-09 12:14:11.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00091 Training loss: 10.702733993530273
2025-12-09 12:14:11.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.00092 Training loss: 10.798579216003418
2025-12-09 12:14:11.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.00093 Training loss: 10.666215896606445
2025-12-09 12:14:11.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00094 Training loss: 10.665013313293457
2025-12-09 12:14:11.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00095 Training loss: 10.640548706054688
2025-12-09 12:14:11.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00096 Training loss: 10.627167701721191
2025-12-09 12:14:11.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0009699999999999999 Training loss: 10.641465187072754
2025-12-09 12:14:11.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00098 Training loss: 10.386173248291016
2025-12-09 12:14:11.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00099 Training loss: 10.642126083374023
2025-12-09 12:14:11.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.001 Training loss: 10.565640449523926
2025-12-09 12:14:11.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0009999999072578703 Training loss: 10.339362144470215
2025-12-09 12:14:11.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.0009999996290315154 Training loss: 10.400166511535645
2025-12-09 12:14:12.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0009999991653210384 Training loss: 10.58289623260498
2025-12-09 12:14:12.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0009999985161266117 Training loss: 10.401163101196289
2025-12-09 12:14:12.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0009999976814484759 Training loss: 10.467560768127441
2025-12-09 12:14:12.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0009999966612869405 Training loss: 10.343805313110352
2025-12-09 12:14:12.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0009999954556423843 Training loss: 10.406518936157227
2025-12-09 12:14:12.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0009999940645152542 Training loss: 10.70779800415039
2025-12-09 12:14:12.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0009999924879060664 Training loss: 10.424886703491211
2025-12-09 12:14:12.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.000999990725815406 Training loss: 10.362953186035156
2025-12-09 12:14:12.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.0009999887782439264 Training loss: 10.334183692932129
2025-12-09 12:14:12.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.00099998664519235 Training loss: 10.541529655456543
2025-12-09 12:14:12.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0009999843266614685 Training loss: 10.375160217285156
2025-12-09 12:14:12.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0009999818226521416 Training loss: 10.379766464233398
2025-12-09 12:14:12.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0009999791331652982 Training loss: 10.328569412231445
2025-12-09 12:14:13.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0009999762582019365 Training loss: 10.302287101745605
2025-12-09 12:14:13.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.0009999731977631226 Training loss: 10.36513900756836
2025-12-09 12:14:13.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.0009999699518499921 Training loss: 10.571564674377441
2025-12-09 12:14:13.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0009999665204637486 Training loss: 10.166818618774414
2025-12-09 12:14:13.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.0009999629036056656 Training loss: 10.268672943115234
2025-12-09 12:14:13.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0009999591012770847 Training loss: 10.088716506958008
2025-12-09 12:14:13.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0009999551134794165 Training loss: 10.304351806640625
2025-12-09 12:14:13.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00099995094021414 Training loss: 10.310420036315918
2025-12-09 12:14:13.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0009999465814828036 Training loss: 10.3003511428833
2025-12-09 12:14:13.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0009999420372870244 Training loss: 10.277103424072266
2025-12-09 12:14:13.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.0009999373076284876 Training loss: 10.193211555480957
2025-12-09 12:14:13.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0009999323925089486 Training loss: 9.87710189819336
2025-12-09 12:14:13.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00099992729193023 Training loss: 10.264482498168945
2025-12-09 12:14:14.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0009999220058942244 Training loss: 10.264426231384277
2025-12-09 12:14:14.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.0009999165344028926 Training loss: 10.140997886657715
2025-12-09 12:14:14.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.0009999108774582644 Training loss: 10.808541297912598
2025-12-09 12:14:14.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0009999050350624381 Training loss: 10.508787155151367
2025-12-09 12:14:14.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0009998990072175814 Training loss: 9.758505821228027
2025-12-09 12:14:14.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0009998927939259303 Training loss: 10.051041603088379
2025-12-09 12:14:14.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.0009998863951897897 Training loss: 10.105506896972656
2025-12-09 12:14:14.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0009998798110115333 Training loss: 10.377866744995117
2025-12-09 12:14:14.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0009998730413936037 Training loss: 10.080635070800781
2025-12-09 12:14:14.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0009998660863385124 Training loss: 9.708001136779785
2025-12-09 12:14:14.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0009998589458488389 Training loss: 10.26118278503418
2025-12-09 12:14:14.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0009998516199272328 Training loss: 10.088712692260742
2025-12-09 12:14:14.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.0009998441085764113 Training loss: 10.018677711486816
2025-12-09 12:14:15.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0009998364117991612 Training loss: 9.888498306274414
2025-12-09 12:14:15.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0009998285295983375 Training loss: 10.163795471191406
2025-12-09 12:14:15.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0009998204619768645 Training loss: 9.988375663757324
2025-12-09 12:14:15.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0009998122089377348 Training loss: 10.01899528503418
2025-12-09 12:14:15.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0009998037704840102 Training loss: 10.094916343688965
2025-12-09 12:14:15.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.000999795146618821 Training loss: 10.238777160644531
2025-12-09 12:14:15.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0009997863373453664 Training loss: 9.97895622253418
2025-12-09 12:14:15.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.000999777342666914 Training loss: 10.012293815612793
2025-12-09 12:14:15.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0009997681625868013 Training loss: 10.028534889221191
2025-12-09 12:14:15.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0009997587971084334 Training loss: 10.229820251464844
2025-12-09 12:14:15.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.0009997492462352846 Training loss: 10.135037422180176
2025-12-09 12:14:15.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0009997395099708981 Training loss: 9.89293384552002
2025-12-09 12:14:15.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0009997295883188856 Training loss: 10.102710723876953
2025-12-09 12:14:16.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0009997194812829276 Training loss: 9.998825073242188
2025-12-09 12:14:16.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0009997091888667737 Training loss: 10.076513290405273
2025-12-09 12:14:16.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.000999698711074242 Training loss: 9.974190711975098
2025-12-09 12:14:16.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0009996880479092197 Training loss: 9.987001419067383
2025-12-09 12:14:16.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.000999677199375662 Training loss: 9.893168449401855
2025-12-09 12:14:16.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0009996661654775938 Training loss: 10.303302764892578
2025-12-09 12:14:16.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.000999654946219108 Training loss: 9.927331924438477
2025-12-09 12:14:16.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.000999643541604367 Training loss: 9.980388641357422
2025-12-09 12:14:16.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.000999631951637601 Training loss: 10.239320755004883
2025-12-09 12:14:16.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0009996201763231099 Training loss: 9.825177192687988
2025-12-09 12:14:16.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0009996082156652618 Training loss: 9.972468376159668
2025-12-09 12:14:16.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.000999596069668494 Training loss: 9.896614074707031
2025-12-09 12:14:16.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.000999583738337312 Training loss: 9.746320724487305
2025-12-09 12:14:17.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0009995712216762903 Training loss: 9.926618576049805
2025-12-09 12:14:17.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.0009995585196900722 Training loss: 9.9508695602417
2025-12-09 12:14:17.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.00099954563238337 Training loss: 9.90578842163086
2025-12-09 12:14:17.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0009995325597609644 Training loss: 9.936841011047363
2025-12-09 12:14:17.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.000999519301827705 Training loss: 9.88404655456543
2025-12-09 12:14:17.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0009995058585885095 Training loss: 9.843181610107422
2025-12-09 12:14:17.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.0009994922300483656 Training loss: 9.752551078796387
2025-12-09 12:14:17.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.000999478416212329 Training loss: 10.246854782104492
2025-12-09 12:14:17.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.0009994644170855237 Training loss: 9.838154792785645
2025-12-09 12:14:17.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0009994502326731434 Training loss: 9.958260536193848
2025-12-09 12:14:17.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0009994358629804498 Training loss: 9.895411491394043
2025-12-09 12:14:17.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0009994213080127738 Training loss: 9.929621696472168
2025-12-09 12:14:17.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0009994065677755147 Training loss: 10.141862869262695
2025-12-09 12:14:18.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0009993916422741409 Training loss: 9.915180206298828
2025-12-09 12:14:18.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.000999376531514189 Training loss: 9.816656112670898
2025-12-09 12:14:18.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0009993612355012646 Training loss: 10.061928749084473
2025-12-09 12:14:18.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.0009993457542410422 Training loss: 9.87685775756836
2025-12-09 12:14:18.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.000999330087739265 Training loss: 9.831527709960938
2025-12-09 12:14:18.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0009993142360017445 Training loss: 9.835268020629883
2025-12-09 12:14:18.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0009992981990343613 Training loss: 10.063946723937988
2025-12-09 12:14:18.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0009992819768430649 Training loss: 9.782739639282227
2025-12-09 12:14:18.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0009992655694338725 Training loss: 9.923466682434082
2025-12-09 12:14:18.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0009992489768128714 Training loss: 9.936555862426758
2025-12-09 12:14:18.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0009992321989862165 Training loss: 9.859561920166016
2025-12-09 12:14:18.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0009992152359601322 Training loss: 9.896315574645996
2025-12-09 12:14:18.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.000999198087740911 Training loss: 9.934821128845215
2025-12-09 12:14:19.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0009991807543349145 Training loss: 9.87403392791748
2025-12-09 12:14:19.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.000999163235748573 Training loss: 9.932236671447754
2025-12-09 12:14:19.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0009991455319883849 Training loss: 9.842342376708984
2025-12-09 12:14:19.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0009991276430609181 Training loss: 9.79159927368164
2025-12-09 12:14:19.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.0009991095689728087 Training loss: 9.861549377441406
2025-12-09 12:14:19.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0009990913097307613 Training loss: 9.819891929626465
2025-12-09 12:14:19.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0009990728653415503 Training loss: 9.739299774169922
2025-12-09 12:14:19.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0009990542358120174 Training loss: 9.799897193908691
2025-12-09 12:14:19.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0009990354211490736 Training loss: 9.800600051879883
2025-12-09 12:14:19.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0009990164213596986 Training loss: 9.794225692749023
2025-12-09 12:14:19.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0009989972364509408 Training loss: 10.014196395874023
2025-12-09 12:14:19.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0009989778664299172 Training loss: 9.974810600280762
2025-12-09 12:14:19.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0009989583113038133 Training loss: 9.96364974975586
2025-12-09 12:14:20.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0009989385710798837 Training loss: 10.03183364868164
2025-12-09 12:14:20.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.0009989186457654514 Training loss: 9.775753021240234
2025-12-09 12:14:20.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.0009988985353679076 Training loss: 9.946028709411621
2025-12-09 12:14:20.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0009988782398947132 Training loss: 9.71817684173584
2025-12-09 12:14:20.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0009988577593533967 Training loss: 9.898778915405273
2025-12-09 12:14:20.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.000998837093751556 Training loss: 9.731721878051758
2025-12-09 12:14:20.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0009988162430968576 Training loss: 9.800786018371582
2025-12-09 12:14:20.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.000998795207397036 Training loss: 9.707310676574707
2025-12-09 12:14:20.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.000998773986659895 Training loss: 9.643139839172363
2025-12-09 12:14:20.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.0009987525808933069 Training loss: 9.806110382080078
2025-12-09 12:14:20.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0009987309901052122 Training loss: 9.960412979125977
2025-12-09 12:14:20.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.000998709214303621 Training loss: 9.757940292358398
2025-12-09 12:14:20.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.000998687253496611 Training loss: 9.620915412902832
2025-12-09 12:14:21.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0009986651076923287 Training loss: 9.617178916931152
2025-12-09 12:14:21.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0009986427768989903 Training loss: 10.125335693359375
2025-12-09 12:14:21.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0009986202611248793 Training loss: 9.873661994934082
2025-12-09 12:14:21.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0009985975603783483 Training loss: 9.713912963867188
2025-12-09 12:14:21.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.000998574674667819 Training loss: 9.957595825195312
2025-12-09 12:14:21.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0009985516040017807 Training loss: 9.9072847366333
2025-12-09 12:14:21.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0009985283483887923 Training loss: 10.294549942016602
2025-12-09 12:14:21.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0009985049078374806 Training loss: 9.845613479614258
2025-12-09 12:14:21.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0009984812823565416 Training loss: 9.601187705993652
2025-12-09 12:14:21.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0009984574719547395 Training loss: 9.576029777526855
2025-12-09 12:14:21.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.000998433476640907 Training loss: 9.893952369689941
2025-12-09 12:14:21.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0009984092964239462 Training loss: 9.809127807617188
2025-12-09 12:14:21.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0009983849313128263 Training loss: 9.787736892700195
2025-12-09 12:14:21.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0009983603813165868 Training loss: 9.661595344543457
2025-12-09 12:14:22.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0009983356464443346 Training loss: 9.937420845031738
2025-12-09 12:14:22.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0009983107267052458 Training loss: 9.542031288146973
2025-12-09 12:14:22.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0009982856221085643 Training loss: 9.646912574768066
2025-12-09 12:14:22.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0009982603326636036 Training loss: 9.810104370117188
2025-12-09 12:14:22.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0009982348583797453 Training loss: 9.939801216125488
2025-12-09 12:14:22.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.0009982091992664392 Training loss: 9.707310676574707
2025-12-09 12:14:22.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0009981833553332044 Training loss: 10.234066009521484
2025-12-09 12:14:22.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0009981573265896281 Training loss: 9.694148063659668
2025-12-09 12:14:22.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.000998131113045366 Training loss: 9.976901054382324
2025-12-09 12:14:22.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0009981047147101425 Training loss: 9.816132545471191
2025-12-09 12:14:22.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0009980781315937506 Training loss: 9.717423439025879
2025-12-09 12:14:22.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.000998051363706052 Training loss: 9.705645561218262
2025-12-09 12:14:22.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0009980244110569766 Training loss: 9.166106224060059
2025-12-09 12:14:23.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0009979972736565226 Training loss: 9.765244483947754
2025-12-09 12:14:23.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0009979699515147579 Training loss: 9.6483736038208
2025-12-09 12:14:23.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0009979424446418172 Training loss: 9.464617729187012
2025-12-09 12:14:23.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0009979147530479056 Training loss: 9.750479698181152
2025-12-09 12:14:23.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0009978868767432953 Training loss: 9.93730354309082
2025-12-09 12:14:23.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0009978588157383277 Training loss: 9.580856323242188
2025-12-09 12:14:23.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0009978305700434125 Training loss: 10.163007736206055
2025-12-09 12:14:23.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.000997802139669028 Training loss: 9.862813949584961
2025-12-09 12:14:23.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0009977735246257209 Training loss: 9.610896110534668
2025-12-09 12:14:23.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0009977447249241065 Training loss: 9.651891708374023
2025-12-09 12:14:23.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.0009977157405748687 Training loss: 9.812966346740723
2025-12-09 12:14:23.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0009976865715887596 Training loss: 9.620508193969727
2025-12-09 12:14:23.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0009976572179766 Training loss: 9.861287117004395
2025-12-09 12:14:24.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0009976276797492793 Training loss: 9.63589859008789
2025-12-09 12:14:24.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0009975979569177551 Training loss: 10.146946907043457
2025-12-09 12:14:24.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0009975680494930539 Training loss: 9.790369987487793
2025-12-09 12:14:24.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00099753795748627 Training loss: 9.492576599121094
2025-12-09 12:14:24.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0009975076809085669 Training loss: 9.726304054260254
2025-12-09 12:14:24.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0009974772197711762 Training loss: 9.619202613830566
2025-12-09 12:14:24.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.000997446574085398 Training loss: 9.48311710357666
2025-12-09 12:14:24.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0009974157438626008 Training loss: 9.964547157287598
2025-12-09 12:14:24.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0009973847291142217 Training loss: 9.206533432006836
2025-12-09 12:14:24.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0009973535298517663 Training loss: 9.740767478942871
2025-12-09 12:14:24.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0009973221460868086 Training loss: 9.716066360473633
2025-12-09 12:14:24.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0009972905778309906 Training loss: 9.651981353759766
2025-12-09 12:14:24.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0009972588250960234 Training loss: 9.057987213134766
2025-12-09 12:14:25.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0009972268878936862 Training loss: 10.121420860290527
2025-12-09 12:14:25.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.000997194766235827 Training loss: 9.66585922241211
2025-12-09 12:14:25.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0009971624601343614 Training loss: 9.449387550354004
2025-12-09 12:14:25.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0009971299696012743 Training loss: 9.704618453979492
2025-12-09 12:14:25.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0009970972946486186 Training loss: 9.889945030212402
2025-12-09 12:14:25.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0009970644352885157 Training loss: 9.907380104064941
2025-12-09 12:14:25.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0009970313915331553 Training loss: 9.706388473510742
2025-12-09 12:14:25.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0009969981633947955 Training loss: 9.65759563446045
2025-12-09 12:14:25.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0009969647508857632 Training loss: 9.709383010864258
2025-12-09 12:14:25.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.000996931154018453 Training loss: 9.668489456176758
2025-12-09 12:14:25.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0009968973728053288 Training loss: 9.799570083618164
2025-12-09 12:14:25.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0009968634072589219 Training loss: 9.630340576171875
2025-12-09 12:14:25.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0009968292573918325 Training loss: 9.872504234313965
2025-12-09 12:14:26.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.0009967949232167295 Training loss: 9.502740859985352
2025-12-09 12:14:26.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0009967604047463492 Training loss: 9.598477363586426
2025-12-09 12:14:26.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0009967257019934974 Training loss: 9.748791694641113
2025-12-09 12:14:26.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0009966908149710476 Training loss: 9.857009887695312
2025-12-09 12:14:26.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0009966557436919415 Training loss: 9.795441627502441
2025-12-09 12:14:26.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.00099662048816919 Training loss: 9.62965202331543
2025-12-09 12:14:26.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.000996585048415871 Training loss: 9.609634399414062
2025-12-09 12:14:26.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0009965494244451323 Training loss: 9.353875160217285
2025-12-09 12:14:26.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0009965136162701888 Training loss: 9.839652061462402
2025-12-09 12:14:26.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0009964776239043244 Training loss: 9.725805282592773
2025-12-09 12:14:26.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0009964414473608912 Training loss: 9.704694747924805
2025-12-09 12:14:26.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0009964050866533092 Training loss: 9.639777183532715
2025-12-09 12:14:26.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.0009963685417950677 Training loss: 9.53536319732666
2025-12-09 12:14:27.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.000996331812799723 Training loss: 9.504867553710938
2025-12-09 12:14:27.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0009962948996809007 Training loss: 9.829997062683105
2025-12-09 12:14:27.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0009962578024522947 Training loss: 9.686617851257324
2025-12-09 12:14:27.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0009962205211276665 Training loss: 9.638209342956543
2025-12-09 12:14:27.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0009961830557208464 Training loss: 9.59583854675293
2025-12-09 12:14:27.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.000996145406245733 Training loss: 9.723678588867188
2025-12-09 12:14:27.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0009961075727162928 Training loss: 9.677201271057129
2025-12-09 12:14:27.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0009960695551465611 Training loss: 9.55446720123291
2025-12-09 12:14:27.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.000996031353550641 Training loss: 9.566052436828613
2025-12-09 12:14:27.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0009959929679427047 Training loss: 9.683981895446777
2025-12-09 12:14:27.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0009959543983369913 Training loss: 9.469609260559082
2025-12-09 12:14:27.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.000995915644747809 Training loss: 10.23240852355957
2025-12-09 12:14:27.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0009958767071895347 Training loss: 9.565679550170898
2025-12-09 12:14:28.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.0009958375856766127 Training loss: 9.379372596740723
2025-12-09 12:14:28.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0009957982802235555 Training loss: 9.521459579467773
2025-12-09 12:14:28.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0009957587908449449 Training loss: 9.61388874053955
2025-12-09 12:14:28.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0009957191175554295 Training loss: 9.751590728759766
2025-12-09 12:14:28.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0009956792603697273 Training loss: 9.722394943237305
2025-12-09 12:14:28.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.000995639219302624 Training loss: 9.735336303710938
2025-12-09 12:14:28.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0009955989943689733 Training loss: 9.730027198791504
2025-12-09 12:14:28.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0009955585855836978 Training loss: 9.711788177490234
2025-12-09 12:14:28.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0009955179929617875 Training loss: 9.57736873626709
2025-12-09 12:14:28.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0009954772165183012 Training loss: 9.701176643371582
2025-12-09 12:14:28.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0009954362562683658 Training loss: 9.76754379272461
2025-12-09 12:14:28.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.000995395112227176 Training loss: 9.493337631225586
2025-12-09 12:14:28.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.000995353784409995 Training loss: 9.89025592803955
2025-12-09 12:14:29.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0009953122728321542 Training loss: 9.573568344116211
2025-12-09 12:14:29.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0009952705775090529 Training loss: 9.688319206237793
2025-12-09 12:14:29.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0009952286984561591 Training loss: 9.693008422851562
2025-12-09 12:14:29.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0009951866356890083 Training loss: 9.684039115905762
2025-12-09 12:14:29.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0009951443892232048 Training loss: 9.64135456085205
2025-12-09 12:14:29.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0009951019590744203 Training loss: 9.639863967895508
2025-12-09 12:14:29.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0009950593452583952 Training loss: 9.696945190429688
2025-12-09 12:14:29.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0009950165477909379 Training loss: 9.478108406066895
2025-12-09 12:14:29.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0009949735666879252 Training loss: 9.555996894836426
2025-12-09 12:14:29.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.000994930401965301 Training loss: 9.293835639953613
2025-12-09 12:14:29.804 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.000994887053639079 Training loss: 9.834113121032715
2025-12-09 12:14:29.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0009948435217253394 Training loss: 9.61922836303711
2025-12-09 12:14:29.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.0009947998062402312 Training loss: 9.6685791015625
2025-12-09 12:14:30.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.000994755907199972 Training loss: 9.92864990234375
2025-12-09 12:14:30.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0009947118246208461 Training loss: 9.664045333862305
2025-12-09 12:14:30.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0009946675585192075 Training loss: 9.79265308380127
2025-12-09 12:14:30.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0009946231089114773 Training loss: 9.662537574768066
2025-12-09 12:14:30.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.000994578475814145 Training loss: 9.556462287902832
2025-12-09 12:14:30.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0009945336592437678 Training loss: 9.4854154586792
2025-12-09 12:14:30.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0009944886592169711 Training loss: 9.706950187683105
2025-12-09 12:14:30.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.000994443475750449 Training loss: 9.641226768493652
2025-12-09 12:14:30.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.000994398108860963 Training loss: 9.893388748168945
2025-12-09 12:14:30.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0009943525585653428 Training loss: 9.768280029296875
2025-12-09 12:14:30.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0009943068248804859 Training loss: 9.656693458557129
2025-12-09 12:14:30.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.000994260907823358 Training loss: 9.556772232055664
2025-12-09 12:14:30.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0009942148074109933 Training loss: 9.819674491882324
2025-12-09 12:14:31.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0009941685236604934 Training loss: 9.494086265563965
2025-12-09 12:14:31.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.0009941220565890278 Training loss: 9.510493278503418
2025-12-09 12:14:31.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.000994075406213835 Training loss: 9.753829002380371
2025-12-09 12:14:31.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.0009940285725522201 Training loss: 9.538898468017578
2025-12-09 12:14:31.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0009939815556215576 Training loss: 9.784527778625488
2025-12-09 12:14:31.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0009939343554392886 Training loss: 9.569034576416016
2025-12-09 12:14:31.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.0009938869720229233 Training loss: 9.601922988891602
2025-12-09 12:14:31.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0009938394053900395 Training loss: 9.719890594482422
2025-12-09 12:14:31.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0009937916555582827 Training loss: 9.710355758666992
2025-12-09 12:14:31.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.000993743722545367 Training loss: 10.019452095031738
2025-12-09 12:14:31.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.0009936956063690734 Training loss: 9.6141939163208
2025-12-09 12:14:31.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0009936473070472518 Training loss: 9.724645614624023
2025-12-09 12:14:31.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0009935988245978198 Training loss: 9.513538360595703
2025-12-09 12:14:32.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0009935501590387628 Training loss: 9.490447044372559
2025-12-09 12:14:32.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0009935013103881344 Training loss: 9.577765464782715
2025-12-09 12:14:32.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0009934522786640555 Training loss: 9.521897315979004
2025-12-09 12:14:32.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0009934030638847156 Training loss: 9.493483543395996
2025-12-09 12:14:32.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0009933536660683717 Training loss: 9.714594841003418
2025-12-09 12:14:32.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0009933040852333488 Training loss: 9.675437927246094
2025-12-09 12:14:32.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00099325432139804 Training loss: 9.625822067260742
2025-12-09 12:14:32.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.0009932043745809064 Training loss: 9.19245433807373
2025-12-09 12:14:32.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.000993154244800476 Training loss: 9.626669883728027
2025-12-09 12:14:32.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.0009931039320753457 Training loss: 9.575508117675781
2025-12-09 12:14:32.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00099305343642418 Training loss: 9.889020919799805
2025-12-09 12:14:32.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0009930027578657114 Training loss: 9.570030212402344
2025-12-09 12:14:32.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0009929518964187393 Training loss: 9.522686958312988
2025-12-09 12:14:33.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0009929008521021325 Training loss: 8.960053443908691
2025-12-09 12:14:33.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.0009928496249348266 Training loss: 9.863341331481934
2025-12-09 12:14:33.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.000992798214935825 Training loss: 9.640645980834961
2025-12-09 12:14:33.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0009927466221241995 Training loss: 9.764440536499023
2025-12-09 12:14:33.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0009926948465190893 Training loss: 9.841599464416504
2025-12-09 12:14:33.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0009926428881397015 Training loss: 9.801623344421387
2025-12-09 12:14:33.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0009925907470053111 Training loss: 9.686901092529297
2025-12-09 12:14:33.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.0009925384231352606 Training loss: 9.932045936584473
2025-12-09 12:14:33.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0009924859165489608 Training loss: 9.44894790649414
2025-12-09 12:14:33.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0009924332272658897 Training loss: 9.607897758483887
2025-12-09 12:14:33.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.0009923803553055937 Training loss: 9.593184471130371
2025-12-09 12:14:33.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0009923273006876864 Training loss: 9.313167572021484
2025-12-09 12:14:33.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0009922740634318494 Training loss: 9.520581245422363
2025-12-09 12:14:34.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.0009922206435578323 Training loss: 9.582747459411621
2025-12-09 12:14:34.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0009921670410854518 Training loss: 9.21311092376709
2025-12-09 12:14:34.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0009921132560345928 Training loss: 9.590195655822754
2025-12-09 12:14:34.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0009920592884252082 Training loss: 9.464911460876465
2025-12-09 12:14:34.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0009920051382773178 Training loss: 9.550729751586914
2025-12-09 12:14:34.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.00099195080561101 Training loss: 9.677017211914062
2025-12-09 12:14:34.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.0009918962904464407 Training loss: 9.351542472839355
2025-12-09 12:14:34.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.0009918415928038325 Training loss: 9.646343231201172
2025-12-09 12:14:34.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.000991786712703477 Training loss: 9.586238861083984
2025-12-09 12:14:34.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0009917316501657334 Training loss: 9.50987434387207
2025-12-09 12:14:34.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0009916764052110274 Training loss: 9.681591987609863
2025-12-09 12:14:34.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0009916209778598536 Training loss: 9.437589645385742
2025-12-09 12:14:34.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0009915653681327736 Training loss: 9.288883209228516
2025-12-09 12:14:35.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.000991509576050417 Training loss: 9.872588157653809
2025-12-09 12:14:35.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0009914536016334807 Training loss: 9.562880516052246
2025-12-09 12:14:35.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0009913974449027297 Training loss: 9.723015785217285
2025-12-09 12:14:35.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.0009913411058789963 Training loss: 9.596343994140625
2025-12-09 12:14:35.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0009912845845831805 Training loss: 9.477376937866211
2025-12-09 12:14:35.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00099122788103625 Training loss: 9.552889823913574
2025-12-09 12:14:35.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0009911709952592396 Training loss: 9.40401554107666
2025-12-09 12:14:35.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.0009911139272732526 Training loss: 9.37482738494873
2025-12-09 12:14:35.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.0009910566770994593 Training loss: 8.875777244567871
2025-12-09 12:14:35.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0009909992447590978 Training loss: 9.522111892700195
2025-12-09 12:14:35.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.0009909416302734736 Training loss: 9.487106323242188
2025-12-09 12:14:35.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.0009908838336639598 Training loss: 9.627016067504883
2025-12-09 12:14:35.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.000990825854951997 Training loss: 9.296530723571777
2025-12-09 12:14:36.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0009907676941590937 Training loss: 9.430365562438965
2025-12-09 12:14:36.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0009907093513068259 Training loss: 9.412697792053223
2025-12-09 12:14:36.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.0009906508264168365 Training loss: 9.466468811035156
2025-12-09 12:14:36.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.0009905921195108368 Training loss: 9.262206077575684
2025-12-09 12:14:36.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0009905332306106049 Training loss: 9.83061695098877
2025-12-09 12:14:36.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.000990474159737987 Training loss: 9.444234848022461
2025-12-09 12:14:36.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0009904149069148963 Training loss: 9.460968971252441
2025-12-09 12:14:36.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.000990355472163314 Training loss: 9.563316345214844
2025-12-09 12:14:36.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0009902958555052881 Training loss: 9.545907020568848
2025-12-09 12:14:36.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0009902360569629348 Training loss: 9.288515090942383
2025-12-09 12:14:36.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0009901760765584375 Training loss: 9.915884971618652
2025-12-09 12:14:36.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.000990115914314047 Training loss: 9.524582862854004
2025-12-09 12:14:36.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0009900555702520816 Training loss: 9.1097993850708
2025-12-09 12:14:37.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.000989995044394927 Training loss: 9.503280639648438
2025-12-09 12:14:37.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0009899343367650365 Training loss: 9.795097351074219
2025-12-09 12:14:37.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0009898734473849304 Training loss: 9.595098495483398
2025-12-09 12:14:37.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0009898123762771972 Training loss: 9.654828071594238
2025-12-09 12:14:37.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.000989751123464492 Training loss: 9.011266708374023
2025-12-09 12:14:37.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0009896896889695376 Training loss: 9.594165802001953
2025-12-09 12:14:37.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0009896280728151248 Training loss: 9.241744041442871
2025-12-09 12:14:37.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.0009895662750241108 Training loss: 9.620161056518555
2025-12-09 12:14:37.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0009895042956194209 Training loss: 9.548480033874512
2025-12-09 12:14:37.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0009894421346240473 Training loss: 9.54227352142334
2025-12-09 12:14:37.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0009893797920610496 Training loss: 9.691231727600098
2025-12-09 12:14:37.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0009893172679535552 Training loss: 9.609166145324707
2025-12-09 12:14:37.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0009892545623247585 Training loss: 9.446932792663574
2025-12-09 12:14:37.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0009891916751979218 Training loss: 9.406843185424805
2025-12-09 12:14:38.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.0009891286065963733 Training loss: 9.693370819091797
2025-12-09 12:14:38.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0009890653565435101 Training loss: 9.545675277709961
2025-12-09 12:14:38.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0009890019250627959 Training loss: 9.466765403747559
2025-12-09 12:14:38.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0009889383121777617 Training loss: 9.489747047424316
2025-12-09 12:14:38.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.000988874517912006 Training loss: 9.439937591552734
2025-12-09 12:14:38.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0009888105422891941 Training loss: 9.799609184265137
2025-12-09 12:14:38.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0009887463853330593 Training loss: 9.970544815063477
2025-12-09 12:14:38.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.000988682047067402 Training loss: 9.26480484008789
2025-12-09 12:14:38.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.000988617527516089 Training loss: 9.483539581298828
2025-12-09 12:14:38.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0009885528267030556 Training loss: 9.832549095153809
2025-12-09 12:14:38.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0009884879446523036 Training loss: 9.622152328491211
2025-12-09 12:14:38.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.000988422881387902 Training loss: 9.422737121582031
2025-12-09 12:14:38.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0009883576369339876 Training loss: 9.585110664367676
2025-12-09 12:14:39.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0009882922113147636 Training loss: 8.968408584594727
2025-12-09 12:14:39.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0009882266045545011 Training loss: 9.684853553771973
2025-12-09 12:14:39.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.0009881608166775384 Training loss: 9.384488105773926
2025-12-09 12:14:39.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.0009880948477082802 Training loss: 9.189764022827148
2025-12-09 12:14:39.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0009880286976711992 Training loss: 9.707064628601074
2025-12-09 12:14:39.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.000987962366590835 Training loss: 9.538280487060547
2025-12-09 12:14:39.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.0009878958544917943 Training loss: 9.536511421203613
2025-12-09 12:14:39.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.0009878291613987509 Training loss: 9.743739128112793
2025-12-09 12:14:39.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.000987762287336446 Training loss: 9.47214412689209
2025-12-09 12:14:39.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0009876952323296876 Training loss: 9.545012474060059
2025-12-09 12:14:39.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0009876279964033511 Training loss: 9.511198997497559
2025-12-09 12:14:39.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.000987560579582379 Training loss: 9.744124412536621
2025-12-09 12:14:39.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.0009874929818917805 Training loss: 9.662240028381348
2025-12-09 12:14:40.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.0009874252033566326 Training loss: 9.529867172241211
2025-12-09 12:14:40.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.000987357244002079 Training loss: 9.817645072937012
2025-12-09 12:14:40.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00098728910385333 Training loss: 9.791735649108887
2025-12-09 12:14:40.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.000987220782935664 Training loss: 9.394601821899414
2025-12-09 12:14:40.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0009871522812744257 Training loss: 9.480975151062012
2025-12-09 12:14:40.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.0009870835988950268 Training loss: 9.593070030212402
2025-12-09 12:14:40.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0009870147358229467 Training loss: 9.590020179748535
2025-12-09 12:14:40.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0009869456920837312 Training loss: 9.550543785095215
2025-12-09 12:14:40.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0009868764677029933 Training loss: 9.123157501220703
2025-12-09 12:14:40.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.0009868070627064133 Training loss: 9.786066055297852
2025-12-09 12:14:40.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0009867374771197384 Training loss: 9.495807647705078
2025-12-09 12:14:40.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0009866677109687822 Training loss: 9.526435852050781
2025-12-09 12:14:40.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0009865977642794259 Training loss: 9.336725234985352
2025-12-09 12:14:41.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0009865276370776177 Training loss: 9.563823699951172
2025-12-09 12:14:41.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0009864573293893724 Training loss: 9.87905216217041
2025-12-09 12:14:41.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.000986386841240772 Training loss: 9.361503601074219
2025-12-09 12:14:41.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0009863161726579655 Training loss: 9.564611434936523
2025-12-09 12:14:41.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0009862453236671685 Training loss: 9.72788143157959
2025-12-09 12:14:41.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.000986174294294664 Training loss: 9.54112434387207
2025-12-09 12:14:41.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0009861030845668014 Training loss: 9.455611228942871
2025-12-09 12:14:41.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.0009860316945099973 Training loss: 9.578819274902344
2025-12-09 12:14:41.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0009859601241507354 Training loss: 9.540850639343262
2025-12-09 12:14:41.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0009858883735155658 Training loss: 9.338876724243164
2025-12-09 12:14:41.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0009858164426311058 Training loss: 9.388510704040527
2025-12-09 12:14:41.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.0009857443315240395 Training loss: 9.627022743225098
2025-12-09 12:14:41.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.000985672040221118 Training loss: 9.403579711914062
2025-12-09 12:14:42.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.000985599568749159 Training loss: 9.399186134338379
2025-12-09 12:14:42.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.000985526917135047 Training loss: 9.547783851623535
2025-12-09 12:14:42.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0009854540854057337 Training loss: 9.675711631774902
2025-12-09 12:14:42.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.000985381073588237 Training loss: 9.244953155517578
2025-12-09 12:14:42.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.0009853078817096423 Training loss: 9.598271369934082
2025-12-09 12:14:42.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0009852345097971016 Training loss: 9.690232276916504
2025-12-09 12:14:42.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.0009851609578778332 Training loss: 9.46442985534668
2025-12-09 12:14:42.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0009850872259791227 Training loss: 9.651870727539062
2025-12-09 12:14:42.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0009850133141283226 Training loss: 9.449543952941895
2025-12-09 12:14:42.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0009849392223528514 Training loss: 9.40703010559082
2025-12-09 12:14:42.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.000984864950680195 Training loss: 9.723350524902344
2025-12-09 12:14:42.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.000984790499137906 Training loss: 9.472957611083984
2025-12-09 12:14:42.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0009847158677536033 Training loss: 9.281075477600098
2025-12-09 12:14:43.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.000984641056554973 Training loss: 9.653611183166504
2025-12-09 12:14:43.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.0009845660655697678 Training loss: 9.348809242248535
2025-12-09 12:14:43.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.0009844908948258067 Training loss: 9.541969299316406
2025-12-09 12:14:43.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.000984415544350976 Training loss: 9.544339179992676
2025-12-09 12:14:43.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.000984340014173228 Training loss: 9.553791999816895
2025-12-09 12:14:43.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0009842643043205823 Training loss: 9.382577896118164
2025-12-09 12:14:43.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0009841884148211247 Training loss: 9.617494583129883
2025-12-09 12:14:43.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.000984112345703008 Training loss: 9.587732315063477
2025-12-09 12:14:43.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.000984036096994451 Training loss: 9.767134666442871
2025-12-09 12:14:43.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0009839596687237402 Training loss: 9.517291069030762
2025-12-09 12:14:43.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0009838830609192278 Training loss: 9.595922470092773
2025-12-09 12:14:43.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.0009838062736093327 Training loss: 9.433237075805664
2025-12-09 12:14:43.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0009837293068225407 Training loss: 9.339583396911621
2025-12-09 12:14:44.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0009836521605874043 Training loss: 9.458809852600098
2025-12-09 12:14:44.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0009835748349325422 Training loss: 9.364314079284668
2025-12-09 12:14:44.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0009834973298866393 Training loss: 9.41850757598877
2025-12-09 12:14:44.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0009834196454784484 Training loss: 9.214896202087402
2025-12-09 12:14:44.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0009833417817367873 Training loss: 9.715106010437012
2025-12-09 12:14:44.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.0009832637386905413 Training loss: 9.54790210723877
2025-12-09 12:14:44.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0009831855163686617 Training loss: 9.501055717468262
2025-12-09 12:14:44.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0009831071148001668 Training loss: 9.708953857421875
2025-12-09 12:14:44.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0009830285340141408 Training loss: 9.485238075256348
2025-12-09 12:14:44.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0009829497740397348 Training loss: 9.449358940124512
2025-12-09 12:14:44.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0009828708349061664 Training loss: 9.495603561401367
2025-12-09 12:14:44.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0009827917166427196 Training loss: 9.61135196685791
2025-12-09 12:14:44.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0009827124192787445 Training loss: 9.715503692626953
2025-12-09 12:14:45.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.000982632942843658 Training loss: 9.490310668945312
2025-12-09 12:14:45.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0009825532873669433 Training loss: 9.493711471557617
2025-12-09 12:14:45.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0009824734528781505 Training loss: 9.562206268310547
2025-12-09 12:14:45.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.0009823934394068952 Training loss: 9.577675819396973
2025-12-09 12:14:45.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.0009823132469828602 Training loss: 9.463102340698242
2025-12-09 12:14:45.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.000982232875635794 Training loss: 9.586915016174316
2025-12-09 12:14:45.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.0009821523253955122 Training loss: 9.638273239135742
2025-12-09 12:14:45.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.0009820715962918964 Training loss: 9.425559043884277
2025-12-09 12:14:45.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0009819906883548942 Training loss: 9.47166633605957
2025-12-09 12:14:45.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0009819096016145203 Training loss: 9.54011344909668
2025-12-09 12:14:45.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.000981828336100855 Training loss: 9.51074504852295
2025-12-09 12:14:45.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0009817468918440454 Training loss: 9.33651351928711
2025-12-09 12:14:45.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0009816652688743048 Training loss: 9.382554054260254
2025-12-09 12:14:46.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0009815834672219127 Training loss: 9.64608097076416
2025-12-09 12:14:46.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.000981501486917215 Training loss: 9.173944473266602
2025-12-09 12:14:46.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0009814193279906237 Training loss: 9.368086814880371
2025-12-09 12:14:46.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.000981336990472617 Training loss: 9.48245620727539
2025-12-09 12:14:46.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00098125447439374 Training loss: 9.492286682128906
2025-12-09 12:14:46.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0009811717797846033 Training loss: 9.583510398864746
2025-12-09 12:14:46.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.000981088906675884 Training loss: 10.027813911437988
2025-12-09 12:14:46.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0009810058550983253 Training loss: 9.43752384185791
2025-12-09 12:14:46.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.000980922625082737 Training loss: 9.47677993774414
2025-12-09 12:14:46.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0009808392166599947 Training loss: 9.339994430541992
2025-12-09 12:14:46.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.0009807556298610403 Training loss: 9.55685806274414
2025-12-09 12:14:46.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.0009806718647168817 Training loss: 9.686355590820312
2025-12-09 12:14:46.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.0009805879212585933 Training loss: 9.529613494873047
2025-12-09 12:14:47.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0009805037995173154 Training loss: 10.062671661376953
2025-12-09 12:14:47.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0009804194995242548 Training loss: 9.506930351257324
2025-12-09 12:14:47.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.0009803350213106836 Training loss: 9.514068603515625
2025-12-09 12:14:47.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.000980250364907941 Training loss: 9.592607498168945
2025-12-09 12:14:47.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0009801655303474318 Training loss: 9.10313606262207
2025-12-09 12:14:47.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.000980080517660627 Training loss: 9.748236656188965
2025-12-09 12:14:47.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.0009799953268790633 Training loss: 9.687057495117188
2025-12-09 12:14:47.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.000979909958034344 Training loss: 9.422239303588867
2025-12-09 12:14:47.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0009798244111581382 Training loss: 9.262455940246582
2025-12-09 12:14:47.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0009797386862821812 Training loss: 9.392571449279785
2025-12-09 12:14:47.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0009796527834382745 Training loss: 9.428932189941406
2025-12-09 12:14:47.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.0009795667026582847 Training loss: 9.76506519317627
2025-12-09 12:14:47.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0009794804439741454 Training loss: 9.397760391235352
2025-12-09 12:14:48.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.000979394007417856 Training loss: 9.399194717407227
2025-12-09 12:14:48.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0009793073930214817 Training loss: 8.88891887664795
2025-12-09 12:14:48.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0009792206008171535 Training loss: 9.597229957580566
2025-12-09 12:14:48.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0009791336308370687 Training loss: 9.367192268371582
2025-12-09 12:14:48.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0009790464831134903 Training loss: 10.407187461853027
2025-12-09 12:14:48.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0009789591576787476 Training loss: 9.439911842346191
2025-12-09 12:14:48.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0009788716545652352 Training loss: 9.56030559539795
2025-12-09 12:14:48.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0009787839738054146 Training loss: 9.491113662719727
2025-12-09 12:14:48.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0009786961154318121 Training loss: 9.206400871276855
2025-12-09 12:14:48.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0009786080794770206 Training loss: 9.81218433380127
2025-12-09 12:14:48.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0009785198659736987 Training loss: 9.460455894470215
2025-12-09 12:14:48.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0009784314749545706 Training loss: 9.813854217529297
2025-12-09 12:14:48.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.0009783429064524269 Training loss: 9.44400691986084
2025-12-09 12:14:49.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0009782541605001234 Training loss: 9.595867156982422
2025-12-09 12:14:49.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.0009781652371305826 Training loss: 9.54599666595459
2025-12-09 12:14:49.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0009780761363767914 Training loss: 10.206159591674805
2025-12-09 12:14:49.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.000977986858271804 Training loss: 9.31332778930664
2025-12-09 12:14:49.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0009778974028487398 Training loss: 9.574307441711426
2025-12-09 12:14:49.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.0009778077701407836 Training loss: 9.51952838897705
2025-12-09 12:14:49.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.0009777179601811866 Training loss: 9.648137092590332
2025-12-09 12:14:49.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0009776279730032654 Training loss: 9.502686500549316
2025-12-09 12:14:49.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.0009775378086404024 Training loss: 9.609862327575684
2025-12-09 12:14:49.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0009774474671260455 Training loss: 9.386524200439453
2025-12-09 12:14:49.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.000977356948493709 Training loss: 9.353564262390137
2025-12-09 12:14:49.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.000977266252776972 Training loss: 9.580150604248047
2025-12-09 12:14:49.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.0009771753800094803 Training loss: 9.474393844604492
2025-12-09 12:14:50.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0009770843302249442 Training loss: 9.669793128967285
2025-12-09 12:14:50.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0009769931034571409 Training loss: 9.552034378051758
2025-12-09 12:14:50.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0009769016997399121 Training loss: 9.708823204040527
2025-12-09 12:14:50.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.000976810119107166 Training loss: 9.632791519165039
2025-12-09 12:14:50.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0009767183615928764 Training loss: 9.443768501281738
2025-12-09 12:14:50.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.000976626427231082 Training loss: 9.456665992736816
2025-12-09 12:14:50.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0009765343160558879 Training loss: 9.441573143005371
2025-12-09 12:14:50.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0009764420281014641 Training loss: 9.530023574829102
2025-12-09 12:14:50.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0009763495634020466 Training loss: 9.529783248901367
2025-12-09 12:14:50.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0009762569219919371 Training loss: 9.725630760192871
2025-12-09 12:14:50.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0009761641039055025 Training loss: 9.582408905029297
2025-12-09 12:14:50.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0009760711091771755 Training loss: 9.493744850158691
2025-12-09 12:14:50.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.0009759779378414542 Training loss: 9.362317085266113
2025-12-09 12:14:51.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0009758845899329021 Training loss: 9.591438293457031
2025-12-09 12:14:51.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0009757910654861482 Training loss: 9.374161720275879
2025-12-09 12:14:51.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0009756973645358876 Training loss: 9.646505355834961
2025-12-09 12:14:51.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0009756034871168799 Training loss: 10.018973350524902
2025-12-09 12:14:51.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0009755094332639511 Training loss: 9.41721248626709
2025-12-09 12:14:51.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.0009754152030119921 Training loss: 9.412041664123535
2025-12-09 12:14:51.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0009753207963959591 Training loss: 9.689454078674316
2025-12-09 12:14:51.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0009752262134508741 Training loss: 9.551933288574219
2025-12-09 12:14:51.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0009751314542118246 Training loss: 9.456253051757812
2025-12-09 12:14:51.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0009750365187139631 Training loss: 9.432573318481445
2025-12-09 12:14:51.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0009749414069925077 Training loss: 9.50078296661377
2025-12-09 12:14:51.880 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0009748461190827421 Training loss: 9.425780296325684
2025-12-09 12:14:51.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0009747506550200146 Training loss: 9.639949798583984
2025-12-09 12:14:52.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.0009746550148397397 Training loss: 9.415623664855957
2025-12-09 12:14:52.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0009745591985773971 Training loss: 9.380480766296387
2025-12-09 12:14:52.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0009744632062685312 Training loss: 9.262877464294434
2025-12-09 12:14:52.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0009743670379487523 Training loss: 9.578110694885254
2025-12-09 12:14:52.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0009742706936537357 Training loss: 9.497486114501953
2025-12-09 12:14:52.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0009741741734192224 Training loss: 9.510083198547363
2025-12-09 12:14:52.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0009740774772810182 Training loss: 9.520320892333984
2025-12-09 12:14:52.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0009739806052749942 Training loss: 9.519530296325684
2025-12-09 12:14:52.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0009738835574370871 Training loss: 9.409748077392578
2025-12-09 12:14:52.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0009737863338032984 Training loss: 9.630501747131348
2025-12-09 12:14:52.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0009736889344096951 Training loss: 9.535115242004395
2025-12-09 12:14:52.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.0009735913592924093 Training loss: 9.079265594482422
2025-12-09 12:14:52.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0009734936084876383 Training loss: 9.795451164245605
2025-12-09 12:14:53.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0009733956820316443 Training loss: 9.41381549835205
2025-12-09 12:14:53.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0009732975799607554 Training loss: 9.671731948852539
2025-12-09 12:14:53.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0009731993023113641 Training loss: 9.503564834594727
2025-12-09 12:14:53.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0009731008491199284 Training loss: 9.381409645080566
2025-12-09 12:14:53.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0009730022204229714 Training loss: 9.399388313293457
2025-12-09 12:14:53.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0009729034162570811 Training loss: 9.472436904907227
2025-12-09 12:14:53.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0009728044366589108 Training loss: 9.545736312866211
2025-12-09 12:14:53.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.0009727052816651788 Training loss: 9.830061912536621
2025-12-09 12:14:53.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0009726059513126685 Training loss: 9.59287166595459
2025-12-09 12:14:53.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0009725064456382282 Training loss: 9.480696678161621
2025-12-09 12:14:53.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0009724067646787717 Training loss: 9.878253936767578
2025-12-09 12:14:53.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0009723069084712771 Training loss: 9.86694622039795
2025-12-09 12:14:53.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0009722068770527882 Training loss: 9.405942916870117
2025-12-09 12:14:54.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0009721066704604133 Training loss: 9.545364379882812
2025-12-09 12:14:54.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0009720062887313262 Training loss: 9.896819114685059
2025-12-09 12:14:54.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.000971905731902765 Training loss: 9.290905952453613
2025-12-09 12:14:54.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0009718050000120333 Training loss: 9.595635414123535
2025-12-09 12:14:54.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0009717040930964996 Training loss: 9.344304084777832
2025-12-09 12:14:54.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0009716030111935968 Training loss: 9.476461410522461
2025-12-09 12:14:54.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.0009715017543408234 Training loss: 9.528063774108887
2025-12-09 12:14:54.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0009714003225757424 Training loss: 9.885488510131836
2025-12-09 12:14:54.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0009712987159359818 Training loss: 9.434247016906738
2025-12-09 12:14:54.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.0009711969344592346 Training loss: 9.584949493408203
2025-12-09 12:14:54.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0009710949781832585 Training loss: 9.497518539428711
2025-12-09 12:14:54.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0009709928471458759 Training loss: 9.408126831054688
2025-12-09 12:14:54.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0009708905413849743 Training loss: 9.467093467712402
2025-12-09 12:14:55.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0009707880609385058 Training loss: 9.433855056762695
2025-12-09 12:14:55.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0009706854058444876 Training loss: 9.40279769897461
2025-12-09 12:14:55.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0009705825761410014 Training loss: 9.347097396850586
2025-12-09 12:14:55.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0009704795718661938 Training loss: 9.361377716064453
2025-12-09 12:14:55.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.000970376393058276 Training loss: 9.457889556884766
2025-12-09 12:14:55.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.0009702730397555246 Training loss: 9.753263473510742
2025-12-09 12:14:55.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0009701695119962799 Training loss: 9.45704174041748
2025-12-09 12:14:55.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.0009700658098189476 Training loss: 9.450897216796875
2025-12-09 12:14:55.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0009699619332619979 Training loss: 9.740973472595215
2025-12-09 12:14:55.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0009698578823639658 Training loss: 9.719575881958008
2025-12-09 12:14:55.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0009697536571634509 Training loss: 9.573390007019043
2025-12-09 12:14:55.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0009696492576991174 Training loss: 9.63094711303711
2025-12-09 12:14:55.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.0009695446840096944 Training loss: 9.35192584991455
2025-12-09 12:14:56.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0009694399361339751 Training loss: 9.754584312438965
2025-12-09 12:14:56.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0009693350141108182 Training loss: 9.479320526123047
2025-12-09 12:14:56.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.000969229917979146 Training loss: 9.34048080444336
2025-12-09 12:14:56.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.000969124647777946 Training loss: 9.754171371459961
2025-12-09 12:14:56.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0009690192035462701 Training loss: 9.540507316589355
2025-12-09 12:14:56.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.0009689135853232349 Training loss: 9.599075317382812
2025-12-09 12:14:56.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0009688077931480212 Training loss: 9.136743545532227
2025-12-09 12:14:56.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0009687018270598749 Training loss: 9.420498847961426
2025-12-09 12:14:56.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0009685956870981059 Training loss: 9.887948989868164
2025-12-09 12:14:56.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0009684893733020888 Training loss: 9.834908485412598
2025-12-09 12:14:56.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0009683828857112626 Training loss: 9.605777740478516
2025-12-09 12:14:56.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0009682762243651309 Training loss: 9.514575004577637
2025-12-09 12:14:56.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0009681693893032617 Training loss: 9.35536003112793
2025-12-09 12:14:57.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.0009680623805652876 Training loss: 9.525830268859863
2025-12-09 12:14:57.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.0009679551981909053 Training loss: 9.618093490600586
2025-12-09 12:14:57.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.000967847842219876 Training loss: 9.63184928894043
2025-12-09 12:14:57.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.0009677403126920255 Training loss: 9.476103782653809
2025-12-09 12:14:57.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0009676326096472441 Training loss: 9.539216995239258
2025-12-09 12:14:57.406 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0009675247331254858 Training loss: 9.451558113098145
2025-12-09 12:14:57.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.0009674166831667697 Training loss: 9.445915222167969
2025-12-09 12:14:57.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0009673084598111788 Training loss: 9.163688659667969
2025-12-09 12:14:57.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0009672000630988605 Training loss: 9.721531867980957
2025-12-09 12:14:57.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.0009670914930700268 Training loss: 9.195724487304688
2025-12-09 12:14:57.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.0009669827497649536 Training loss: 9.613415718078613
2025-12-09 12:14:57.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.0009668738332239813 Training loss: 9.608210563659668
2025-12-09 12:14:57.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0009667647434875144 Training loss: 9.533968925476074
2025-12-09 12:14:58.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0009666554805960219 Training loss: 9.483887672424316
2025-12-09 12:14:58.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.0009665460445900368 Training loss: 9.720917701721191
2025-12-09 12:14:58.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0009664364355101565 Training loss: 9.577486991882324
2025-12-09 12:14:58.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.0009663266533970423 Training loss: 9.527705192565918
2025-12-09 12:14:58.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.0009662166982914202 Training loss: 9.825296401977539
2025-12-09 12:14:58.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00096610657023408 Training loss: 9.78754997253418
2025-12-09 12:14:58.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0009659962692658757 Training loss: 9.385446548461914
2025-12-09 12:14:58.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.0009658857954277254 Training loss: 9.407288551330566
2025-12-09 12:14:58.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0009657751487606115 Training loss: 9.475613594055176
2025-12-09 12:14:58.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0009656643293055805 Training loss: 9.461284637451172
2025-12-09 12:14:58.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0009655533371037426 Training loss: 9.654311180114746
2025-12-09 12:14:58.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.0009654421721962729 Training loss: 9.455816268920898
2025-12-09 12:14:58.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0009653308346244099 Training loss: 9.18259048461914
2025-12-09 12:14:59.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.0009652193244294562 Training loss: 9.350995063781738
2025-12-09 12:14:59.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.0009651076416527786 Training loss: 9.566167831420898
2025-12-09 12:14:59.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.000964995786335808 Training loss: 9.7578763961792
2025-12-09 12:14:59.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.0009648837585200391 Training loss: 9.743718147277832
2025-12-09 12:14:59.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0009647715582470309 Training loss: 9.75238037109375
2025-12-09 12:14:59.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.000964659185558406 Training loss: 9.546688079833984
2025-12-09 12:14:59.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.000964546640495851 Training loss: 9.420342445373535
2025-12-09 12:14:59.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.0009644339231011168 Training loss: 10.096919059753418
2025-12-09 12:14:59.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.0009643210334160178 Training loss: 9.32069206237793
2025-12-09 12:14:59.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0009642079714824328 Training loss: 9.516759872436523
2025-12-09 12:14:59.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.0009640947373423039 Training loss: 9.69282341003418
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.37 GiB is free. Including non-PyTorch memory, this process has 90.83 GiB memory in use. Of the allocated memory 89.58 GiB is allocated by PyTorch, and 500.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 16/10000 [00:00<01:08, 146.41it/s]Tokenizing texts:   0%|          | 36/10000 [00:00<00:56, 175.94it/s]Tokenizing texts:   1%|          | 54/10000 [00:00<01:21, 122.39it/s]Tokenizing texts:   1%|          | 73/10000 [00:00<01:10, 141.46it/s]Tokenizing texts:   1%|          | 98/10000 [00:00<00:57, 173.51it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 213.77it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:53, 185.27it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:53, 184.39it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 197.23it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 202.39it/s]Tokenizing texts:   2%|▏         | 241/10000 [00:01<00:47, 205.07it/s]Tokenizing texts:   3%|▎         | 262/10000 [00:01<00:47, 204.92it/s]Tokenizing texts:   3%|▎         | 291/10000 [00:01<00:42, 227.85it/s]Tokenizing texts:   3%|▎         | 315/10000 [00:01<00:42, 226.72it/s]Tokenizing texts:   3%|▎         | 338/10000 [00:01<00:53, 182.17it/s]Tokenizing texts:   4%|▎         | 363/10000 [00:01<00:48, 198.36it/s]Tokenizing texts:   4%|▍         | 385/10000 [00:02<00:52, 184.70it/s]Tokenizing texts:   4%|▍         | 406/10000 [00:02<00:51, 186.11it/s]Tokenizing texts:   4%|▍         | 426/10000 [00:02<00:51, 185.59it/s]Tokenizing texts:   4%|▍         | 446/10000 [00:02<00:51, 187.11it/s]Tokenizing texts:   5%|▍         | 468/10000 [00:02<00:48, 195.56it/s]Tokenizing texts:   5%|▍         | 497/10000 [00:02<00:43, 217.77it/s]Tokenizing texts:   5%|▌         | 526/10000 [00:02<00:40, 236.77it/s]Tokenizing texts:   6%|▌         | 550/10000 [00:02<00:50, 186.12it/s]Tokenizing texts:   6%|▌         | 580/10000 [00:02<00:44, 211.89it/s]Tokenizing texts:   6%|▌         | 603/10000 [00:03<00:44, 210.72it/s]Tokenizing texts:   6%|▋         | 626/10000 [00:03<00:46, 200.32it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:46, 202.05it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:46, 199.82it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 190.97it/s]Tokenizing texts:   7%|▋         | 721/10000 [00:03<00:46, 200.39it/s]Tokenizing texts:   7%|▋         | 742/10000 [00:03<00:46, 200.17it/s]Tokenizing texts:   8%|▊         | 766/10000 [00:03<00:43, 210.54it/s]Tokenizing texts:   8%|▊         | 788/10000 [00:04<00:45, 202.24it/s]Tokenizing texts:   8%|▊         | 813/10000 [00:04<00:43, 212.19it/s]Tokenizing texts:   8%|▊         | 835/10000 [00:04<00:43, 212.62it/s]Tokenizing texts:   9%|▊         | 861/10000 [00:04<00:40, 225.54it/s]Tokenizing texts:   9%|▉         | 884/10000 [00:04<00:41, 217.79it/s]Tokenizing texts:   9%|▉         | 910/10000 [00:04<00:39, 228.21it/s]Tokenizing texts:   9%|▉         | 940/10000 [00:04<00:36, 247.97it/s]Tokenizing texts:  10%|▉         | 972/10000 [00:04<00:33, 267.84it/s]Tokenizing texts:  10%|█         | 1001/10000 [00:04<00:33, 272.15it/s]Tokenizing texts:  10%|█         | 1032/10000 [00:04<00:31, 283.21it/s]Tokenizing texts:  11%|█         | 1061/10000 [00:05<00:39, 224.89it/s]Tokenizing texts:  11%|█         | 1086/10000 [00:05<00:39, 225.47it/s]Tokenizing texts:  11%|█         | 1110/10000 [00:05<00:39, 227.73it/s]Tokenizing texts:  11%|█▏        | 1134/10000 [00:05<00:45, 193.54it/s]Tokenizing texts:  12%|█▏        | 1159/10000 [00:05<00:43, 204.52it/s]Tokenizing texts:  12%|█▏        | 1184/10000 [00:05<00:40, 215.17it/s]Tokenizing texts:  12%|█▏        | 1207/10000 [00:05<00:43, 202.42it/s]Tokenizing texts:  12%|█▏        | 1235/10000 [00:05<00:41, 210.56it/s]Tokenizing texts:  13%|█▎        | 1257/10000 [00:06<00:42, 204.65it/s]Tokenizing texts:  13%|█▎        | 1291/10000 [00:06<00:36, 240.24it/s]Tokenizing texts:  13%|█▎        | 1316/10000 [00:06<00:36, 234.98it/s]Tokenizing texts:  13%|█▎        | 1341/10000 [00:06<00:42, 204.41it/s]Tokenizing texts:  14%|█▎        | 1368/10000 [00:06<00:39, 219.71it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:39, 215.60it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 233.08it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:34, 250.20it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:07<00:31, 273.36it/s]Tokenizing texts:  15%|█▌        | 1516/10000 [00:07<00:31, 267.29it/s]Tokenizing texts:  15%|█▌        | 1544/10000 [00:07<00:33, 250.81it/s]Tokenizing texts:  16%|█▌        | 1577/10000 [00:07<00:30, 271.94it/s]Tokenizing texts:  16%|█▌        | 1605/10000 [00:07<00:33, 252.66it/s]Tokenizing texts:  16%|█▋        | 1631/10000 [00:07<00:40, 208.43it/s]Tokenizing texts:  17%|█▋        | 1657/10000 [00:07<00:38, 215.76it/s]Tokenizing texts:  17%|█▋        | 1680/10000 [00:07<00:38, 217.07it/s]Tokenizing texts:  17%|█▋        | 1703/10000 [00:07<00:37, 220.19it/s]Tokenizing texts:  17%|█▋        | 1735/10000 [00:08<00:33, 244.17it/s]Tokenizing texts:  18%|█▊        | 1761/10000 [00:08<00:33, 243.89it/s]Tokenizing texts:  18%|█▊        | 1786/10000 [00:08<00:33, 244.87it/s]Tokenizing texts:  18%|█▊        | 1820/10000 [00:08<00:30, 271.29it/s]Tokenizing texts:  18%|█▊        | 1848/10000 [00:08<00:37, 217.45it/s]Tokenizing texts:  19%|█▊        | 1872/10000 [00:08<00:36, 220.33it/s]Tokenizing texts:  19%|█▉        | 1896/10000 [00:08<00:36, 224.03it/s]Tokenizing texts:  19%|█▉        | 1920/10000 [00:08<00:36, 224.19it/s]Tokenizing texts:  19%|█▉        | 1946/10000 [00:09<00:41, 194.45it/s]Tokenizing texts:  20%|█▉        | 1972/10000 [00:09<00:38, 209.76it/s]Tokenizing texts:  20%|██        | 2000/10000 [00:09<00:35, 227.18it/s]Tokenizing texts:  20%|██        | 2026/10000 [00:09<00:34, 232.82it/s]Tokenizing texts:  21%|██        | 2055/10000 [00:09<00:32, 247.43it/s]Tokenizing texts:  21%|██        | 2081/10000 [00:09<00:37, 208.65it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:37, 209.32it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:36, 214.51it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:09<00:38, 205.36it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:35, 223.13it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:31, 250.75it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:29, 259.54it/s]Tokenizing texts:  23%|██▎       | 2274/10000 [00:10<00:28, 269.23it/s]Tokenizing texts:  23%|██▎       | 2302/10000 [00:10<00:29, 259.94it/s]Tokenizing texts:  23%|██▎       | 2329/10000 [00:10<00:36, 210.66it/s]Tokenizing texts:  24%|██▎       | 2365/10000 [00:10<00:30, 246.63it/s]Tokenizing texts:  24%|██▍       | 2399/10000 [00:10<00:28, 267.68it/s]Tokenizing texts:  24%|██▍       | 2428/10000 [00:11<00:30, 247.73it/s]Tokenizing texts:  25%|██▍       | 2455/10000 [00:11<00:30, 250.93it/s]Tokenizing texts:  25%|██▍       | 2482/10000 [00:11<00:30, 246.01it/s]Tokenizing texts:  25%|██▌       | 2508/10000 [00:11<00:30, 245.24it/s]Tokenizing texts:  25%|██▌       | 2538/10000 [00:11<00:28, 258.42it/s]Tokenizing texts:  26%|██▌       | 2565/10000 [00:11<00:33, 222.14it/s]Tokenizing texts:  26%|██▌       | 2589/10000 [00:11<00:33, 221.30it/s]Tokenizing texts:  26%|██▌       | 2612/10000 [00:11<00:36, 203.74it/s]Tokenizing texts:  26%|██▋       | 2641/10000 [00:12<00:33, 222.55it/s]Tokenizing texts:  27%|██▋       | 2667/10000 [00:12<00:31, 230.81it/s]Tokenizing texts:  27%|██▋       | 2691/10000 [00:12<00:32, 224.00it/s]Tokenizing texts:  27%|██▋       | 2714/10000 [00:12<00:35, 203.43it/s]Tokenizing texts:  27%|██▋       | 2749/10000 [00:12<00:30, 240.84it/s]Tokenizing texts:  28%|██▊       | 2776/10000 [00:12<00:29, 244.44it/s]Tokenizing texts:  28%|██▊       | 2802/10000 [00:12<00:39, 181.38it/s]Tokenizing texts:  28%|██▊       | 2837/10000 [00:12<00:32, 218.69it/s]Tokenizing texts:  29%|██▊       | 2863/10000 [00:13<00:35, 202.02it/s]Tokenizing texts:  29%|██▉       | 2890/10000 [00:13<00:32, 217.52it/s]Tokenizing texts:  29%|██▉       | 2923/10000 [00:13<00:28, 244.32it/s]Tokenizing texts:  30%|██▉       | 2950/10000 [00:13<00:31, 222.00it/s]Tokenizing texts:  30%|██▉       | 2974/10000 [00:13<00:31, 223.41it/s]Tokenizing texts:  30%|███       | 3012/10000 [00:13<00:26, 263.46it/s]Tokenizing texts:  30%|███       | 3040/10000 [00:13<00:30, 227.27it/s]Tokenizing texts:  31%|███       | 3074/10000 [00:13<00:27, 253.47it/s]Tokenizing texts:  31%|███       | 3102/10000 [00:14<00:28, 243.68it/s]Tokenizing texts:  31%|███▏      | 3136/10000 [00:14<00:25, 266.90it/s]Tokenizing texts:  32%|███▏      | 3164/10000 [00:14<00:26, 253.77it/s]Tokenizing texts:  32%|███▏      | 3192/10000 [00:14<00:26, 259.78it/s]Tokenizing texts:  32%|███▏      | 3220/10000 [00:14<00:25, 263.84it/s]Tokenizing texts:  32%|███▏      | 3247/10000 [00:14<00:26, 257.03it/s]Tokenizing texts:  33%|███▎      | 3278/10000 [00:14<00:25, 268.46it/s]Tokenizing texts:  33%|███▎      | 3306/10000 [00:14<00:25, 260.68it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:14<00:26, 248.63it/s]Tokenizing texts:  34%|███▎      | 3361/10000 [00:15<00:25, 257.10it/s]Tokenizing texts:  34%|███▍      | 3387/10000 [00:15<00:26, 252.85it/s]Tokenizing texts:  34%|███▍      | 3413/10000 [00:15<00:25, 254.79it/s]Tokenizing texts:  35%|███▍      | 3453/10000 [00:15<00:22, 293.99it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:15<00:22, 289.86it/s]Tokenizing texts:  35%|███▌      | 3517/10000 [00:15<00:26, 246.09it/s]Tokenizing texts:  35%|███▌      | 3543/10000 [00:15<00:27, 233.04it/s]Tokenizing texts:  36%|███▌      | 3570/10000 [00:15<00:26, 241.51it/s]Tokenizing texts:  36%|███▌      | 3600/10000 [00:15<00:25, 253.27it/s]Tokenizing texts:  36%|███▋      | 3626/10000 [00:16<00:25, 251.83it/s]Tokenizing texts:  37%|███▋      | 3652/10000 [00:16<00:25, 249.94it/s]Tokenizing texts:  37%|███▋      | 3678/10000 [00:16<00:25, 247.33it/s]Tokenizing texts:  37%|███▋      | 3705/10000 [00:16<00:24, 252.21it/s]Tokenizing texts:  37%|███▋      | 3732/10000 [00:16<00:24, 256.53it/s]Tokenizing texts:  38%|███▊      | 3758/10000 [00:16<00:27, 225.56it/s]Tokenizing texts:  38%|███▊      | 3784/10000 [00:16<00:26, 233.93it/s]Tokenizing texts:  38%|███▊      | 3809/10000 [00:16<00:30, 206.29it/s]Tokenizing texts:  38%|███▊      | 3841/10000 [00:16<00:26, 234.92it/s]Tokenizing texts:  39%|███▊      | 3868/10000 [00:17<00:25, 243.47it/s]Tokenizing texts:  39%|███▉      | 3895/10000 [00:17<00:24, 249.86it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:25, 234.49it/s]Tokenizing texts:  39%|███▉      | 3946/10000 [00:17<00:26, 229.53it/s]Tokenizing texts:  40%|███▉      | 3979/10000 [00:17<00:23, 256.45it/s]Tokenizing texts:  40%|████      | 4006/10000 [00:17<00:27, 219.27it/s]Tokenizing texts:  40%|████      | 4037/10000 [00:17<00:25, 237.34it/s]Tokenizing texts:  41%|████      | 4067/10000 [00:17<00:23, 252.98it/s]Tokenizing texts:  41%|████      | 4099/10000 [00:17<00:21, 270.71it/s]Tokenizing texts:  41%|████▏     | 4132/10000 [00:18<00:20, 286.50it/s]Tokenizing texts:  42%|████▏     | 4162/10000 [00:18<00:21, 271.74it/s]Tokenizing texts:  42%|████▏     | 4195/10000 [00:18<00:20, 285.38it/s]Tokenizing texts:  42%|████▏     | 4225/10000 [00:18<00:22, 260.13it/s]Tokenizing texts:  43%|████▎     | 4252/10000 [00:18<00:23, 245.68it/s]Tokenizing texts:  43%|████▎     | 4278/10000 [00:18<00:24, 235.73it/s]Tokenizing texts:  43%|████▎     | 4310/10000 [00:18<00:22, 256.95it/s]Tokenizing texts:  43%|████▎     | 4345/10000 [00:18<00:20, 279.68it/s]Tokenizing texts:  44%|████▎     | 4374/10000 [00:19<00:21, 262.81it/s]Tokenizing texts:  44%|████▍     | 4412/10000 [00:19<00:19, 290.93it/s]Tokenizing texts:  44%|████▍     | 4442/10000 [00:19<00:19, 279.33it/s]Tokenizing texts:  45%|████▍     | 4471/10000 [00:19<00:21, 256.92it/s]Tokenizing texts:  45%|████▍     | 4498/10000 [00:19<00:22, 241.24it/s]Tokenizing texts:  45%|████▌     | 4525/10000 [00:19<00:22, 247.84it/s]Tokenizing texts:  46%|████▌     | 4552/10000 [00:19<00:21, 253.15it/s]Tokenizing texts:  46%|████▌     | 4578/10000 [00:19<00:21, 250.75it/s]Tokenizing texts:  46%|████▌     | 4604/10000 [00:19<00:22, 239.27it/s]Tokenizing texts:  46%|████▋     | 4637/10000 [00:20<00:20, 262.02it/s]Tokenizing texts:  47%|████▋     | 4671/10000 [00:20<00:18, 282.30it/s]Tokenizing texts:  47%|████▋     | 4708/10000 [00:20<00:17, 304.10it/s]Tokenizing texts:  47%|████▋     | 4739/10000 [00:20<00:18, 292.20it/s]Tokenizing texts:  48%|████▊     | 4769/10000 [00:20<00:18, 281.95it/s]Tokenizing texts:  48%|████▊     | 4798/10000 [00:20<00:21, 244.67it/s]Tokenizing texts:  48%|████▊     | 4830/10000 [00:20<00:19, 261.08it/s]Tokenizing texts:  49%|████▊     | 4859/10000 [00:20<00:19, 266.23it/s]Tokenizing texts:  49%|████▉     | 4887/10000 [00:20<00:19, 267.67it/s]Tokenizing texts:  49%|████▉     | 4915/10000 [00:21<00:18, 270.43it/s]Tokenizing texts:  49%|████▉     | 4943/10000 [00:21<00:18, 268.69it/s]Tokenizing texts:  50%|████▉     | 4971/10000 [00:21<00:19, 259.94it/s]Tokenizing texts:  50%|█████     | 5010/10000 [00:21<00:16, 295.35it/s]Tokenizing texts:  50%|█████     | 5040/10000 [00:21<00:17, 277.28it/s]Tokenizing texts:  51%|█████     | 5069/10000 [00:21<00:20, 245.12it/s]Tokenizing texts:  51%|█████     | 5095/10000 [00:21<00:20, 242.10it/s]Tokenizing texts:  51%|█████     | 5122/10000 [00:21<00:20, 233.79it/s]Tokenizing texts:  52%|█████▏    | 5150/10000 [00:22<00:19, 244.04it/s]Tokenizing texts:  52%|█████▏    | 5178/10000 [00:22<00:19, 253.02it/s]Tokenizing texts:  52%|█████▏    | 5204/10000 [00:22<00:20, 238.24it/s]Tokenizing texts:  52%|█████▏    | 5231/10000 [00:22<00:19, 242.27it/s]Tokenizing texts:  53%|█████▎    | 5265/10000 [00:22<00:17, 265.94it/s]Tokenizing texts:  53%|█████▎    | 5292/10000 [00:22<00:18, 253.56it/s]Tokenizing texts:  53%|█████▎    | 5327/10000 [00:22<00:16, 279.80it/s]Tokenizing texts:  54%|█████▎    | 5359/10000 [00:22<00:16, 289.94it/s]Tokenizing texts:  54%|█████▍    | 5390/10000 [00:22<00:15, 294.86it/s]Tokenizing texts:  54%|█████▍    | 5423/10000 [00:22<00:15, 302.97it/s]Tokenizing texts:  55%|█████▍    | 5454/10000 [00:23<00:16, 274.38it/s]Tokenizing texts:  55%|█████▍    | 5483/10000 [00:23<00:17, 259.79it/s]Tokenizing texts:  55%|█████▌    | 5510/10000 [00:23<00:20, 217.89it/s]Tokenizing texts:  55%|█████▌    | 5542/10000 [00:23<00:18, 240.17it/s]Tokenizing texts:  56%|█████▌    | 5570/10000 [00:23<00:18, 242.24it/s]Tokenizing texts:  56%|█████▌    | 5600/10000 [00:23<00:17, 256.65it/s]Tokenizing texts:  56%|█████▋    | 5636/10000 [00:23<00:15, 283.01it/s]Tokenizing texts:  57%|█████▋    | 5672/10000 [00:23<00:14, 298.16it/s]Tokenizing texts:  57%|█████▋    | 5703/10000 [00:24<00:14, 297.51it/s]Tokenizing texts:  57%|█████▋    | 5734/10000 [00:24<00:15, 284.03it/s]Tokenizing texts:  58%|█████▊    | 5763/10000 [00:24<00:16, 258.87it/s]Tokenizing texts:  58%|█████▊    | 5790/10000 [00:24<00:19, 220.20it/s]Tokenizing texts:  58%|█████▊    | 5814/10000 [00:24<00:19, 218.11it/s]Tokenizing texts:  58%|█████▊    | 5837/10000 [00:24<00:19, 214.46it/s]Tokenizing texts:  59%|█████▊    | 5874/10000 [00:24<00:17, 239.26it/s]Tokenizing texts:  59%|█████▉    | 5899/10000 [00:24<00:16, 241.34it/s]Tokenizing texts:  59%|█████▉    | 5924/10000 [00:25<00:17, 238.68it/s]Tokenizing texts:  60%|█████▉    | 5952/10000 [00:25<00:16, 248.62it/s]Tokenizing texts:  60%|█████▉    | 5980/10000 [00:25<00:15, 254.48it/s]Tokenizing texts:  60%|██████    | 6008/10000 [00:25<00:15, 251.70it/s]Tokenizing texts:  60%|██████    | 6034/10000 [00:25<00:15, 250.39it/s]Tokenizing texts:  61%|██████    | 6066/10000 [00:25<00:15, 262.17it/s]Tokenizing texts:  61%|██████    | 6093/10000 [00:25<00:15, 259.44it/s]Tokenizing texts:  61%|██████    | 6120/10000 [00:25<00:14, 260.44it/s]Tokenizing texts:  62%|██████▏   | 6154/10000 [00:25<00:13, 277.62it/s]Tokenizing texts:  62%|██████▏   | 6182/10000 [00:25<00:13, 276.15it/s]Tokenizing texts:  62%|██████▏   | 6210/10000 [00:26<00:13, 275.56it/s]Tokenizing texts:  62%|██████▏   | 6238/10000 [00:26<00:15, 249.22it/s]Tokenizing texts:  63%|██████▎   | 6264/10000 [00:26<00:15, 246.11it/s]Tokenizing texts:  63%|██████▎   | 6290/10000 [00:26<00:14, 249.46it/s]Tokenizing texts:  63%|██████▎   | 6316/10000 [00:26<00:15, 234.94it/s]Tokenizing texts:  63%|██████▎   | 6340/10000 [00:26<00:15, 229.91it/s]Tokenizing texts:  64%|██████▎   | 6368/10000 [00:26<00:14, 242.21it/s]Tokenizing texts:  64%|██████▍   | 6395/10000 [00:26<00:14, 242.13it/s]Tokenizing texts:  64%|██████▍   | 6423/10000 [00:26<00:14, 252.40it/s]Tokenizing texts:  65%|██████▍   | 6453/10000 [00:27<00:13, 263.68it/s]Tokenizing texts:  65%|██████▍   | 6480/10000 [00:27<00:14, 240.96it/s]Tokenizing texts:  65%|██████▌   | 6506/10000 [00:27<00:15, 232.56it/s]Tokenizing texts:  65%|██████▌   | 6531/10000 [00:27<00:14, 236.73it/s]Tokenizing texts:  66%|██████▌   | 6572/10000 [00:27<00:12, 284.20it/s]Tokenizing texts:  66%|██████▌   | 6602/10000 [00:27<00:12, 272.11it/s]Tokenizing texts:  66%|██████▋   | 6630/10000 [00:27<00:12, 265.46it/s]Tokenizing texts:  67%|██████▋   | 6658/10000 [00:27<00:12, 268.29it/s]Tokenizing texts:  67%|██████▋   | 6686/10000 [00:27<00:12, 266.20it/s]Tokenizing texts:  67%|██████▋   | 6713/10000 [00:28<00:12, 255.30it/s]Tokenizing texts:  67%|██████▋   | 6739/10000 [00:28<00:13, 246.91it/s]Tokenizing texts:  68%|██████▊   | 6770/10000 [00:28<00:12, 264.16it/s]Tokenizing texts:  68%|██████▊   | 6804/10000 [00:28<00:11, 285.61it/s]Tokenizing texts:  68%|██████▊   | 6837/10000 [00:28<00:10, 293.88it/s]Tokenizing texts:  69%|██████▊   | 6869/10000 [00:28<00:10, 289.70it/s]Tokenizing texts:  69%|██████▉   | 6899/10000 [00:28<00:13, 237.49it/s]Tokenizing texts:  69%|██████▉   | 6925/10000 [00:28<00:13, 233.23it/s]Tokenizing texts:  70%|██████▉   | 6950/10000 [00:29<00:13, 234.14it/s]Tokenizing texts:  70%|██████▉   | 6978/10000 [00:29<00:12, 243.43it/s]Tokenizing texts:  70%|███████   | 7009/10000 [00:29<00:11, 259.14it/s]Tokenizing texts:  70%|███████   | 7036/10000 [00:29<00:11, 249.47it/s]Tokenizing texts:  71%|███████   | 7062/10000 [00:29<00:13, 211.41it/s]Tokenizing texts:  71%|███████   | 7086/10000 [00:29<00:13, 217.44it/s]Tokenizing texts:  71%|███████   | 7112/10000 [00:29<00:12, 227.53it/s]Tokenizing texts:  71%|███████▏  | 7142/10000 [00:29<00:11, 246.96it/s]Tokenizing texts:  72%|███████▏  | 7168/10000 [00:29<00:11, 237.20it/s]Tokenizing texts:  72%|███████▏  | 7195/10000 [00:30<00:11, 245.86it/s]Tokenizing texts:  72%|███████▏  | 7224/10000 [00:30<00:10, 256.05it/s]Tokenizing texts:  73%|███████▎  | 7251/10000 [00:30<00:11, 244.20it/s]Tokenizing texts:  73%|███████▎  | 7297/10000 [00:30<00:09, 296.89it/s]Tokenizing texts:  73%|███████▎  | 7331/10000 [00:30<00:08, 306.41it/s]Tokenizing texts:  74%|███████▎  | 7362/10000 [00:30<00:09, 274.47it/s]Tokenizing texts:  74%|███████▍  | 7391/10000 [00:30<00:09, 263.74it/s]Tokenizing texts:  74%|███████▍  | 7418/10000 [00:30<00:11, 222.10it/s]Tokenizing texts:  74%|███████▍  | 7442/10000 [00:31<00:12, 205.70it/s]Tokenizing texts:  75%|███████▍  | 7474/10000 [00:31<00:10, 231.95it/s]Tokenizing texts:  75%|███████▌  | 7509/10000 [00:31<00:09, 261.27it/s]Tokenizing texts:  75%|███████▌  | 7538/10000 [00:31<00:09, 268.35it/s]Tokenizing texts:  76%|███████▌  | 7566/10000 [00:31<00:09, 262.08it/s]Tokenizing texts:  76%|███████▌  | 7603/10000 [00:31<00:08, 289.03it/s]Tokenizing texts:  76%|███████▋  | 7633/10000 [00:31<00:08, 272.38it/s]Tokenizing texts:  77%|███████▋  | 7663/10000 [00:31<00:08, 276.65it/s]Tokenizing texts:  77%|███████▋  | 7696/10000 [00:31<00:07, 289.16it/s]Tokenizing texts:  77%|███████▋  | 7726/10000 [00:32<00:08, 261.22it/s]Tokenizing texts:  78%|███████▊  | 7753/10000 [00:32<00:08, 258.86it/s]Tokenizing texts:  78%|███████▊  | 7780/10000 [00:32<00:08, 247.01it/s]Tokenizing texts:  78%|███████▊  | 7806/10000 [00:32<00:09, 232.67it/s]Tokenizing texts:  78%|███████▊  | 7835/10000 [00:32<00:08, 245.89it/s]Tokenizing texts:  79%|███████▊  | 7865/10000 [00:32<00:08, 259.78it/s]Tokenizing texts:  79%|███████▉  | 7892/10000 [00:32<00:08, 240.51it/s]Tokenizing texts:  79%|███████▉  | 7929/10000 [00:32<00:07, 272.30it/s]Tokenizing texts:  80%|███████▉  | 7961/10000 [00:32<00:07, 279.43it/s]Tokenizing texts:  80%|███████▉  | 7995/10000 [00:33<00:06, 294.20it/s]Tokenizing texts:  80%|████████  | 8025/10000 [00:33<00:06, 287.55it/s]Tokenizing texts:  81%|████████  | 8055/10000 [00:33<00:11, 173.08it/s]Tokenizing texts:  81%|████████  | 8092/10000 [00:33<00:09, 209.66it/s]Tokenizing texts:  81%|████████  | 8119/10000 [00:33<00:08, 209.61it/s]Tokenizing texts:  81%|████████▏ | 8148/10000 [00:33<00:08, 226.57it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:33<00:07, 239.11it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:34<00:07, 249.44it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 214.49it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 224.81it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 257.83it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 253.58it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:34<00:06, 257.32it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:34<00:06, 256.72it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:34<00:06, 232.40it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:35<00:06, 258.03it/s]Tokenizing texts:  85%|████████▍ | 8472/10000 [00:35<00:05, 263.48it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:35<00:05, 278.25it/s]Tokenizing texts:  85%|████████▌ | 8533/10000 [00:35<00:05, 260.91it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:35<00:05, 267.44it/s]Tokenizing texts:  86%|████████▌ | 8590/10000 [00:35<00:05, 262.55it/s]Tokenizing texts:  86%|████████▌ | 8624/10000 [00:35<00:04, 283.78it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:35<00:04, 285.61it/s]Tokenizing texts:  87%|████████▋ | 8687/10000 [00:35<00:04, 295.15it/s]Tokenizing texts:  87%|████████▋ | 8723/10000 [00:36<00:04, 310.57it/s]Tokenizing texts:  88%|████████▊ | 8755/10000 [00:36<00:04, 281.11it/s]Tokenizing texts:  88%|████████▊ | 8785/10000 [00:36<00:04, 284.45it/s]Tokenizing texts:  88%|████████▊ | 8814/10000 [00:36<00:05, 218.08it/s]Tokenizing texts:  88%|████████▊ | 8839/10000 [00:36<00:05, 219.72it/s]Tokenizing texts:  89%|████████▊ | 8872/10000 [00:36<00:04, 245.67it/s]Tokenizing texts:  89%|████████▉ | 8911/10000 [00:36<00:03, 280.72it/s]Tokenizing texts:  89%|████████▉ | 8941/10000 [00:36<00:03, 283.74it/s]Tokenizing texts:  90%|████████▉ | 8971/10000 [00:36<00:03, 275.51it/s]Tokenizing texts:  90%|█████████ | 9000/10000 [00:37<00:04, 247.47it/s]Tokenizing texts:  90%|█████████ | 9030/10000 [00:37<00:03, 259.29it/s]Tokenizing texts:  91%|█████████ | 9057/10000 [00:37<00:03, 250.77it/s]Tokenizing texts:  91%|█████████ | 9085/10000 [00:37<00:03, 256.72it/s]Tokenizing texts:  91%|█████████ | 9115/10000 [00:37<00:03, 267.42it/s]Tokenizing texts:  91%|█████████▏| 9146/10000 [00:37<00:03, 275.79it/s]Tokenizing texts:  92%|█████████▏| 9175/10000 [00:37<00:02, 279.41it/s]Tokenizing texts:  92%|█████████▏| 9205/10000 [00:37<00:02, 283.26it/s]Tokenizing texts:  92%|█████████▏| 9234/10000 [00:37<00:02, 274.92it/s]Tokenizing texts:  93%|█████████▎| 9262/10000 [00:38<00:02, 274.91it/s]Tokenizing texts:  93%|█████████▎| 9290/10000 [00:38<00:02, 254.70it/s]Tokenizing texts:  93%|█████████▎| 9316/10000 [00:38<00:02, 232.24it/s]Tokenizing texts:  93%|█████████▎| 9348/10000 [00:38<00:02, 254.47it/s]Tokenizing texts:  94%|█████████▍| 9375/10000 [00:38<00:02, 221.85it/s]Tokenizing texts:  94%|█████████▍| 9403/10000 [00:38<00:02, 230.44it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:38<00:02, 230.50it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:38<00:02, 234.70it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:39<00:02, 243.84it/s]Tokenizing texts:  95%|█████████▌| 9504/10000 [00:39<00:02, 183.63it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 223.54it/s]Tokenizing texts:  96%|█████████▌| 9566/10000 [00:39<00:01, 230.43it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 234.43it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 232.52it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 235.02it/s]Tokenizing texts:  97%|█████████▋| 9673/10000 [00:39<00:01, 238.99it/s]Tokenizing texts:  97%|█████████▋| 9698/10000 [00:40<00:01, 236.30it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:40<00:01, 239.81it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 208.06it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 216.56it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 262.28it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 255.11it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 260.60it/s]Tokenizing texts:  99%|█████████▉| 9907/10000 [00:40<00:00, 287.88it/s]Tokenizing texts:  99%|█████████▉| 9937/10000 [00:40<00:00, 284.56it/s]Tokenizing texts: 100%|█████████▉| 9968/10000 [00:41<00:00, 291.61it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:41<00:00, 243.23it/s]
2025-12-09 12:16:00.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 12.024267196655273
2025-12-09 12:16:00.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 12.030048370361328
2025-12-09 12:16:00.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 12.044998168945312
2025-12-09 12:16:00.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 12.014548301696777
2025-12-09 12:16:00.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 12.041017532348633
2025-12-09 12:16:00.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 12.05860424041748
2025-12-09 12:16:00.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 12.04239559173584
2025-12-09 12:16:00.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 12.050808906555176
2025-12-09 12:16:01.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 12.016907691955566
2025-12-09 12:16:01.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 12.0440092086792
2025-12-09 12:16:01.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 12.010168075561523
2025-12-09 12:16:01.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 12.021406173706055
2025-12-09 12:16:01.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 12.054570198059082
2025-12-09 12:16:01.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 12.007979393005371
2025-12-09 12:16:01.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 11.979269027709961
2025-12-09 12:16:01.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 12.01866340637207
2025-12-09 12:16:01.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 11.992287635803223
2025-12-09 12:16:01.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 11.972848892211914
2025-12-09 12:16:01.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 11.961433410644531
2025-12-09 12:16:01.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 12.02200984954834
2025-12-09 12:16:01.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 11.9581298828125
2025-12-09 12:16:02.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 11.951875686645508
2025-12-09 12:16:02.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 11.898673057556152
2025-12-09 12:16:02.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 11.938273429870605
2025-12-09 12:16:02.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 11.873459815979004
2025-12-09 12:16:02.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 11.898334503173828
2025-12-09 12:16:02.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 11.80880069732666
2025-12-09 12:16:02.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 11.820951461791992
2025-12-09 12:16:02.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 11.840654373168945
2025-12-09 12:16:02.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 11.82915210723877
2025-12-09 12:16:02.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 11.78573989868164
2025-12-09 12:16:02.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 11.737135887145996
2025-12-09 12:16:02.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 11.72135066986084
2025-12-09 12:16:02.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 11.577619552612305
2025-12-09 12:16:03.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 11.629162788391113
2025-12-09 12:16:03.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 11.60847282409668
2025-12-09 12:16:03.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 11.51427936553955
2025-12-09 12:16:03.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 11.558639526367188
2025-12-09 12:16:03.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 11.480337142944336
2025-12-09 12:16:03.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 11.400237083435059
2025-12-09 12:16:03.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 11.47065544128418
2025-12-09 12:16:03.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 11.341158866882324
2025-12-09 12:16:03.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 11.390970230102539
2025-12-09 12:16:03.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 11.354901313781738
2025-12-09 12:16:03.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 11.258203506469727
2025-12-09 12:16:03.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 11.186676025390625
2025-12-09 12:16:03.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 11.047822952270508
2025-12-09 12:16:03.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 11.143980026245117
2025-12-09 12:16:04.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 11.017434120178223
2025-12-09 12:16:04.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 11.021531105041504
2025-12-09 12:16:04.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 10.812978744506836
2025-12-09 12:16:04.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 10.97402572631836
2025-12-09 12:16:04.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 10.872167587280273
2025-12-09 12:16:04.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 10.896175384521484
2025-12-09 12:16:04.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 10.721477508544922
2025-12-09 12:16:04.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 10.798027992248535
2025-12-09 12:16:04.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 11.192351341247559
2025-12-09 12:16:04.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 11.227590560913086
2025-12-09 12:16:04.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 10.61880111694336
2025-12-09 12:16:04.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 10.831496238708496
2025-12-09 12:16:04.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 10.537388801574707
2025-12-09 12:16:05.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 10.475534439086914
2025-12-09 12:16:05.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 10.547052383422852
2025-12-09 12:16:05.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 10.634500503540039
2025-12-09 12:16:05.289 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 10.683850288391113
2025-12-09 12:16:05.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 10.380910873413086
2025-12-09 12:16:05.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 10.123600006103516
2025-12-09 12:16:05.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 10.367328643798828
2025-12-09 12:16:05.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 10.174383163452148
2025-12-09 12:16:05.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 10.232754707336426
2025-12-09 12:16:05.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 10.271026611328125
2025-12-09 12:16:05.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 10.364805221557617
2025-12-09 12:16:05.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 10.129598617553711
2025-12-09 12:16:05.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 10.157048225402832
2025-12-09 12:16:06.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 10.249174118041992
2025-12-09 12:16:06.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 10.095078468322754
2025-12-09 12:16:06.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 10.230103492736816
2025-12-09 12:16:06.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 10.051762580871582
2025-12-09 12:16:06.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 9.978800773620605
2025-12-09 12:16:06.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 10.231526374816895
2025-12-09 12:16:06.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 10.003779411315918
2025-12-09 12:16:06.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 10.278371810913086
2025-12-09 12:16:06.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 10.062847137451172
2025-12-09 12:16:06.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 9.986542701721191
2025-12-09 12:16:06.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 9.964905738830566
2025-12-09 12:16:06.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 10.08197021484375
2025-12-09 12:16:06.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 9.700592041015625
2025-12-09 12:16:07.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 9.862425804138184
2025-12-09 12:16:07.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 9.781599044799805
2025-12-09 12:16:07.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 9.8959321975708
2025-12-09 12:16:07.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 10.065193176269531
2025-12-09 12:16:07.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 9.945561408996582
2025-12-09 12:16:07.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 10.251777648925781
2025-12-09 12:16:07.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 9.768040657043457
2025-12-09 12:16:07.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 9.619107246398926
2025-12-09 12:16:07.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 9.95107364654541
2025-12-09 12:16:07.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 9.867955207824707
2025-12-09 12:16:07.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 9.853203773498535
2025-12-09 12:16:07.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 10.02677059173584
2025-12-09 12:16:07.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 9.929947853088379
2025-12-09 12:16:08.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0029999997217736107 Training loss: 9.957551956176758
2025-12-09 12:16:08.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002999998887094546 Training loss: 10.110867500305176
2025-12-09 12:16:08.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0029999974959631155 Training loss: 9.79988956451416
2025-12-09 12:16:08.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0029999955483798347 Training loss: 9.741684913635254
2025-12-09 12:16:08.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0029999930443454273 Training loss: 9.793586730957031
2025-12-09 12:16:08.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.002999989983860821 Training loss: 9.906944274902344
2025-12-09 12:16:08.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0029999863669271528 Training loss: 9.280632019042969
2025-12-09 12:16:08.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0029999821935457623 Training loss: 9.982831001281738
2025-12-09 12:16:08.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0029999774637181993 Training loss: 9.720170974731445
2025-12-09 12:16:08.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.002999972177446218 Training loss: 9.69477367401123
2025-12-09 12:16:08.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.002999966334731779 Training loss: 9.720417022705078
2025-12-09 12:16:08.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0029999599355770503 Training loss: 9.775152206420898
2025-12-09 12:16:08.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0029999529799844054 Training loss: 9.924912452697754
2025-12-09 12:16:09.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0029999454679564244 Training loss: 10.032919883728027
2025-12-09 12:16:09.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.002999937399495895 Training loss: 9.712276458740234
2025-12-09 12:16:09.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0029999287746058094 Training loss: 10.333159446716309
2025-12-09 12:16:09.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.002999919593289368 Training loss: 9.71900463104248
2025-12-09 12:16:09.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.002999909855549976 Training loss: 9.724533081054688
2025-12-09 12:16:09.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0029998995613912463 Training loss: 9.871687889099121
2025-12-09 12:16:09.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.002999888710816997 Training loss: 9.701870918273926
2025-12-09 12:16:09.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.002999877303831254 Training loss: 10.169604301452637
2025-12-09 12:16:09.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.002999865340438249 Training loss: 9.808095932006836
2025-12-09 12:16:09.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0029998528206424202 Training loss: 9.542765617370605
2025-12-09 12:16:09.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0029998397444484107 Training loss: 9.321769714355469
2025-12-09 12:16:09.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0029998261118610726 Training loss: 9.569223403930664
2025-12-09 12:16:10.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.002999811922885463 Training loss: 9.352450370788574
2025-12-09 12:16:10.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0029997971775268454 Training loss: 9.689104080200195
2025-12-09 12:16:10.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00299978187579069 Training loss: 9.59543514251709
2025-12-09 12:16:10.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0029997660176826735 Training loss: 9.687918663024902
2025-12-09 12:16:10.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.002999749603208678 Training loss: 9.565632820129395
2025-12-09 12:16:10.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.002999732632374793 Training loss: 9.77469253540039
2025-12-09 12:16:10.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.002999715105187314 Training loss: 9.772965431213379
2025-12-09 12:16:10.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.002999697021652744 Training loss: 9.598884582519531
2025-12-09 12:16:10.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.002999678381777791 Training loss: 9.772110939025879
2025-12-09 12:16:10.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.002999659185569369 Training loss: 9.777083396911621
2025-12-09 12:16:10.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.0029996394330345996 Training loss: 9.468250274658203
2025-12-09 12:16:10.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0029996191241808113 Training loss: 9.635578155517578
2025-12-09 12:16:10.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.002999598259015537 Training loss: 9.694147109985352
2025-12-09 12:16:11.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.002999576837546517 Training loss: 9.215314865112305
2025-12-09 12:16:11.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0029995548597816983 Training loss: 9.620136260986328
2025-12-09 12:16:11.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.002999532325729234 Training loss: 9.188176155090332
2025-12-09 12:16:11.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0029995092353974837 Training loss: 9.781842231750488
2025-12-09 12:16:11.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.002999485588795013 Training loss: 9.700920104980469
2025-12-09 12:16:11.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0029994613859305936 Training loss: 9.582155227661133
2025-12-09 12:16:11.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0029994366268132045 Training loss: 9.26371955871582
2025-12-09 12:16:11.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.0029994113114520304 Training loss: 9.501544952392578
2025-12-09 12:16:11.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0029993854398564627 Training loss: 10.193399429321289
2025-12-09 12:16:11.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0029993590120360987 Training loss: 9.854642868041992
2025-12-09 12:16:11.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0029993320280007423 Training loss: 9.66426944732666
2025-12-09 12:16:11.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.002999304487760404 Training loss: 9.72213077545166
2025-12-09 12:16:11.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.0029992763913253002 Training loss: 9.603127479553223
2025-12-09 12:16:12.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.002999247738705854 Training loss: 9.551066398620605
2025-12-09 12:16:12.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0029992185299126946 Training loss: 9.61121940612793
2025-12-09 12:16:12.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0029991887649566565 Training loss: 9.53122615814209
2025-12-09 12:16:12.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.002999158443848783 Training loss: 9.577966690063477
2025-12-09 12:16:12.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0029991275666003212 Training loss: 9.442188262939453
2025-12-09 12:16:12.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.0029990961332227264 Training loss: 9.46190071105957
2025-12-09 12:16:12.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.002999064143727659 Training loss: 9.59204387664795
2025-12-09 12:16:12.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0029990315981269864 Training loss: 9.720100402832031
2025-12-09 12:16:12.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0029989984964327813 Training loss: 9.58468246459961
2025-12-09 12:16:12.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0029989648386573244 Training loss: 9.646653175354004
2025-12-09 12:16:12.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.002998930624813101 Training loss: 9.623867988586426
2025-12-09 12:16:12.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.002998895854912803 Training loss: 9.491554260253906
2025-12-09 12:16:12.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.0029988605289693296 Training loss: 9.638243675231934
2025-12-09 12:16:12.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0029988246469957857 Training loss: 9.560232162475586
2025-12-09 12:16:13.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.002998788209005482 Training loss: 9.512137413024902
2025-12-09 12:16:13.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.002998751215011936 Training loss: 9.745166778564453
2025-12-09 12:16:13.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0029987136650288706 Training loss: 9.343249320983887
2025-12-09 12:16:13.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.002998675559070217 Training loss: 9.556411743164062
2025-12-09 12:16:13.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.00299863689715011 Training loss: 9.569089889526367
2025-12-09 12:16:13.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0029985976792828934 Training loss: 9.456480979919434
2025-12-09 12:16:13.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.0029985579054831145 Training loss: 9.488321304321289
2025-12-09 12:16:13.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0029985175757655286 Training loss: 9.486700057983398
2025-12-09 12:16:13.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.002998476690145097 Training loss: 9.630712509155273
2025-12-09 12:16:13.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0029984352486369867 Training loss: 9.315751075744629
2025-12-09 12:16:13.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.002998393251256571 Training loss: 9.553813934326172
2025-12-09 12:16:13.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0029983506980194303 Training loss: 8.671069145202637
2025-12-09 12:16:13.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.0029983075889413497 Training loss: 9.48643970489502
2025-12-09 12:16:14.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.0029982639240383217 Training loss: 9.567416191101074
2025-12-09 12:16:14.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.0029982197033265445 Training loss: 9.511649131774902
2025-12-09 12:16:14.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0029981749268224228 Training loss: 9.456369400024414
2025-12-09 12:16:14.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.0029981295945425666 Training loss: 9.591076850891113
2025-12-09 12:16:14.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.002998083706503794 Training loss: 9.494033813476562
2025-12-09 12:16:14.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.002998037262723127 Training loss: 9.603808403015137
2025-12-09 12:16:14.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.002997990263217795 Training loss: 9.639784812927246
2025-12-09 12:16:14.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0029979427080052334 Training loss: 9.645179748535156
2025-12-09 12:16:14.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.002997894597103084 Training loss: 9.266925811767578
2025-12-09 12:16:14.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0029978459305291943 Training loss: 9.235666275024414
2025-12-09 12:16:14.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0029977967083016175 Training loss: 9.524900436401367
2025-12-09 12:16:14.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.002997746930438614 Training loss: 9.510200500488281
2025-12-09 12:16:14.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0029976965969586494 Training loss: 9.681041717529297
2025-12-09 12:16:15.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0029976457078803964 Training loss: 9.574689865112305
2025-12-09 12:16:15.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0029975942632227332 Training loss: 9.384693145751953
2025-12-09 12:16:15.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.002997542263004744 Training loss: 9.744258880615234
2025-12-09 12:16:15.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.002997489707245719 Training loss: 9.4008207321167
2025-12-09 12:16:15.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0029974365959651544 Training loss: 9.530940055847168
2025-12-09 12:16:15.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0029973829291827544 Training loss: 9.623276710510254
2025-12-09 12:16:15.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.002997328706918426 Training loss: 9.596612930297852
2025-12-09 12:16:15.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0029972739291922843 Training loss: 9.669483184814453
2025-12-09 12:16:15.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0029972185960246514 Training loss: 9.5020170211792
2025-12-09 12:16:15.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0029971627074360523 Training loss: 9.485213279724121
2025-12-09 12:16:15.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.0029971062634472205 Training loss: 9.205092430114746
2025-12-09 12:16:15.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.0029970492640790957 Training loss: 9.96835994720459
2025-12-09 12:16:15.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.002996991709352822 Training loss: 9.735901832580566
2025-12-09 12:16:16.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0029969335992897513 Training loss: 9.104571342468262
2025-12-09 12:16:16.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0029968749339114404 Training loss: 9.349067687988281
2025-12-09 12:16:16.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0029968157132396513 Training loss: 9.409013748168945
2025-12-09 12:16:16.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.002996755937296354 Training loss: 9.42534065246582
2025-12-09 12:16:16.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.002996695606103723 Training loss: 9.77735424041748
2025-12-09 12:16:16.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0029966347196841393 Training loss: 9.623948097229004
2025-12-09 12:16:16.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.00299657327806019 Training loss: 9.612764358520508
2025-12-09 12:16:16.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0029965112812546683 Training loss: 9.55854606628418
2025-12-09 12:16:16.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0029964487292905725 Training loss: 9.658710479736328
2025-12-09 12:16:16.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0029963856221911075 Training loss: 9.414606094360352
2025-12-09 12:16:16.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.002996321959979685 Training loss: 9.780546188354492
2025-12-09 12:16:16.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00299625774267992 Training loss: 9.491350173950195
2025-12-09 12:16:16.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0029961929703156364 Training loss: 9.5531644821167
2025-12-09 12:16:17.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.002996127642910863 Training loss: 9.538506507873535
2025-12-09 12:16:17.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0029960617604898325 Training loss: 9.65483570098877
2025-12-09 12:16:17.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.002995995323076986 Training loss: 9.628771781921387
2025-12-09 12:16:17.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.002995928330696971 Training loss: 9.41217041015625
2025-12-09 12:16:17.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.002995860783374638 Training loss: 9.626004219055176
2025-12-09 12:16:17.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0029957926811350452 Training loss: 9.533929824829102
2025-12-09 12:16:17.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0029957240240034567 Training loss: 9.605578422546387
2025-12-09 12:16:17.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0029956548120053422 Training loss: 9.39479923248291
2025-12-09 12:16:17.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0029955850451663765 Training loss: 9.511984825134277
2025-12-09 12:16:17.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0029955147235124417 Training loss: 9.290897369384766
2025-12-09 12:16:17.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.002995443847069625 Training loss: 9.367731094360352
2025-12-09 12:16:17.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0029953724158642185 Training loss: 9.529491424560547
2025-12-09 12:16:17.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0029953004299227213 Training loss: 9.350485801696777
2025-12-09 12:16:18.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.002995227889271838 Training loss: 9.522795677185059
2025-12-09 12:16:18.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.002995154793938479 Training loss: 9.489496231079102
2025-12-09 12:16:18.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.0029950811439497607 Training loss: 9.572196006774902
2025-12-09 12:16:18.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0029950069393330043 Training loss: 9.427127838134766
2025-12-09 12:16:18.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.002994932180115737 Training loss: 9.63280200958252
2025-12-09 12:16:18.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0029948568663256928 Training loss: 9.551048278808594
2025-12-09 12:16:18.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0029947809979908114 Training loss: 9.382096290588379
2025-12-09 12:16:18.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.002994704575139236 Training loss: 9.475234031677246
2025-12-09 12:16:18.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.002994627597799318 Training loss: 9.436227798461914
2025-12-09 12:16:18.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0029945500659996132 Training loss: 9.84658145904541
2025-12-09 12:16:18.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0029944719797688844 Training loss: 9.417877197265625
2025-12-09 12:16:18.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.002994393339136098 Training loss: 9.445592880249023
2025-12-09 12:16:18.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0029943141441304277 Training loss: 9.622904777526855
2025-12-09 12:16:19.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0029942343947812517 Training loss: 9.50822925567627
2025-12-09 12:16:19.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.002994154091118156 Training loss: 9.539823532104492
2025-12-09 12:16:19.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0029940732331709295 Training loss: 9.391936302185059
2025-12-09 12:16:19.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.0029939918209695676 Training loss: 9.806024551391602
2025-12-09 12:16:19.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0029939098545442733 Training loss: 9.553793907165527
2025-12-09 12:16:19.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0029938273339254516 Training loss: 9.623029708862305
2025-12-09 12:16:19.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.002993744259143717 Training loss: 9.641209602355957
2025-12-09 12:16:19.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.002993660630229886 Training loss: 9.444461822509766
2025-12-09 12:16:19.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0029935764472149833 Training loss: 9.505025863647461
2025-12-09 12:16:19.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0029934917101302376 Training loss: 9.585330963134766
2025-12-09 12:16:19.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.002993406419007084 Training loss: 9.33229923248291
2025-12-09 12:16:19.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.0029933205738771626 Training loss: 9.61073112487793
2025-12-09 12:16:19.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0029932341747723194 Training loss: 9.829262733459473
2025-12-09 12:16:20.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.002993147221724606 Training loss: 9.679767608642578
2025-12-09 12:16:20.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0029930597147662785 Training loss: 9.661776542663574
2025-12-09 12:16:20.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.0029929716539297997 Training loss: 9.398561477661133
2025-12-09 12:16:20.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.0029928830392478376 Training loss: 9.62486457824707
2025-12-09 12:16:20.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.002992793870753265 Training loss: 9.755523681640625
2025-12-09 12:16:20.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0029927041484791614 Training loss: 9.474492073059082
2025-12-09 12:16:20.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00299261387245881 Training loss: 9.764665603637695
2025-12-09 12:16:20.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0029925230427257006 Training loss: 9.61962604522705
2025-12-09 12:16:20.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0029924316593135285 Training loss: 9.596590995788574
2025-12-09 12:16:20.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0029923397222561938 Training loss: 9.373687744140625
2025-12-09 12:16:20.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0029922472315878023 Training loss: 9.586187362670898
2025-12-09 12:16:20.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.002992154187342665 Training loss: 9.51445484161377
2025-12-09 12:16:20.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.002992060589555299 Training loss: 9.291923522949219
2025-12-09 12:16:21.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0029919664382604253 Training loss: 9.541047096252441
2025-12-09 12:16:21.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0029918717334929718 Training loss: 9.45482063293457
2025-12-09 12:16:21.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00299177647528807 Training loss: 9.628026008605957
2025-12-09 12:16:21.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.002991680663681059 Training loss: 9.655082702636719
2025-12-09 12:16:21.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.002991584298707481 Training loss: 9.748641014099121
2025-12-09 12:16:21.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.002991487380403084 Training loss: 9.541589736938477
2025-12-09 12:16:21.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.002991389908803823 Training loss: 9.443746566772461
2025-12-09 12:16:21.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0029912918839458554 Training loss: 9.688247680664062
2025-12-09 12:16:21.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.002991193305865547 Training loss: 9.647560119628906
2025-12-09 12:16:21.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0029910941745994657 Training loss: 9.250069618225098
2025-12-09 12:16:21.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0029909944901843864 Training loss: 9.530627250671387
2025-12-09 12:16:21.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.002990894252657289 Training loss: 8.862994194030762
2025-12-09 12:16:21.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0029907934620553595 Training loss: 9.584985733032227
2025-12-09 12:16:22.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0029906921184159863 Training loss: 9.702744483947754
2025-12-09 12:16:22.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029905902217767654 Training loss: 9.602036476135254
2025-12-09 12:16:22.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029904877721754976 Training loss: 9.52615737915039
2025-12-09 12:16:22.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002990384769650188 Training loss: 9.421480178833008
2025-12-09 12:16:22.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0029902812142390475 Training loss: 9.440927505493164
2025-12-09 12:16:22.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0029901771059804923 Training loss: 9.714567184448242
2025-12-09 12:16:22.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0029900724449131427 Training loss: 9.488678932189941
2025-12-09 12:16:22.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0029899672310758248 Training loss: 9.493684768676758
2025-12-09 12:16:22.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0029898614645075695 Training loss: 9.603487014770508
2025-12-09 12:16:22.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.002989755145247613 Training loss: 9.341974258422852
2025-12-09 12:16:22.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002989648273335397 Training loss: 9.545735359191895
2025-12-09 12:16:22.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0029895408488105667 Training loss: 9.689003944396973
2025-12-09 12:16:22.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0029894328717129737 Training loss: 9.538588523864746
2025-12-09 12:16:23.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0029893243420826736 Training loss: 9.604409217834473
2025-12-09 12:16:23.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.002989215259959928 Training loss: 9.341399192810059
2025-12-09 12:16:23.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002989105625385203 Training loss: 9.57245922088623
2025-12-09 12:16:23.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.002988995438399169 Training loss: 9.493836402893066
2025-12-09 12:16:23.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0029888846990427024 Training loss: 9.568458557128906
2025-12-09 12:16:23.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.002988773407356884 Training loss: 9.488405227661133
2025-12-09 12:16:23.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0029886615633829996 Training loss: 9.531139373779297
2025-12-09 12:16:23.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.002988549167162539 Training loss: 9.595377922058105
2025-12-09 12:16:23.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0029884362187371986 Training loss: 9.55685043334961
2025-12-09 12:16:23.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0029883227181488783 Training loss: 9.710479736328125
2025-12-09 12:16:23.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.002988208665439683 Training loss: 9.710593223571777
2025-12-09 12:16:23.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0029880940606519233 Training loss: 9.550383567810059
2025-12-09 12:16:23.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.002987978903828114 Training loss: 9.303019523620605
2025-12-09 12:16:24.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0029878631950109734 Training loss: 9.474717140197754
2025-12-09 12:16:24.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0029877469342434273 Training loss: 9.576811790466309
2025-12-09 12:16:24.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.002987630121568604 Training loss: 9.379566192626953
2025-12-09 12:16:24.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.002987512757029838 Training loss: 9.685284614562988
2025-12-09 12:16:24.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0029873948406706667 Training loss: 9.540350914001465
2025-12-09 12:16:24.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0029872763725348342 Training loss: 9.3441743850708
2025-12-09 12:16:24.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0029871573526662884 Training loss: 9.891968727111816
2025-12-09 12:16:24.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0029870377811091822 Training loss: 9.53328800201416
2025-12-09 12:16:24.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.002986917657907872 Training loss: 9.463130950927734
2025-12-09 12:16:24.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.00298679698310692 Training loss: 8.60487174987793
2025-12-09 12:16:24.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.002986675756751093 Training loss: 9.715242385864258
2025-12-09 12:16:24.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0029865539788853624 Training loss: 9.617271423339844
2025-12-09 12:16:24.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0029864316495549037 Training loss: 9.47681713104248
2025-12-09 12:16:25.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0029863087688050973 Training loss: 9.586777687072754
2025-12-09 12:16:25.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.002986185336681528 Training loss: 9.65115737915039
2025-12-09 12:16:25.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0029860613532299847 Training loss: 9.652012825012207
2025-12-09 12:16:25.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0029859368184964627 Training loss: 9.74710750579834
2025-12-09 12:16:25.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.002985811732527159 Training loss: 9.557132720947266
2025-12-09 12:16:25.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0029856860953684774 Training loss: 9.597989082336426
2025-12-09 12:16:25.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0029855599070670253 Training loss: 9.447511672973633
2025-12-09 12:16:25.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0029854331676696143 Training loss: 9.558205604553223
2025-12-09 12:16:25.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0029853058772232608 Training loss: 9.600244522094727
2025-12-09 12:16:25.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0029851780357751856 Training loss: 9.733155250549316
2025-12-09 12:16:25.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.002985049643372814 Training loss: 9.525125503540039
2025-12-09 12:16:25.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0029849207000637755 Training loss: 9.499452590942383
2025-12-09 12:16:25.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0029847912058959033 Training loss: 9.454212188720703
2025-12-09 12:16:26.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.002984661160917237 Training loss: 9.729186058044434
2025-12-09 12:16:26.118 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.002984530565176018 Training loss: 9.499289512634277
2025-12-09 12:16:26.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.002984399418720694 Training loss: 9.470978736877441
2025-12-09 12:16:26.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0029842677215999158 Training loss: 9.552567481994629
2025-12-09 12:16:26.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.002984135473862539 Training loss: 9.639021873474121
2025-12-09 12:16:26.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.002984002675557623 Training loss: 9.589764595031738
2025-12-09 12:16:26.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.002983869326734432 Training loss: 10.02407455444336
2025-12-09 12:16:26.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0029837354274424347 Training loss: 9.632492065429688
2025-12-09 12:16:26.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.002983600977731303 Training loss: 9.081780433654785
2025-12-09 12:16:26.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0029834659776509136 Training loss: 9.43415355682373
2025-12-09 12:16:26.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0029833304272513473 Training loss: 9.205355644226074
2025-12-09 12:16:26.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.002983194326582889 Training loss: 9.621922492980957
2025-12-09 12:16:26.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0029830576756960285 Training loss: 9.594614028930664
2025-12-09 12:16:27.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0029829204746414577 Training loss: 9.770735740661621
2025-12-09 12:16:27.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0029827827234700744 Training loss: 9.925622940063477
2025-12-09 12:16:27.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00298264442223298 Training loss: 9.57961368560791
2025-12-09 12:16:27.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.0029825055709814803 Training loss: 9.541751861572266
2025-12-09 12:16:27.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.002982366169767084 Training loss: 9.572173118591309
2025-12-09 12:16:27.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.002982226218641505 Training loss: 9.436506271362305
2025-12-09 12:16:27.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.002982085717656661 Training loss: 9.890666007995605
2025-12-09 12:16:27.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0029819446668646723 Training loss: 9.301189422607422
2025-12-09 12:16:27.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0029818030663178656 Training loss: 9.42600154876709
2025-12-09 12:16:27.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00298166091606877 Training loss: 9.613747596740723
2025-12-09 12:16:27.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0029815182161701185 Training loss: 8.845274925231934
2025-12-09 12:16:27.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0029813749666748484 Training loss: 9.629897117614746
2025-12-09 12:16:27.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0029812311676361003 Training loss: 9.2155122756958
2025-12-09 12:16:28.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00298108681910722 Training loss: 8.993453025817871
2025-12-09 12:16:28.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0029809419211417553 Training loss: 9.621447563171387
2025-12-09 12:16:28.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0029807964737934593 Training loss: 9.459946632385254
2025-12-09 12:16:28.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0029806504771162884 Training loss: 9.609301567077637
2025-12-09 12:16:28.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0029805039311644028 Training loss: 9.618473052978516
2025-12-09 12:16:28.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.002980356835992166 Training loss: 9.999166488647461
2025-12-09 12:16:28.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0029802091916541463 Training loss: 9.546310424804688
2025-12-09 12:16:28.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.002980060998205115 Training loss: 9.588443756103516
2025-12-09 12:16:28.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0029799122557000466 Training loss: 9.604683876037598
2025-12-09 12:16:28.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.0029797629641941203 Training loss: 9.41662311553955
2025-12-09 12:16:28.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.002979613123742719 Training loss: 9.560885429382324
2025-12-09 12:16:28.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0029794627344014277 Training loss: 9.411450386047363
2025-12-09 12:16:28.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.002979311796226037 Training loss: 9.949519157409668
2025-12-09 12:16:29.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00297916030927254 Training loss: 9.448962211608887
2025-12-09 12:16:29.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0029790082735971337 Training loss: 9.27272891998291
2025-12-09 12:16:29.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0029788556892562184 Training loss: 9.7153959274292
2025-12-09 12:16:29.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.002978702556306398 Training loss: 9.326899528503418
2025-12-09 12:16:29.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.00297854887480448 Training loss: 9.637229919433594
2025-12-09 12:16:29.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.002978394644807475 Training loss: 9.62520980834961
2025-12-09 12:16:29.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0029782398663725984 Training loss: 9.437212944030762
2025-12-09 12:16:29.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0029780845395572676 Training loss: 9.527874946594238
2025-12-09 12:16:29.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0029779286644191043 Training loss: 9.555160522460938
2025-12-09 12:16:29.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.002977772241015933 Training loss: 8.962450981140137
2025-12-09 12:16:29.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.002977615269405782 Training loss: 9.515411376953125
2025-12-09 12:16:29.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0029774577496468825 Training loss: 9.17330551147461
2025-12-09 12:16:29.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0029772996817976696 Training loss: 9.587197303771973
2025-12-09 12:16:30.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.002977141065916781 Training loss: 9.6254301071167
2025-12-09 12:16:30.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0029769819020630597 Training loss: 9.451336860656738
2025-12-09 12:16:30.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0029768221902955485 Training loss: 9.54971981048584
2025-12-09 12:16:30.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.002976661930673497 Training loss: 9.771803855895996
2025-12-09 12:16:30.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.002976501123256355 Training loss: 9.65779972076416
2025-12-09 12:16:30.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0029763397681037787 Training loss: 9.56623363494873
2025-12-09 12:16:30.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.0029761778652756246 Training loss: 9.62370491027832
2025-12-09 12:16:30.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.0029760154148319538 Training loss: 9.52963638305664
2025-12-09 12:16:30.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0029758524168330305 Training loss: 9.540240287780762
2025-12-09 12:16:30.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.0029756888713393216 Training loss: 9.763920783996582
2025-12-09 12:16:30.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.002975524778411498 Training loss: 9.63623332977295
2025-12-09 12:16:30.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0029753601381104318 Training loss: 9.459364891052246
2025-12-09 12:16:30.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.0029751949504972 Training loss: 9.664322853088379
2025-12-09 12:16:31.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0029750292156330823 Training loss: 9.621678352355957
2025-12-09 12:16:31.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0029748629335795604 Training loss: 9.61216926574707
2025-12-09 12:16:31.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.002974696104398321 Training loss: 9.291147232055664
2025-12-09 12:16:31.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.002974528728151251 Training loss: 10.0809907913208
2025-12-09 12:16:31.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0029743608049004424 Training loss: 9.753205299377441
2025-12-09 12:16:31.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.002974192334708189 Training loss: 9.538651466369629
2025-12-09 12:16:31.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.002974023317636989 Training loss: 9.645369529724121
2025-12-09 12:16:31.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0029738537537495413 Training loss: 9.534046173095703
2025-12-09 12:16:31.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.0029736836431087494 Training loss: 9.445696830749512
2025-12-09 12:16:31.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0029735129857777188 Training loss: 9.684082984924316
2025-12-09 12:16:31.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.002973341781819758 Training loss: 9.673885345458984
2025-12-09 12:16:31.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.002973170031298378 Training loss: 9.473320960998535
2025-12-09 12:16:31.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0029729977342772937 Training loss: 9.430768966674805
2025-12-09 12:16:32.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.0029728248908204207 Training loss: 9.640727996826172
2025-12-09 12:16:32.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.0029726515009918793 Training loss: 9.184867858886719
2025-12-09 12:16:32.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0029724775648559913 Training loss: 9.519246101379395
2025-12-09 12:16:32.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0029723030824772814 Training loss: 9.47592830657959
2025-12-09 12:16:32.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.002972128053920478 Training loss: 9.539436340332031
2025-12-09 12:16:32.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00297195247925051 Training loss: 9.475079536437988
2025-12-09 12:16:32.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.0029717763585325103 Training loss: 9.958781242370605
2025-12-09 12:16:32.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.002971599691831815 Training loss: 9.69279670715332
2025-12-09 12:16:32.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.0029714224792139607 Training loss: 9.65684986114502
2025-12-09 12:16:32.747 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.002971244720744689 Training loss: 9.74117660522461
2025-12-09 12:16:32.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.0029710664164899416 Training loss: 9.416032791137695
2025-12-09 12:16:32.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0029708875665158643 Training loss: 9.49895191192627
2025-12-09 12:16:32.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0029707081708888047 Training loss: 9.82079029083252
2025-12-09 12:16:33.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0029705282296753127 Training loss: 9.572121620178223
2025-12-09 12:16:33.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.002970347742942141 Training loss: 9.647329330444336
2025-12-09 12:16:33.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.002970166710756245 Training loss: 9.295740127563477
2025-12-09 12:16:33.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.002969985133184781 Training loss: 9.326733589172363
2025-12-09 12:16:33.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0029698030102951094 Training loss: 9.480841636657715
2025-12-09 12:16:33.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0029696203421547916 Training loss: 9.721881866455078
2025-12-09 12:16:33.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0029694371288315913 Training loss: 9.830397605895996
2025-12-09 12:16:33.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.0029692533703934757 Training loss: 8.903940200805664
2025-12-09 12:16:33.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.002969069066908613 Training loss: 9.48189926147461
2025-12-09 12:16:33.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0029688842184453744 Training loss: 9.486452102661133
2025-12-09 12:16:33.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.002968698825072332 Training loss: 9.520160675048828
2025-12-09 12:16:33.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0029685128868582626 Training loss: 9.465407371520996
2025-12-09 12:16:33.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0029683264038721418 Training loss: 9.533485412597656
2025-12-09 12:16:34.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0029681393761831487 Training loss: 9.57681655883789
2025-12-09 12:16:34.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.002967951803860666 Training loss: 9.383279800415039
2025-12-09 12:16:34.213 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.002967763686974276 Training loss: 8.802754402160645
2025-12-09 12:16:34.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.002967575025593765 Training loss: 9.402323722839355
2025-12-09 12:16:34.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.00296738581978912 Training loss: 9.577728271484375
2025-12-09 12:16:34.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0029671960696305306 Training loss: 9.502580642700195
2025-12-09 12:16:34.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.002967005775188388 Training loss: 9.717162132263184
2025-12-09 12:16:34.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0029668149365332853 Training loss: 9.398416519165039
2025-12-09 12:16:34.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.002966623553736018 Training loss: 9.562500953674316
2025-12-09 12:16:34.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0029664316268675824 Training loss: 9.41861629486084
2025-12-09 12:16:34.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0029662391559991783 Training loss: 9.298389434814453
2025-12-09 12:16:34.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0029660461412022057 Training loss: 9.210808753967285
2025-12-09 12:16:34.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.002965852582548267 Training loss: 9.554868698120117
2025-12-09 12:16:35.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0029656584801091668 Training loss: 8.863747596740723
2025-12-09 12:16:35.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0029654638339569107 Training loss: 9.476123809814453
2025-12-09 12:16:35.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.002965268644163706 Training loss: 9.396323204040527
2025-12-09 12:16:35.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0029650729108019625 Training loss: 9.233482360839844
2025-12-09 12:16:35.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.002964876633944291 Training loss: 9.584402084350586
2025-12-09 12:16:35.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.0029646798136635038 Training loss: 9.319867134094238
2025-12-09 12:16:35.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.002964482450032615 Training loss: 9.587183952331543
2025-12-09 12:16:35.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.002964284543124841 Training loss: 9.452611923217773
2025-12-09 12:16:35.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0029640860930135976 Training loss: 9.367155075073242
2025-12-09 12:16:35.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.002963887099772505 Training loss: 9.352011680603027
2025-12-09 12:16:35.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.0029636875634753827 Training loss: 9.325849533081055
2025-12-09 12:16:35.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.002963487484196253 Training loss: 9.480679512023926
2025-12-09 12:16:35.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.002963286862009338 Training loss: 9.39728832244873
2025-12-09 12:16:36.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0029630856969890627 Training loss: 9.236291885375977
2025-12-09 12:16:36.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0029628839892100536 Training loss: 9.47652530670166
2025-12-09 12:16:36.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.002962681738747137 Training loss: 9.438436508178711
2025-12-09 12:16:36.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.002962478945675342 Training loss: 9.41916275024414
2025-12-09 12:16:36.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.002962275610069898 Training loss: 9.412542343139648
2025-12-09 12:16:36.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.002962071732006237 Training loss: 9.220414161682129
2025-12-09 12:16:36.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00296186731155999 Training loss: 9.376840591430664
2025-12-09 12:16:36.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0029616623488069923 Training loss: 9.234816551208496
2025-12-09 12:16:36.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0029614568438232768 Training loss: 9.56785774230957
2025-12-09 12:16:36.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.0029612507966850807 Training loss: 9.413115501403809
2025-12-09 12:16:36.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0029610442074688398 Training loss: 9.279918670654297
2025-12-09 12:16:36.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0029608370762511937 Training loss: 9.48355484008789
2025-12-09 12:16:36.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0029606294031089804 Training loss: 9.262818336486816
2025-12-09 12:16:37.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.00296042118811924 Training loss: 9.251300811767578
2025-12-09 12:16:37.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.002960212431359215 Training loss: 9.412346839904785
2025-12-09 12:16:37.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.0029600031329063466 Training loss: 9.310370445251465
2025-12-09 12:16:37.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.002959793292838278 Training loss: 9.31380844116211
2025-12-09 12:16:37.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.002959582911232853 Training loss: 8.67150592803955
2025-12-09 12:16:37.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0029593719881681173 Training loss: 9.398473739624023
2025-12-09 12:16:37.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.002959160523722316 Training loss: 9.442413330078125
2025-12-09 12:16:37.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0029589485179738963 Training loss: 9.355364799499512
2025-12-09 12:16:37.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0029587359710015054 Training loss: 9.374746322631836
2025-12-09 12:16:37.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0029585228828839915 Training loss: 9.300979614257812
2025-12-09 12:16:37.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.002958309253700404 Training loss: 8.975418090820312
2025-12-09 12:16:37.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.002958095083529992 Training loss: 9.358044624328613
2025-12-09 12:16:37.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0029578803724522058 Training loss: 9.214962005615234
2025-12-09 12:16:38.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.002957665120546697 Training loss: 9.486837387084961
2025-12-09 12:16:38.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0029574493278933175 Training loss: 9.233968734741211
2025-12-09 12:16:38.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.002957232994572119 Training loss: 9.06673812866211
2025-12-09 12:16:38.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0029570161206633546 Training loss: 9.138460159301758
2025-12-09 12:16:38.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0029567987062474772 Training loss: 9.205246925354004
2025-12-09 12:16:38.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.002956580751405141 Training loss: 9.180876731872559
2025-12-09 12:16:38.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.002956362256217201 Training loss: 8.98560905456543
2025-12-09 12:16:38.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0029561432207647113 Training loss: 9.059612274169922
2025-12-09 12:16:38.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.002955923645128927 Training loss: 9.757888793945312
2025-12-09 12:16:38.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.0029557035293913047 Training loss: 9.402297973632812
2025-12-09 12:16:38.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.0029554828736334995 Training loss: 9.328624725341797
2025-12-09 12:16:38.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0029552616779373684 Training loss: 9.056567192077637
2025-12-09 12:16:39.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.0029550399423849674 Training loss: 9.226826667785645
2025-12-09 12:16:39.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.002954817667058554 Training loss: 8.994226455688477
2025-12-09 12:16:39.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.002954594852040585 Training loss: 9.416035652160645
2025-12-09 12:16:39.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0029543714974137178 Training loss: 9.343220710754395
2025-12-09 12:16:39.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.00295414760326081 Training loss: 9.196839332580566
2025-12-09 12:16:39.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.002953923169664919 Training loss: 9.456274032592773
2025-12-09 12:16:39.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.002953698196709303 Training loss: 9.270310401916504
2025-12-09 12:16:39.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.00295347268447742 Training loss: 9.169058799743652
2025-12-09 12:16:39.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.002953246633052928 Training loss: 9.098851203918457
2025-12-09 12:16:39.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0029530200425196837 Training loss: 8.402982711791992
2025-12-09 12:16:39.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.0029527929129617467 Training loss: 9.184422492980957
2025-12-09 12:16:39.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.002952565244463374 Training loss: 9.158456802368164
2025-12-09 12:16:39.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0029523370371090235 Training loss: 9.25639820098877
2025-12-09 12:16:40.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.002952108290983353 Training loss: 9.147337913513184
2025-12-09 12:16:40.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.002951879006171221 Training loss: 9.204399108886719
2025-12-09 12:16:40.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0029516491827576833 Training loss: 8.294008255004883
2025-12-09 12:16:40.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.0029514188208279984 Training loss: 8.338339805603027
2025-12-09 12:16:40.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0029511879204676223 Training loss: 9.042611122131348
2025-12-09 12:16:40.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0029509564817622133 Training loss: 9.201276779174805
2025-12-09 12:16:40.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0029507245047976265 Training loss: 9.089393615722656
2025-12-09 12:16:40.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.002950491989659918 Training loss: 9.792877197265625
2025-12-09 12:16:40.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0029502589364353454 Training loss: 9.024571418762207
2025-12-09 12:16:40.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0029500253452103622 Training loss: 8.681230545043945
2025-12-09 12:16:40.785 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.0029497912160716235 Training loss: 9.113551139831543
2025-12-09 12:16:40.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.002949556549105985 Training loss: 9.020648002624512
2025-12-09 12:16:40.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0029493213444005 Training loss: 8.969304084777832
2025-12-09 12:16:41.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.002949085602042422 Training loss: 9.356931686401367
2025-12-09 12:16:41.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.0029488493221192045 Training loss: 8.906538963317871
2025-12-09 12:16:41.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.002948612504718499 Training loss: 9.114449501037598
2025-12-09 12:16:41.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0029483751499281585 Training loss: 9.474995613098145
2025-12-09 12:16:41.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0029481372578362332 Training loss: 8.797767639160156
2025-12-09 12:16:41.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.002947898828530974 Training loss: 8.918211936950684
2025-12-09 12:16:41.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.00294765986210083 Training loss: 9.089473724365234
2025-12-09 12:16:41.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0029474203586344516 Training loss: 9.049004554748535
2025-12-09 12:16:41.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.0029471803182206857 Training loss: 8.988728523254395
2025-12-09 12:16:41.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.0029469397409485807 Training loss: 8.509325981140137
2025-12-09 12:16:41.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0029466986269073825 Training loss: 9.376230239868164
2025-12-09 12:16:41.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.0029464569761865366 Training loss: 9.304475784301758
2025-12-09 12:16:41.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.002946214788875689 Training loss: 9.32750129699707
2025-12-09 12:16:42.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.0029459720650646826 Training loss: 9.040985107421875
2025-12-09 12:16:42.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0029457288048435606 Training loss: 8.863494873046875
2025-12-09 12:16:42.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.002945485008302565 Training loss: 9.021661758422852
2025-12-09 12:16:42.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0029452406755321363 Training loss: 9.00477123260498
2025-12-09 12:16:42.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0029449958066229145 Training loss: 9.090420722961426
2025-12-09 12:16:42.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0029447504016657383 Training loss: 9.139474868774414
2025-12-09 12:16:42.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.002944504460751645 Training loss: 8.973185539245605
2025-12-09 12:16:42.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.002944257983971871 Training loss: 9.14085578918457
2025-12-09 12:16:42.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.002944010971417851 Training loss: 9.293137550354004
2025-12-09 12:16:42.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00294376342318122 Training loss: 8.919214248657227
2025-12-09 12:16:42.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.00294351533935381 Training loss: 9.12099838256836
2025-12-09 12:16:42.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.002943266720027652 Training loss: 9.015776634216309
2025-12-09 12:16:42.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0029430175652949762 Training loss: 8.895827293395996
2025-12-09 12:16:43.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0029427678752482114 Training loss: 7.991640567779541
2025-12-09 12:16:43.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0029425176499799843 Training loss: 8.796234130859375
2025-12-09 12:16:43.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.002942266889583121 Training loss: 8.203845977783203
2025-12-09 12:16:43.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.0029420155941506454 Training loss: 9.134202003479004
2025-12-09 12:16:43.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00294176376377578 Training loss: 8.967780113220215
2025-12-09 12:16:43.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0029415113985519466 Training loss: 8.991778373718262
2025-12-09 12:16:43.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0029412584985727642 Training loss: 9.286026000976562
2025-12-09 12:16:43.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.002941005063932051 Training loss: 9.017598152160645
2025-12-09 12:16:43.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.002940751094723823 Training loss: 8.955307960510254
2025-12-09 12:16:43.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0029404965910422953 Training loss: 9.742033958435059
2025-12-09 12:16:43.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0029402415529818805 Training loss: 8.946366310119629
2025-12-09 12:16:43.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.0029399859806371895 Training loss: 9.17198657989502
2025-12-09 12:16:43.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.002939729874103032 Training loss: 8.788299560546875
2025-12-09 12:16:44.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.002939473233474415 Training loss: 9.102885246276855
2025-12-09 12:16:44.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.002939216058846544 Training loss: 8.794160842895508
2025-12-09 12:16:44.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0029389583503148234 Training loss: 8.966475486755371
2025-12-09 12:16:44.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.0029387001079748537 Training loss: 8.90625
2025-12-09 12:16:44.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0029384413319224366 Training loss: 8.964856147766113
2025-12-09 12:16:44.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.002938182022253568 Training loss: 8.815251350402832
2025-12-09 12:16:44.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.002937922179064445 Training loss: 8.997275352478027
2025-12-09 12:16:44.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.00293766180245146 Training loss: 8.975824356079102
2025-12-09 12:16:44.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.0029374008925112057 Training loss: 8.792723655700684
2025-12-09 12:16:44.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0029371394493404707 Training loss: 8.799601554870605
2025-12-09 12:16:44.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.0029368774730362426 Training loss: 9.025912284851074
2025-12-09 12:16:44.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.002936614963695706 Training loss: 8.880305290222168
2025-12-09 12:16:44.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.002936351921416244 Training loss: 8.394671440124512
2025-12-09 12:16:45.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0029360883462954362 Training loss: 8.613177299499512
2025-12-09 12:16:45.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.002935824238431062 Training loss: 8.682658195495605
2025-12-09 12:16:45.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0029355595979210962 Training loss: 8.368349075317383
2025-12-09 12:16:45.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.002935294424863712 Training loss: 8.462855339050293
2025-12-09 12:16:45.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.002935028719357281 Training loss: 7.799623966217041
2025-12-09 12:16:45.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.0029347624815003704 Training loss: 8.25006103515625
2025-12-09 12:16:45.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.0029344957113917472 Training loss: 8.836103439331055
2025-12-09 12:16:45.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.002934228409130374 Training loss: 9.048035621643066
2025-12-09 12:16:45.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0029339605748154125 Training loss: 8.788352012634277
2025-12-09 12:16:45.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0029336922085462193 Training loss: 8.703643798828125
2025-12-09 12:16:45.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.002933423310422351 Training loss: 9.090145111083984
2025-12-09 12:16:45.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00293315388054356 Training loss: 9.264366149902344
2025-12-09 12:16:45.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.002932883919009796 Training loss: 8.82280445098877
2025-12-09 12:16:46.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.002932613425921207 Training loss: 8.817281723022461
2025-12-09 12:16:46.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.002932342401378137 Training loss: 8.754405975341797
2025-12-09 12:16:46.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0029320708454811267 Training loss: 9.181951522827148
2025-12-09 12:16:46.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.002931798758330916 Training loss: 8.744832992553711
2025-12-09 12:16:46.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.002931526140028441 Training loss: 8.979411125183105
2025-12-09 12:16:46.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0029312529906748326 Training loss: 8.784424781799316
2025-12-09 12:16:46.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0029309793103714224 Training loss: 9.048528671264648
2025-12-09 12:16:46.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.002930705099219736 Training loss: 9.03657054901123
2025-12-09 12:16:46.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0029304303573214983 Training loss: 8.848872184753418
2025-12-09 12:16:46.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0029301550847786293 Training loss: 8.466318130493164
2025-12-09 12:16:46.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0029298792816932462 Training loss: 8.753170013427734
2025-12-09 12:16:46.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0029296029481676636 Training loss: 8.961546897888184
2025-12-09 12:16:46.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0029293260843043924 Training loss: 8.598791122436523
2025-12-09 12:16:47.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0029290486902061397 Training loss: 8.76272964477539
2025-12-09 12:16:47.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0029287707659758117 Training loss: 8.655576705932617
2025-12-09 12:16:47.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0029284923117165076 Training loss: 8.669281005859375
2025-12-09 12:16:47.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0029282133275315265 Training loss: 9.000816345214844
2025-12-09 12:16:47.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.0029279338135243626 Training loss: 8.798831939697266
2025-12-09 12:16:47.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0029276537697987062 Training loss: 9.086702346801758
2025-12-09 12:16:47.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.0029273731964584446 Training loss: 8.915675163269043
2025-12-09 12:16:47.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0029270920936076625 Training loss: 8.670605659484863
2025-12-09 12:16:47.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.0029268104613506397 Training loss: 8.708213806152344
2025-12-09 12:16:47.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.0029265282997918535 Training loss: 8.545083999633789
2025-12-09 12:16:47.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.0029262456090359762 Training loss: 8.767526626586914
2025-12-09 12:16:47.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.002925962389187877 Training loss: 8.790420532226562
2025-12-09 12:16:47.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0029256786403526226 Training loss: 9.19388198852539
2025-12-09 12:16:48.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0029253943626354737 Training loss: 8.776461601257324
2025-12-09 12:16:48.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0029251095561418894 Training loss: 8.886541366577148
2025-12-09 12:16:48.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0029248242209775235 Training loss: 8.439096450805664
2025-12-09 12:16:48.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.002924538357248226 Training loss: 8.766740798950195
2025-12-09 12:16:48.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0029242519650600437 Training loss: 8.753813743591309
2025-12-09 12:16:48.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.002923965044519219 Training loss: 8.71469497680664
2025-12-09 12:16:48.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.002923677595732191 Training loss: 8.880568504333496
2025-12-09 12:16:48.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0029233896188055933 Training loss: 8.90860652923584
2025-12-09 12:16:48.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0029231011138462566 Training loss: 8.678508758544922
2025-12-09 12:16:48.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0029228120809612072 Training loss: 8.752118110656738
2025-12-09 12:16:48.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.002922522520257667 Training loss: 8.895167350769043
2025-12-09 12:16:48.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0029222324318430542 Training loss: 8.50835132598877
2025-12-09 12:16:48.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0029219418158249826 Training loss: 8.685866355895996
2025-12-09 12:16:49.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0029216506723112614 Training loss: 8.434123992919922
2025-12-09 12:16:49.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0029213590014098953 Training loss: 8.807912826538086
2025-12-09 12:16:49.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.002921066803229085 Training loss: 8.795696258544922
2025-12-09 12:16:49.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.002920774077877228 Training loss: 8.62409782409668
2025-12-09 12:16:49.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0029204808254629146 Training loss: 8.63919448852539
2025-12-09 12:16:49.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.002920187046094933 Training loss: 8.813116073608398
2025-12-09 12:16:49.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.002919892739882266 Training loss: 9.15286636352539
2025-12-09 12:16:49.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0029195979069340924 Training loss: 9.131982803344727
2025-12-09 12:16:49.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0029193025473597855 Training loss: 8.799570083618164
2025-12-09 12:16:49.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0029190066612689142 Training loss: 8.596378326416016
2025-12-09 12:16:49.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0029187102487712433 Training loss: 8.952079772949219
2025-12-09 12:16:49.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0029184133099767326 Training loss: 8.730551719665527
2025-12-09 12:16:50.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.0029181158449955364 Training loss: 8.521834373474121
2025-12-09 12:16:50.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0029178178539380054 Training loss: 8.735437393188477
2025-12-09 12:16:50.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0029175193369146844 Training loss: 8.604974746704102
2025-12-09 12:16:50.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.002917220294036315 Training loss: 8.294682502746582
2025-12-09 12:16:50.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0029169207254138314 Training loss: 8.798830032348633
2025-12-09 12:16:50.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.0029166206311583647 Training loss: 8.715524673461914
2025-12-09 12:16:50.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.00291632001138124 Training loss: 8.729571342468262
2025-12-09 12:16:50.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0029160188661939783 Training loss: 8.437636375427246
2025-12-09 12:16:50.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.002915717195708295 Training loss: 8.828031539916992
2025-12-09 12:16:50.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0029154150000361 Training loss: 8.314270973205566
2025-12-09 12:16:50.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0029151122792894987 Training loss: 8.656627655029297
2025-12-09 12:16:50.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.00291480903358079 Training loss: 8.599956512451172
2025-12-09 12:16:50.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00291450526302247 Training loss: 8.571654319763184
2025-12-09 12:16:51.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0029142009677272274 Training loss: 7.7149739265441895
2025-12-09 12:16:51.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.0029138961478079456 Training loss: 8.569557189941406
2025-12-09 12:16:51.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.002913590803377704 Training loss: 9.268027305603027
2025-12-09 12:16:51.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0029132849345497756 Training loss: 7.725094795227051
2025-12-09 12:16:51.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0029129785414376275 Training loss: 8.719278335571289
2025-12-09 12:16:51.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0029126716241549225 Training loss: 8.99618148803711
2025-12-09 12:16:51.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0029123641828155173 Training loss: 8.499914169311523
2025-12-09 12:16:51.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.0029120562175334627 Training loss: 8.565044403076172
2025-12-09 12:16:51.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0029117477284230043 Training loss: 8.543782234191895
2025-12-09 12:16:51.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0029114387155985814 Training loss: 8.266096115112305
2025-12-09 12:16:51.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0029111291791748283 Training loss: 8.924657821655273
2025-12-09 12:16:51.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.002910819119266574 Training loss: 8.5638427734375
2025-12-09 12:16:51.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0029105085359888397 Training loss: 8.497645378112793
2025-12-09 12:16:52.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.0029101974294568427 Training loss: 8.570106506347656
2025-12-09 12:16:52.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0029098857997859936 Training loss: 8.540412902832031
2025-12-09 12:16:52.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0029095736470918974 Training loss: 8.78285026550293
2025-12-09 12:16:52.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0029092609714903525 Training loss: 8.213239669799805
2025-12-09 12:16:52.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.002908947773097352 Training loss: 8.395943641662598
2025-12-09 12:16:52.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.002908634052029083 Training loss: 8.587701797485352
2025-12-09 12:16:52.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0029083198084019256 Training loss: 8.390528678894043
2025-12-09 12:16:52.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0029080050423324543 Training loss: 8.657302856445312
2025-12-09 12:16:52.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.002907689753937438 Training loss: 8.51025390625
2025-12-09 12:16:52.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.0029073739433338377 Training loss: 8.733620643615723
2025-12-09 12:16:52.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0029070576106388106 Training loss: 8.434286117553711
2025-12-09 12:16:52.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.002906740755969705 Training loss: 8.62035846710205
2025-12-09 12:16:52.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.002906423379444064 Training loss: 8.601058006286621
2025-12-09 12:16:53.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.0029061054811796248 Training loss: 9.350693702697754
2025-12-09 12:16:53.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0029057870612943177 Training loss: 8.795721054077148
2025-12-09 12:16:53.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0029054681199062664 Training loss: 9.100000381469727
2025-12-09 12:16:53.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.002905148657133788 Training loss: 8.313495635986328
2025-12-09 12:16:53.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0029048286730953927 Training loss: 8.65875244140625
2025-12-09 12:16:53.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.002904508167909785 Training loss: 8.574625015258789
2025-12-09 12:16:53.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.002904187141695863 Training loss: 8.741064071655273
2025-12-09 12:16:53.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.002903865594572716 Training loss: 8.313350677490234
2025-12-09 12:16:53.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.002903543526659628 Training loss: 8.848682403564453
2025-12-09 12:16:53.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.0029032209380760766 Training loss: 8.664220809936523
2025-12-09 12:16:53.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0029028978289417323 Training loss: 8.930109977722168
2025-12-09 12:16:53.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.002902574199376457 Training loss: 8.37121295928955
2025-12-09 12:16:53.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.002902250049500309 Training loss: 8.481518745422363
2025-12-09 12:16:54.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0029019253794335363 Training loss: 8.591337203979492
2025-12-09 12:16:54.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0029016001892965817 Training loss: 8.58333969116211
2025-12-09 12:16:54.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.00290127447921008 Training loss: 8.395973205566406
2025-12-09 12:16:54.274 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.0029009482492948608 Training loss: 8.925212860107422
2025-12-09 12:16:54.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.002900621499671944 Training loss: 9.001579284667969
2025-12-09 12:16:54.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0029002942304625435 Training loss: 8.683782577514648
2025-12-09 12:16:54.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.0028999664417880657 Training loss: 8.44072437286377
2025-12-09 12:16:54.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.0028996381337701104 Training loss: 8.21291732788086
2025-12-09 12:16:54.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0028993093065304695 Training loss: 8.60464096069336
2025-12-09 12:16:54.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.002898979960191127 Training loss: 8.682043075561523
2025-12-09 12:16:54.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.002898650094874261 Training loss: 8.308127403259277
2025-12-09 12:16:54.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00289831971070224 Training loss: 8.471393585205078
2025-12-09 12:16:54.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.002897988807797627 Training loss: 8.488303184509277
2025-12-09 12:16:55.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.002897657386283176 Training loss: 8.523402214050293
2025-12-09 12:16:55.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0028973254462818345 Training loss: 8.563413619995117
2025-12-09 12:16:55.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.002896992987916741 Training loss: 8.397370338439941
2025-12-09 12:16:55.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.0028966600113112277 Training loss: 8.604233741760254
2025-12-09 12:16:55.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.002896326516588819 Training loss: 8.789735794067383
2025-12-09 12:16:55.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.0028959925038732296 Training loss: 8.06296443939209
2025-12-09 12:16:55.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.0028956579732883686 Training loss: 8.441934585571289
2025-12-09 12:16:55.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.002895322924958336 Training loss: 8.670072555541992
2025-12-09 12:16:55.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.002894987359007424 Training loss: 8.157548904418945
2025-12-09 12:16:55.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.0028946512755601175 Training loss: 8.53042984008789
2025-12-09 12:16:55.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.0028943146747410927 Training loss: 8.474655151367188
2025-12-09 12:16:55.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.002893977556675218 Training loss: 8.74514389038086
2025-12-09 12:16:55.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.002893639921487553 Training loss: 8.404834747314453
2025-12-09 12:16:56.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.0028933017693033502 Training loss: 8.617761611938477
2025-12-09 12:16:56.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.002892963100248053 Training loss: 8.39754867553711
2025-12-09 12:16:56.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.0028926239144472982 Training loss: 8.523600578308105
2025-12-09 12:16:56.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.002892284212026912 Training loss: 8.255760192871094
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.39 GiB is free. Including non-PyTorch memory, this process has 90.83 GiB memory in use. Of the allocated memory 89.58 GiB is allocated by PyTorch, and 500.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Tokenizing texts:   0%|          | 0/10000 [00:00<?, ?it/s]Tokenizing texts:   0%|          | 15/10000 [00:00<01:06, 149.43it/s]Tokenizing texts:   0%|          | 34/10000 [00:00<00:58, 170.80it/s]Tokenizing texts:   1%|          | 52/10000 [00:00<01:24, 117.87it/s]Tokenizing texts:   1%|          | 72/10000 [00:00<01:10, 141.28it/s]Tokenizing texts:   1%|          | 97/10000 [00:00<00:57, 171.76it/s]Tokenizing texts:   1%|▏         | 129/10000 [00:00<00:46, 213.66it/s]Tokenizing texts:   2%|▏         | 152/10000 [00:00<00:52, 186.30it/s]Tokenizing texts:   2%|▏         | 173/10000 [00:00<00:52, 185.43it/s]Tokenizing texts:   2%|▏         | 197/10000 [00:01<00:49, 198.41it/s]Tokenizing texts:   2%|▏         | 219/10000 [00:01<00:48, 203.75it/s]Tokenizing texts:   2%|▏         | 241/10000 [00:01<00:47, 206.83it/s]Tokenizing texts:   3%|▎         | 263/10000 [00:01<00:47, 206.18it/s]Tokenizing texts:   3%|▎         | 293/10000 [00:01<00:43, 224.60it/s]Tokenizing texts:   3%|▎         | 319/10000 [00:01<00:43, 224.14it/s]Tokenizing texts:   3%|▎         | 342/10000 [00:01<00:51, 189.16it/s]Tokenizing texts:   4%|▎         | 368/10000 [00:01<00:46, 205.04it/s]Tokenizing texts:   4%|▍         | 390/10000 [00:02<00:51, 188.39it/s]Tokenizing texts:   4%|▍         | 412/10000 [00:02<00:49, 193.84it/s]Tokenizing texts:   4%|▍         | 433/10000 [00:02<00:50, 188.17it/s]Tokenizing texts:   5%|▍         | 453/10000 [00:02<00:51, 186.47it/s]Tokenizing texts:   5%|▍         | 481/10000 [00:02<00:45, 210.76it/s]Tokenizing texts:   5%|▌         | 507/10000 [00:02<00:42, 223.61it/s]Tokenizing texts:   5%|▌         | 530/10000 [00:02<00:50, 188.37it/s]Tokenizing texts:   6%|▌         | 551/10000 [00:02<00:48, 193.73it/s]Tokenizing texts:   6%|▌         | 581/10000 [00:02<00:43, 218.34it/s]Tokenizing texts:   6%|▌         | 604/10000 [00:03<00:42, 218.85it/s]Tokenizing texts:   6%|▋         | 627/10000 [00:03<00:46, 203.54it/s]Tokenizing texts:   7%|▋         | 651/10000 [00:03<00:45, 205.43it/s]Tokenizing texts:   7%|▋         | 677/10000 [00:03<00:45, 202.83it/s]Tokenizing texts:   7%|▋         | 698/10000 [00:03<00:48, 193.67it/s]Tokenizing texts:   7%|▋         | 722/10000 [00:03<00:45, 204.43it/s]Tokenizing texts:   7%|▋         | 743/10000 [00:03<00:45, 204.24it/s]Tokenizing texts:   8%|▊         | 768/10000 [00:03<00:42, 215.92it/s]Tokenizing texts:   8%|▊         | 790/10000 [00:04<00:45, 203.74it/s]Tokenizing texts:   8%|▊         | 814/10000 [00:04<00:43, 211.22it/s]Tokenizing texts:   8%|▊         | 837/10000 [00:04<00:42, 214.67it/s]Tokenizing texts:   9%|▊         | 863/10000 [00:04<00:40, 227.31it/s]Tokenizing texts:   9%|▉         | 886/10000 [00:04<00:41, 221.31it/s]Tokenizing texts:   9%|▉         | 911/10000 [00:04<00:39, 227.71it/s]Tokenizing texts:   9%|▉         | 944/10000 [00:04<00:35, 256.19it/s]Tokenizing texts:  10%|▉         | 974/10000 [00:04<00:34, 265.06it/s]Tokenizing texts:  10%|█         | 1004/10000 [00:04<00:32, 273.39it/s]Tokenizing texts:  10%|█         | 1036/10000 [00:04<00:31, 286.80it/s]Tokenizing texts:  11%|█         | 1065/10000 [00:05<00:39, 225.00it/s]Tokenizing texts:  11%|█         | 1090/10000 [00:05<00:40, 220.98it/s]Tokenizing texts:  11%|█         | 1116/10000 [00:05<00:38, 230.23it/s]Tokenizing texts:  11%|█▏        | 1141/10000 [00:05<00:44, 198.46it/s]Tokenizing texts:  12%|█▏        | 1165/10000 [00:05<00:43, 202.85it/s]Tokenizing texts:  12%|█▏        | 1193/10000 [00:05<00:40, 215.76it/s]Tokenizing texts:  12%|█▏        | 1216/10000 [00:05<00:40, 218.58it/s]Tokenizing texts:  12%|█▏        | 1239/10000 [00:05<00:40, 214.34it/s]Tokenizing texts:  13%|█▎        | 1261/10000 [00:06<00:41, 211.20it/s]Tokenizing texts:  13%|█▎        | 1296/10000 [00:06<00:34, 248.86it/s]Tokenizing texts:  13%|█▎        | 1322/10000 [00:06<00:39, 217.95it/s]Tokenizing texts:  13%|█▎        | 1345/10000 [00:06<00:42, 205.51it/s]Tokenizing texts:  14%|█▎        | 1370/10000 [00:06<00:39, 215.92it/s]Tokenizing texts:  14%|█▍        | 1395/10000 [00:06<00:39, 219.82it/s]Tokenizing texts:  14%|█▍        | 1424/10000 [00:06<00:36, 237.44it/s]Tokenizing texts:  15%|█▍        | 1454/10000 [00:06<00:33, 254.66it/s]Tokenizing texts:  15%|█▍        | 1488/10000 [00:06<00:30, 277.12it/s]Tokenizing texts:  15%|█▌        | 1517/10000 [00:07<00:31, 270.83it/s]Tokenizing texts:  15%|█▌        | 1545/10000 [00:07<00:33, 254.95it/s]Tokenizing texts:  16%|█▌        | 1578/10000 [00:07<00:30, 275.46it/s]Tokenizing texts:  16%|█▌        | 1607/10000 [00:07<00:40, 208.88it/s]Tokenizing texts:  16%|█▋        | 1635/10000 [00:07<00:37, 224.57it/s]Tokenizing texts:  17%|█▋        | 1660/10000 [00:07<00:36, 229.60it/s]Tokenizing texts:  17%|█▋        | 1685/10000 [00:07<00:37, 221.01it/s]Tokenizing texts:  17%|█▋        | 1709/10000 [00:07<00:36, 224.96it/s]Tokenizing texts:  17%|█▋        | 1744/10000 [00:08<00:32, 254.82it/s]Tokenizing texts:  18%|█▊        | 1771/10000 [00:08<00:32, 252.33it/s]Tokenizing texts:  18%|█▊        | 1797/10000 [00:08<00:33, 246.66it/s]Tokenizing texts:  18%|█▊        | 1830/10000 [00:08<00:30, 268.93it/s]Tokenizing texts:  19%|█▊        | 1858/10000 [00:08<00:36, 223.68it/s]Tokenizing texts:  19%|█▉        | 1882/10000 [00:08<00:36, 221.63it/s]Tokenizing texts:  19%|█▉        | 1910/10000 [00:08<00:34, 234.37it/s]Tokenizing texts:  19%|█▉        | 1935/10000 [00:08<00:36, 224.00it/s]Tokenizing texts:  20%|█▉        | 1959/10000 [00:09<00:42, 189.96it/s]Tokenizing texts:  20%|█▉        | 1990/10000 [00:09<00:36, 217.48it/s]Tokenizing texts:  20%|██        | 2017/10000 [00:09<00:35, 226.38it/s]Tokenizing texts:  21%|██        | 2051/10000 [00:09<00:31, 254.87it/s]Tokenizing texts:  21%|██        | 2078/10000 [00:09<00:32, 245.41it/s]Tokenizing texts:  21%|██        | 2104/10000 [00:09<00:39, 199.69it/s]Tokenizing texts:  21%|██▏       | 2128/10000 [00:09<00:38, 206.88it/s]Tokenizing texts:  22%|██▏       | 2151/10000 [00:09<00:39, 200.54it/s]Tokenizing texts:  22%|██▏       | 2180/10000 [00:10<00:35, 218.77it/s]Tokenizing texts:  22%|██▏       | 2214/10000 [00:10<00:31, 247.14it/s]Tokenizing texts:  22%|██▏       | 2244/10000 [00:10<00:30, 257.39it/s]Tokenizing texts:  23%|██▎       | 2274/10000 [00:10<00:28, 268.58it/s]Tokenizing texts:  23%|██▎       | 2302/10000 [00:10<00:29, 259.43it/s]Tokenizing texts:  23%|██▎       | 2329/10000 [00:10<00:36, 210.83it/s]Tokenizing texts:  24%|██▎       | 2365/10000 [00:10<00:30, 246.79it/s]Tokenizing texts:  24%|██▍       | 2399/10000 [00:10<00:28, 267.91it/s]Tokenizing texts:  24%|██▍       | 2428/10000 [00:10<00:30, 248.27it/s]Tokenizing texts:  25%|██▍       | 2455/10000 [00:11<00:30, 251.33it/s]Tokenizing texts:  25%|██▍       | 2482/10000 [00:11<00:30, 246.70it/s]Tokenizing texts:  25%|██▌       | 2508/10000 [00:11<00:30, 245.85it/s]Tokenizing texts:  25%|██▌       | 2537/10000 [00:11<00:28, 257.67it/s]Tokenizing texts:  26%|██▌       | 2564/10000 [00:11<00:33, 221.08it/s]Tokenizing texts:  26%|██▌       | 2588/10000 [00:11<00:33, 222.41it/s]Tokenizing texts:  26%|██▌       | 2612/10000 [00:11<00:36, 204.68it/s]Tokenizing texts:  26%|██▋       | 2640/10000 [00:11<00:32, 223.65it/s]Tokenizing texts:  27%|██▋       | 2667/10000 [00:12<00:31, 231.10it/s]Tokenizing texts:  27%|██▋       | 2691/10000 [00:12<00:32, 223.49it/s]Tokenizing texts:  27%|██▋       | 2714/10000 [00:12<00:35, 203.38it/s]Tokenizing texts:  27%|██▋       | 2749/10000 [00:12<00:30, 239.76it/s]Tokenizing texts:  28%|██▊       | 2776/10000 [00:12<00:29, 243.09it/s]Tokenizing texts:  28%|██▊       | 2801/10000 [00:12<00:40, 179.67it/s]Tokenizing texts:  28%|██▊       | 2837/10000 [00:12<00:32, 219.35it/s]Tokenizing texts:  29%|██▊       | 2863/10000 [00:12<00:35, 202.85it/s]Tokenizing texts:  29%|██▉       | 2891/10000 [00:13<00:32, 220.50it/s]Tokenizing texts:  29%|██▉       | 2923/10000 [00:13<00:28, 244.98it/s]Tokenizing texts:  30%|██▉       | 2950/10000 [00:13<00:31, 224.40it/s]Tokenizing texts:  30%|██▉       | 2975/10000 [00:13<00:30, 227.27it/s]Tokenizing texts:  30%|███       | 3013/10000 [00:13<00:26, 264.10it/s]Tokenizing texts:  30%|███       | 3041/10000 [00:13<00:30, 228.87it/s]Tokenizing texts:  31%|███       | 3074/10000 [00:13<00:27, 253.71it/s]Tokenizing texts:  31%|███       | 3102/10000 [00:13<00:28, 243.49it/s]Tokenizing texts:  31%|███▏      | 3136/10000 [00:14<00:25, 267.02it/s]Tokenizing texts:  32%|███▏      | 3164/10000 [00:14<00:27, 252.81it/s]Tokenizing texts:  32%|███▏      | 3192/10000 [00:14<00:26, 259.45it/s]Tokenizing texts:  32%|███▏      | 3220/10000 [00:14<00:25, 263.21it/s]Tokenizing texts:  32%|███▏      | 3247/10000 [00:14<00:26, 256.23it/s]Tokenizing texts:  33%|███▎      | 3278/10000 [00:14<00:25, 267.37it/s]Tokenizing texts:  33%|███▎      | 3306/10000 [00:14<00:25, 260.40it/s]Tokenizing texts:  33%|███▎      | 3333/10000 [00:14<00:26, 248.48it/s]Tokenizing texts:  34%|███▎      | 3362/10000 [00:14<00:25, 258.91it/s]Tokenizing texts:  34%|███▍      | 3389/10000 [00:15<00:26, 249.67it/s]Tokenizing texts:  34%|███▍      | 3416/10000 [00:15<00:25, 254.79it/s]Tokenizing texts:  35%|███▍      | 3457/10000 [00:15<00:22, 296.71it/s]Tokenizing texts:  35%|███▍      | 3487/10000 [00:15<00:22, 291.24it/s]Tokenizing texts:  35%|███▌      | 3517/10000 [00:15<00:26, 245.85it/s]Tokenizing texts:  35%|███▌      | 3543/10000 [00:15<00:27, 233.83it/s]Tokenizing texts:  36%|███▌      | 3570/10000 [00:15<00:26, 242.60it/s]Tokenizing texts:  36%|███▌      | 3600/10000 [00:15<00:25, 254.29it/s]Tokenizing texts:  36%|███▋      | 3627/10000 [00:15<00:24, 255.79it/s]Tokenizing texts:  37%|███▋      | 3654/10000 [00:16<00:24, 253.95it/s]Tokenizing texts:  37%|███▋      | 3680/10000 [00:16<00:25, 248.26it/s]Tokenizing texts:  37%|███▋      | 3706/10000 [00:16<00:25, 248.18it/s]Tokenizing texts:  37%|███▋      | 3733/10000 [00:16<00:24, 250.89it/s]Tokenizing texts:  38%|███▊      | 3759/10000 [00:16<00:27, 230.19it/s]Tokenizing texts:  38%|███▊      | 3785/10000 [00:16<00:26, 234.64it/s]Tokenizing texts:  38%|███▊      | 3809/10000 [00:16<00:29, 208.75it/s]Tokenizing texts:  38%|███▊      | 3842/10000 [00:16<00:26, 233.98it/s]Tokenizing texts:  39%|███▊      | 3870/10000 [00:16<00:25, 243.87it/s]Tokenizing texts:  39%|███▉      | 3896/10000 [00:17<00:25, 240.86it/s]Tokenizing texts:  39%|███▉      | 3921/10000 [00:17<00:25, 239.91it/s]Tokenizing texts:  39%|███▉      | 3946/10000 [00:17<00:25, 233.53it/s]Tokenizing texts:  40%|███▉      | 3980/10000 [00:17<00:23, 259.13it/s]Tokenizing texts:  40%|████      | 4007/10000 [00:17<00:26, 222.76it/s]Tokenizing texts:  40%|████      | 4037/10000 [00:17<00:24, 238.74it/s]Tokenizing texts:  41%|████      | 4067/10000 [00:17<00:23, 253.78it/s]Tokenizing texts:  41%|████      | 4099/10000 [00:17<00:21, 271.48it/s]Tokenizing texts:  41%|████▏     | 4132/10000 [00:17<00:20, 287.75it/s]Tokenizing texts:  42%|████▏     | 4162/10000 [00:18<00:21, 273.34it/s]Tokenizing texts:  42%|████▏     | 4195/10000 [00:18<00:20, 286.49it/s]Tokenizing texts:  42%|████▏     | 4225/10000 [00:18<00:22, 261.24it/s]Tokenizing texts:  43%|████▎     | 4252/10000 [00:18<00:23, 246.84it/s]Tokenizing texts:  43%|████▎     | 4278/10000 [00:18<00:24, 237.29it/s]Tokenizing texts:  43%|████▎     | 4310/10000 [00:18<00:21, 258.81it/s]Tokenizing texts:  43%|████▎     | 4345/10000 [00:18<00:20, 281.38it/s]Tokenizing texts:  44%|████▎     | 4374/10000 [00:18<00:21, 264.77it/s]Tokenizing texts:  44%|████▍     | 4412/10000 [00:19<00:19, 293.27it/s]Tokenizing texts:  44%|████▍     | 4442/10000 [00:19<00:19, 282.53it/s]Tokenizing texts:  45%|████▍     | 4471/10000 [00:19<00:21, 259.60it/s]Tokenizing texts:  45%|████▍     | 4498/10000 [00:19<00:22, 243.58it/s]Tokenizing texts:  45%|████▌     | 4525/10000 [00:19<00:21, 250.20it/s]Tokenizing texts:  46%|████▌     | 4553/10000 [00:19<00:21, 250.85it/s]Tokenizing texts:  46%|████▌     | 4581/10000 [00:19<00:20, 258.07it/s]Tokenizing texts:  46%|████▌     | 4608/10000 [00:19<00:22, 244.65it/s]Tokenizing texts:  46%|████▋     | 4641/10000 [00:19<00:20, 266.54it/s]Tokenizing texts:  47%|████▋     | 4676/10000 [00:20<00:18, 288.54it/s]Tokenizing texts:  47%|████▋     | 4712/10000 [00:20<00:17, 307.53it/s]Tokenizing texts:  47%|████▋     | 4744/10000 [00:20<00:19, 274.53it/s]Tokenizing texts:  48%|████▊     | 4774/10000 [00:20<00:18, 280.16it/s]Tokenizing texts:  48%|████▊     | 4803/10000 [00:20<00:20, 250.25it/s]Tokenizing texts:  48%|████▊     | 4835/10000 [00:20<00:19, 267.60it/s]Tokenizing texts:  49%|████▊     | 4865/10000 [00:20<00:18, 275.02it/s]Tokenizing texts:  49%|████▉     | 4895/10000 [00:20<00:18, 281.91it/s]Tokenizing texts:  49%|████▉     | 4924/10000 [00:20<00:18, 267.47it/s]Tokenizing texts:  50%|████▉     | 4953/10000 [00:21<00:18, 271.44it/s]Tokenizing texts:  50%|████▉     | 4981/10000 [00:21<00:18, 273.76it/s]Tokenizing texts:  50%|█████     | 5011/10000 [00:21<00:17, 281.08it/s]Tokenizing texts:  50%|█████     | 5040/10000 [00:21<00:17, 278.34it/s]Tokenizing texts:  51%|█████     | 5069/10000 [00:21<00:20, 245.20it/s]Tokenizing texts:  51%|█████     | 5095/10000 [00:21<00:20, 242.73it/s]Tokenizing texts:  51%|█████     | 5122/10000 [00:21<00:20, 234.28it/s]Tokenizing texts:  52%|█████▏    | 5150/10000 [00:21<00:19, 244.81it/s]Tokenizing texts:  52%|█████▏    | 5178/10000 [00:21<00:19, 253.33it/s]Tokenizing texts:  52%|█████▏    | 5204/10000 [00:22<00:20, 238.20it/s]Tokenizing texts:  52%|█████▏    | 5231/10000 [00:22<00:19, 242.50it/s]Tokenizing texts:  53%|█████▎    | 5265/10000 [00:22<00:17, 266.35it/s]Tokenizing texts:  53%|█████▎    | 5292/10000 [00:22<00:18, 254.06it/s]Tokenizing texts:  53%|█████▎    | 5327/10000 [00:22<00:16, 280.23it/s]Tokenizing texts:  54%|█████▎    | 5359/10000 [00:22<00:15, 290.25it/s]Tokenizing texts:  54%|█████▍    | 5390/10000 [00:22<00:15, 294.70it/s]Tokenizing texts:  54%|█████▍    | 5423/10000 [00:22<00:15, 302.45it/s]Tokenizing texts:  55%|█████▍    | 5454/10000 [00:22<00:16, 273.18it/s]Tokenizing texts:  55%|█████▍    | 5482/10000 [00:23<00:17, 257.18it/s]Tokenizing texts:  55%|█████▌    | 5509/10000 [00:23<00:20, 217.69it/s]Tokenizing texts:  55%|█████▌    | 5542/10000 [00:23<00:18, 240.76it/s]Tokenizing texts:  56%|█████▌    | 5570/10000 [00:23<00:18, 242.87it/s]Tokenizing texts:  56%|█████▌    | 5599/10000 [00:23<00:17, 254.99it/s]Tokenizing texts:  56%|█████▋    | 5632/10000 [00:23<00:15, 274.44it/s]Tokenizing texts:  57%|█████▋    | 5669/10000 [00:23<00:14, 300.74it/s]Tokenizing texts:  57%|█████▋    | 5701/10000 [00:23<00:14, 301.34it/s]Tokenizing texts:  57%|█████▋    | 5732/10000 [00:24<00:15, 279.88it/s]Tokenizing texts:  58%|█████▊    | 5761/10000 [00:24<00:16, 255.29it/s]Tokenizing texts:  58%|█████▊    | 5788/10000 [00:24<00:19, 218.14it/s]Tokenizing texts:  58%|█████▊    | 5812/10000 [00:24<00:19, 219.10it/s]Tokenizing texts:  58%|█████▊    | 5835/10000 [00:24<00:19, 214.54it/s]Tokenizing texts:  59%|█████▊    | 5871/10000 [00:24<00:16, 251.78it/s]Tokenizing texts:  59%|█████▉    | 5898/10000 [00:24<00:16, 250.28it/s]Tokenizing texts:  59%|█████▉    | 5924/10000 [00:24<00:17, 235.65it/s]Tokenizing texts:  60%|█████▉    | 5952/10000 [00:25<00:16, 246.05it/s]Tokenizing texts:  60%|█████▉    | 5980/10000 [00:25<00:15, 252.51it/s]Tokenizing texts:  60%|██████    | 6008/10000 [00:25<00:15, 250.55it/s]Tokenizing texts:  60%|██████    | 6034/10000 [00:25<00:15, 250.28it/s]Tokenizing texts:  61%|██████    | 6066/10000 [00:25<00:15, 262.16it/s]Tokenizing texts:  61%|██████    | 6093/10000 [00:25<00:15, 259.69it/s]Tokenizing texts:  61%|██████    | 6120/10000 [00:25<00:14, 261.19it/s]Tokenizing texts:  62%|██████▏   | 6154/10000 [00:25<00:13, 278.70it/s]Tokenizing texts:  62%|██████▏   | 6182/10000 [00:25<00:13, 277.44it/s]Tokenizing texts:  62%|██████▏   | 6210/10000 [00:25<00:13, 276.77it/s]Tokenizing texts:  62%|██████▏   | 6238/10000 [00:26<00:15, 248.92it/s]Tokenizing texts:  63%|██████▎   | 6264/10000 [00:26<00:15, 245.62it/s]Tokenizing texts:  63%|██████▎   | 6290/10000 [00:26<00:14, 248.82it/s]Tokenizing texts:  63%|██████▎   | 6316/10000 [00:26<00:15, 234.40it/s]Tokenizing texts:  63%|██████▎   | 6340/10000 [00:26<00:15, 229.11it/s]Tokenizing texts:  64%|██████▎   | 6368/10000 [00:26<00:15, 241.66it/s]Tokenizing texts:  64%|██████▍   | 6395/10000 [00:26<00:14, 241.64it/s]Tokenizing texts:  64%|██████▍   | 6423/10000 [00:26<00:14, 251.96it/s]Tokenizing texts:  65%|██████▍   | 6453/10000 [00:26<00:13, 263.59it/s]Tokenizing texts:  65%|██████▍   | 6480/10000 [00:27<00:14, 241.35it/s]Tokenizing texts:  65%|██████▌   | 6506/10000 [00:27<00:14, 233.06it/s]Tokenizing texts:  65%|██████▌   | 6531/10000 [00:27<00:14, 237.45it/s]Tokenizing texts:  66%|██████▌   | 6572/10000 [00:27<00:12, 284.81it/s]Tokenizing texts:  66%|██████▌   | 6602/10000 [00:27<00:12, 273.10it/s]Tokenizing texts:  66%|██████▋   | 6630/10000 [00:27<00:12, 266.49it/s]Tokenizing texts:  67%|██████▋   | 6658/10000 [00:27<00:12, 269.72it/s]Tokenizing texts:  67%|██████▋   | 6686/10000 [00:27<00:12, 267.67it/s]Tokenizing texts:  67%|██████▋   | 6713/10000 [00:27<00:12, 256.20it/s]Tokenizing texts:  67%|██████▋   | 6739/10000 [00:28<00:13, 248.05it/s]Tokenizing texts:  68%|██████▊   | 6770/10000 [00:28<00:12, 265.01it/s]Tokenizing texts:  68%|██████▊   | 6804/10000 [00:28<00:11, 285.76it/s]Tokenizing texts:  68%|██████▊   | 6837/10000 [00:28<00:10, 293.82it/s]Tokenizing texts:  69%|██████▊   | 6869/10000 [00:28<00:10, 290.02it/s]Tokenizing texts:  69%|██████▉   | 6899/10000 [00:28<00:13, 237.93it/s]Tokenizing texts:  69%|██████▉   | 6925/10000 [00:28<00:13, 233.22it/s]Tokenizing texts:  70%|██████▉   | 6950/10000 [00:28<00:13, 234.15it/s]Tokenizing texts:  70%|██████▉   | 6978/10000 [00:29<00:12, 244.16it/s]Tokenizing texts:  70%|███████   | 7009/10000 [00:29<00:11, 260.40it/s]Tokenizing texts:  70%|███████   | 7036/10000 [00:29<00:11, 250.69it/s]Tokenizing texts:  71%|███████   | 7062/10000 [00:29<00:13, 212.25it/s]Tokenizing texts:  71%|███████   | 7086/10000 [00:29<00:13, 218.05it/s]Tokenizing texts:  71%|███████   | 7112/10000 [00:29<00:12, 227.85it/s]Tokenizing texts:  71%|███████▏  | 7142/10000 [00:29<00:11, 246.60it/s]Tokenizing texts:  72%|███████▏  | 7168/10000 [00:29<00:11, 236.44it/s]Tokenizing texts:  72%|███████▏  | 7195/10000 [00:29<00:11, 244.29it/s]Tokenizing texts:  72%|███████▏  | 7224/10000 [00:30<00:10, 254.80it/s]Tokenizing texts:  72%|███████▎  | 7250/10000 [00:30<00:11, 242.78it/s]Tokenizing texts:  73%|███████▎  | 7295/10000 [00:30<00:09, 298.58it/s]Tokenizing texts:  73%|███████▎  | 7328/10000 [00:30<00:08, 305.96it/s]Tokenizing texts:  74%|███████▎  | 7360/10000 [00:30<00:09, 274.51it/s]Tokenizing texts:  74%|███████▍  | 7389/10000 [00:30<00:09, 263.27it/s]Tokenizing texts:  74%|███████▍  | 7416/10000 [00:30<00:11, 220.35it/s]Tokenizing texts:  74%|███████▍  | 7442/10000 [00:30<00:12, 206.93it/s]Tokenizing texts:  75%|███████▍  | 7474/10000 [00:31<00:10, 233.29it/s]Tokenizing texts:  75%|███████▌  | 7510/10000 [00:31<00:09, 262.93it/s]Tokenizing texts:  75%|███████▌  | 7539/10000 [00:31<00:09, 268.86it/s]Tokenizing texts:  76%|███████▌  | 7568/10000 [00:31<00:09, 260.98it/s]Tokenizing texts:  76%|███████▌  | 7607/10000 [00:31<00:08, 295.07it/s]Tokenizing texts:  76%|███████▋  | 7638/10000 [00:31<00:08, 272.56it/s]Tokenizing texts:  77%|███████▋  | 7668/10000 [00:31<00:08, 279.83it/s]Tokenizing texts:  77%|███████▋  | 7701/10000 [00:31<00:07, 288.84it/s]Tokenizing texts:  77%|███████▋  | 7731/10000 [00:31<00:08, 268.15it/s]Tokenizing texts:  78%|███████▊  | 7759/10000 [00:32<00:09, 239.67it/s]Tokenizing texts:  78%|███████▊  | 7791/10000 [00:32<00:08, 256.34it/s]Tokenizing texts:  78%|███████▊  | 7818/10000 [00:32<00:09, 236.59it/s]Tokenizing texts:  78%|███████▊  | 7845/10000 [00:32<00:08, 245.02it/s]Tokenizing texts:  79%|███████▉  | 7875/10000 [00:32<00:08, 256.81it/s]Tokenizing texts:  79%|███████▉  | 7902/10000 [00:32<00:08, 254.53it/s]Tokenizing texts:  79%|███████▉  | 7938/10000 [00:32<00:07, 283.16it/s]Tokenizing texts:  80%|███████▉  | 7967/10000 [00:32<00:07, 280.63it/s]Tokenizing texts:  80%|███████▉  | 7999/10000 [00:32<00:06, 291.51it/s]Tokenizing texts:  80%|████████  | 8029/10000 [00:33<00:06, 287.63it/s]Tokenizing texts:  81%|████████  | 8058/10000 [00:33<00:11, 170.70it/s]Tokenizing texts:  81%|████████  | 8094/10000 [00:33<00:09, 201.14it/s]Tokenizing texts:  81%|████████  | 8120/10000 [00:33<00:08, 210.27it/s]Tokenizing texts:  81%|████████▏ | 8149/10000 [00:33<00:08, 227.84it/s]Tokenizing texts:  82%|████████▏ | 8177/10000 [00:33<00:07, 240.22it/s]Tokenizing texts:  82%|████████▏ | 8212/10000 [00:33<00:07, 250.97it/s]Tokenizing texts:  82%|████████▏ | 8239/10000 [00:34<00:08, 214.85it/s]Tokenizing texts:  83%|████████▎ | 8265/10000 [00:34<00:07, 225.39it/s]Tokenizing texts:  83%|████████▎ | 8301/10000 [00:34<00:06, 259.17it/s]Tokenizing texts:  83%|████████▎ | 8329/10000 [00:34<00:06, 254.93it/s]Tokenizing texts:  84%|████████▎ | 8356/10000 [00:34<00:06, 258.81it/s]Tokenizing texts:  84%|████████▍ | 8383/10000 [00:34<00:06, 258.16it/s]Tokenizing texts:  84%|████████▍ | 8410/10000 [00:34<00:06, 233.57it/s]Tokenizing texts:  84%|████████▍ | 8444/10000 [00:34<00:05, 259.49it/s]Tokenizing texts:  85%|████████▍ | 8472/10000 [00:35<00:05, 264.94it/s]Tokenizing texts:  85%|████████▌ | 8504/10000 [00:35<00:05, 280.06it/s]Tokenizing texts:  85%|████████▌ | 8533/10000 [00:35<00:05, 262.90it/s]Tokenizing texts:  86%|████████▌ | 8562/10000 [00:35<00:05, 269.48it/s]Tokenizing texts:  86%|████████▌ | 8590/10000 [00:35<00:05, 264.67it/s]Tokenizing texts:  86%|████████▋ | 8625/10000 [00:35<00:04, 287.96it/s]Tokenizing texts:  87%|████████▋ | 8655/10000 [00:35<00:04, 287.18it/s]Tokenizing texts:  87%|████████▋ | 8688/10000 [00:35<00:04, 284.98it/s]Tokenizing texts:  87%|████████▋ | 8725/10000 [00:35<00:04, 285.99it/s]Tokenizing texts:  88%|████████▊ | 8757/10000 [00:36<00:04, 292.76it/s]Tokenizing texts:  88%|████████▊ | 8787/10000 [00:36<00:04, 294.09it/s]Tokenizing texts:  88%|████████▊ | 8817/10000 [00:36<00:05, 221.10it/s]Tokenizing texts:  88%|████████▊ | 8842/10000 [00:36<00:05, 222.87it/s]Tokenizing texts:  89%|████████▉ | 8876/10000 [00:36<00:04, 250.13it/s]Tokenizing texts:  89%|████████▉ | 8914/10000 [00:36<00:03, 274.85it/s]Tokenizing texts:  89%|████████▉ | 8946/10000 [00:36<00:03, 285.11it/s]Tokenizing texts:  90%|████████▉ | 8976/10000 [00:36<00:03, 282.25it/s]Tokenizing texts:  90%|█████████ | 9006/10000 [00:37<00:04, 245.87it/s]Tokenizing texts:  90%|█████████ | 9038/10000 [00:37<00:03, 262.75it/s]Tokenizing texts:  91%|█████████ | 9066/10000 [00:37<00:03, 251.99it/s]Tokenizing texts:  91%|█████████ | 9097/10000 [00:37<00:03, 267.05it/s]Tokenizing texts:  91%|█████████▏| 9125/10000 [00:37<00:03, 257.33it/s]Tokenizing texts:  92%|█████████▏| 9158/10000 [00:37<00:03, 271.67it/s]Tokenizing texts:  92%|█████████▏| 9191/10000 [00:37<00:02, 286.84it/s]Tokenizing texts:  92%|█████████▏| 9221/10000 [00:37<00:02, 285.02it/s]Tokenizing texts:  92%|█████████▎| 9250/10000 [00:37<00:02, 278.02it/s]Tokenizing texts:  93%|█████████▎| 9279/10000 [00:38<00:02, 247.75it/s]Tokenizing texts:  93%|█████████▎| 9310/10000 [00:38<00:02, 258.23it/s]Tokenizing texts:  93%|█████████▎| 9337/10000 [00:38<00:02, 249.14it/s]Tokenizing texts:  94%|█████████▎| 9363/10000 [00:38<00:02, 212.80it/s]Tokenizing texts:  94%|█████████▍| 9400/10000 [00:38<00:02, 250.08it/s]Tokenizing texts:  94%|█████████▍| 9427/10000 [00:38<00:02, 228.26it/s]Tokenizing texts:  95%|█████████▍| 9452/10000 [00:38<00:02, 232.91it/s]Tokenizing texts:  95%|█████████▍| 9479/10000 [00:38<00:02, 241.95it/s]Tokenizing texts:  95%|█████████▌| 9505/10000 [00:39<00:02, 185.90it/s]Tokenizing texts:  95%|█████████▌| 9540/10000 [00:39<00:02, 221.96it/s]Tokenizing texts:  96%|█████████▌| 9566/10000 [00:39<00:01, 229.05it/s]Tokenizing texts:  96%|█████████▌| 9596/10000 [00:39<00:01, 233.91it/s]Tokenizing texts:  96%|█████████▌| 9621/10000 [00:39<00:01, 232.30it/s]Tokenizing texts:  96%|█████████▋| 9648/10000 [00:39<00:01, 234.98it/s]Tokenizing texts:  97%|█████████▋| 9673/10000 [00:39<00:01, 238.92it/s]Tokenizing texts:  97%|█████████▋| 9698/10000 [00:39<00:01, 235.82it/s]Tokenizing texts:  97%|█████████▋| 9724/10000 [00:39<00:01, 239.41it/s]Tokenizing texts:  97%|█████████▋| 9749/10000 [00:40<00:01, 207.81it/s]Tokenizing texts:  98%|█████████▊| 9774/10000 [00:40<00:01, 215.65it/s]Tokenizing texts:  98%|█████████▊| 9813/10000 [00:40<00:00, 260.70it/s]Tokenizing texts:  98%|█████████▊| 9841/10000 [00:40<00:00, 253.69it/s]Tokenizing texts:  99%|█████████▊| 9871/10000 [00:40<00:00, 259.09it/s]Tokenizing texts:  99%|█████████▉| 9907/10000 [00:40<00:00, 286.11it/s]Tokenizing texts:  99%|█████████▉| 9937/10000 [00:40<00:00, 283.11it/s]Tokenizing texts: 100%|█████████▉| 9968/10000 [00:40<00:00, 290.23it/s]Tokenizing texts: 100%|██████████| 10000/10000 [00:40<00:00, 244.08it/s]
2025-12-09 12:17:56.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 12.033109664916992
2025-12-09 12:17:56.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 12.050241470336914
2025-12-09 12:17:57.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 12.053863525390625
2025-12-09 12:17:57.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 12.056864738464355
2025-12-09 12:17:57.208 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 12.02119255065918
2025-12-09 12:17:57.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 12.006023406982422
2025-12-09 12:17:57.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 12.02318286895752
2025-12-09 12:17:57.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 12.016715049743652
2025-12-09 12:17:57.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 11.987963676452637
2025-12-09 12:17:57.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 12.000905990600586
2025-12-09 12:17:57.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 11.97700023651123
2025-12-09 12:17:57.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 11.997869491577148
2025-12-09 12:17:57.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 11.941247940063477
2025-12-09 12:17:57.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 11.959638595581055
2025-12-09 12:17:57.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 11.910857200622559
2025-12-09 12:17:58.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 11.893643379211426
2025-12-09 12:17:58.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 11.81752872467041
2025-12-09 12:17:58.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 11.856987953186035
2025-12-09 12:17:58.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 11.80951976776123
2025-12-09 12:17:58.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 11.759660720825195
2025-12-09 12:17:58.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 11.698113441467285
2025-12-09 12:17:58.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 11.55764389038086
2025-12-09 12:17:58.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 11.631705284118652
2025-12-09 12:17:58.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 11.47314167022705
2025-12-09 12:17:58.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 11.417311668395996
2025-12-09 12:17:58.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 11.32460880279541
2025-12-09 12:17:58.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 11.314221382141113
2025-12-09 12:17:58.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 11.077343940734863
2025-12-09 12:17:59.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 11.202746391296387
2025-12-09 12:17:59.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 10.966338157653809
2025-12-09 12:17:59.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 11.053139686584473
2025-12-09 12:17:59.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 10.966968536376953
2025-12-09 12:17:59.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 10.798657417297363
2025-12-09 12:17:59.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 10.64126205444336
2025-12-09 12:17:59.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 10.787705421447754
2025-12-09 12:17:59.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 10.671910285949707
2025-12-09 12:17:59.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 10.724297523498535
2025-12-09 12:17:59.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 10.403419494628906
2025-12-09 12:17:59.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 10.769765853881836
2025-12-09 12:17:59.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 10.482829093933105
2025-12-09 12:17:59.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 10.355799674987793
2025-12-09 12:18:00.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 10.168853759765625
2025-12-09 12:18:00.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 10.219493865966797
2025-12-09 12:18:00.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 10.446229934692383
2025-12-09 12:18:00.421 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 10.305816650390625
2025-12-09 12:18:00.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 10.167817115783691
2025-12-09 12:18:00.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 10.198702812194824
2025-12-09 12:18:00.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 9.965880393981934
2025-12-09 12:18:00.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 10.039380073547363
2025-12-09 12:18:00.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 10.026716232299805
2025-12-09 12:18:00.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 10.039234161376953
2025-12-09 12:18:00.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 10.162800788879395
2025-12-09 12:18:01.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 10.007779121398926
2025-12-09 12:18:01.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 9.883824348449707
2025-12-09 12:18:01.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 10.199663162231445
2025-12-09 12:18:01.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 10.036397933959961
2025-12-09 12:18:01.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 9.7857084274292
2025-12-09 12:18:01.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 10.030525207519531
2025-12-09 12:18:01.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 9.651308059692383
2025-12-09 12:18:01.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 10.010781288146973
2025-12-09 12:18:01.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 9.827994346618652
2025-12-09 12:18:01.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 9.742321968078613
2025-12-09 12:18:01.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 9.914456367492676
2025-12-09 12:18:01.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 9.559660911560059
2025-12-09 12:18:01.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 9.71556568145752
2025-12-09 12:18:02.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 9.748998641967773
2025-12-09 12:18:02.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 9.708845138549805
2025-12-09 12:18:02.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 9.725923538208008
2025-12-09 12:18:02.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 9.74397087097168
2025-12-09 12:18:02.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 9.453361511230469
2025-12-09 12:18:02.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 9.842940330505371
2025-12-09 12:18:02.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 9.834053993225098
2025-12-09 12:18:02.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 9.571474075317383
2025-12-09 12:18:02.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 9.693559646606445
2025-12-09 12:18:02.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 9.643701553344727
2025-12-09 12:18:02.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 9.683027267456055
2025-12-09 12:18:02.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 9.678601264953613
2025-12-09 12:18:02.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 9.705016136169434
2025-12-09 12:18:03.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 9.659521102905273
2025-12-09 12:18:03.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 9.86998462677002
2025-12-09 12:18:03.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 9.456888198852539
2025-12-09 12:18:03.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 9.467070579528809
2025-12-09 12:18:03.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 9.70854663848877
2025-12-09 12:18:03.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 9.899545669555664
2025-12-09 12:18:03.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 9.433671951293945
2025-12-09 12:18:03.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 9.284196853637695
2025-12-09 12:18:03.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 9.774889945983887
2025-12-09 12:18:03.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 9.776246070861816
2025-12-09 12:18:03.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 9.508951187133789
2025-12-09 12:18:03.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 9.027754783630371
2025-12-09 12:18:03.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 9.474617004394531
2025-12-09 12:18:04.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 9.548543930053711
2025-12-09 12:18:04.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 9.027787208557129
2025-12-09 12:18:04.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 9.458733558654785
2025-12-09 12:18:04.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 9.693700790405273
2025-12-09 12:18:04.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 9.210953712463379
2025-12-09 12:18:04.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 9.491228103637695
2025-12-09 12:18:04.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 9.562190055847168
2025-12-09 12:18:04.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 9.410178184509277
2025-12-09 12:18:04.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 9.434613227844238
2025-12-09 12:18:04.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999999072578702 Training loss: 9.267559051513672
2025-12-09 12:18:04.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.009999996290315153 Training loss: 9.50208854675293
2025-12-09 12:18:04.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009999991653210385 Training loss: 9.722994804382324
2025-12-09 12:18:04.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009999985161266116 Training loss: 9.389378547668457
2025-12-09 12:18:04.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009999976814484758 Training loss: 9.499696731567383
2025-12-09 12:18:05.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009999966612869405 Training loss: 9.101239204406738
2025-12-09 12:18:05.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009999954556423843 Training loss: 9.414799690246582
2025-12-09 12:18:05.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.009999940645152541 Training loss: 9.628926277160645
2025-12-09 12:18:05.298 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.009999924879060665 Training loss: 9.565876007080078
2025-12-09 12:18:05.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.00999990725815406 Training loss: 9.549372673034668
2025-12-09 12:18:05.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009999887782439263 Training loss: 9.525192260742188
2025-12-09 12:18:05.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0099998664519235 Training loss: 9.40392780303955
2025-12-09 12:18:05.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009999843266614685 Training loss: 9.474981307983398
2025-12-09 12:18:05.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009999818226521415 Training loss: 9.33033561706543
2025-12-09 12:18:05.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.009999791331652984 Training loss: 9.692671775817871
2025-12-09 12:18:05.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009999762582019366 Training loss: 9.535855293273926
2025-12-09 12:18:05.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.009999731977631227 Training loss: 9.668279647827148
2025-12-09 12:18:06.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.00999969951849992 Training loss: 9.796473503112793
2025-12-09 12:18:06.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.009999665204637487 Training loss: 9.442193984985352
2025-12-09 12:18:06.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009999629036056657 Training loss: 8.961463928222656
2025-12-09 12:18:06.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.009999591012770847 Training loss: 9.559117317199707
2025-12-09 12:18:06.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.009999551134794164 Training loss: 9.873346328735352
2025-12-09 12:18:06.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.0099995094021414 Training loss: 9.492359161376953
2025-12-09 12:18:06.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.009999465814828036 Training loss: 9.494683265686035
2025-12-09 12:18:06.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.009999420372870242 Training loss: 9.56955337524414
2025-12-09 12:18:06.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.009999373076284877 Training loss: 9.114977836608887
2025-12-09 12:18:06.744 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009999323925089485 Training loss: 9.514140129089355
2025-12-09 12:18:06.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009999272919302301 Training loss: 9.68748950958252
2025-12-09 12:18:06.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009999220058942245 Training loss: 9.860045433044434
2025-12-09 12:18:06.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009999165344028926 Training loss: 9.325875282287598
2025-12-09 12:18:07.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.009999108774582644 Training loss: 9.525025367736816
2025-12-09 12:18:07.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009999050350624381 Training loss: 9.280986785888672
2025-12-09 12:18:07.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.009998990072175813 Training loss: 9.543394088745117
2025-12-09 12:18:07.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009998927939259302 Training loss: 9.462995529174805
2025-12-09 12:18:07.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009998863951897896 Training loss: 9.592405319213867
2025-12-09 12:18:07.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009998798110115333 Training loss: 9.603764533996582
2025-12-09 12:18:07.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.009998730413936037 Training loss: 9.515212059020996
2025-12-09 12:18:07.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.009998660863385123 Training loss: 8.974093437194824
2025-12-09 12:18:07.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00999858945848839 Training loss: 10.206304550170898
2025-12-09 12:18:07.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009998516199272327 Training loss: 9.46072006225586
2025-12-09 12:18:07.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009998441085764113 Training loss: 9.519230842590332
2025-12-09 12:18:07.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.009998364117991612 Training loss: 9.57584285736084
2025-12-09 12:18:07.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.009998285295983376 Training loss: 9.491803169250488
2025-12-09 12:18:08.043 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009998204619768645 Training loss: 9.62713623046875
2025-12-09 12:18:08.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.00999812208937735 Training loss: 9.698055267333984
2025-12-09 12:18:08.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.009998037704840102 Training loss: 9.6583890914917
2025-12-09 12:18:08.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00999795146618821 Training loss: 9.699783325195312
2025-12-09 12:18:08.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009997863373453663 Training loss: 9.563682556152344
2025-12-09 12:18:08.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.00999777342666914 Training loss: 9.655677795410156
2025-12-09 12:18:08.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009997681625868013 Training loss: 9.394180297851562
2025-12-09 12:18:08.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009997587971084335 Training loss: 9.585360527038574
2025-12-09 12:18:08.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009997492462352845 Training loss: 9.609359741210938
2025-12-09 12:18:08.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009997395099708982 Training loss: 9.643036842346191
2025-12-09 12:18:08.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.009997295883188855 Training loss: 9.433152198791504
2025-12-09 12:18:08.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009997194812829277 Training loss: 9.476155281066895
2025-12-09 12:18:08.959 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009997091888667738 Training loss: 9.558395385742188
2025-12-09 12:18:09.035 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009996987110742421 Training loss: 9.607769012451172
2025-12-09 12:18:09.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009996880479092198 Training loss: 9.730914115905762
2025-12-09 12:18:09.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.009996771993756622 Training loss: 9.704875946044922
2025-12-09 12:18:09.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.009996661654775939 Training loss: 9.51901912689209
2025-12-09 12:18:09.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.009996549462191081 Training loss: 9.876799583435059
2025-12-09 12:18:09.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00999643541604367 Training loss: 9.375198364257812
2025-12-09 12:18:09.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.00999631951637601 Training loss: 9.409224510192871
2025-12-09 12:18:09.572 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.009996201763231098 Training loss: 9.461658477783203
2025-12-09 12:18:09.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.009996082156652618 Training loss: 9.514028549194336
2025-12-09 12:18:09.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.009995960696684939 Training loss: 9.197660446166992
2025-12-09 12:18:09.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.009995837383373118 Training loss: 9.412994384765625
2025-12-09 12:18:09.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.009995712216762901 Training loss: 9.524065017700195
2025-12-09 12:18:09.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.009995585196900723 Training loss: 9.533980369567871
2025-12-09 12:18:10.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.009995456323833701 Training loss: 9.06484317779541
2025-12-09 12:18:10.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.009995325597609645 Training loss: 9.58047866821289
2025-12-09 12:18:10.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00999519301827705 Training loss: 9.31237506866455
2025-12-09 12:18:10.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.009995058585885095 Training loss: 9.603304862976074
2025-12-09 12:18:10.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.009994922300483657 Training loss: 9.630472183227539
2025-12-09 12:18:10.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.00999478416212329 Training loss: 9.3885498046875
2025-12-09 12:18:10.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.009994644170855237 Training loss: 9.297779083251953
2025-12-09 12:18:10.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.009994502326731434 Training loss: 9.268507957458496
2025-12-09 12:18:10.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.009994358629804499 Training loss: 9.48758602142334
2025-12-09 12:18:10.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.009994213080127738 Training loss: 9.5798978805542
2025-12-09 12:18:10.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.009994065677755147 Training loss: 9.598852157592773
2025-12-09 12:18:10.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00999391642274141 Training loss: 9.582460403442383
2025-12-09 12:18:10.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00999376531514189 Training loss: 9.438458442687988
2025-12-09 12:18:11.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.009993612355012647 Training loss: 8.815786361694336
2025-12-09 12:18:11.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.009993457542410423 Training loss: 9.420443534851074
2025-12-09 12:18:11.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00999330087739265 Training loss: 9.286880493164062
2025-12-09 12:18:11.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.009993142360017445 Training loss: 9.601381301879883
2025-12-09 12:18:11.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.009992981990343614 Training loss: 9.284857749938965
2025-12-09 12:18:11.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.009992819768430647 Training loss: 9.62885856628418
2025-12-09 12:18:11.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.009992655694338725 Training loss: 9.304141998291016
2025-12-09 12:18:11.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.009992489768128714 Training loss: 9.406278610229492
2025-12-09 12:18:11.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.009992321989862165 Training loss: 9.45346450805664
2025-12-09 12:18:11.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.009992152359601322 Training loss: 9.298212051391602
2025-12-09 12:18:11.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.00999198087740911 Training loss: 9.412908554077148
2025-12-09 12:18:11.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.009991807543349147 Training loss: 9.379579544067383
2025-12-09 12:18:11.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.009991632357485729 Training loss: 9.539390563964844
2025-12-09 12:18:12.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.009991455319883848 Training loss: 9.252677917480469
2025-12-09 12:18:12.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.00999127643060918 Training loss: 9.448800086975098
2025-12-09 12:18:12.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.009991095689728087 Training loss: 9.381814956665039
2025-12-09 12:18:12.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.009990913097307614 Training loss: 9.205907821655273
2025-12-09 12:18:12.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.009990728653415505 Training loss: 9.052393913269043
2025-12-09 12:18:12.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.009990542358120174 Training loss: 9.475314140319824
2025-12-09 12:18:12.481 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.009990354211490735 Training loss: 8.756414413452148
2025-12-09 12:18:12.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.009990164213596987 Training loss: 9.479975700378418
2025-12-09 12:18:12.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.009989972364509407 Training loss: 9.220748901367188
2025-12-09 12:18:12.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.009989778664299172 Training loss: 9.278955459594727
2025-12-09 12:18:12.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.009989583113038134 Training loss: 9.211577415466309
2025-12-09 12:18:12.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.009989385710798838 Training loss: 9.261090278625488
2025-12-09 12:18:12.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.009989186457654514 Training loss: 8.761716842651367
2025-12-09 12:18:13.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.009988985353679076 Training loss: 9.09543228149414
2025-12-09 12:18:13.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.009988782398947132 Training loss: 9.091553688049316
2025-12-09 12:18:13.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.009988577593533967 Training loss: 9.212494850158691
2025-12-09 12:18:13.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00998837093751556 Training loss: 9.21969985961914
2025-12-09 12:18:13.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.009988162430968575 Training loss: 9.148789405822754
2025-12-09 12:18:13.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.009987952073970359 Training loss: 9.091429710388184
2025-12-09 12:18:13.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00998773986659895 Training loss: 9.044578552246094
2025-12-09 12:18:13.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.009987525808933069 Training loss: 9.170472145080566
2025-12-09 12:18:13.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.009987309901052122 Training loss: 9.719107627868652
2025-12-09 12:18:13.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.009987092143036209 Training loss: 9.28540325164795
2025-12-09 12:18:13.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.009986872534966109 Training loss: 9.151240348815918
2025-12-09 12:18:13.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.009986651076923288 Training loss: 9.032743453979492
2025-12-09 12:18:13.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.009986427768989904 Training loss: 9.188170433044434
2025-12-09 12:18:14.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.009986202611248794 Training loss: 9.265732765197754
2025-12-09 12:18:14.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.009985975603783484 Training loss: 8.897225379943848
2025-12-09 12:18:14.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.00998574674667819 Training loss: 8.90546703338623
2025-12-09 12:18:14.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.009985516040017807 Training loss: 8.918780326843262
2025-12-09 12:18:14.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.009985283483887922 Training loss: 8.863312721252441
2025-12-09 12:18:14.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.009985049078374806 Training loss: 9.171808242797852
2025-12-09 12:18:14.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.009984812823565416 Training loss: 8.994400024414062
2025-12-09 12:18:14.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.009984574719547395 Training loss: 9.079230308532715
2025-12-09 12:18:14.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.00998433476640907 Training loss: 8.941970825195312
2025-12-09 12:18:14.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00998409296423946 Training loss: 8.829413414001465
2025-12-09 12:18:14.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.009983849313128264 Training loss: 8.903266906738281
2025-12-09 12:18:14.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.009983603813165869 Training loss: 8.827102661132812
2025-12-09 12:18:14.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.009983356464443347 Training loss: 9.022817611694336
2025-12-09 12:18:15.010 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.009983107267052456 Training loss: 8.90223217010498
2025-12-09 12:18:15.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.009982856221085643 Training loss: 8.967856407165527
2025-12-09 12:18:15.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.009982603326636037 Training loss: 8.95923900604248
2025-12-09 12:18:15.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.009982348583797453 Training loss: 8.808396339416504
2025-12-09 12:18:15.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.009982091992664392 Training loss: 9.500321388244629
2025-12-09 12:18:15.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.009981833553332044 Training loss: 9.103540420532227
2025-12-09 12:18:15.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.009981573265896281 Training loss: 8.889738082885742
2025-12-09 12:18:15.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00998131113045366 Training loss: 8.94625186920166
2025-12-09 12:18:15.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.009981047147101425 Training loss: 9.007918357849121
2025-12-09 12:18:15.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.009980781315937506 Training loss: 8.29340934753418
2025-12-09 12:18:15.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.00998051363706052 Training loss: 9.084076881408691
2025-12-09 12:18:15.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.009980244110569764 Training loss: 8.738554000854492
2025-12-09 12:18:15.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.009979972736565226 Training loss: 8.828662872314453
2025-12-09 12:18:16.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.009979699515147577 Training loss: 8.616186141967773
2025-12-09 12:18:16.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.009979424446418172 Training loss: 8.071342468261719
2025-12-09 12:18:16.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.009979147530479055 Training loss: 8.821106910705566
2025-12-09 12:18:16.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.009978868767432954 Training loss: 9.061890602111816
2025-12-09 12:18:16.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.009978588157383277 Training loss: 8.885103225708008
2025-12-09 12:18:16.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.009978305700434125 Training loss: 9.286272048950195
2025-12-09 12:18:16.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00997802139669028 Training loss: 8.763372421264648
2025-12-09 12:18:16.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.009977735246257209 Training loss: 8.7783784866333
2025-12-09 12:18:16.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.009977447249241066 Training loss: 8.682066917419434
2025-12-09 12:18:16.699 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.009977157405748687 Training loss: 8.709463119506836
2025-12-09 12:18:16.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.009976865715887595 Training loss: 9.243444442749023
2025-12-09 12:18:16.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.009976572179765998 Training loss: 8.724929809570312
2025-12-09 12:18:16.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.009976276797492793 Training loss: 8.790757179260254
2025-12-09 12:18:17.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.009975979569177552 Training loss: 8.698400497436523
2025-12-09 12:18:17.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.009975680494930538 Training loss: 8.815591812133789
2025-12-09 12:18:17.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.0099753795748627 Training loss: 9.010306358337402
2025-12-09 12:18:17.238 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.00997507680908567 Training loss: 8.408255577087402
2025-12-09 12:18:17.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.009974772197711762 Training loss: 8.8868989944458
2025-12-09 12:18:17.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.009974465740853979 Training loss: 8.90587329864502
2025-12-09 12:18:17.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.009974157438626008 Training loss: 7.948544502258301
2025-12-09 12:18:17.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.009973847291142217 Training loss: 8.601494789123535
2025-12-09 12:18:17.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.009973535298517662 Training loss: 8.519122123718262
2025-12-09 12:18:17.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.009973221460868084 Training loss: 8.607878684997559
2025-12-09 12:18:17.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.009972905778309905 Training loss: 8.693693161010742
2025-12-09 12:18:17.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.009972588250960234 Training loss: 9.282678604125977
2025-12-09 12:18:17.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.009972268878936864 Training loss: 8.81921672821045
2025-12-09 12:18:18.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.009971947662358269 Training loss: 7.706153869628906
2025-12-09 12:18:18.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.009971624601343614 Training loss: 8.58562183380127
2025-12-09 12:18:18.160 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.009971299696012743 Training loss: 8.776609420776367
2025-12-09 12:18:18.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.009970972946486186 Training loss: 8.954887390136719
2025-12-09 12:18:18.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.009970644352885156 Training loss: 8.480779647827148
2025-12-09 12:18:18.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.009970313915331553 Training loss: 8.75213623046875
2025-12-09 12:18:18.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.009969981633947956 Training loss: 8.575552940368652
2025-12-09 12:18:18.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.009969647508857631 Training loss: 8.483006477355957
2025-12-09 12:18:18.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.00996931154018453 Training loss: 8.663230895996094
2025-12-09 12:18:18.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.009968973728053289 Training loss: 8.482128143310547
2025-12-09 12:18:18.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.009968634072589218 Training loss: 9.361907958984375
2025-12-09 12:18:18.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.009968292573918324 Training loss: 8.449963569641113
2025-12-09 12:18:18.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.009967949232167294 Training loss: 8.602527618408203
2025-12-09 12:18:19.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.009967604047463493 Training loss: 8.340642929077148
2025-12-09 12:18:19.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.009967257019934974 Training loss: 8.50272274017334
2025-12-09 12:18:19.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.009966908149710475 Training loss: 8.308753967285156
2025-12-09 12:18:19.235 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.009966557436919416 Training loss: 8.779483795166016
2025-12-09 12:18:19.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.009966204881691898 Training loss: 8.706218719482422
2025-12-09 12:18:19.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.00996585048415871 Training loss: 8.469399452209473
2025-12-09 12:18:19.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.009965494244451324 Training loss: 8.846869468688965
2025-12-09 12:18:19.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00996513616270189 Training loss: 8.558686256408691
2025-12-09 12:18:19.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.009964776239043245 Training loss: 8.433727264404297
2025-12-09 12:18:19.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.009964414473608912 Training loss: 8.734590530395508
2025-12-09 12:18:19.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.009964050866533094 Training loss: 8.475847244262695
2025-12-09 12:18:19.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.009963685417950676 Training loss: 8.281517028808594
2025-12-09 12:18:19.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00996331812799723 Training loss: 8.486810684204102
2025-12-09 12:18:20.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.009962948996809008 Training loss: 8.55611801147461
2025-12-09 12:18:20.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.009962578024522948 Training loss: 8.469574928283691
2025-12-09 12:18:20.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.009962205211276665 Training loss: 8.27117919921875
2025-12-09 12:18:20.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.009961830557208463 Training loss: 8.802705764770508
2025-12-09 12:18:20.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.009961454062457329 Training loss: 8.646233558654785
2025-12-09 12:18:20.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.009961075727162927 Training loss: 8.45084285736084
2025-12-09 12:18:20.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.009960695551465611 Training loss: 8.454205513000488
2025-12-09 12:18:20.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.009960313535506412 Training loss: 8.150959014892578
2025-12-09 12:18:20.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.009959929679427047 Training loss: 8.60751724243164
2025-12-09 12:18:20.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.009959543983369913 Training loss: 7.8789448738098145
2025-12-09 12:18:20.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.009959156447478091 Training loss: 8.622891426086426
2025-12-09 12:18:20.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.009958767071895348 Training loss: 8.386128425598145
2025-12-09 12:18:20.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.009958375856766127 Training loss: 8.702574729919434
2025-12-09 12:18:21.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.009957982802235556 Training loss: 8.679229736328125
2025-12-09 12:18:21.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.009957587908449448 Training loss: 8.334508895874023
2025-12-09 12:18:21.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.009957191175554294 Training loss: 8.664057731628418
2025-12-09 12:18:21.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.009956792603697273 Training loss: 8.465394973754883
2025-12-09 12:18:21.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.009956392193026239 Training loss: 8.122749328613281
2025-12-09 12:18:21.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.009955989943689734 Training loss: 8.173124313354492
2025-12-09 12:18:21.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.009955585855836977 Training loss: 8.552000999450684
2025-12-09 12:18:21.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.009955179929617875 Training loss: 8.415356636047363
2025-12-09 12:18:21.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.009954772165183012 Training loss: 8.587668418884277
2025-12-09 12:18:21.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.009954362562683658 Training loss: 8.464500427246094
2025-12-09 12:18:21.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00995395112227176 Training loss: 8.54471492767334
2025-12-09 12:18:21.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00995353784409995 Training loss: 8.496785163879395
2025-12-09 12:18:21.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.009953122728321542 Training loss: 8.082563400268555
2025-12-09 12:18:22.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00995270577509053 Training loss: 8.899504661560059
2025-12-09 12:18:22.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.009952286984561591 Training loss: 8.346611022949219
2025-12-09 12:18:22.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.009951866356890084 Training loss: 8.246329307556152
2025-12-09 12:18:22.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.009951443892232048 Training loss: 8.477096557617188
2025-12-09 12:18:22.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.009951019590744202 Training loss: 8.279233932495117
2025-12-09 12:18:22.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.009950593452583952 Training loss: 7.94000244140625
2025-12-09 12:18:22.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00995016547790938 Training loss: 8.430434226989746
2025-12-09 12:18:22.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.009949735666879251 Training loss: 8.514267921447754
2025-12-09 12:18:22.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.009949304019653011 Training loss: 8.556478500366211
2025-12-09 12:18:22.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00994887053639079 Training loss: 9.188305854797363
2025-12-09 12:18:22.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.009948435217253393 Training loss: 8.184436798095703
2025-12-09 12:18:22.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.009947998062402312 Training loss: 8.475860595703125
2025-12-09 12:18:22.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.009947559071999719 Training loss: 8.381136894226074
2025-12-09 12:18:23.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.009947118246208461 Training loss: 8.406375885009766
2025-12-09 12:18:23.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.009946675585192076 Training loss: 8.64594841003418
2025-12-09 12:18:23.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.009946231089114773 Training loss: 7.444934844970703
2025-12-09 12:18:23.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.009945784758141448 Training loss: 8.23687744140625
2025-12-09 12:18:23.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.009945336592437678 Training loss: 8.182853698730469
2025-12-09 12:18:23.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.009944886592169713 Training loss: 8.343463897705078
2025-12-09 12:18:23.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.009944434757504492 Training loss: 8.196660041809082
2025-12-09 12:18:23.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.009943981088609631 Training loss: 8.375994682312012
2025-12-09 12:18:23.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.009943525585653428 Training loss: 8.416716575622559
2025-12-09 12:18:23.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.009943068248804858 Training loss: 8.404899597167969
2025-12-09 12:18:23.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.009942609078233581 Training loss: 8.387758255004883
2025-12-09 12:18:23.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.009942148074109933 Training loss: 8.376049041748047
2025-12-09 12:18:23.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.009941685236604934 Training loss: 8.183777809143066
2025-12-09 12:18:23.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.009941220565890278 Training loss: 8.490521430969238
2025-12-09 12:18:24.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00994075406213835 Training loss: 8.520001411437988
2025-12-09 12:18:24.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.009940285725522203 Training loss: 8.223978042602539
2025-12-09 12:18:24.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.009939815556215575 Training loss: 8.492168426513672
2025-12-09 12:18:24.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.009939343554392886 Training loss: 8.536721229553223
2025-12-09 12:18:24.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.009938869720229233 Training loss: 8.26021957397461
2025-12-09 12:18:24.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.009938394053900394 Training loss: 8.28504467010498
2025-12-09 12:18:24.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.009937916555582828 Training loss: 8.187788963317871
2025-12-09 12:18:24.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.009937437225453668 Training loss: 8.308798789978027
2025-12-09 12:18:24.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.009936956063690734 Training loss: 8.54697036743164
2025-12-09 12:18:24.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.009936473070472518 Training loss: 9.167689323425293
2025-12-09 12:18:24.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.009935988245978198 Training loss: 8.10490894317627
2025-12-09 12:18:24.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.009935501590387627 Training loss: 8.665728569030762
2025-12-09 12:18:24.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.009935013103881342 Training loss: 8.582534790039062
2025-12-09 12:18:25.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.009934522786640554 Training loss: 8.876405715942383
2025-12-09 12:18:25.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.009934030638847154 Training loss: 7.977227687835693
2025-12-09 12:18:25.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.009933536660683716 Training loss: 8.332927703857422
2025-12-09 12:18:25.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.009933040852333487 Training loss: 8.06020450592041
2025-12-09 12:18:25.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.009932543213980401 Training loss: 8.319673538208008
2025-12-09 12:18:25.461 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.009932043745809064 Training loss: 8.326967239379883
2025-12-09 12:18:25.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.009931542448004758 Training loss: 8.27148151397705
2025-12-09 12:18:25.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.009931039320753456 Training loss: 8.124750137329102
2025-12-09 12:18:25.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.0099305343642418 Training loss: 8.465529441833496
2025-12-09 12:18:25.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.009930027578657113 Training loss: 8.390970230102539
2025-12-09 12:18:25.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.009929518964187393 Training loss: 8.47886848449707
2025-12-09 12:18:25.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.009929008521021325 Training loss: 8.358597755432129
2025-12-09 12:18:25.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.009928496249348266 Training loss: 8.365065574645996
2025-12-09 12:18:26.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.00992798214935825 Training loss: 8.296602249145508
2025-12-09 12:18:26.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.009927466221241995 Training loss: 8.50226879119873
2025-12-09 12:18:26.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.009926948465190892 Training loss: 8.468674659729004
2025-12-09 12:18:26.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.009926428881397015 Training loss: 8.260053634643555
2025-12-09 12:18:26.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.00992590747005311 Training loss: 7.777721881866455
2025-12-09 12:18:26.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.009925384231352606 Training loss: 8.22221851348877
2025-12-09 12:18:26.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.009924859165489608 Training loss: 8.180424690246582
2025-12-09 12:18:26.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.009924332272658898 Training loss: 8.338675498962402
2025-12-09 12:18:26.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.009923803553055936 Training loss: 9.058414459228516
2025-12-09 12:18:26.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.009923273006876865 Training loss: 8.304154396057129
2025-12-09 12:18:26.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.009922740634318495 Training loss: 8.008533477783203
2025-12-09 12:18:26.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.009922206435578323 Training loss: 8.54913330078125
2025-12-09 12:18:26.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.009921670410854518 Training loss: 8.196712493896484
2025-12-09 12:18:27.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.009921132560345929 Training loss: 8.522690773010254
2025-12-09 12:18:27.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.009920592884252082 Training loss: 8.549213409423828
2025-12-09 12:18:27.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.009920051382773179 Training loss: 8.403721809387207
2025-12-09 12:18:27.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.009919508056110101 Training loss: 8.438544273376465
2025-12-09 12:18:27.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.009918962904464406 Training loss: 8.348958015441895
2025-12-09 12:18:27.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.009918415928038325 Training loss: 8.829483032226562
2025-12-09 12:18:27.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.009917867127034772 Training loss: 8.270638465881348
2025-12-09 12:18:27.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.009917316501657334 Training loss: 8.211703300476074
2025-12-09 12:18:27.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.009916764052110274 Training loss: 8.495074272155762
2025-12-09 12:18:27.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.009916209778598535 Training loss: 7.495213031768799
2025-12-09 12:18:27.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.009915653681327737 Training loss: 8.068768501281738
2025-12-09 12:18:27.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.009915095760504169 Training loss: 8.43764877319336
2025-12-09 12:18:27.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.009914536016334808 Training loss: 8.242621421813965
2025-12-09 12:18:28.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.009913974449027297 Training loss: 8.316285133361816
2025-12-09 12:18:28.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.009913411058789963 Training loss: 8.387666702270508
2025-12-09 12:18:28.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.009912845845831806 Training loss: 8.475130081176758
2025-12-09 12:18:28.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.009912278810362498 Training loss: 8.354496955871582
2025-12-09 12:18:28.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.009911709952592397 Training loss: 8.211387634277344
2025-12-09 12:18:28.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.009911139272732527 Training loss: 7.9806952476501465
2025-12-09 12:18:28.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.009910566770994594 Training loss: 8.275483131408691
2025-12-09 12:18:28.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.00990999244759098 Training loss: 8.252955436706543
2025-12-09 12:18:28.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.009909416302734736 Training loss: 7.676573276519775
2025-12-09 12:18:28.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.009908838336639598 Training loss: 8.593535423278809
2025-12-09 12:18:28.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.00990825854951997 Training loss: 8.350264549255371
2025-12-09 12:18:28.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.009907676941590938 Training loss: 8.427680969238281
2025-12-09 12:18:28.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.009907093513068259 Training loss: 8.227387428283691
2025-12-09 12:18:29.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.009906508264168366 Training loss: 8.333324432373047
2025-12-09 12:18:29.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.009905921195108367 Training loss: 8.487390518188477
2025-12-09 12:18:29.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.00990533230610605 Training loss: 8.172642707824707
2025-12-09 12:18:29.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00990474159737987 Training loss: 7.924649715423584
2025-12-09 12:18:29.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.009904149069148962 Training loss: 8.528740882873535
2025-12-09 12:18:29.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.009903554721633139 Training loss: 8.079413414001465
2025-12-09 12:18:29.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.00990295855505288 Training loss: 8.128307342529297
2025-12-09 12:18:29.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.009902360569629348 Training loss: 8.631054878234863
2025-12-09 12:18:29.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.009901760765584376 Training loss: 8.278841018676758
2025-12-09 12:18:29.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.00990115914314047 Training loss: 7.998721599578857
2025-12-09 12:18:29.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.009900555702520816 Training loss: 8.519329071044922
2025-12-09 12:18:29.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.00989995044394927 Training loss: 8.33905029296875
2025-12-09 12:18:29.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.009899343367650365 Training loss: 8.118453025817871
2025-12-09 12:18:30.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.009898734473849305 Training loss: 8.552522659301758
2025-12-09 12:18:30.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.00989812376277197 Training loss: 8.391572952270508
2025-12-09 12:18:30.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00989751123464492 Training loss: 8.6102876663208
2025-12-09 12:18:30.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.009896896889695377 Training loss: 8.121796607971191
2025-12-09 12:18:30.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.009896280728151248 Training loss: 8.363822937011719
2025-12-09 12:18:30.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.009895662750241108 Training loss: 8.689094543457031
2025-12-09 12:18:30.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.009895042956194209 Training loss: 8.274195671081543
2025-12-09 12:18:30.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.009894421346240473 Training loss: 8.098653793334961
2025-12-09 12:18:30.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.009893797920610495 Training loss: 8.437159538269043
2025-12-09 12:18:30.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.009893172679535553 Training loss: 8.453669548034668
2025-12-09 12:18:30.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.009892545623247586 Training loss: 8.4019775390625
2025-12-09 12:18:30.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.009891916751979217 Training loss: 8.01339054107666
2025-12-09 12:18:30.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.009891286065963734 Training loss: 8.118042945861816
2025-12-09 12:18:31.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.0098906535654351 Training loss: 8.234270095825195
2025-12-09 12:18:31.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.00989001925062796 Training loss: 7.952238082885742
2025-12-09 12:18:31.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.009889383121777617 Training loss: 8.90357780456543
2025-12-09 12:18:31.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.00988874517912006 Training loss: 8.444622039794922
2025-12-09 12:18:31.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.009888105422891941 Training loss: 8.285539627075195
2025-12-09 12:18:31.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.009887463853330595 Training loss: 8.492364883422852
2025-12-09 12:18:31.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.009886820470674018 Training loss: 8.142444610595703
2025-12-09 12:18:31.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.00988617527516089 Training loss: 8.064000129699707
2025-12-09 12:18:31.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.009885528267030555 Training loss: 8.080425262451172
2025-12-09 12:18:31.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.009884879446523035 Training loss: 8.407453536987305
2025-12-09 12:18:31.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00988422881387902 Training loss: 8.26144790649414
2025-12-09 12:18:31.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.009883576369339874 Training loss: 8.458321571350098
2025-12-09 12:18:31.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.009882922113147636 Training loss: 8.281438827514648
2025-12-09 12:18:32.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.009882266045545011 Training loss: 8.429040908813477
2025-12-09 12:18:32.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.009881608166775383 Training loss: 8.471421241760254
2025-12-09 12:18:32.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.009880948477082803 Training loss: 8.269275665283203
2025-12-09 12:18:32.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.009880286976711991 Training loss: 8.38789176940918
2025-12-09 12:18:32.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.00987962366590835 Training loss: 8.481114387512207
2025-12-09 12:18:32.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.009878958544917943 Training loss: 8.175619125366211
2025-12-09 12:18:32.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00987829161398751 Training loss: 8.137457847595215
2025-12-09 12:18:32.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00987762287336446 Training loss: 8.220794677734375
2025-12-09 12:18:32.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.009876952323296877 Training loss: 8.241880416870117
2025-12-09 12:18:32.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.009876279964033513 Training loss: 8.240532875061035
2025-12-09 12:18:32.844 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00987560579582379 Training loss: 8.50969123840332
2025-12-09 12:18:32.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.009874929818917806 Training loss: 7.725229740142822
2025-12-09 12:18:32.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.009874252033566327 Training loss: 8.294733047485352
2025-12-09 12:18:33.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.009873572440020791 Training loss: 8.345890998840332
2025-12-09 12:18:33.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.0098728910385333 Training loss: 8.163468360900879
2025-12-09 12:18:33.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.009872207829356642 Training loss: 8.22435188293457
2025-12-09 12:18:33.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.009871522812744256 Training loss: 8.06135368347168
2025-12-09 12:18:33.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.009870835988950269 Training loss: 8.160719871520996
2025-12-09 12:18:33.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.009870147358229466 Training loss: 8.309088706970215
2025-12-09 12:18:33.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.009869456920837311 Training loss: 7.8228864669799805
2025-12-09 12:18:33.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.009868764677029934 Training loss: 8.837689399719238
2025-12-09 12:18:33.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.009868070627064135 Training loss: 8.703965187072754
2025-12-09 12:18:33.768 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.009867374771197384 Training loss: 8.300915718078613
2025-12-09 12:18:33.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.009866677109687822 Training loss: 8.5315580368042
2025-12-09 12:18:33.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.009865977642794259 Training loss: 8.26186752319336
2025-12-09 12:18:33.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.009865276370776178 Training loss: 8.432605743408203
2025-12-09 12:18:34.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.009864573293893723 Training loss: 9.294005393981934
2025-12-09 12:18:34.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00986386841240772 Training loss: 8.221858978271484
2025-12-09 12:18:34.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.009863161726579655 Training loss: 8.237571716308594
2025-12-09 12:18:34.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.009862453236671684 Training loss: 8.344240188598633
2025-12-09 12:18:34.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.009861742942946639 Training loss: 8.179702758789062
2025-12-09 12:18:34.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.009861030845668013 Training loss: 8.470541954040527
2025-12-09 12:18:34.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.009860316945099973 Training loss: 8.08188533782959
2025-12-09 12:18:34.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.009859601241507353 Training loss: 8.068307876586914
2025-12-09 12:18:34.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.009858883735155657 Training loss: 8.418405532836914
2025-12-09 12:18:34.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.009858164426311058 Training loss: 8.679558753967285
2025-12-09 12:18:34.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.009857443315240397 Training loss: 8.042106628417969
2025-12-09 12:18:34.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.00985672040221118 Training loss: 8.220394134521484
2025-12-09 12:18:34.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.00985599568749159 Training loss: 8.186993598937988
2025-12-09 12:18:35.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00985526917135047 Training loss: 8.285058975219727
2025-12-09 12:18:35.151 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.009854540854057337 Training loss: 8.394316673278809
2025-12-09 12:18:35.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00985381073588237 Training loss: 8.070834159851074
2025-12-09 12:18:35.304 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.009853078817096423 Training loss: 8.288176536560059
2025-12-09 12:18:35.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.009852345097971017 Training loss: 8.037498474121094
2025-12-09 12:18:35.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.009851609578778332 Training loss: 7.906260967254639
2025-12-09 12:18:35.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.009850872259791229 Training loss: 8.394837379455566
2025-12-09 12:18:35.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.009850133141283225 Training loss: 8.337364196777344
2025-12-09 12:18:35.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.009849392223528514 Training loss: 8.164421081542969
2025-12-09 12:18:35.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00984864950680195 Training loss: 8.266753196716309
2025-12-09 12:18:35.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00984790499137906 Training loss: 8.257272720336914
2025-12-09 12:18:35.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.009847158677536034 Training loss: 8.518094062805176
2025-12-09 12:18:36.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00984641056554973 Training loss: 8.352766036987305
2025-12-09 12:18:36.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.009845660655697678 Training loss: 8.401651382446289
2025-12-09 12:18:36.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.009844908948258067 Training loss: 8.003069877624512
2025-12-09 12:18:36.229 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.009844155443509759 Training loss: 8.965609550476074
2025-12-09 12:18:36.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.009843400141732279 Training loss: 8.33400821685791
2025-12-09 12:18:36.388 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.009842643043205822 Training loss: 8.254582405090332
2025-12-09 12:18:36.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.009841884148211246 Training loss: 8.23324203491211
2025-12-09 12:18:36.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.009841123457030079 Training loss: 8.217268943786621
2025-12-09 12:18:36.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.009840360969944511 Training loss: 8.14108943939209
2025-12-09 12:18:36.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.009839596687237401 Training loss: 8.557023048400879
2025-12-09 12:18:36.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.009838830609192278 Training loss: 8.386680603027344
2025-12-09 12:18:36.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.009838062736093327 Training loss: 8.238659858703613
2025-12-09 12:18:36.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.009837293068225408 Training loss: 8.368819236755371
2025-12-09 12:18:37.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.009836521605874044 Training loss: 8.326744079589844
2025-12-09 12:18:37.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.009835748349325421 Training loss: 8.3407564163208
2025-12-09 12:18:37.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.009834973298866394 Training loss: 8.457497596740723
2025-12-09 12:18:37.236 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.009834196454784484 Training loss: 8.394754409790039
2025-12-09 12:18:37.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.009833417817367874 Training loss: 7.566822052001953
2025-12-09 12:18:37.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.009832637386905412 Training loss: 8.433248519897461
2025-12-09 12:18:37.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.009831855163686617 Training loss: 7.15172815322876
2025-12-09 12:18:37.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.009831071148001667 Training loss: 8.376151084899902
2025-12-09 12:18:37.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.009830285340141407 Training loss: 8.566800117492676
2025-12-09 12:18:37.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.009829497740397349 Training loss: 8.301520347595215
2025-12-09 12:18:37.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.009828708349061663 Training loss: 8.444025039672852
2025-12-09 12:18:37.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.009827917166427195 Training loss: 8.027871131896973
2025-12-09 12:18:37.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.009827124192787444 Training loss: 8.352889060974121
2025-12-09 12:18:38.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.00982632942843658 Training loss: 8.227787017822266
2025-12-09 12:18:38.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.009825532873669433 Training loss: 8.274871826171875
2025-12-09 12:18:38.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.009824734528781505 Training loss: 8.248312950134277
2025-12-09 12:18:38.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.009823934394068952 Training loss: 8.823235511779785
2025-12-09 12:18:38.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.009823132469828601 Training loss: 8.03432846069336
2025-12-09 12:18:38.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.009822328756357942 Training loss: 8.334630966186523
2025-12-09 12:18:38.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.009821523253955123 Training loss: 8.304035186767578
2025-12-09 12:18:38.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.009820715962918964 Training loss: 7.948334693908691
2025-12-09 12:18:38.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.009819906883548942 Training loss: 8.19129753112793
2025-12-09 12:18:38.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.009819096016145203 Training loss: 8.107949256896973
2025-12-09 12:18:38.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.00981828336100855 Training loss: 8.386455535888672
2025-12-09 12:18:38.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.009817468918440455 Training loss: 8.668288230895996
2025-12-09 12:18:38.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.009816652688743049 Training loss: 8.295780181884766
2025-12-09 12:18:39.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.009815834672219127 Training loss: 8.389416694641113
2025-12-09 12:18:39.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00981501486917215 Training loss: 8.17410945892334
2025-12-09 12:18:39.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.009814193279906236 Training loss: 8.177461624145508
2025-12-09 12:18:39.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.00981336990472617 Training loss: 7.991596221923828
2025-12-09 12:18:39.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.0098125447439374 Training loss: 7.9798760414123535
2025-12-09 12:18:39.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.009811717797846033 Training loss: 8.438453674316406
2025-12-09 12:18:39.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.00981088906675884 Training loss: 8.146933555603027
2025-12-09 12:18:39.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.009810058550983254 Training loss: 8.285935401916504
2025-12-09 12:18:39.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.009809226250827372 Training loss: 8.653616905212402
2025-12-09 12:18:39.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.009808392166599948 Training loss: 8.297067642211914
2025-12-09 12:18:39.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.009807556298610402 Training loss: 8.10252571105957
2025-12-09 12:18:39.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.009806718647168818 Training loss: 8.229031562805176
2025-12-09 12:18:39.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.009805879212585933 Training loss: 8.133898735046387
2025-12-09 12:18:40.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.009805037995173155 Training loss: 7.964211463928223
2025-12-09 12:18:40.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.009804194995242549 Training loss: 7.959569454193115
2025-12-09 12:18:40.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.009803350213106837 Training loss: 8.328849792480469
2025-12-09 12:18:40.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.00980250364907941 Training loss: 8.247663497924805
2025-12-09 12:18:40.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.009801655303474318 Training loss: 8.0667724609375
2025-12-09 12:18:40.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.009800805176606269 Training loss: 7.9983015060424805
2025-12-09 12:18:40.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.009799953268790632 Training loss: 8.010025978088379
2025-12-09 12:18:40.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.00979909958034344 Training loss: 7.636908531188965
2025-12-09 12:18:40.625 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.009798244111581382 Training loss: 8.5270414352417
2025-12-09 12:18:40.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.009797386862821814 Training loss: 8.442537307739258
2025-12-09 12:18:40.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.009796527834382745 Training loss: 8.368358612060547
2025-12-09 12:18:40.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.009795667026582846 Training loss: 8.38937759399414
2025-12-09 12:18:40.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.009794804439741454 Training loss: 8.393265724182129
2025-12-09 12:18:41.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.00979394007417856 Training loss: 7.858076095581055
2025-12-09 12:18:41.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.009793073930214816 Training loss: 8.201826095581055
2025-12-09 12:18:41.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.009792206008171534 Training loss: 8.147510528564453
2025-12-09 12:18:41.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.009791336308370686 Training loss: 7.9454779624938965
2025-12-09 12:18:41.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.009790464831134903 Training loss: 8.425211906433105
2025-12-09 12:18:41.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.009789591576787476 Training loss: 8.233338356018066
2025-12-09 12:18:41.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.009788716545652353 Training loss: 8.286417007446289
2025-12-09 12:18:41.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.009787839738054147 Training loss: 8.092540740966797
2025-12-09 12:18:41.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.009786961154318121 Training loss: 8.184700012207031
2025-12-09 12:18:41.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.009786080794770207 Training loss: 8.5410737991333
2025-12-09 12:18:41.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.009785198659736987 Training loss: 8.232721328735352
2025-12-09 12:18:41.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.009784314749545706 Training loss: 8.426377296447754
2025-12-09 12:18:41.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00978342906452427 Training loss: 8.05370044708252
2025-12-09 12:18:42.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.009782541605001235 Training loss: 8.13687515258789
2025-12-09 12:18:42.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.009781652371305825 Training loss: 8.409302711486816
2025-12-09 12:18:42.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.009780761363767914 Training loss: 8.336414337158203
2025-12-09 12:18:42.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.009779868582718041 Training loss: 8.823573112487793
2025-12-09 12:18:42.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.009778974028487398 Training loss: 8.10010814666748
2025-12-09 12:18:42.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.009778077701407838 Training loss: 7.97252082824707
2025-12-09 12:18:42.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.009777179601811866 Training loss: 8.191556930541992
2025-12-09 12:18:42.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.009776279730032653 Training loss: 8.32783031463623
2025-12-09 12:18:42.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.009775378086404022 Training loss: 8.12126636505127
2025-12-09 12:18:42.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.009774474671260455 Training loss: 8.241543769836426
2025-12-09 12:18:42.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.00977356948493709 Training loss: 8.663741111755371
2025-12-09 12:18:42.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.00977266252776972 Training loss: 7.8577799797058105
2025-12-09 12:18:42.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.009771753800094802 Training loss: 8.127300262451172
2025-12-09 12:18:43.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.009770843302249442 Training loss: 8.292799949645996
2025-12-09 12:18:43.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.009769931034571407 Training loss: 8.254129409790039
2025-12-09 12:18:43.169 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.00976901699739912 Training loss: 8.11943531036377
2025-12-09 12:18:43.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.009768101191071661 Training loss: 8.13982105255127
2025-12-09 12:18:43.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.009767183615928763 Training loss: 8.541923522949219
2025-12-09 12:18:43.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.009766264272310822 Training loss: 8.211973190307617
2025-12-09 12:18:43.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.009765343160558878 Training loss: 8.9341459274292
2025-12-09 12:18:43.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.009764420281014641 Training loss: 8.356892585754395
2025-12-09 12:18:43.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.009763495634020467 Training loss: 8.051895141601562
2025-12-09 12:18:43.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.009762569219919371 Training loss: 7.963217258453369
2025-12-09 12:18:43.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.009761641039055026 Training loss: 8.315582275390625
2025-12-09 12:18:43.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.009760711091771755 Training loss: 8.260262489318848
2025-12-09 12:18:43.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.009759779378414542 Training loss: 7.881075382232666
2025-12-09 12:18:44.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.009758845899329021 Training loss: 7.928182125091553
2025-12-09 12:18:44.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.009757910654861483 Training loss: 8.650500297546387
2025-12-09 12:18:44.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.009756973645358876 Training loss: 8.015713691711426
2025-12-09 12:18:44.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.009756034871168799 Training loss: 8.021345138549805
2025-12-09 12:18:44.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.009755094332639512 Training loss: 8.18182373046875
2025-12-09 12:18:44.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00975415203011992 Training loss: 8.252063751220703
2025-12-09 12:18:44.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.00975320796395959 Training loss: 7.924902439117432
2025-12-09 12:18:44.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.009752262134508742 Training loss: 8.464384078979492
2025-12-09 12:18:44.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.009751314542118247 Training loss: 8.433717727661133
2025-12-09 12:18:44.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.009750365187139632 Training loss: 7.898402690887451
2025-12-09 12:18:44.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.009749414069925078 Training loss: 8.23186206817627
2025-12-09 12:18:44.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.00974846119082742 Training loss: 8.041950225830078
2025-12-09 12:18:44.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.009747506550200145 Training loss: 8.33053970336914
2025-12-09 12:18:45.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.009746550148397396 Training loss: 8.051173210144043
2025-12-09 12:18:45.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.00974559198577397 Training loss: 8.465473175048828
2025-12-09 12:18:45.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.009744632062685311 Training loss: 8.160633087158203
2025-12-09 12:18:45.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.009743670379487522 Training loss: 8.17113208770752
2025-12-09 12:18:45.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.009742706936537358 Training loss: 8.015335083007812
2025-12-09 12:18:45.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.009741741734192224 Training loss: 8.054099082946777
2025-12-09 12:18:45.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.009740774772810181 Training loss: 8.415130615234375
2025-12-09 12:18:45.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.009739806052749942 Training loss: 8.412219047546387
2025-12-09 12:18:45.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.009738835574370872 Training loss: 7.976346969604492
2025-12-09 12:18:45.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.009737863338032985 Training loss: 8.53411865234375
2025-12-09 12:18:45.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.009736889344096951 Training loss: 8.103094100952148
2025-12-09 12:18:45.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.009735913592924092 Training loss: 8.103584289550781
2025-12-09 12:18:45.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.009734936084876382 Training loss: 7.957823276519775
2025-12-09 12:18:46.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.009733956820316443 Training loss: 8.545740127563477
2025-12-09 12:18:46.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.009732975799607553 Training loss: 8.213394165039062
2025-12-09 12:18:46.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.00973199302311364 Training loss: 8.341292381286621
2025-12-09 12:18:46.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.009731008491199285 Training loss: 8.064846992492676
2025-12-09 12:18:46.327 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.009730022204229714 Training loss: 8.214176177978516
2025-12-09 12:18:46.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.009729034162570812 Training loss: 8.004847526550293
2025-12-09 12:18:46.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.009728044366589108 Training loss: 8.3090181350708
2025-12-09 12:18:46.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.009727052816651788 Training loss: 8.021251678466797
2025-12-09 12:18:46.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.009726059513126686 Training loss: 7.600644111633301
2025-12-09 12:18:46.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.009725064456382283 Training loss: 8.492830276489258
2025-12-09 12:18:46.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.009724067646787717 Training loss: 7.921415328979492
2025-12-09 12:18:46.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.00972306908471277 Training loss: 8.409300804138184
2025-12-09 12:18:46.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.009722068770527881 Training loss: 8.161665916442871
2025-12-09 12:18:47.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.009721066704604134 Training loss: 8.113433837890625
2025-12-09 12:18:47.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.00972006288731326 Training loss: 8.632691383361816
2025-12-09 12:18:47.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.00971905731902765 Training loss: 8.521771430969238
2025-12-09 12:18:47.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.009718050000120333 Training loss: 8.386129379272461
2025-12-09 12:18:47.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.009717040930964996 Training loss: 8.448204040527344
2025-12-09 12:18:47.412 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.009716030111935968 Training loss: 8.25856876373291
2025-12-09 12:18:47.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.009715017543408233 Training loss: 8.26527214050293
2025-12-09 12:18:47.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.009714003225757424 Training loss: 8.216800689697266
2025-12-09 12:18:47.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.009712987159359818 Training loss: 8.414441108703613
2025-12-09 12:18:47.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.009711969344592347 Training loss: 8.156570434570312
2025-12-09 12:18:47.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.009710949781832585 Training loss: 8.107081413269043
2025-12-09 12:18:47.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.009709928471458759 Training loss: 8.034012794494629
2025-12-09 12:18:47.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.009708905413849743 Training loss: 7.915136337280273
2025-12-09 12:18:48.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.009707880609385058 Training loss: 8.243769645690918
2025-12-09 12:18:48.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.009706854058444877 Training loss: 8.536270141601562
2025-12-09 12:18:48.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.009705825761410014 Training loss: 8.395049095153809
2025-12-09 12:18:48.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.009704795718661938 Training loss: 8.174651145935059
2025-12-09 12:18:48.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.00970376393058276 Training loss: 8.478495597839355
2025-12-09 12:18:48.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.009702730397555245 Training loss: 8.32658863067627
2025-12-09 12:18:48.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.009701695119962798 Training loss: 8.641324043273926
2025-12-09 12:18:48.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.009700658098189475 Training loss: 8.417317390441895
2025-12-09 12:18:48.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.009699619332619978 Training loss: 8.069452285766602
2025-12-09 12:18:48.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.009698578823639658 Training loss: 8.273359298706055
2025-12-09 12:18:48.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.009697536571634508 Training loss: 8.591164588928223
2025-12-09 12:18:48.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.009696492576991175 Training loss: 8.433286666870117
2025-12-09 12:18:48.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.009695446840096945 Training loss: 8.230937957763672
2025-12-09 12:18:49.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.009694399361339753 Training loss: 8.215946197509766
2025-12-09 12:18:49.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.009693350141108182 Training loss: 8.047670364379883
2025-12-09 12:18:49.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00969229917979146 Training loss: 8.08730411529541
2025-12-09 12:18:49.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.009691246477779459 Training loss: 7.9928717613220215
2025-12-09 12:18:49.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0096901920354627 Training loss: 8.119400024414062
2025-12-09 12:18:49.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.009689135853232349 Training loss: 8.032145500183105
2025-12-09 12:18:49.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.009688077931480212 Training loss: 7.902033805847168
2025-12-09 12:18:49.565 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.009687018270598749 Training loss: 8.039005279541016
2025-12-09 12:18:49.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.009685956870981059 Training loss: 8.059621810913086
2025-12-09 12:18:49.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.009684893733020887 Training loss: 7.992299556732178
2025-12-09 12:18:49.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.009683828857112626 Training loss: 9.027633666992188
2025-12-09 12:18:49.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.009682762243651308 Training loss: 8.287603378295898
2025-12-09 12:18:49.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.009681693893032617 Training loss: 8.225998878479004
2025-12-09 12:18:50.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.009680623805652875 Training loss: 8.143762588500977
2025-12-09 12:18:50.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.009679551981909052 Training loss: 8.251227378845215
2025-12-09 12:18:50.180 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.00967847842219876 Training loss: 8.398039817810059
2025-12-09 12:18:50.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.009677403126920255 Training loss: 8.38248062133789
2025-12-09 12:18:50.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.00967632609647244 Training loss: 8.281961441040039
2025-12-09 12:18:50.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.009675247331254857 Training loss: 8.076805114746094
2025-12-09 12:18:50.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.009674166831667696 Training loss: 8.445862770080566
2025-12-09 12:18:50.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.009673084598111788 Training loss: 8.328433990478516
2025-12-09 12:18:50.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.009672000630988605 Training loss: 7.945059776306152
2025-12-09 12:18:50.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.009670914930700268 Training loss: 8.127059936523438
2025-12-09 12:18:50.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.009669827497649537 Training loss: 8.237884521484375
2025-12-09 12:18:50.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.009668738332239813 Training loss: 8.20384407043457
2025-12-09 12:18:50.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.009667647434875144 Training loss: 8.378986358642578
2025-12-09 12:18:51.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.00966655480596022 Training loss: 8.067622184753418
2025-12-09 12:18:51.101 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.009665460445900368 Training loss: 8.356551170349121
2025-12-09 12:18:51.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.009664364355101564 Training loss: 8.19151496887207
2025-12-09 12:18:51.255 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.009663266533970424 Training loss: 8.531879425048828
2025-12-09 12:18:51.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.009662166982914203 Training loss: 8.199342727661133
2025-12-09 12:18:51.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.0096610657023408 Training loss: 8.253971099853516
2025-12-09 12:18:51.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.009659962692658756 Training loss: 7.623075485229492
2025-12-09 12:18:51.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.009658857954277254 Training loss: 8.073691368103027
2025-12-09 12:18:51.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.009657751487606114 Training loss: 8.284178733825684
2025-12-09 12:18:51.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.009656643293055805 Training loss: 8.39189338684082
2025-12-09 12:18:51.789 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.009655533371037426 Training loss: 8.33584213256836
2025-12-09 12:18:51.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.009654421721962729 Training loss: 8.210790634155273
2025-12-09 12:18:51.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.009653308346244099 Training loss: 8.105138778686523
2025-12-09 12:18:52.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.009652193244294562 Training loss: 8.872456550598145
2025-12-09 12:18:52.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.009651076416527786 Training loss: 8.319326400756836
2025-12-09 12:18:52.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.00964995786335808 Training loss: 8.350479125976562
2025-12-09 12:18:52.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.009648837585200392 Training loss: 7.9673075675964355
2025-12-09 12:18:52.332 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.00964771558247031 Training loss: 7.576232433319092
2025-12-09 12:18:52.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.00964659185558406 Training loss: 8.454954147338867
2025-12-09 12:18:52.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.00964546640495851 Training loss: 8.22585678100586
2025-12-09 12:18:52.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.009644339231011169 Training loss: 8.131301879882812
2025-12-09 12:18:52.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.009643210334160178 Training loss: 8.37994384765625
2025-12-09 12:18:52.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.009642079714824328 Training loss: 8.081025123596191
2025-12-09 12:18:52.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.00964094737342304 Training loss: 8.34720516204834
Traceback (most recent call last):
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 83, in <module>
    train(
  File "/work/nvme/bdhh/yxu21/ds-shu-200/experiments/main.py", line 16, in train
    loss.backward()
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/vllm-workspace/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 94.50 GiB of which 3.40 GiB is free. Including non-PyTorch memory, this process has 90.83 GiB memory in use. Of the allocated memory 89.58 GiB is allocated by PyTorch, and 500.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
