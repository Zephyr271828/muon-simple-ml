2025-12-09 12:03:26.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 9.210470199584961
2025-12-09 12:03:26.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 9.208928108215332
2025-12-09 12:03:26.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 9.208910942077637
2025-12-09 12:03:26.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 9.209657669067383
2025-12-09 12:03:26.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 9.20874309539795
2025-12-09 12:03:26.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 9.207987785339355
2025-12-09 12:03:26.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 9.207764625549316
2025-12-09 12:03:26.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 9.20699691772461
2025-12-09 12:03:26.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 9.206282615661621
2025-12-09 12:03:26.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 9.20443058013916
2025-12-09 12:03:26.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 9.20409870147705
2025-12-09 12:03:26.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 9.203482627868652
2025-12-09 12:03:26.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 9.202958106994629
2025-12-09 12:03:26.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 9.201371192932129
2025-12-09 12:03:26.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 9.199092864990234
2025-12-09 12:03:26.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 9.197945594787598
2025-12-09 12:03:26.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 9.197216987609863
2025-12-09 12:03:26.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 9.193087577819824
2025-12-09 12:03:26.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 9.19147777557373
2025-12-09 12:03:26.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 9.19011402130127
2025-12-09 12:03:26.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 9.186257362365723
2025-12-09 12:03:26.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 9.183893203735352
2025-12-09 12:03:26.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 9.181035995483398
2025-12-09 12:03:26.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 9.174363136291504
2025-12-09 12:03:26.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 9.171165466308594
2025-12-09 12:03:26.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 9.172521591186523
2025-12-09 12:03:26.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 9.166775703430176
2025-12-09 12:03:26.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 9.160114288330078
2025-12-09 12:03:26.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 9.148968696594238
2025-12-09 12:03:26.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 9.15009880065918
2025-12-09 12:03:26.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 9.137142181396484
2025-12-09 12:03:26.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 9.127141952514648
2025-12-09 12:03:26.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 9.114133834838867
2025-12-09 12:03:26.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 9.105562210083008
2025-12-09 12:03:26.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 9.09489631652832
2025-12-09 12:03:26.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 9.073454856872559
2025-12-09 12:03:26.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 9.032629013061523
2025-12-09 12:03:26.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 9.019078254699707
2025-12-09 12:03:26.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 8.975287437438965
2025-12-09 12:03:26.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 8.92182731628418
2025-12-09 12:03:26.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 8.876191139221191
2025-12-09 12:03:26.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 8.774328231811523
2025-12-09 12:03:26.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 8.689545631408691
2025-12-09 12:03:26.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 8.600035667419434
2025-12-09 12:03:26.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 8.386239051818848
2025-12-09 12:03:26.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 8.310145378112793
2025-12-09 12:03:26.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 8.169756889343262
2025-12-09 12:03:26.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 8.013375282287598
2025-12-09 12:03:26.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 7.903594017028809
2025-12-09 12:03:26.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 7.7548933029174805
2025-12-09 12:03:26.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 7.615947246551514
2025-12-09 12:03:27.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 7.54857063293457
2025-12-09 12:03:27.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 7.4529218673706055
2025-12-09 12:03:27.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 7.324344158172607
2025-12-09 12:03:27.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 7.182264804840088
2025-12-09 12:03:27.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 7.146724224090576
2025-12-09 12:03:27.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 7.122191905975342
2025-12-09 12:03:27.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 7.097362518310547
2025-12-09 12:03:27.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 7.024107933044434
2025-12-09 12:03:27.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 7.0494794845581055
2025-12-09 12:03:27.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 6.930126190185547
2025-12-09 12:03:27.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 6.96366548538208
2025-12-09 12:03:27.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 6.939218521118164
2025-12-09 12:03:27.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 6.836644172668457
2025-12-09 12:03:27.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 6.829689025878906
2025-12-09 12:03:27.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 6.85658597946167
2025-12-09 12:03:27.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 6.848033905029297
2025-12-09 12:03:27.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 6.7638115882873535
2025-12-09 12:03:27.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 6.802125930786133
2025-12-09 12:03:27.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 6.7989912033081055
2025-12-09 12:03:27.295 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 6.731551170349121
2025-12-09 12:03:27.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 6.764561176300049
2025-12-09 12:03:27.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 6.73665714263916
2025-12-09 12:03:27.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 6.734529972076416
2025-12-09 12:03:27.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 6.733067989349365
2025-12-09 12:03:27.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 6.747648239135742
2025-12-09 12:03:27.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 6.638643741607666
2025-12-09 12:03:27.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 6.7759881019592285
2025-12-09 12:03:27.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 6.662108898162842
2025-12-09 12:03:27.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 6.73314905166626
2025-12-09 12:03:27.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 6.766542911529541
2025-12-09 12:03:27.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 6.757148265838623
2025-12-09 12:03:27.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 6.713791370391846
2025-12-09 12:03:27.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 6.674701690673828
2025-12-09 12:03:27.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 6.756387233734131
2025-12-09 12:03:27.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 6.719552516937256
2025-12-09 12:03:27.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 6.729499816894531
2025-12-09 12:03:27.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 6.731727600097656
2025-12-09 12:03:27.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 6.671203136444092
2025-12-09 12:03:27.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 6.730409145355225
2025-12-09 12:03:27.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 6.7323808670043945
2025-12-09 12:03:27.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 6.789694309234619
2025-12-09 12:03:27.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 6.720399379730225
2025-12-09 12:03:27.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 6.737796783447266
2025-12-09 12:03:27.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 6.717953205108643
2025-12-09 12:03:27.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 6.679248332977295
2025-12-09 12:03:27.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 6.749728202819824
2025-12-09 12:03:27.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 6.771157741546631
2025-12-09 12:03:27.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 6.806411266326904
2025-12-09 12:03:27.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 6.672481536865234
2025-12-09 12:03:27.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0002909538931178862 Training loss: 6.727003574371338
2025-12-09 12:03:27.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00026490666646784665 Training loss: 6.758341312408447
2025-12-09 12:03:27.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000225 Training loss: 6.681698799133301
2025-12-09 12:03:27.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00017604722665003956 Training loss: 6.7508544921875
2025-12-09 12:03:27.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00012395277334996044 Training loss: 6.7509965896606445
2025-12-09 12:03:27.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 7.500000000000002e-05 Training loss: 6.7605509757995605
2025-12-09 12:03:27.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 3.509333353215331e-05 Training loss: 6.71946907043457
2025-12-09 12:03:27.863 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113751e-06 Training loss: 6.731075286865234
2025-12-09 12:03:27.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 6.7166900634765625
