2025-12-09 12:06:25.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 9.21070671081543
2025-12-09 12:06:25.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 9.211498260498047
2025-12-09 12:06:25.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 9.20983600616455
2025-12-09 12:06:25.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 9.210346221923828
2025-12-09 12:06:25.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 9.21045207977295
2025-12-09 12:06:25.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 9.210097312927246
2025-12-09 12:06:25.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 9.21127986907959
2025-12-09 12:06:25.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 9.210306167602539
2025-12-09 12:06:25.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 9.210771560668945
2025-12-09 12:06:25.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 9.209915161132812
2025-12-09 12:06:25.669 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 9.210298538208008
2025-12-09 12:06:25.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 9.210596084594727
2025-12-09 12:06:25.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 9.210412979125977
2025-12-09 12:06:25.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 9.209940910339355
2025-12-09 12:06:25.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 9.21056079864502
2025-12-09 12:06:25.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 9.209907531738281
2025-12-09 12:06:25.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 9.210492134094238
2025-12-09 12:06:25.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 9.21011734008789
2025-12-09 12:06:25.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 9.21005916595459
2025-12-09 12:06:25.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 9.209327697753906
2025-12-09 12:06:25.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 9.20950698852539
2025-12-09 12:06:25.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 9.210524559020996
2025-12-09 12:06:25.858 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 9.209683418273926
2025-12-09 12:06:25.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 9.208869934082031
2025-12-09 12:06:25.888 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 9.2097749710083
2025-12-09 12:06:25.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 9.210206985473633
2025-12-09 12:06:25.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 9.20969295501709
2025-12-09 12:06:25.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 9.208860397338867
2025-12-09 12:06:25.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 9.20798110961914
2025-12-09 12:06:25.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 9.208712577819824
2025-12-09 12:06:25.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 9.208771705627441
2025-12-09 12:06:26.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 9.20816421508789
2025-12-09 12:06:26.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 9.207880973815918
2025-12-09 12:06:26.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 9.207921981811523
2025-12-09 12:06:26.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 9.207528114318848
2025-12-09 12:06:26.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 9.207056045532227
2025-12-09 12:06:26.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 9.207371711730957
2025-12-09 12:06:26.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 9.207441329956055
2025-12-09 12:06:26.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 9.207262992858887
2025-12-09 12:06:26.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 9.207125663757324
2025-12-09 12:06:26.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 9.206172943115234
2025-12-09 12:06:26.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 9.206742286682129
2025-12-09 12:06:26.171 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 9.20680046081543
2025-12-09 12:06:26.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 9.20602798461914
2025-12-09 12:06:26.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 9.205410957336426
2025-12-09 12:06:26.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 9.205883979797363
2025-12-09 12:06:26.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 9.205810546875
2025-12-09 12:06:26.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 9.205482482910156
2025-12-09 12:06:26.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 9.205771446228027
2025-12-09 12:06:26.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 9.205453872680664
2025-12-09 12:06:26.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 9.205107688903809
2025-12-09 12:06:26.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 9.204365730285645
2025-12-09 12:06:26.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 9.20447826385498
2025-12-09 12:06:26.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 9.20429801940918
2025-12-09 12:06:26.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 9.2036714553833
2025-12-09 12:06:26.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 9.202563285827637
2025-12-09 12:06:26.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 9.203913688659668
2025-12-09 12:06:26.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 9.20321273803711
2025-12-09 12:06:26.415 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 9.202956199645996
2025-12-09 12:06:26.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 9.20206356048584
2025-12-09 12:06:26.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 9.202898025512695
2025-12-09 12:06:26.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 9.201386451721191
2025-12-09 12:06:26.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 9.201436042785645
2025-12-09 12:06:26.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 9.200857162475586
2025-12-09 12:06:26.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 9.200390815734863
2025-12-09 12:06:26.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 9.201284408569336
2025-12-09 12:06:26.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 9.200096130371094
2025-12-09 12:06:26.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 9.20004940032959
2025-12-09 12:06:26.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 9.199929237365723
2025-12-09 12:06:26.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 9.199563980102539
2025-12-09 12:06:26.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 9.199190139770508
2025-12-09 12:06:26.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 9.198697090148926
2025-12-09 12:06:26.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 9.198880195617676
2025-12-09 12:06:26.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 9.197277069091797
2025-12-09 12:06:26.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 9.197827339172363
2025-12-09 12:06:26.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 9.197293281555176
2025-12-09 12:06:26.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 9.1973295211792
2025-12-09 12:06:26.705 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 9.19669246673584
2025-12-09 12:06:26.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 9.196587562561035
2025-12-09 12:06:26.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 9.195343971252441
2025-12-09 12:06:26.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 9.19727897644043
2025-12-09 12:06:26.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 9.195211410522461
2025-12-09 12:06:26.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 9.194452285766602
2025-12-09 12:06:26.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 9.1947021484375
2025-12-09 12:06:26.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 9.19462776184082
2025-12-09 12:06:26.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 9.193660736083984
2025-12-09 12:06:26.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 9.193757057189941
2025-12-09 12:06:26.855 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 9.193964004516602
2025-12-09 12:06:26.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 9.19311809539795
2025-12-09 12:06:26.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 9.19212532043457
2025-12-09 12:06:26.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 9.193155288696289
2025-12-09 12:06:26.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 9.192254066467285
2025-12-09 12:06:26.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 9.190948486328125
2025-12-09 12:06:26.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 9.19106674194336
2025-12-09 12:06:26.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 9.19128704071045
2025-12-09 12:06:26.997 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 9.190295219421387
2025-12-09 12:06:27.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 9.18956470489502
2025-12-09 12:06:27.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 9.190181732177734
2025-12-09 12:06:27.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 9.189826965332031
2025-12-09 12:06:27.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 9.189397811889648
2025-12-09 12:06:27.072 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.002909538931178863 Training loss: 9.18880558013916
2025-12-09 12:06:27.087 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002649066664678467 Training loss: 9.18875503540039
2025-12-09 12:06:27.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0022500000000000003 Training loss: 9.188403129577637
2025-12-09 12:06:27.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0017604722665003959 Training loss: 9.18795394897461
2025-12-09 12:06:27.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0012395277334996046 Training loss: 9.18618392944336
2025-12-09 12:06:27.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0007500000000000003 Training loss: 9.188094139099121
2025-12-09 12:06:27.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0003509333353215332 Training loss: 9.18581485748291
2025-12-09 12:06:27.177 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113752e-05 Training loss: 9.187644958496094
2025-12-09 12:06:27.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 9.188776016235352
