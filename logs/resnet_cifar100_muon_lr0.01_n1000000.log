2025-12-09 12:09:58.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 4.873716831207275
2025-12-09 12:09:58.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 4.773757457733154
2025-12-09 12:09:58.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 4.9079270362854
2025-12-09 12:09:58.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 4.854887008666992
2025-12-09 12:09:58.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 4.803802013397217
2025-12-09 12:09:59.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 4.9105634689331055
2025-12-09 12:09:59.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 4.870787143707275
2025-12-09 12:09:59.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 4.858786582946777
2025-12-09 12:09:59.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 4.7845845222473145
2025-12-09 12:09:59.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 4.837215900421143
2025-12-09 12:09:59.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 4.847665309906006
2025-12-09 12:09:59.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 4.6451334953308105
2025-12-09 12:09:59.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 4.744940757751465
2025-12-09 12:09:59.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 4.736783981323242
2025-12-09 12:09:59.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 4.602291584014893
2025-12-09 12:09:59.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 4.604862213134766
2025-12-09 12:09:59.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 4.655924320220947
2025-12-09 12:09:59.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 4.648123264312744
2025-12-09 12:09:59.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 4.591699600219727
2025-12-09 12:09:59.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 4.505617618560791
2025-12-09 12:09:59.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 4.40597677230835
2025-12-09 12:09:59.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 4.416983604431152
2025-12-09 12:09:59.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 4.510196208953857
2025-12-09 12:09:59.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 4.353265762329102
2025-12-09 12:09:59.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 4.153914928436279
2025-12-09 12:09:59.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 4.336745738983154
2025-12-09 12:09:59.629 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 4.364870548248291
2025-12-09 12:09:59.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 4.117680549621582
2025-12-09 12:09:59.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 4.274554252624512
2025-12-09 12:09:59.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 4.366634368896484
2025-12-09 12:09:59.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 4.182000160217285
2025-12-09 12:09:59.777 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 4.408579349517822
2025-12-09 12:09:59.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 4.032167434692383
2025-12-09 12:09:59.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 4.248020648956299
2025-12-09 12:09:59.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 4.072471618652344
2025-12-09 12:09:59.894 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 4.001396179199219
2025-12-09 12:09:59.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 4.07572078704834
2025-12-09 12:09:59.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 4.140365123748779
2025-12-09 12:09:59.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 4.011847019195557
2025-12-09 12:10:00.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 4.103078842163086
2025-12-09 12:10:00.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 4.285792827606201
2025-12-09 12:10:00.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 4.221397399902344
2025-12-09 12:10:00.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 4.053434371948242
2025-12-09 12:10:00.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 4.085414409637451
2025-12-09 12:10:00.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 4.207278251647949
2025-12-09 12:10:00.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 3.927719831466675
2025-12-09 12:10:00.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 4.125540733337402
2025-12-09 12:10:00.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 4.0250043869018555
2025-12-09 12:10:00.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 4.056691646575928
2025-12-09 12:10:00.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 3.982571601867676
2025-12-09 12:10:00.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 4.143777847290039
2025-12-09 12:10:00.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 3.949434280395508
2025-12-09 12:10:00.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 3.8406498432159424
2025-12-09 12:10:00.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 4.206794261932373
2025-12-09 12:10:00.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 4.07515811920166
2025-12-09 12:10:00.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 4.182270526885986
2025-12-09 12:10:00.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 3.7981722354888916
2025-12-09 12:10:00.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 3.7260775566101074
2025-12-09 12:10:00.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 4.056958198547363
2025-12-09 12:10:00.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 3.8204147815704346
2025-12-09 12:10:00.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 3.9547832012176514
2025-12-09 12:10:00.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 4.202723503112793
2025-12-09 12:10:00.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 4.067487716674805
2025-12-09 12:10:00.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 4.028541564941406
2025-12-09 12:10:00.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 3.9488630294799805
2025-12-09 12:10:00.791 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 3.7986466884613037
2025-12-09 12:10:00.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 4.162458419799805
2025-12-09 12:10:00.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 3.991375207901001
2025-12-09 12:10:00.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 3.973848581314087
2025-12-09 12:10:00.908 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 3.8944191932678223
2025-12-09 12:10:00.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 3.804138660430908
2025-12-09 12:10:00.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 4.11467981338501
2025-12-09 12:10:00.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 4.043365001678467
2025-12-09 12:10:01.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 3.923389434814453
2025-12-09 12:10:01.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 3.933879852294922
2025-12-09 12:10:01.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 3.9889347553253174
2025-12-09 12:10:01.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 4.203179836273193
2025-12-09 12:10:01.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 3.6650052070617676
2025-12-09 12:10:01.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 3.8747429847717285
2025-12-09 12:10:01.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 3.5727553367614746
2025-12-09 12:10:01.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 3.842240571975708
2025-12-09 12:10:01.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 4.047804355621338
2025-12-09 12:10:01.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 3.823007106781006
2025-12-09 12:10:01.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 3.709409475326538
2025-12-09 12:10:01.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 3.9068946838378906
2025-12-09 12:10:01.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 3.9430644512176514
2025-12-09 12:10:01.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 3.9717516899108887
2025-12-09 12:10:01.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 3.929222583770752
2025-12-09 12:10:01.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 3.860926389694214
2025-12-09 12:10:01.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 3.763129472732544
2025-12-09 12:10:01.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 3.8656086921691895
2025-12-09 12:10:01.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 4.064054012298584
2025-12-09 12:10:01.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 4.39816427230835
2025-12-09 12:10:01.618 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 3.9446256160736084
2025-12-09 12:10:01.648 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 3.9572463035583496
2025-12-09 12:10:01.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 3.7304437160491943
2025-12-09 12:10:01.707 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 3.8506176471710205
2025-12-09 12:10:01.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 3.6202476024627686
2025-12-09 12:10:01.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 3.6710100173950195
2025-12-09 12:10:01.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 4.18840217590332
2025-12-09 12:10:01.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999708626830616 Training loss: 3.741240978240967
2025-12-09 12:10:01.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.009998834541281799 Training loss: 4.211655139923096
2025-12-09 12:10:01.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009997377845227575 Training loss: 3.9232752323150635
2025-12-09 12:10:01.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009995338708444804 Training loss: 3.7332465648651123
2025-12-09 12:10:01.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009992717368593385 Training loss: 3.88285231590271
2025-12-09 12:10:01.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009989514131188558 Training loss: 3.73427152633667
2025-12-09 12:10:02.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009985729369565299 Training loss: 3.6996657848358154
2025-12-09 12:10:02.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0099813635248348 Training loss: 3.9719433784484863
2025-12-09 12:10:02.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.00997641710583307 Training loss: 3.6444039344787598
2025-12-09 12:10:02.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.009970890689061622 Training loss: 3.884121894836426
2025-12-09 12:10:02.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009964784918620281 Training loss: 3.80865740776062
2025-12-09 12:10:02.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.009958100506132127 Training loss: 4.090987205505371
2025-12-09 12:10:02.182 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009950838230660534 Training loss: 3.837148666381836
2025-12-09 12:10:02.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009942998938618395 Training loss: 3.891568660736084
2025-12-09 12:10:02.241 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.009934583543669454 Training loss: 3.622014045715332
2025-12-09 12:10:02.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009925593026621833 Training loss: 3.79780912399292
2025-12-09 12:10:02.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.009916028435313709 Training loss: 4.090449810028076
2025-12-09 12:10:02.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.009905890884491196 Training loss: 3.794830560684204
2025-12-09 12:10:02.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.00989518155567842 Training loss: 3.6722934246063232
2025-12-09 12:10:02.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009883901697039808 Training loss: 3.5664925575256348
2025-12-09 12:10:02.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.009872052623234632 Training loss: 3.89508056640625
2025-12-09 12:10:02.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00985963571526376 Training loss: 3.7873010635375977
2025-12-09 12:10:02.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.009846652420308728 Training loss: 3.8984107971191406
2025-12-09 12:10:02.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.009833104251563056 Training loss: 3.9405548572540283
2025-12-09 12:10:02.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.00981899278805589 Training loss: 3.7393641471862793
2025-12-09 12:10:02.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.009804319674467968 Training loss: 3.671917676925659
2025-12-09 12:10:02.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009789086620939936 Training loss: 3.7335472106933594
2025-12-09 12:10:02.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009773295402873026 Training loss: 3.711927652359009
2025-12-09 12:10:02.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009756947860722143 Training loss: 3.7056658267974854
2025-12-09 12:10:02.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009740045899781353 Training loss: 3.806854248046875
2025-12-09 12:10:02.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.009722591489961827 Training loss: 3.5709996223449707
2025-12-09 12:10:02.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009704586665562249 Training loss: 3.701220750808716
2025-12-09 12:10:02.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.00968603352503172 Training loss: 3.676114082336426
2025-12-09 12:10:02.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009666934230725179 Training loss: 3.5405843257904053
2025-12-09 12:10:02.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009647291008651398 Training loss: 3.820183515548706
2025-12-09 12:10:02.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009627106148213521 Training loss: 3.644517183303833
2025-12-09 12:10:02.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.009606382001942255 Training loss: 3.9635019302368164
2025-12-09 12:10:02.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00958512098522167 Training loss: 3.6055748462677
2025-12-09 12:10:02.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0095633255760077 Training loss: 3.327970504760742
2025-12-09 12:10:02.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009540998314539327 Training loss: 3.442556381225586
2025-12-09 12:10:03.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009518141803042527 Training loss: 3.578702926635742
2025-12-09 12:10:03.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.009494758705426976 Training loss: 3.5906271934509277
2025-12-09 12:10:03.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.009470851746975581 Training loss: 3.310724973678589
2025-12-09 12:10:03.104 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009446423714026845 Training loss: 4.270821571350098
2025-12-09 12:10:03.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.009421477453650118 Training loss: 3.430147409439087
2025-12-09 12:10:03.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.009396015873313781 Training loss: 3.47463059425354
2025-12-09 12:10:03.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00937004194054638 Training loss: 3.8948771953582764
2025-12-09 12:10:03.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009343558682590757 Training loss: 3.7046470642089844
2025-12-09 12:10:03.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.009316569186051234 Training loss: 3.802342414855957
2025-12-09 12:10:03.282 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009289076596533871 Training loss: 3.4040064811706543
2025-12-09 12:10:03.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009261084118279847 Training loss: 3.2948176860809326
2025-12-09 12:10:03.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009232595013792002 Training loss: 3.6563923358917236
2025-12-09 12:10:03.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009203612603454604 Training loss: 3.568525791168213
2025-12-09 12:10:03.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.009174140265146355 Training loss: 3.343338966369629
2025-12-09 12:10:03.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009144181433846706 Training loss: 3.3981316089630127
2025-12-09 12:10:03.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009113739601235507 Training loss: 3.5411157608032227
2025-12-09 12:10:03.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009082818315286054 Training loss: 3.3964405059814453
2025-12-09 12:10:03.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009051421179851587 Training loss: 3.556246757507324
2025-12-09 12:10:03.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.00901955185424525 Training loss: 3.6515491008758545
2025-12-09 12:10:03.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.008987214052813604 Training loss: 3.363617420196533
2025-12-09 12:10:03.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00895441154450373 Training loss: 3.6612894535064697
2025-12-09 12:10:03.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.008921148152423945 Training loss: 3.5216197967529297
2025-12-09 12:10:03.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.008887427753398248 Training loss: 3.5571680068969727
2025-12-09 12:10:03.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.008853254277514447 Training loss: 3.0415561199188232
2025-12-09 12:10:03.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.008818631707666134 Training loss: 3.5958752632141113
2025-12-09 12:10:03.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.008783564079088476 Training loss: 3.5985286235809326
2025-12-09 12:10:03.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.008748055478887904 Training loss: 3.4127068519592285
2025-12-09 12:10:03.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.008712110045565767 Training loss: 3.5214405059814453
2025-12-09 12:10:03.845 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.008675731968536002 Training loss: 3.3203277587890625
2025-12-09 12:10:03.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.008638925487636848 Training loss: 3.327277660369873
2025-12-09 12:10:03.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0086016948926367 Training loss: 3.4919097423553467
2025-12-09 12:10:03.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.008564044522734147 Training loss: 3.608690023422241
2025-12-09 12:10:03.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.008525978766052229 Training loss: 3.646653413772583
2025-12-09 12:10:03.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.008487502059127015 Training loss: 3.4083175659179688
2025-12-09 12:10:04.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.008448618886390521 Training loss: 3.5550172328948975
2025-12-09 12:10:04.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00840933377964806 Training loss: 3.4982292652130127
2025-12-09 12:10:04.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.008369651317550054 Training loss: 3.242204427719116
2025-12-09 12:10:04.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.008329576125058406 Training loss: 3.4513959884643555
2025-12-09 12:10:04.142 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.008289112872907454 Training loss: 3.460871458053589
2025-12-09 12:10:04.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.008248266277059607 Training loss: 3.1848456859588623
2025-12-09 12:10:04.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0082070410981557 Training loss: 3.4442296028137207
2025-12-09 12:10:04.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00816544214096015 Training loss: 3.103973150253296
2025-12-09 12:10:04.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.008123474253800956 Training loss: 3.6491663455963135
2025-12-09 12:10:04.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.008081142328004637 Training loss: 3.4527194499969482
2025-12-09 12:10:04.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.008038451297326145 Training loss: 3.554202079772949
2025-12-09 12:10:04.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.007995406137373847 Training loss: 3.3031489849090576
2025-12-09 12:10:04.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.007952011865029614 Training loss: 3.3193883895874023
2025-12-09 12:10:04.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.007908273537864113 Training loss: 3.1939351558685303
2025-12-09 12:10:04.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.007864196253547348 Training loss: 3.5612661838531494
2025-12-09 12:10:04.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.007819785149254532 Training loss: 3.079118251800537
2025-12-09 12:10:04.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.00777504540106735 Training loss: 3.232170343399048
2025-12-09 12:10:04.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.007729982223370691 Training loss: 3.146273612976074
2025-12-09 12:10:04.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.00768460086824492 Training loss: 3.433720350265503
2025-12-09 12:10:04.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.007638906624853743 Training loss: 3.435047149658203
2025-12-09 12:10:04.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.007592904818827774 Training loss: 3.2310047149658203
2025-12-09 12:10:04.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.007546600811643816 Training loss: 3.209449052810669
2025-12-09 12:10:04.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0075 Training loss: 3.5194687843322754
2025-12-09 12:10:04.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.007453107815186802 Training loss: 2.9904723167419434
2025-12-09 12:10:04.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.007405929722454026 Training loss: 3.191807508468628
2025-12-09 12:10:04.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.007358471220373831 Training loss: 3.486999750137329
2025-12-09 12:10:04.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.007310737840199885 Training loss: 3.3835055828094482
2025-12-09 12:10:04.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.007262735145222695 Training loss: 3.7291574478149414
2025-12-09 12:10:04.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.007214468730121208 Training loss: 3.3953256607055664
2025-12-09 12:10:04.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.007165944220310766 Training loss: 3.2815346717834473
2025-12-09 12:10:04.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.007117167271287452 Training loss: 3.4599180221557617
2025-12-09 12:10:04.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.007068143567968957 Training loss: 3.3672451972961426
2025-12-09 12:10:04.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.0070188788240320085 Training loss: 3.068624973297119
2025-12-09 12:10:05.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.006969378781246436 Training loss: 3.400958776473999
2025-12-09 12:10:05.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.006919649208805981 Training loss: 3.19297456741333
2025-12-09 12:10:05.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0068696959026558965 Training loss: 3.085484743118286
2025-12-09 12:10:05.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.006819524684817438 Training loss: 3.3314692974090576
2025-12-09 12:10:05.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0067691414027093045 Training loss: 3.274589776992798
2025-12-09 12:10:05.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.006718551928466133 Training loss: 2.895446538925171
2025-12-09 12:10:05.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.006667762158254104 Training loss: 2.8968822956085205
2025-12-09 12:10:05.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.006616778011583743 Training loss: 3.090118408203125
2025-12-09 12:10:05.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.006565605430620013 Training loss: 3.1561453342437744
2025-12-09 12:10:05.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.006514250379489753 Training loss: 3.3745100498199463
2025-12-09 12:10:05.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.006462718843586571 Training loss: 3.494124174118042
2025-12-09 12:10:05.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.0064110168288732386 Training loss: 3.271164655685425
2025-12-09 12:10:05.363 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.006359150361181715 Training loss: 3.1986684799194336
2025-12-09 12:10:05.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.006307125485510829 Training loss: 3.166670799255371
2025-12-09 12:10:05.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0062549482653217435 Training loss: 3.2698147296905518
2025-12-09 12:10:05.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0062026247818312685 Training loss: 3.451256036758423
2025-12-09 12:10:05.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0061501611333030885 Training loss: 3.049833059310913
2025-12-09 12:10:05.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.006097563434337026 Training loss: 3.2973926067352295
2025-12-09 12:10:05.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.006044837815156376 Training loss: 3.2220420837402344
2025-12-09 12:10:05.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.005991990420893449 Training loss: 3.2206661701202393
2025-12-09 12:10:05.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.005939027410873351 Training loss: 3.1045167446136475
2025-12-09 12:10:05.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0058859549578961145 Training loss: 3.3213627338409424
2025-12-09 12:10:05.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.005832779247517273 Training loss: 3.206376791000366
2025-12-09 12:10:05.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0057795064773269325 Training loss: 3.2721996307373047
2025-12-09 12:10:05.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.005726142856227452 Training loss: 3.1008057594299316
2025-12-09 12:10:05.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.005672694603709794 Training loss: 3.2923452854156494
2025-12-09 12:10:05.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.005619167949128652 Training loss: 3.090176820755005
2025-12-09 12:10:05.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.005565569130976423 Training loss: 3.2260541915893555
2025-12-09 12:10:05.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.005511904396156113 Training loss: 3.1251041889190674
2025-12-09 12:10:05.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.005458179999253274 Training loss: 3.1518666744232178
2025-12-09 12:10:05.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.005404402201807022 Training loss: 3.0710225105285645
2025-12-09 12:10:05.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00535057727158027 Training loss: 3.417673110961914
2025-12-09 12:10:05.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.005296711481829226 Training loss: 3.0498390197753906
2025-12-09 12:10:05.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.005242811110572242 Training loss: 2.9844443798065186
2025-12-09 12:10:06.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.005188882439858117 Training loss: 3.166271924972534
2025-12-09 12:10:06.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.005134931755033936 Training loss: 3.0119173526763916
2025-12-09 12:10:06.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.005080965344012508 Training loss: 2.982626438140869
2025-12-09 12:10:06.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.005026989496539522 Training loss: 3.171799659729004
2025-12-09 12:10:06.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.004973010503460479 Training loss: 2.9850282669067383
2025-12-09 12:10:06.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.004919034655987493 Training loss: 2.804065227508545
2025-12-09 12:10:06.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.004865068244966066 Training loss: 3.0164666175842285
2025-12-09 12:10:06.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0048111175601418844 Training loss: 3.2515177726745605
2025-12-09 12:10:06.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0047571888894277605 Training loss: 3.0706331729888916
2025-12-09 12:10:06.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0047032885181707736 Training loss: 3.0027694702148438
2025-12-09 12:10:06.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.004649422728419729 Training loss: 3.093916654586792
2025-12-09 12:10:06.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0045955977981929795 Training loss: 3.167428493499756
2025-12-09 12:10:06.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.004541820000746727 Training loss: 2.8998639583587646
2025-12-09 12:10:06.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.004488095603843887 Training loss: 2.935512065887451
2025-12-09 12:10:06.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.004434430869023579 Training loss: 2.914829969406128
2025-12-09 12:10:06.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00438083205087135 Training loss: 2.8172199726104736
2025-12-09 12:10:06.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.004327305396290207 Training loss: 2.82548451423645
2025-12-09 12:10:06.529 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00427385714377255 Training loss: 2.911376953125
2025-12-09 12:10:06.559 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.004220493522673068 Training loss: 2.8177642822265625
2025-12-09 12:10:06.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.004167220752482727 Training loss: 2.9105498790740967
2025-12-09 12:10:06.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0041140450421038866 Training loss: 2.6710002422332764
2025-12-09 12:10:06.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00406097258912665 Training loss: 3.0625553131103516
2025-12-09 12:10:06.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.004008009579106551 Training loss: 3.324012279510498
2025-12-09 12:10:06.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.003955162184843625 Training loss: 2.9133903980255127
2025-12-09 12:10:06.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.003902436565662977 Training loss: 2.9875333309173584
2025-12-09 12:10:06.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.003849838866696913 Training loss: 3.170703172683716
2025-12-09 12:10:06.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.003797375218168733 Training loss: 3.0430169105529785
2025-12-09 12:10:06.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0037450517346782563 Training loss: 2.997544765472412
2025-12-09 12:10:06.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0036928745144891727 Training loss: 3.11118745803833
2025-12-09 12:10:06.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0036408496388182854 Training loss: 2.863250732421875
2025-12-09 12:10:06.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.0035889831711267616 Training loss: 2.8659539222717285
2025-12-09 12:10:06.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00353728115641343 Training loss: 3.206066370010376
2025-12-09 12:10:06.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.003485749620510247 Training loss: 2.6818695068359375
2025-12-09 12:10:07.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0034343945693799884 Training loss: 2.8646082878112793
2025-12-09 12:10:07.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0033832219884162586 Training loss: 2.6950888633728027
2025-12-09 12:10:07.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0033322378417458983 Training loss: 2.945742607116699
2025-12-09 12:10:07.096 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.0032814480715338667 Training loss: 2.928621530532837
2025-12-09 12:10:07.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.0032308585972906966 Training loss: 2.7707128524780273
2025-12-09 12:10:07.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.0031804753151825627 Training loss: 3.169172525405884
2025-12-09 12:10:07.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0031303040973441033 Training loss: 3.0200772285461426
2025-12-09 12:10:07.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.003080350791194019 Training loss: 2.980520009994507
2025-12-09 12:10:07.247 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0030306212187535654 Training loss: 2.9899098873138428
2025-12-09 12:10:07.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0029811211759679926 Training loss: 2.749290704727173
2025-12-09 12:10:07.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0029318564320310442 Training loss: 2.879971504211426
2025-12-09 12:10:07.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.002882832728712551 Training loss: 2.585149049758911
2025-12-09 12:10:07.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.0028340557796892353 Training loss: 3.0066049098968506
2025-12-09 12:10:07.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0027855312698787903 Training loss: 2.8694992065429688
2025-12-09 12:10:07.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.002737264854777306 Training loss: 2.798083782196045
2025-12-09 12:10:07.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0026892621598001154 Training loss: 2.820956230163574
2025-12-09 12:10:07.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0026415287796261707 Training loss: 2.9036147594451904
2025-12-09 12:10:07.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0025940702775459745 Training loss: 2.7314376831054688
2025-12-09 12:10:07.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.002546892184813198 Training loss: 2.537431001663208
2025-12-09 12:10:07.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.0025000000000000014 Training loss: 2.701753616333008
2025-12-09 12:10:07.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0024533991883561868 Training loss: 3.0303149223327637
2025-12-09 12:10:07.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.002407095181172227 Training loss: 2.893786668777466
2025-12-09 12:10:07.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0023610933751462555 Training loss: 2.754795551300049
2025-12-09 12:10:07.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.002315399131755081 Training loss: 2.890143394470215
2025-12-09 12:10:07.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0022700177766293095 Training loss: 2.8813395500183105
2025-12-09 12:10:07.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.002224954598932651 Training loss: 2.8800082206726074
2025-12-09 12:10:07.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0021802148507454673 Training loss: 2.8097918033599854
2025-12-09 12:10:07.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0021358037464526513 Training loss: 2.731369733810425
2025-12-09 12:10:07.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0020917264621358876 Training loss: 2.759659767150879
2025-12-09 12:10:07.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0020479881349703883 Training loss: 2.839604616165161
2025-12-09 12:10:07.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0020045938626261544 Training loss: 2.702310562133789
2025-12-09 12:10:07.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0019615487026738545 Training loss: 2.9167792797088623
2025-12-09 12:10:07.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0019188576719953632 Training loss: 2.8397274017333984
2025-12-09 12:10:07.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0018765257461990442 Training loss: 2.853736400604248
2025-12-09 12:10:08.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0018345578590398509 Training loss: 2.746478319168091
2025-12-09 12:10:08.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0017929589018443016 Training loss: 2.9488909244537354
2025-12-09 12:10:08.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0017517337229403945 Training loss: 2.678009271621704
2025-12-09 12:10:08.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.001710887127092548 Training loss: 2.924659013748169
2025-12-09 12:10:08.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0016704238749415956 Training loss: 2.763211965560913
2025-12-09 12:10:08.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0016303486824499457 Training loss: 2.807802677154541
2025-12-09 12:10:08.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0015906662203519412 Training loss: 2.9860126972198486
2025-12-09 12:10:08.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0015513811136094785 Training loss: 2.7866318225860596
2025-12-09 12:10:08.256 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.001512497940872986 Training loss: 2.5023550987243652
2025-12-09 12:10:08.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.001474021233947772 Training loss: 2.7475526332855225
2025-12-09 12:10:08.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0014359554772658551 Training loss: 2.775404453277588
2025-12-09 12:10:08.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0013983051073632995 Training loss: 2.6705129146575928
2025-12-09 12:10:08.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0013610745123631535 Training loss: 2.4461781978607178
2025-12-09 12:10:08.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0013242680314639993 Training loss: 2.654829740524292
2025-12-09 12:10:08.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.0012878899544342326 Training loss: 2.651118040084839
2025-12-09 12:10:08.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.0012519445211120978 Training loss: 2.286097288131714
2025-12-09 12:10:08.496 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.0012164359209115233 Training loss: 2.4876368045806885
2025-12-09 12:10:08.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.0011813682923338654 Training loss: 2.577550172805786
2025-12-09 12:10:08.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0011467457224855543 Training loss: 2.66644287109375
2025-12-09 12:10:08.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0011125722466017547 Training loss: 2.685816764831543
2025-12-09 12:10:08.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0010788518475760545 Training loss: 2.364914655685425
2025-12-09 12:10:08.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.0010455884554962725 Training loss: 2.487959623336792
2025-12-09 12:10:08.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.001012785947186397 Training loss: 2.406618118286133
2025-12-09 12:10:08.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00098044814575475 Training loss: 2.426147699356079
2025-12-09 12:10:08.733 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0009485788201484125 Training loss: 2.661572217941284
2025-12-09 12:10:08.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0009171816847139447 Training loss: 2.466866970062256
2025-12-09 12:10:08.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.000886260398764494 Training loss: 2.5715768337249756
2025-12-09 12:10:08.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.0008558185661532941 Training loss: 2.6854443550109863
2025-12-09 12:10:08.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.000825859734853645 Training loss: 2.5296905040740967
2025-12-09 12:10:08.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0007963873965453961 Training loss: 2.595163345336914
2025-12-09 12:10:08.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.000767404986207999 Training loss: 2.4914751052856445
2025-12-09 12:10:08.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0007389158817201541 Training loss: 2.542816162109375
2025-12-09 12:10:08.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0007109234034661288 Training loss: 2.297574996948242
2025-12-09 12:10:09.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0006834308139487672 Training loss: 2.4554057121276855
2025-12-09 12:10:09.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0006564413174092443 Training loss: 2.517836332321167
2025-12-09 12:10:09.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0006299580594536214 Training loss: 2.476773738861084
2025-12-09 12:10:09.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0006039841266862189 Training loss: 2.6756651401519775
2025-12-09 12:10:09.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0005785225463498828 Training loss: 2.7046191692352295
2025-12-09 12:10:09.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0005535762859731547 Training loss: 2.682389736175537
2025-12-09 12:10:09.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0005291482530244179 Training loss: 2.313730001449585
2025-12-09 12:10:09.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0005052412945730239 Training loss: 2.6381802558898926
2025-12-09 12:10:09.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.00048185819695747425 Training loss: 3.046520233154297
2025-12-09 12:10:09.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00045900168546067266 Training loss: 2.66825008392334
2025-12-09 12:10:09.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00043667442399229983 Training loss: 2.5858383178710938
2025-12-09 12:10:09.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.0004148790147783288 Training loss: 2.6120762825012207
2025-12-09 12:10:09.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00039361799805774536 Training loss: 2.5570764541625977
2025-12-09 12:10:09.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0003728938517864794 Training loss: 2.7587404251098633
2025-12-09 12:10:09.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.00035270899134860366 Training loss: 2.3748087882995605
2025-12-09 12:10:09.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00033306576927482126 Training loss: 2.333617687225342
2025-12-09 12:10:09.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0003139664749682825 Training loss: 2.830509662628174
2025-12-09 12:10:09.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002954133344377524 Training loss: 2.4305169582366943
2025-12-09 12:10:09.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.00027740851003817346 Training loss: 2.509709596633911
2025-12-09 12:10:09.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00025995410021864785 Training loss: 2.5977673530578613
2025-12-09 12:10:09.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.0002430521392778573 Training loss: 2.565417528152466
2025-12-09 12:10:09.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.00022670459712697378 Training loss: 2.5876688957214355
2025-12-09 12:10:09.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0002109133790600648 Training loss: 2.2121081352233887
2025-12-09 12:10:09.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00019568032553203218 Training loss: 2.428361415863037
2025-12-09 12:10:09.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0001810072119441103 Training loss: 2.8054234981536865
2025-12-09 12:10:09.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.00016689574843694433 Training loss: 2.3395142555236816
2025-12-09 12:10:09.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.000153347579691272 Training loss: 2.76367449760437
2025-12-09 12:10:09.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0001403642847362402 Training loss: 2.7854692935943604
2025-12-09 12:10:09.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00012794737676536993 Training loss: 2.6557304859161377
2025-12-09 12:10:09.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00011609830296019141 Training loss: 2.3366920948028564
2025-12-09 12:10:09.890 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0001048184443215816 Training loss: 2.4964990615844727
2025-12-09 12:10:09.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 9.410911550880474e-05 Training loss: 2.3588688373565674
2025-12-09 12:10:09.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 8.397156468629208e-05 Training loss: 2.579059600830078
2025-12-09 12:10:09.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 7.44069733781677e-05 Training loss: 2.614393711090088
2025-12-09 12:10:10.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 6.54164563305465e-05 Training loss: 2.548208475112915
2025-12-09 12:10:10.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 5.7001061381606875e-05 Training loss: 2.483823537826538
2025-12-09 12:10:10.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 4.916176933946692e-05 Training loss: 2.7388455867767334
2025-12-09 12:10:10.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 4.1899493867874615e-05 Training loss: 2.7264091968536377
2025-12-09 12:10:10.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 3.521508137971807e-05 Training loss: 2.303849220275879
2025-12-09 12:10:10.159 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 2.9109310938378874e-05 Training loss: 2.7042133808135986
2025-12-09 12:10:10.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 2.358289416693027e-05 Training loss: 2.736959934234619
2025-12-09 12:10:10.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 1.863647516520017e-05 Training loss: 2.490659475326538
2025-12-09 12:10:10.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 1.4270630434701781e-05 Training loss: 2.360684394836426
2025-12-09 12:10:10.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 1.0485868811441756e-05 Training loss: 2.4220926761627197
2025-12-09 12:10:10.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 7.2826314066154475e-06 Training loss: 2.611109495162964
2025-12-09 12:10:10.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 4.661291555196345e-06 Training loss: 2.7542381286621094
2025-12-09 12:10:10.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 2.6221547724253337e-06 Training loss: 2.6955223083496094
2025-12-09 12:10:10.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 1.1654587182013953e-06 Training loss: 2.4426469802856445
2025-12-09 12:10:10.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 2.9137316938265825e-07 Training loss: 2.48650860786438
2025-12-09 12:10:10.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0 Training loss: 2.494539976119995
