2025-12-09 13:09:50.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 12.228447914123535
2025-12-09 13:09:51.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 12.176467895507812
2025-12-09 13:09:51.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 12.1774263381958
2025-12-09 13:09:52.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 12.156624794006348
2025-12-09 13:09:52.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 12.167801856994629
2025-12-09 13:09:52.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 12.142793655395508
2025-12-09 13:09:53.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 12.175086975097656
2025-12-09 13:09:53.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 12.11697769165039
2025-12-09 13:09:53.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 12.092219352722168
2025-12-09 13:09:54.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 12.046205520629883
2025-12-09 13:09:54.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 12.010110855102539
2025-12-09 13:09:55.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 12.0075044631958
2025-12-09 13:09:55.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 11.949197769165039
2025-12-09 13:09:55.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 11.920598030090332
2025-12-09 13:09:56.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 11.821887016296387
2025-12-09 13:09:56.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 11.699545860290527
2025-12-09 13:09:56.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 11.586967468261719
2025-12-09 13:09:57.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 11.711678504943848
2025-12-09 13:09:57.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 11.222427368164062
2025-12-09 13:09:58.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 10.995390892028809
2025-12-09 13:09:58.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 10.998819351196289
2025-12-09 13:09:58.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 10.984935760498047
2025-12-09 13:09:59.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 10.695796012878418
2025-12-09 13:09:59.520 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 10.347335815429688
2025-12-09 13:09:59.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 10.543725967407227
2025-12-09 13:10:00.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 9.985557556152344
2025-12-09 13:10:00.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 10.173218727111816
2025-12-09 13:10:01.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 10.036910057067871
2025-12-09 13:10:01.377 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 10.219732284545898
2025-12-09 13:10:01.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 10.167370796203613
2025-12-09 13:10:02.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 10.089715003967285
2025-12-09 13:10:02.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 9.791729927062988
2025-12-09 13:10:02.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 9.889993667602539
2025-12-09 13:10:03.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 9.972036361694336
2025-12-09 13:10:03.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 9.931248664855957
2025-12-09 13:10:03.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 9.694828987121582
2025-12-09 13:10:04.346 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 10.17264461517334
2025-12-09 13:10:04.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 9.58043098449707
2025-12-09 13:10:05.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 9.535674095153809
2025-12-09 13:10:05.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 9.885869979858398
2025-12-09 13:10:05.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 9.510890007019043
2025-12-09 13:10:06.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 9.468329429626465
2025-12-09 13:10:06.570 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 9.5087890625
2025-12-09 13:10:06.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 9.756381034851074
2025-12-09 13:10:07.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 9.55583381652832
2025-12-09 13:10:07.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 9.342890739440918
2025-12-09 13:10:08.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 9.609959602355957
2025-12-09 13:10:08.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 9.512639999389648
2025-12-09 13:10:08.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 9.370044708251953
2025-12-09 13:10:09.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 9.261642456054688
2025-12-09 13:10:09.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 9.29152774810791
2025-12-09 13:10:09.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 9.238359451293945
2025-12-09 13:10:10.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 9.603260040283203
2025-12-09 13:10:10.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 9.123373031616211
2025-12-09 13:10:11.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 9.110001564025879
2025-12-09 13:10:11.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 9.065703392028809
2025-12-09 13:10:11.761 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 9.183055877685547
2025-12-09 13:10:12.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 8.948360443115234
2025-12-09 13:10:12.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 9.093331336975098
2025-12-09 13:10:12.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 9.348267555236816
2025-12-09 13:10:13.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 9.079964637756348
2025-12-09 13:10:13.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 8.850421905517578
2025-12-09 13:10:13.985 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 9.34522533416748
2025-12-09 13:10:14.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 9.421954154968262
2025-12-09 13:10:14.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 8.978281021118164
2025-12-09 13:10:15.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 9.184998512268066
2025-12-09 13:10:15.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 8.713006019592285
2025-12-09 13:10:15.842 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 8.984136581420898
2025-12-09 13:10:16.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 9.038206100463867
2025-12-09 13:10:16.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 8.82160472869873
2025-12-09 13:10:16.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 8.968396186828613
2025-12-09 13:10:17.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 8.887715339660645
2025-12-09 13:10:17.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 9.000792503356934
2025-12-09 13:10:18.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 9.109031677246094
2025-12-09 13:10:18.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 8.827005386352539
2025-12-09 13:10:18.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 9.024970054626465
2025-12-09 13:10:19.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 8.949843406677246
2025-12-09 13:10:19.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 8.879640579223633
2025-12-09 13:10:19.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 9.201669692993164
2025-12-09 13:10:20.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 8.896008491516113
2025-12-09 13:10:20.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 9.037449836730957
2025-12-09 13:10:21.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 8.99584674835205
2025-12-09 13:10:21.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 8.939580917358398
2025-12-09 13:10:21.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 8.752754211425781
2025-12-09 13:10:22.146 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 8.998023986816406
2025-12-09 13:10:22.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 8.947823524475098
2025-12-09 13:10:22.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 8.872363090515137
2025-12-09 13:10:23.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 8.757092475891113
2025-12-09 13:10:23.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 8.743398666381836
2025-12-09 13:10:24.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 8.983194351196289
2025-12-09 13:10:24.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 9.12671184539795
2025-12-09 13:10:24.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 8.842978477478027
2025-12-09 13:10:25.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 8.967425346374512
2025-12-09 13:10:25.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 8.840327262878418
2025-12-09 13:10:25.853 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 9.592591285705566
2025-12-09 13:10:26.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 8.747812271118164
2025-12-09 13:10:26.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 8.805548667907715
2025-12-09 13:10:26.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 8.785333633422852
2025-12-09 13:10:27.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 9.090217590332031
2025-12-09 13:10:27.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 8.78970718383789
2025-12-09 13:10:28.075 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999999029798808 Training loss: 8.864959716796875
2025-12-09 13:10:28.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00999999611919561 Training loss: 8.933235168457031
2025-12-09 13:10:28.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009999991268191535 Training loss: 9.165722846984863
2025-12-09 13:10:29.191 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009999984476788465 Training loss: 8.946503639221191
2025-12-09 13:10:29.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009999975744989035 Training loss: 8.744743347167969
2025-12-09 13:10:29.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009999965072796636 Training loss: 8.883211135864258
2025-12-09 13:10:30.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009999952460215409 Training loss: 9.31610107421875
2025-12-09 13:10:30.673 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.009999937907250246 Training loss: 8.952108383178711
2025-12-09 13:10:31.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.009999921413906798 Training loss: 9.366732597351074
2025-12-09 13:10:31.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.009999902980191464 Training loss: 8.982998847961426
2025-12-09 13:10:31.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009999882606111399 Training loss: 9.07873821258545
2025-12-09 13:10:32.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.009999860291674507 Training loss: 8.897778511047363
2025-12-09 13:10:32.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009999836036889453 Training loss: 8.767288208007812
2025-12-09 13:10:32.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009999809841765645 Training loss: 9.239409446716309
2025-12-09 13:10:33.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00999978170631325 Training loss: 8.942790031433105
2025-12-09 13:10:33.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009999751630543188 Training loss: 8.874521255493164
2025-12-09 13:10:34.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00999971961446713 Training loss: 9.67358684539795
2025-12-09 13:10:34.382 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.009999685658097501 Training loss: 8.95768928527832
2025-12-09 13:10:34.753 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.009999649761447477 Training loss: 9.15085506439209
2025-12-09 13:10:35.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009999611924530994 Training loss: 8.839241981506348
2025-12-09 13:10:35.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.00999957214736273 Training loss: 9.052416801452637
2025-12-09 13:10:35.866 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.009999530429958124 Training loss: 9.013389587402344
2025-12-09 13:10:36.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.009999486772333366 Training loss: 9.041997909545898
2025-12-09 13:10:36.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0099994411745054 Training loss: 8.89342212677002
2025-12-09 13:10:36.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.009999393636491919 Training loss: 8.801093101501465
2025-12-09 13:10:37.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00999934415831137 Training loss: 8.89303970336914
2025-12-09 13:10:37.724 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009999292739982958 Training loss: 9.284602165222168
2025-12-09 13:10:38.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009999239381526638 Training loss: 8.7337646484375
2025-12-09 13:10:38.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009999184082963117 Training loss: 9.143253326416016
2025-12-09 13:10:38.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009999126844313852 Training loss: 8.841092109680176
2025-12-09 13:10:39.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00999906766560106 Training loss: 9.010751724243164
2025-12-09 13:10:39.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009999006546847707 Training loss: 9.280981063842773
2025-12-09 13:10:39.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.009998943488077507 Training loss: 8.960319519042969
2025-12-09 13:10:40.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009998878489314937 Training loss: 8.864572525024414
2025-12-09 13:10:40.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009998811550585221 Training loss: 9.114290237426758
2025-12-09 13:10:41.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009998742671914335 Training loss: 8.988884925842285
2025-12-09 13:10:41.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.00999867185332901 Training loss: 9.259868621826172
2025-12-09 13:10:41.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00999859909485673 Training loss: 8.811807632446289
2025-12-09 13:10:42.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00999852439652573 Training loss: 9.15906810760498
2025-12-09 13:10:42.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009998447758365002 Training loss: 8.962761878967285
2025-12-09 13:10:42.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009998369180404282 Training loss: 9.06021785736084
2025-12-09 13:10:43.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00999828866267407 Training loss: 9.00865650177002
2025-12-09 13:10:43.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.00999820620520561 Training loss: 9.5128755569458
2025-12-09 13:10:44.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009998121808030905 Training loss: 9.110533714294434
2025-12-09 13:10:44.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.009998035471182706 Training loss: 9.063050270080566
2025-12-09 13:10:44.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00999794719469452 Training loss: 9.205594062805176
2025-12-09 13:10:45.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.009997856978600603 Training loss: 9.057448387145996
2025-12-09 13:10:45.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009997764822935967 Training loss: 8.992290496826172
2025-12-09 13:10:45.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.009997670727736378 Training loss: 9.141718864440918
2025-12-09 13:10:46.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009997574693038351 Training loss: 8.743653297424316
2025-12-09 13:10:46.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009997476718879152 Training loss: 9.168517112731934
2025-12-09 13:10:47.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009997376805296809 Training loss: 9.171615600585938
2025-12-09 13:10:47.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009997274952330094 Training loss: 9.00473403930664
2025-12-09 13:10:47.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00999717116001853 Training loss: 9.16842269897461
2025-12-09 13:10:48.113 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009997065428402403 Training loss: 9.255667686462402
2025-12-09 13:10:48.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009996957757522741 Training loss: 8.989107131958008
2025-12-09 13:10:48.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009996848147421333 Training loss: 9.139580726623535
2025-12-09 13:10:49.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009996736598140715 Training loss: 9.340619087219238
2025-12-09 13:10:49.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.009996623109724174 Training loss: 9.258831977844238
2025-12-09 13:10:49.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.009996507682215754 Training loss: 9.044055938720703
2025-12-09 13:10:50.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.009996390315660254 Training loss: 9.445371627807617
2025-12-09 13:10:50.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.009996271010103216 Training loss: 9.774422645568848
2025-12-09 13:10:51.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.009996149765590946 Training loss: 9.391633033752441
2025-12-09 13:10:51.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00999602658217049 Training loss: 9.312009811401367
2025-12-09 13:10:51.825 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.009995901459889657 Training loss: 9.049168586730957
2025-12-09 13:10:52.195 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.009995774398797007 Training loss: 9.215801239013672
2025-12-09 13:10:52.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.009995645398941846 Training loss: 9.19323444366455
2025-12-09 13:10:52.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.009995514460374237 Training loss: 9.162668228149414
2025-12-09 13:10:53.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.009995381583144995 Training loss: 9.451207160949707
2025-12-09 13:10:53.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.009995246767305689 Training loss: 9.276504516601562
2025-12-09 13:10:54.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.009995110012908634 Training loss: 9.505694389343262
2025-12-09 13:10:54.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.009994971320006905 Training loss: 9.154692649841309
2025-12-09 13:10:54.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.009994830688654326 Training loss: 9.506285667419434
2025-12-09 13:10:55.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.009994688118905472 Training loss: 9.5762357711792
2025-12-09 13:10:55.537 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.009994543610815672 Training loss: 9.271133422851562
2025-12-09 13:10:55.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.009994397164441006 Training loss: 9.10254955291748
2025-12-09 13:10:56.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.009994248779838311 Training loss: 9.129833221435547
2025-12-09 13:10:56.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.009994098457065167 Training loss: 9.20254135131836
2025-12-09 13:10:57.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.009993946196179913 Training loss: 9.147496223449707
2025-12-09 13:10:57.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.009993791997241638 Training loss: 9.381064414978027
2025-12-09 13:10:57.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.009993635860310187 Training loss: 9.416991233825684
2025-12-09 13:10:58.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00999347778544615 Training loss: 9.257408142089844
2025-12-09 13:10:58.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.009993317772710874 Training loss: 9.388641357421875
2025-12-09 13:10:58.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.009993155822166457 Training loss: 9.28558349609375
2025-12-09 13:10:59.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.009992991933875747 Training loss: 9.064892768859863
2025-12-09 13:10:59.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.009992826107902348 Training loss: 9.66897201538086
2025-12-09 13:10:59.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.009992658344310614 Training loss: 9.231343269348145
2025-12-09 13:11:00.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00999248864316565 Training loss: 9.425575256347656
2025-12-09 13:11:00.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.009992317004533314 Training loss: 9.230413436889648
2025-12-09 13:11:01.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.009992143428480213 Training loss: 9.229246139526367
2025-12-09 13:11:01.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.009991967915073714 Training loss: 9.465941429138184
2025-12-09 13:11:01.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.009991790464381926 Training loss: 9.250617980957031
2025-12-09 13:11:02.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.009991611076473714 Training loss: 9.207152366638184
2025-12-09 13:11:02.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.009991429751418698 Training loss: 9.343293190002441
2025-12-09 13:11:02.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.009991246489287245 Training loss: 9.202630043029785
2025-12-09 13:11:03.337 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.009991061290150474 Training loss: 9.248703956604004
2025-12-09 13:11:03.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.009990874154080258 Training loss: 9.102326393127441
2025-12-09 13:11:04.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.009990685081149222 Training loss: 9.319246292114258
2025-12-09 13:11:04.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.009990494071430742 Training loss: 9.206050872802734
2025-12-09 13:11:04.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.009990301124998944 Training loss: 9.317939758300781
2025-12-09 13:11:05.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.009990106241928705 Training loss: 9.32448673248291
2025-12-09 13:11:05.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.009989909422295658 Training loss: 9.136221885681152
2025-12-09 13:11:05.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.009989710666176184 Training loss: 9.349414825439453
2025-12-09 13:11:06.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.009989509973647417 Training loss: 9.48200798034668
2025-12-09 13:11:06.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.009989307344787242 Training loss: 9.279471397399902
2025-12-09 13:11:07.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.009989102779674294 Training loss: 9.203923225402832
2025-12-09 13:11:07.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00998889627838796 Training loss: 9.187878608703613
2025-12-09 13:11:07.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00998868784100838 Training loss: 9.710264205932617
2025-12-09 13:11:08.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.009988477467616446 Training loss: 8.979211807250977
2025-12-09 13:11:08.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0099882651582938 Training loss: 9.248551368713379
2025-12-09 13:11:08.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.009988050913122831 Training loss: 9.227609634399414
2025-12-09 13:11:09.296 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.009987834732186687 Training loss: 9.125322341918945
2025-12-09 13:11:09.668 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.009987616615569263 Training loss: 9.361985206604004
2025-12-09 13:11:10.039 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.009987396563355204 Training loss: 9.011301040649414
2025-12-09 13:11:10.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.009987174575629911 Training loss: 9.136917114257812
2025-12-09 13:11:10.783 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.009986950652479532 Training loss: 9.456768035888672
2025-12-09 13:11:11.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.009986724793990967 Training loss: 9.12902545928955
2025-12-09 13:11:11.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.009986497000251867 Training loss: 8.99203872680664
2025-12-09 13:11:11.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.009986267271350633 Training loss: 9.383419036865234
2025-12-09 13:11:12.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.00998603560737642 Training loss: 9.149850845336914
2025-12-09 13:11:12.638 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.009985802008419132 Training loss: 9.124327659606934
2025-12-09 13:11:13.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.009985566474569425 Training loss: 8.824821472167969
2025-12-09 13:11:13.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.009985329005918702 Training loss: 8.778244018554688
2025-12-09 13:11:13.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.009985089602559125 Training loss: 8.954407691955566
2025-12-09 13:11:14.129 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.009984848264583597 Training loss: 9.049784660339355
2025-12-09 13:11:14.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00998460499208578 Training loss: 9.6630220413208
2025-12-09 13:11:14.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00998435978516008 Training loss: 9.067483901977539
2025-12-09 13:11:15.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00998411264390166 Training loss: 8.973881721496582
2025-12-09 13:11:15.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.009983863568406429 Training loss: 8.93305492401123
2025-12-09 13:11:15.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.009983612558771048 Training loss: 8.682831764221191
2025-12-09 13:11:16.358 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00998335961509293 Training loss: 9.116209983825684
2025-12-09 13:11:16.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.009983104737470239 Training loss: 9.134395599365234
2025-12-09 13:11:17.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.009982847926001886 Training loss: 8.898098945617676
2025-12-09 13:11:17.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.009982589180787534 Training loss: 8.746655464172363
2025-12-09 13:11:17.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.009982328501927597 Training loss: 8.704415321350098
2025-12-09 13:11:18.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.009982065889523242 Training loss: 8.440322875976562
2025-12-09 13:11:18.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00998180134367638 Training loss: 8.803951263427734
2025-12-09 13:11:18.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.009981534864489678 Training loss: 8.493977546691895
2025-12-09 13:11:19.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.009981266452066553 Training loss: 8.945446014404297
2025-12-09 13:11:19.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.009980996106511169 Training loss: 8.737678527832031
2025-12-09 13:11:20.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.009980723827928441 Training loss: 8.885041236877441
2025-12-09 13:11:20.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.009980449616424037 Training loss: 9.442167282104492
2025-12-09 13:11:20.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00998017347210437 Training loss: 8.369009971618652
2025-12-09 13:11:21.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.009979895395076608 Training loss: 8.663230895996094
2025-12-09 13:11:21.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.009979615385448668 Training loss: 8.584721565246582
2025-12-09 13:11:21.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.009979333443329217 Training loss: 8.721433639526367
2025-12-09 13:11:22.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00997904956882767 Training loss: 8.675638198852539
2025-12-09 13:11:22.695 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.009978763762054192 Training loss: 8.830130577087402
2025-12-09 13:11:23.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0099784760231197 Training loss: 8.295258522033691
2025-12-09 13:11:23.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.00997818635213586 Training loss: 8.78605842590332
2025-12-09 13:11:23.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.009977894749215089 Training loss: 8.452340126037598
2025-12-09 13:11:24.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00997760121447055 Training loss: 8.705872535705566
2025-12-09 13:11:24.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.009977305748016158 Training loss: 8.484517097473145
2025-12-09 13:11:24.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00997700834996658 Training loss: 8.429856300354004
2025-12-09 13:11:25.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.009976709020437229 Training loss: 8.325092315673828
2025-12-09 13:11:25.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.00997640775954427 Training loss: 8.702852249145508
2025-12-09 13:11:26.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.009976104567404616 Training loss: 8.586064338684082
2025-12-09 13:11:26.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.009975799444135928 Training loss: 8.497361183166504
2025-12-09 13:11:26.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.009975492389856622 Training loss: 8.758792877197266
2025-12-09 13:11:27.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.009975183404685856 Training loss: 8.998326301574707
2025-12-09 13:11:27.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.009974872488743543 Training loss: 8.773110389709473
2025-12-09 13:11:27.912 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.009974559642150344 Training loss: 8.616877555847168
2025-12-09 13:11:28.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.009974244865027668 Training loss: 8.608402252197266
2025-12-09 13:11:28.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.009973928157497673 Training loss: 8.34932804107666
2025-12-09 13:11:29.028 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.009973609519683268 Training loss: 8.427875518798828
2025-12-09 13:11:29.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.009973288951708112 Training loss: 8.481014251708984
2025-12-09 13:11:29.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.009972966453696608 Training loss: 8.842296600341797
2025-12-09 13:11:30.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.009972642025773911 Training loss: 8.459600448608398
2025-12-09 13:11:30.514 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.009972315668065928 Training loss: 8.44632625579834
2025-12-09 13:11:30.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00997198738069931 Training loss: 8.587074279785156
2025-12-09 13:11:31.264 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.009971657163801459 Training loss: 8.660039901733398
2025-12-09 13:11:31.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.009971325017500525 Training loss: 8.451679229736328
2025-12-09 13:11:32.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00997099094192541 Training loss: 8.848788261413574
2025-12-09 13:11:32.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.009970654937205762 Training loss: 8.409294128417969
2025-12-09 13:11:32.755 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.009970317003471976 Training loss: 9.017101287841797
2025-12-09 13:11:33.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.009969977140855197 Training loss: 9.010394096374512
2025-12-09 13:11:33.499 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.009969635349487322 Training loss: 9.133064270019531
2025-12-09 13:11:33.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.009969291629500991 Training loss: 8.728806495666504
2025-12-09 13:11:34.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.009968945981029596 Training loss: 8.698768615722656
2025-12-09 13:11:34.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.009968598404207276 Training loss: 8.345735549926758
2025-12-09 13:11:34.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.009968248899168919 Training loss: 8.208115577697754
2025-12-09 13:11:35.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.00996789746605016 Training loss: 8.694747924804688
2025-12-09 13:11:35.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.009967544104987387 Training loss: 9.647924423217773
2025-12-09 13:11:36.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.009967188816117727 Training loss: 8.48486614227295
2025-12-09 13:11:36.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.009966831599579066 Training loss: 8.853681564331055
2025-12-09 13:11:36.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00996647245551003 Training loss: 8.407713890075684
2025-12-09 13:11:37.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.009966111384049996 Training loss: 8.982681274414062
2025-12-09 13:11:37.598 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.009965748385339089 Training loss: 8.454930305480957
2025-12-09 13:11:37.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.00996538345951818 Training loss: 8.541191101074219
2025-12-09 13:11:38.342 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.009965016606728895 Training loss: 8.238876342773438
2025-12-09 13:11:38.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.009964647827113595 Training loss: 9.06275463104248
2025-12-09 13:11:39.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.009964277120815402 Training loss: 8.447680473327637
2025-12-09 13:11:39.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.009963904487978178 Training loss: 8.189151763916016
2025-12-09 13:11:39.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.009963529928746533 Training loss: 8.35023021697998
2025-12-09 13:11:40.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.009963153443265827 Training loss: 8.363040924072266
2025-12-09 13:11:40.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.00996277503168217 Training loss: 8.640978813171387
2025-12-09 13:11:40.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00996239469414241 Training loss: 8.97992992401123
2025-12-09 13:11:41.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.009962012430794153 Training loss: 8.370939254760742
2025-12-09 13:11:41.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.009961628241785746 Training loss: 8.306798934936523
2025-12-09 13:11:42.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.009961242127266288 Training loss: 8.52563190460205
2025-12-09 13:11:42.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.009960854087385618 Training loss: 8.516481399536133
2025-12-09 13:11:42.815 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.00996046412229433 Training loss: 8.028878211975098
2025-12-09 13:11:43.186 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.009960072232143761 Training loss: 8.248695373535156
2025-12-09 13:11:43.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.009959678417085997 Training loss: 8.775650024414062
2025-12-09 13:11:43.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.009959282677273869 Training loss: 8.54509162902832
2025-12-09 13:11:44.310 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.009958885012860954 Training loss: 8.414705276489258
2025-12-09 13:11:44.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.009958485424001582 Training loss: 8.313929557800293
2025-12-09 13:11:45.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.009958083910850821 Training loss: 8.201604843139648
2025-12-09 13:11:45.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.009957680473564495 Training loss: 8.642755508422852
2025-12-09 13:11:45.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.009957275112299165 Training loss: 8.306745529174805
2025-12-09 13:11:46.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.009956867827212149 Training loss: 8.59276008605957
2025-12-09 13:11:46.544 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.009956458618461502 Training loss: 8.618143081665039
2025-12-09 13:11:46.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.009956047486206033 Training loss: 8.472160339355469
2025-12-09 13:11:47.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.00995563443060529 Training loss: 8.40075969696045
2025-12-09 13:11:47.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00995521945181958 Training loss: 8.270065307617188
2025-12-09 13:11:48.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00995480255000994 Training loss: 8.5399169921875
2025-12-09 13:11:48.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.009954383725338167 Training loss: 8.221206665039062
2025-12-09 13:11:48.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.009953962977966795 Training loss: 8.298680305480957
2025-12-09 13:11:49.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00995354030805911 Training loss: 8.584012985229492
2025-12-09 13:11:49.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.009953115715779141 Training loss: 7.982574462890625
2025-12-09 13:11:49.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.009952689201291663 Training loss: 8.419824600219727
2025-12-09 13:11:50.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0099522607647622 Training loss: 8.394527435302734
2025-12-09 13:11:50.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00995183040635702 Training loss: 8.649023056030273
2025-12-09 13:11:51.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.009951398126243134 Training loss: 8.00965690612793
2025-12-09 13:11:51.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.009950963924588304 Training loss: 8.119402885437012
2025-12-09 13:11:51.759 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.009950527801561034 Training loss: 8.327729225158691
2025-12-09 13:11:52.130 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.009950089757330574 Training loss: 8.099409103393555
2025-12-09 13:11:52.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.009949649792066922 Training loss: 8.256771087646484
2025-12-09 13:11:52.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00994920790594082 Training loss: 8.579849243164062
2025-12-09 13:11:53.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.009948764099123755 Training loss: 8.185922622680664
2025-12-09 13:11:53.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.00994831837178796 Training loss: 8.218714714050293
2025-12-09 13:11:53.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.009947870724106411 Training loss: 8.206968307495117
2025-12-09 13:11:54.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.009947421156252837 Training loss: 8.361903190612793
2025-12-09 13:11:54.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.009946969668401697 Training loss: 8.283296585083008
2025-12-09 13:11:55.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.009946516260728214 Training loss: 8.406719207763672
2025-12-09 13:11:55.488 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.009946060933408342 Training loss: 8.516447067260742
2025-12-09 13:11:55.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.009945603686618785 Training loss: 8.441115379333496
2025-12-09 13:11:56.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.009945144520536991 Training loss: 8.477124214172363
2025-12-09 13:11:56.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.009944683435341155 Training loss: 8.526995658874512
2025-12-09 13:11:56.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.009944220431210215 Training loss: 7.94723653793335
2025-12-09 13:11:57.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.009943755508323854 Training loss: 8.161864280700684
2025-12-09 13:11:57.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.009943288666862497 Training loss: 8.378137588500977
2025-12-09 13:11:58.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.00994281990700732 Training loss: 8.186347007751465
2025-12-09 13:11:58.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.009942349228940238 Training loss: 8.155170440673828
2025-12-09 13:11:58.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00994187663284391 Training loss: 8.494901657104492
2025-12-09 13:11:59.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.009941402118901743 Training loss: 8.175789833068848
2025-12-09 13:11:59.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.009940925687297887 Training loss: 8.283684730529785
2025-12-09 13:11:59.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.009940447338217234 Training loss: 8.555160522460938
2025-12-09 13:12:00.334 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.009939967071845425 Training loss: 8.6202974319458
2025-12-09 13:12:00.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.009939484888368837 Training loss: 8.716303825378418
2025-12-09 13:12:01.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.009939000787974602 Training loss: 8.574373245239258
2025-12-09 13:12:01.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.009938514770850585 Training loss: 8.555651664733887
2025-12-09 13:12:01.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.009938026837185403 Training loss: 8.262391090393066
2025-12-09 13:12:02.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.009937536987168413 Training loss: 8.800002098083496
2025-12-09 13:12:02.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.009937045220989716 Training loss: 8.476533889770508
2025-12-09 13:12:02.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.009936551538840153 Training loss: 9.132368087768555
2025-12-09 13:12:03.320 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.009936055940911319 Training loss: 8.66287899017334
2025-12-09 13:12:03.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.009935558427395541 Training loss: 9.182419776916504
2025-12-09 13:12:04.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.009935058998485898 Training loss: 9.118325233459473
2025-12-09 13:12:04.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.009934557654376204 Training loss: 9.24512767791748
2025-12-09 13:12:04.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.009934054395261025 Training loss: 9.281805992126465
2025-12-09 13:12:05.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.009933549221335665 Training loss: 8.727169036865234
2025-12-09 13:12:05.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.009933042132796171 Training loss: 8.75688362121582
2025-12-09 13:12:05.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.009932533129839334 Training loss: 8.12627124786377
2025-12-09 13:12:06.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00993202221266269 Training loss: 10.00216293334961
2025-12-09 13:12:06.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.009931509381464514 Training loss: 9.192999839782715
2025-12-09 13:12:07.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.009930994636443828 Training loss: 9.576756477355957
2025-12-09 13:12:07.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.009930477977800391 Training loss: 11.195716857910156
2025-12-09 13:12:07.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.009929959405734712 Training loss: 10.858930587768555
2025-12-09 13:12:08.164 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.009929438920448038 Training loss: 10.692230224609375
2025-12-09 13:12:08.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.009928916522142357 Training loss: 11.614816665649414
2025-12-09 13:12:08.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0099283922110204 Training loss: 11.399797439575195
2025-12-09 13:12:09.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.009927865987285648 Training loss: 10.950042724609375
2025-12-09 13:12:09.646 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.009927337851142314 Training loss: 10.989275932312012
2025-12-09 13:12:10.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.009926807802795359 Training loss: 10.476880073547363
2025-12-09 13:12:10.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.009926275842450481 Training loss: 11.036757469177246
2025-12-09 13:12:10.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.009925741970314128 Training loss: 10.912335395812988
2025-12-09 13:12:11.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.009925206186593483 Training loss: 10.780916213989258
2025-12-09 13:12:11.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.009924668491496473 Training loss: 10.757296562194824
2025-12-09 13:12:11.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.009924128885231769 Training loss: 10.55020523071289
2025-12-09 13:12:12.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.009923587368008779 Training loss: 10.776464462280273
2025-12-09 13:12:12.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.009923043940037657 Training loss: 10.594496726989746
2025-12-09 13:12:12.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.009922498601529295 Training loss: 10.425519943237305
2025-12-09 13:12:13.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.00992195135269533 Training loss: 10.498388290405273
2025-12-09 13:12:13.722 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.009921402193748138 Training loss: 9.945602416992188
2025-12-09 13:12:14.093 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.009920851124900838 Training loss: 10.18868350982666
2025-12-09 13:12:14.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.009920298146367286 Training loss: 10.269847869873047
2025-12-09 13:12:14.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.009919743258362085 Training loss: 10.082464218139648
2025-12-09 13:12:15.204 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.009919186461100576 Training loss: 9.994592666625977
2025-12-09 13:12:15.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.009918627754798839 Training loss: 10.0178804397583
2025-12-09 13:12:15.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0099180671396737 Training loss: 9.868131637573242
2025-12-09 13:12:16.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.009917504615942721 Training loss: 9.955007553100586
2025-12-09 13:12:16.687 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.009916940183824205 Training loss: 9.844023704528809
2025-12-09 13:12:17.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.009916373843537201 Training loss: 10.276814460754395
2025-12-09 13:12:17.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.009915805595301492 Training loss: 9.689563751220703
2025-12-09 13:12:17.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.009915235439337602 Training loss: 9.812110900878906
2025-12-09 13:12:18.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.009914663375866804 Training loss: 9.785568237304688
2025-12-09 13:12:18.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.009914089405111097 Training loss: 9.596806526184082
2025-12-09 13:12:18.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.009913513527293234 Training loss: 9.283040046691895
2025-12-09 13:12:19.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.009912935742636698 Training loss: 9.50669002532959
2025-12-09 13:12:19.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.009912356051365718 Training loss: 9.451751708984375
2025-12-09 13:12:20.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.009911774453705257 Training loss: 9.268180847167969
2025-12-09 13:12:20.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00991119094988103 Training loss: 9.408090591430664
2025-12-09 13:12:20.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.009910605540119475 Training loss: 9.336223602294922
2025-12-09 13:12:21.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.009910018224647781 Training loss: 9.123523712158203
2025-12-09 13:12:21.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.009909429003693876 Training loss: 8.598420143127441
2025-12-09 13:12:21.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.009908837877486422 Training loss: 9.36140251159668
2025-12-09 13:12:22.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.009908244846254825 Training loss: 8.832862854003906
2025-12-09 13:12:22.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.009907649910229228 Training loss: 9.283405303955078
2025-12-09 13:12:22.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.009907053069640516 Training loss: 9.40083122253418
2025-12-09 13:12:23.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.009906454324720308 Training loss: 8.802804946899414
2025-12-09 13:12:23.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.009905853675700968 Training loss: 9.267644882202148
2025-12-09 13:12:24.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.009905251122815597 Training loss: 9.101616859436035
2025-12-09 13:12:24.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00990464666629803 Training loss: 8.975057601928711
2025-12-09 13:12:24.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.009904040306382847 Training loss: 11.228934288024902
2025-12-09 13:12:25.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.009903432043305365 Training loss: 9.983576774597168
2025-12-09 13:12:25.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.009902821877301638 Training loss: 10.314884185791016
2025-12-09 13:12:25.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00990220980860846 Training loss: 10.551383972167969
2025-12-09 13:12:26.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.009901595837463363 Training loss: 10.889707565307617
2025-12-09 13:12:26.688 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.009900979964104618 Training loss: 10.912153244018555
2025-12-09 13:12:27.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00990036218877123 Training loss: 10.87401294708252
2025-12-09 13:12:27.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.00989974251170295 Training loss: 10.780677795410156
2025-12-09 13:12:27.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00989912093314026 Training loss: 10.697880744934082
2025-12-09 13:12:28.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.009898497453324384 Training loss: 10.30872631072998
2025-12-09 13:12:28.539 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.009897872072497281 Training loss: 10.212142944335938
2025-12-09 13:12:28.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.00989724479090165 Training loss: 9.743542671203613
2025-12-09 13:12:29.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.009896615608780924 Training loss: 9.550501823425293
2025-12-09 13:12:29.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.00989598452637928 Training loss: 9.993632316589355
2025-12-09 13:12:30.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.009895351543941628 Training loss: 9.342411041259766
2025-12-09 13:12:30.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.009894716661713616 Training loss: 9.236868858337402
2025-12-09 13:12:30.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.009894079879941628 Training loss: 9.467195510864258
2025-12-09 13:12:31.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.009893441198872787 Training loss: 9.679627418518066
2025-12-09 13:12:31.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.009892800618754954 Training loss: 9.244816780090332
2025-12-09 13:12:31.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.009892158139836724 Training loss: 9.490766525268555
2025-12-09 13:12:32.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.00989151376236743 Training loss: 9.398930549621582
2025-12-09 13:12:32.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.009890867486597146 Training loss: 9.399369239807129
2025-12-09 13:12:32.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.009890219312776677 Training loss: 9.16834831237793
2025-12-09 13:12:33.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.009889569241157564 Training loss: 9.232344627380371
2025-12-09 13:12:33.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.00988891727199209 Training loss: 8.998865127563477
2025-12-09 13:12:34.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.009888263405533271 Training loss: 9.365270614624023
2025-12-09 13:12:34.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.009887607642034859 Training loss: 8.908317565917969
2025-12-09 13:12:34.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.009886949981751346 Training loss: 9.354024887084961
2025-12-09 13:12:35.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.009886290424937952 Training loss: 8.967437744140625
2025-12-09 13:12:35.583 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.009885628971850642 Training loss: 8.285024642944336
2025-12-09 13:12:35.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.009884965622746112 Training loss: 9.672759056091309
2025-12-09 13:12:36.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.009884300377881794 Training loss: 9.889303207397461
2025-12-09 13:12:36.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.009883633237515857 Training loss: 9.03799819946289
2025-12-09 13:12:37.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.009882964201907207 Training loss: 8.991358757019043
2025-12-09 13:12:37.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.00988229327131548 Training loss: 9.509421348571777
2025-12-09 13:12:37.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.009881620446001056 Training loss: 9.693193435668945
2025-12-09 13:12:38.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.00988094572622504 Training loss: 9.631263732910156
2025-12-09 13:12:38.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.00988026911224928 Training loss: 9.652572631835938
2025-12-09 13:12:38.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.00987959060433636 Training loss: 9.507336616516113
2025-12-09 13:12:39.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.009878910202749589 Training loss: 9.509023666381836
2025-12-09 13:12:39.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.009878227907753022 Training loss: 9.564977645874023
2025-12-09 13:12:40.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.009877543719611444 Training loss: 9.412973403930664
2025-12-09 13:12:40.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.009876857638590373 Training loss: 9.130678176879883
2025-12-09 13:12:40.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.009876169664956067 Training loss: 9.737207412719727
2025-12-09 13:12:41.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.009875479798975512 Training loss: 9.621660232543945
2025-12-09 13:12:41.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.009874788040916432 Training loss: 9.181953430175781
2025-12-09 13:12:41.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.009874094391047288 Training loss: 9.128803253173828
2025-12-09 13:12:42.259 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.009873398849637267 Training loss: 8.855826377868652
2025-12-09 13:12:42.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.009872701416956299 Training loss: 9.447500228881836
2025-12-09 13:12:43.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.009872002093275042 Training loss: 8.828648567199707
2025-12-09 13:12:43.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.00987130087886489 Training loss: 9.031810760498047
2025-12-09 13:12:43.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.009870597773997972 Training loss: 8.869590759277344
2025-12-09 13:12:44.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.009869892778947148 Training loss: 9.020797729492188
2025-12-09 13:12:44.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.009869185893986013 Training loss: 8.832709312438965
2025-12-09 13:12:44.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.009868477119388895 Training loss: 8.898728370666504
2025-12-09 13:12:45.224 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.009867766455430856 Training loss: 8.907358169555664
2025-12-09 13:12:45.594 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.009867053902387693 Training loss: 8.947744369506836
2025-12-09 13:12:45.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.009866339460535929 Training loss: 8.623112678527832
2025-12-09 13:12:46.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.009865623130152828 Training loss: 8.9430570602417
2025-12-09 13:12:46.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.009864904911516384 Training loss: 8.807302474975586
2025-12-09 13:12:47.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.009864184804905323 Training loss: 8.637237548828125
2025-12-09 13:12:47.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.009863462810599103 Training loss: 8.855793952941895
2025-12-09 13:12:47.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.009862738928877922 Training loss: 8.75036334991455
2025-12-09 13:12:48.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.009862013160022696 Training loss: 8.907853126525879
2025-12-09 13:12:48.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.009861285504315085 Training loss: 8.478752136230469
2025-12-09 13:12:48.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00986055596203748 Training loss: 8.840555191040039
2025-12-09 13:12:49.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.009859824533472998 Training loss: 8.79007625579834
2025-12-09 13:12:49.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.009859091218905498 Training loss: 8.997928619384766
2025-12-09 13:12:50.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.00985835601861956 Training loss: 8.578655242919922
2025-12-09 13:12:50.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.009857618932900504 Training loss: 8.647725105285645
2025-12-09 13:12:50.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.009856879962034375 Training loss: 8.811426162719727
2025-12-09 13:12:51.165 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.009856139106307955 Training loss: 8.84260082244873
2025-12-09 13:12:51.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.009855396366008757 Training loss: 8.546354293823242
2025-12-09 13:12:51.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.009854651741425023 Training loss: 8.650622367858887
2025-12-09 13:12:52.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.009853905232845728 Training loss: 8.44255542755127
2025-12-09 13:12:52.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.009853156840560576 Training loss: 8.5891752243042
2025-12-09 13:12:53.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.009852406564860004 Training loss: 9.106389045715332
2025-12-09 13:12:53.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.009851654406035179 Training loss: 8.60027027130127
2025-12-09 13:12:53.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.009850900364378 Training loss: 8.739046096801758
2025-12-09 13:12:54.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.009850144440181096 Training loss: 8.79409122467041
2025-12-09 13:12:54.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.009849386633737824 Training loss: 8.706872940063477
2025-12-09 13:12:54.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.009848626945342278 Training loss: 8.616816520690918
2025-12-09 13:12:55.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.009847865375289276 Training loss: 8.915752410888672
2025-12-09 13:12:55.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.009847101923874366 Training loss: 9.689776420593262
2025-12-09 13:12:55.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.009846336591393832 Training loss: 8.642672538757324
2025-12-09 13:12:56.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.009845569378144686 Training loss: 8.382413864135742
2025-12-09 13:12:56.737 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.009844800284424663 Training loss: 8.594144821166992
2025-12-09 13:12:57.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.00984402931053224 Training loss: 8.912498474121094
2025-12-09 13:12:57.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.009843256456766609 Training loss: 9.035158157348633
2025-12-09 13:12:57.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.009842481723427705 Training loss: 8.373581886291504
2025-12-09 13:12:58.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.009841705110816185 Training loss: 8.484698295593262
2025-12-09 13:12:58.596 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00984092661923344 Training loss: 8.265260696411133
2025-12-09 13:12:58.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.009840146248981585 Training loss: 8.380967140197754
2025-12-09 13:12:59.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.009839364000363466 Training loss: 8.261665344238281
2025-12-09 13:12:59.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00983857987368266 Training loss: 8.309547424316406
2025-12-09 13:13:00.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.009837793869243468 Training loss: 8.784241676330566
2025-12-09 13:13:00.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.009837005987350926 Training loss: 8.65254020690918
2025-12-09 13:13:00.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.009836216228310797 Training loss: 8.490094184875488
2025-12-09 13:13:01.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.009835424592429568 Training loss: 8.446672439575195
2025-12-09 13:13:01.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.009834631080014457 Training loss: 8.399996757507324
2025-12-09 13:13:01.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.009833835691373412 Training loss: 8.289497375488281
2025-12-09 13:13:02.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.00983303842681511 Training loss: 8.276774406433105
2025-12-09 13:13:02.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.009832239286648949 Training loss: 8.429899215698242
2025-12-09 13:13:03.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.009831438271185065 Training loss: 8.970438957214355
2025-12-09 13:13:03.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.009830635380734313 Training loss: 8.503022193908691
2025-12-09 13:13:03.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.009829830615608279 Training loss: 8.485600471496582
2025-12-09 13:13:04.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.009829023976119278 Training loss: 8.739173889160156
2025-12-09 13:13:04.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.009828215462580352 Training loss: 8.26645278930664
2025-12-09 13:13:04.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.009827405075305266 Training loss: 8.179166793823242
2025-12-09 13:13:05.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.009826592814608518 Training loss: 8.674980163574219
2025-12-09 13:13:05.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.00982577868080533 Training loss: 8.457673072814941
2025-12-09 13:13:06.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.009824962674211653 Training loss: 8.31863784790039
2025-12-09 13:13:06.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.009824144795144159 Training loss: 8.571975708007812
2025-12-09 13:13:06.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.009823325043920255 Training loss: 8.27828311920166
2025-12-09 13:13:07.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.009822503420858067 Training loss: 8.373564720153809
2025-12-09 13:13:07.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.009821679926276456 Training loss: 8.389744758605957
2025-12-09 13:13:07.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.009820854560494998 Training loss: 8.313326835632324
2025-12-09 13:13:08.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.009820027323834007 Training loss: 8.258461952209473
2025-12-09 13:13:08.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.009819198216614512 Training loss: 8.569479942321777
2025-12-09 13:13:09.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.009818367239158278 Training loss: 8.24058723449707
2025-12-09 13:13:09.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.009817534391787787 Training loss: 8.935025215148926
2025-12-09 13:13:09.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.009816699674826256 Training loss: 8.44576358795166
2025-12-09 13:13:10.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.009815863088597618 Training loss: 8.018842697143555
2025-12-09 13:13:10.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.009815024633426537 Training loss: 8.517646789550781
2025-12-09 13:13:10.897 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0098141843096384 Training loss: 8.480903625488281
2025-12-09 13:13:11.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.009813342117559323 Training loss: 8.189178466796875
2025-12-09 13:13:11.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.009812498057516142 Training loss: 8.570281982421875
2025-12-09 13:13:12.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.009811652129836422 Training loss: 8.39298152923584
2025-12-09 13:13:12.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.009810804334848449 Training loss: 8.311617851257324
2025-12-09 13:13:12.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.009809954672881238 Training loss: 8.253241539001465
2025-12-09 13:13:13.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.009809103144264524 Training loss: 8.370970726013184
2025-12-09 13:13:13.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.009808249749328769 Training loss: 8.185665130615234
2025-12-09 13:13:13.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.009807394488405159 Training loss: 8.664266586303711
2025-12-09 13:13:14.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.009806537361825607 Training loss: 8.232905387878418
2025-12-09 13:13:14.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.009805678369922742 Training loss: 8.133962631225586
2025-12-09 13:13:14.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.009804817513029926 Training loss: 8.71971321105957
2025-12-09 13:13:15.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.009803954791481238 Training loss: 8.615044593811035
2025-12-09 13:13:15.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.009803090205611487 Training loss: 8.437287330627441
2025-12-09 13:13:16.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.009802223755756198 Training loss: 8.532258987426758
2025-12-09 13:13:16.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.009801355442251625 Training loss: 8.611583709716797
2025-12-09 13:13:16.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.009800485265434745 Training loss: 8.335166931152344
2025-12-09 13:13:17.232 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.009799613225643253 Training loss: 8.095807075500488
2025-12-09 13:13:17.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.009798739323215573 Training loss: 8.46304702758789
2025-12-09 13:13:17.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00979786355849085 Training loss: 8.91032886505127
2025-12-09 13:13:18.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.009796985931808949 Training loss: 8.512331008911133
2025-12-09 13:13:18.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.009796106443510462 Training loss: 8.331439018249512
2025-12-09 13:13:19.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.009795225093936702 Training loss: 8.08790111541748
2025-12-09 13:13:19.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.009794341883429699 Training loss: 8.892387390136719
2025-12-09 13:13:19.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.009793456812332214 Training loss: 8.35748291015625
2025-12-09 13:13:20.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.009792569880987725 Training loss: 9.566664695739746
2025-12-09 13:13:20.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.009791681089740432 Training loss: 8.459173202514648
2025-12-09 13:13:20.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.009790790438935257 Training loss: 8.368313789367676
2025-12-09 13:13:21.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.009789897928917846 Training loss: 8.102931022644043
2025-12-09 13:13:21.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.009789003560034561 Training loss: 8.804061889648438
2025-12-09 13:13:22.074 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.009788107332632493 Training loss: 8.671090126037598
2025-12-09 13:13:22.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.009787209247059453 Training loss: 8.699127197265625
2025-12-09 13:13:22.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.009786309303663962 Training loss: 8.703975677490234
2025-12-09 13:13:23.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.009785407502795277 Training loss: 8.796463012695312
2025-12-09 13:13:23.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.009784503844803368 Training loss: 9.036885261535645
2025-12-09 13:13:23.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.009783598330038924 Training loss: 9.253085136413574
2025-12-09 13:13:24.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.009782690958853361 Training loss: 8.857900619506836
2025-12-09 13:13:24.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.009781781731598813 Training loss: 8.981711387634277
2025-12-09 13:13:25.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.00978087064862813 Training loss: 9.062100410461426
2025-12-09 13:13:25.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.009779957710294886 Training loss: 8.562967300415039
2025-12-09 13:13:25.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.009779042916953376 Training loss: 8.822884559631348
2025-12-09 13:13:26.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.009778126268958612 Training loss: 8.801102638244629
2025-12-09 13:13:26.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.009777207766666329 Training loss: 8.132596015930176
2025-12-09 13:13:26.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.00977628741043298 Training loss: 8.514755249023438
2025-12-09 13:13:27.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.009775365200615735 Training loss: 8.195518493652344
2025-12-09 13:13:27.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.009774441137572488 Training loss: 8.347167015075684
2025-12-09 13:13:28.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.009773515221661847 Training loss: 8.142433166503906
2025-12-09 13:13:28.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.009772587453243142 Training loss: 8.586560249328613
2025-12-09 13:13:28.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.009771657832676426 Training loss: 8.619710922241211
2025-12-09 13:13:29.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.009770726360322463 Training loss: 8.274099349975586
2025-12-09 13:13:29.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.009769793036542742 Training loss: 8.768365859985352
2025-12-09 13:13:29.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.009768857861699462 Training loss: 8.473889350891113
2025-12-09 13:13:30.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.009767920836155552 Training loss: 8.229280471801758
2025-12-09 13:13:30.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.009766981960274652 Training loss: 8.446586608886719
2025-12-09 13:13:31.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.009766041234421121 Training loss: 8.168609619140625
2025-12-09 13:13:31.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.009765098658960036 Training loss: 8.30630874633789
2025-12-09 13:13:31.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.00976415423425719 Training loss: 8.156416893005371
2025-12-09 13:13:32.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0097632079606791 Training loss: 8.460888862609863
2025-12-09 13:13:32.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.009762259838592994 Training loss: 8.3751220703125
2025-12-09 13:13:32.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.009761309868366819 Training loss: 7.894808292388916
2025-12-09 13:13:33.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.009760358050369242 Training loss: 8.250213623046875
2025-12-09 13:13:33.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.009759404384969644 Training loss: 8.199503898620605
2025-12-09 13:13:33.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.009758448872538121 Training loss: 7.951406955718994
2025-12-09 13:13:34.367 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.009757491513445493 Training loss: 8.355525016784668
2025-12-09 13:13:34.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.009756532308063294 Training loss: 8.147793769836426
2025-12-09 13:13:35.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.009755571256763764 Training loss: 8.477519989013672
2025-12-09 13:13:35.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.009754608359919878 Training loss: 8.666414260864258
2025-12-09 13:13:35.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.009753643617905313 Training loss: 8.208948135375977
2025-12-09 13:13:36.237 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.009752677031094465 Training loss: 8.201213836669922
2025-12-09 13:13:36.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.009751708599862451 Training loss: 8.280441284179688
2025-12-09 13:13:36.982 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.009750738324585098 Training loss: 8.130416870117188
2025-12-09 13:13:37.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.009749766205638952 Training loss: 8.370251655578613
2025-12-09 13:13:37.726 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.009748792243401274 Training loss: 8.369912147521973
2025-12-09 13:13:38.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.009747816438250036 Training loss: 8.468066215515137
2025-12-09 13:13:38.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.009746838790563934 Training loss: 8.321732521057129
2025-12-09 13:13:38.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.009745859300722371 Training loss: 8.385282516479492
2025-12-09 13:13:39.217 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.009744877969105468 Training loss: 8.348361015319824
2025-12-09 13:13:39.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.009743894796094062 Training loss: 8.29699993133545
2025-12-09 13:13:39.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.009742909782069702 Training loss: 8.157179832458496
2025-12-09 13:13:40.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.009741922927414652 Training loss: 8.269774436950684
2025-12-09 13:13:40.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.009740934232511893 Training loss: 8.362128257751465
2025-12-09 13:13:41.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.009739943697745118 Training loss: 8.235089302062988
2025-12-09 13:13:41.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.009738951323498732 Training loss: 8.362451553344727
2025-12-09 13:13:41.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.009737957110157859 Training loss: 8.190342903137207
2025-12-09 13:13:42.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.009736961058108331 Training loss: 7.807857036590576
2025-12-09 13:13:42.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.009735963167736698 Training loss: 8.555281639099121
2025-12-09 13:13:42.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.009734963439430222 Training loss: 8.511462211608887
2025-12-09 13:13:43.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.009733961873576877 Training loss: 8.245444297790527
2025-12-09 13:13:43.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.009732958470565352 Training loss: 8.717205047607422
2025-12-09 13:13:44.062 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.009731953230785049 Training loss: 8.18218994140625
2025-12-09 13:13:44.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.009730946154626078 Training loss: 8.231352806091309
2025-12-09 13:13:44.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.00972993724247927 Training loss: 8.371774673461914
2025-12-09 13:13:45.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.009728926494736164 Training loss: 8.31093692779541
2025-12-09 13:13:45.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.009727913911789008 Training loss: 8.195709228515625
2025-12-09 13:13:45.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.009726899494030768 Training loss: 7.742800712585449
2025-12-09 13:13:46.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.009725883241855119 Training loss: 8.18171501159668
2025-12-09 13:13:46.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.009724865155656449 Training loss: 8.703834533691406
2025-12-09 13:13:47.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.009723845235829857 Training loss: 8.049294471740723
2025-12-09 13:13:47.416 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.009722823482771155 Training loss: 8.379953384399414
2025-12-09 13:13:47.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.009721799896876864 Training loss: 7.997731685638428
2025-12-09 13:13:48.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.009720774478544218 Training loss: 8.220396041870117
2025-12-09 13:13:48.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.009719747228171163 Training loss: 8.175371170043945
2025-12-09 13:13:48.907 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.009718718146156354 Training loss: 8.664199829101562
2025-12-09 13:13:49.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.00971768723289916 Training loss: 8.190731048583984
2025-12-09 13:13:49.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.009716654488799652 Training loss: 8.066977500915527
2025-12-09 13:13:50.031 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.009715619914258624 Training loss: 8.355950355529785
2025-12-09 13:13:50.403 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00971458350967757 Training loss: 8.226592063903809
2025-12-09 13:13:50.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.009713545275458703 Training loss: 8.264175415039062
2025-12-09 13:13:51.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.009712505212004938 Training loss: 8.057500839233398
2025-12-09 13:13:51.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.009711463319719903 Training loss: 7.9794816970825195
2025-12-09 13:13:51.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.009710419599007938 Training loss: 8.307188987731934
2025-12-09 13:13:52.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.009709374050274088 Training loss: 8.401161193847656
2025-12-09 13:13:52.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.009708326673924114 Training loss: 8.159701347351074
2025-12-09 13:13:53.006 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.009707277470364482 Training loss: 8.287335395812988
2025-12-09 13:13:53.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.009706226440002363 Training loss: 8.45374584197998
2025-12-09 13:13:53.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.009705173583245644 Training loss: 7.7779459953308105
2025-12-09 13:13:54.128 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.009704118900502918 Training loss: 8.233938217163086
2025-12-09 13:13:54.500 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.009703062392183489 Training loss: 8.03526782989502
2025-12-09 13:13:54.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.009702004058697363 Training loss: 8.232162475585938
2025-12-09 13:13:55.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.00970094390045526 Training loss: 8.40182113647461
2025-12-09 13:13:55.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00969988191786861 Training loss: 8.12759017944336
2025-12-09 13:13:55.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.009698818111349544 Training loss: 8.366401672363281
2025-12-09 13:13:56.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.009697752481310905 Training loss: 8.040838241577148
2025-12-09 13:13:56.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.009696685028166244 Training loss: 7.96970796585083
2025-12-09 13:13:57.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.00969561575232982 Training loss: 8.621082305908203
2025-12-09 13:13:57.476 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.009694544654216595 Training loss: 7.97683572769165
2025-12-09 13:13:57.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.009693471734242244 Training loss: 8.89610767364502
2025-12-09 13:13:58.225 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.009692396992823146 Training loss: 7.834351062774658
2025-12-09 13:13:58.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.009691320430376385 Training loss: 8.012181282043457
2025-12-09 13:13:58.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.009690242047319756 Training loss: 8.207509994506836
2025-12-09 13:13:59.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.009689161844071757 Training loss: 8.254684448242188
2025-12-09 13:13:59.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.009688079821051594 Training loss: 8.720407485961914
2025-12-09 13:14:00.084 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.009686995978679181 Training loss: 8.188003540039062
2025-12-09 13:14:00.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.009685910317375132 Training loss: 8.297990798950195
2025-12-09 13:14:00.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.009684822837560777 Training loss: 8.424456596374512
2025-12-09 13:14:01.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.00968373353965814 Training loss: 7.855988025665283
2025-12-09 13:14:01.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.009682642424089958 Training loss: 8.125341415405273
2025-12-09 13:14:01.948 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.009681549491279673 Training loss: 8.711525917053223
2025-12-09 13:14:02.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.00968045474165143 Training loss: 8.318947792053223
2025-12-09 13:14:02.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.00967935817563008 Training loss: 8.502359390258789
2025-12-09 13:14:03.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00967825979364118 Training loss: 8.19237232208252
2025-12-09 13:14:03.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.009677159596110986 Training loss: 8.260255813598633
2025-12-09 13:14:03.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.009676057583466471 Training loss: 8.56001091003418
2025-12-09 13:14:04.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.009674953756135297 Training loss: 8.619430541992188
2025-12-09 13:14:04.562 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.009673848114545842 Training loss: 8.401790618896484
2025-12-09 13:14:04.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.009672740659127184 Training loss: 8.23123550415039
2025-12-09 13:14:05.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.009671631390309103 Training loss: 7.939908981323242
2025-12-09 13:14:05.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.009670520308522083 Training loss: 8.415566444396973
2025-12-09 13:14:06.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.009669407414197318 Training loss: 8.27199935913086
2025-12-09 13:14:06.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.009668292707766698 Training loss: 8.562663078308105
2025-12-09 13:14:06.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.009667176189662818 Training loss: 8.313098907470703
2025-12-09 13:14:07.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.009666057860318978 Training loss: 8.138205528259277
2025-12-09 13:14:07.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00966493772016918 Training loss: 8.348596572875977
2025-12-09 13:14:07.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.009663815769648127 Training loss: 8.374784469604492
2025-12-09 13:14:08.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00966269200919123 Training loss: 8.60035228729248
2025-12-09 13:14:08.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.009661566439234593 Training loss: 8.816232681274414
2025-12-09 13:14:09.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.00966043906021503 Training loss: 8.767436027526855
2025-12-09 13:14:09.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.009659309872570057 Training loss: 8.198219299316406
2025-12-09 13:14:09.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.009658178876737887 Training loss: 8.359108924865723
2025-12-09 13:14:10.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.009657046073157436 Training loss: 8.452247619628906
2025-12-09 13:14:10.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.009655911462268327 Training loss: 8.706830978393555
2025-12-09 13:14:10.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.00965477504451088 Training loss: 9.064619064331055
2025-12-09 13:14:11.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.009653636820326113 Training loss: 8.912735939025879
2025-12-09 13:14:11.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.009652496790155752 Training loss: 10.392815589904785
2025-12-09 13:14:12.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.009651354954442217 Training loss: 10.87518310546875
2025-12-09 13:14:12.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.009650211313628636 Training loss: 9.797324180603027
2025-12-09 13:14:12.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.009649065868158831 Training loss: 9.711053848266602
2025-12-09 13:14:13.133 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.009647918618477328 Training loss: 9.110719680786133
2025-12-09 13:14:13.505 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.009646769565029354 Training loss: 9.509180068969727
2025-12-09 13:14:13.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00964561870826083 Training loss: 9.504183769226074
2025-12-09 13:14:14.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.009644466048618386 Training loss: 8.898429870605469
2025-12-09 13:14:14.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.009643311586549342 Training loss: 9.334185600280762
2025-12-09 13:14:14.989 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.009642155322501724 Training loss: 8.994682312011719
2025-12-09 13:14:15.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.009640997256924256 Training loss: 9.475881576538086
2025-12-09 13:14:15.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.009639837390266361 Training loss: 8.914874076843262
2025-12-09 13:14:16.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.009638675722978161 Training loss: 9.091996192932129
2025-12-09 13:14:16.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.009637512255510475 Training loss: 8.86036205291748
2025-12-09 13:14:16.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.009636346988314821 Training loss: 9.392401695251465
2025-12-09 13:14:17.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.009635179921843418 Training loss: 9.016768455505371
2025-12-09 13:14:17.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.009634011056549182 Training loss: 8.512210845947266
2025-12-09 13:14:17.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.009632840392885726 Training loss: 8.857441902160645
2025-12-09 13:14:18.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.009631667931307365 Training loss: 8.707261085510254
2025-12-09 13:14:18.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.009630493672269102 Training loss: 9.190818786621094
2025-12-09 13:14:19.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.009629317616226648 Training loss: 8.829736709594727
2025-12-09 13:14:19.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.009628139763636408 Training loss: 8.70205307006836
2025-12-09 13:14:19.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.009626960114955483 Training loss: 8.571283340454102
2025-12-09 13:14:20.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.009625778670641669 Training loss: 9.374959945678711
2025-12-09 13:14:20.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.009624595431153467 Training loss: 8.753549575805664
2025-12-09 13:14:20.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.009623410396950064 Training loss: 8.556015968322754
2025-12-09 13:14:21.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.009622223568491349 Training loss: 8.372405052185059
2025-12-09 13:14:21.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.00962103494623791 Training loss: 8.695683479309082
2025-12-09 13:14:22.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.009619844530651026 Training loss: 8.601668357849121
2025-12-09 13:14:22.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.009618652322192675 Training loss: 8.36759090423584
2025-12-09 13:14:22.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.00961745832132553 Training loss: 8.100297927856445
2025-12-09 13:14:23.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.009616262528512956 Training loss: 8.307289123535156
2025-12-09 13:14:23.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.009615064944219022 Training loss: 8.479369163513184
2025-12-09 13:14:23.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.009613865568908484 Training loss: 8.004989624023438
2025-12-09 13:14:24.273 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.009612664403046797 Training loss: 8.566488265991211
