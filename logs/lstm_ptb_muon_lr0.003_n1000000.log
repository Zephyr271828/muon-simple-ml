2025-12-09 12:05:10.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 3e-05 Training loss: 9.210151672363281
2025-12-09 12:05:10.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 6e-05 Training loss: 9.20954418182373
2025-12-09 12:05:11.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-05 Training loss: 9.21027946472168
2025-12-09 12:05:11.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.00012 Training loss: 9.210590362548828
2025-12-09 12:05:11.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.00015000000000000001 Training loss: 9.208683013916016
2025-12-09 12:05:11.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.00017999999999999998 Training loss: 9.20920467376709
2025-12-09 12:05:11.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.00021000000000000004 Training loss: 9.207852363586426
2025-12-09 12:05:11.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.00024 Training loss: 9.206955909729004
2025-12-09 12:05:11.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.00027 Training loss: 9.206451416015625
2025-12-09 12:05:11.135 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.00030000000000000003 Training loss: 9.204996109008789
2025-12-09 12:05:11.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.00033 Training loss: 9.202990531921387
2025-12-09 12:05:11.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.00035999999999999997 Training loss: 9.201594352722168
2025-12-09 12:05:11.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.00039000000000000005 Training loss: 9.199909210205078
2025-12-09 12:05:11.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.00042000000000000007 Training loss: 9.197595596313477
2025-12-09 12:05:11.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.00045 Training loss: 9.196330070495605
2025-12-09 12:05:11.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.00048 Training loss: 9.194538116455078
2025-12-09 12:05:11.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.00051 Training loss: 9.189723014831543
2025-12-09 12:05:11.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.00054 Training loss: 9.18807315826416
2025-12-09 12:05:11.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.00057 Training loss: 9.183195114135742
2025-12-09 12:05:11.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.0006000000000000001 Training loss: 9.181389808654785
2025-12-09 12:05:11.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.00063 Training loss: 9.178752899169922
2025-12-09 12:05:11.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.00066 Training loss: 9.175820350646973
2025-12-09 12:05:11.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0006900000000000001 Training loss: 9.170750617980957
2025-12-09 12:05:11.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0007199999999999999 Training loss: 9.167020797729492
2025-12-09 12:05:11.440 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.00075 Training loss: 9.16207504272461
2025-12-09 12:05:11.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0007800000000000001 Training loss: 9.156201362609863
2025-12-09 12:05:11.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0008100000000000001 Training loss: 9.152571678161621
2025-12-09 12:05:11.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0008400000000000001 Training loss: 9.14694881439209
2025-12-09 12:05:11.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.00087 Training loss: 9.139532089233398
2025-12-09 12:05:11.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.0009 Training loss: 9.138635635375977
2025-12-09 12:05:11.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.00093 Training loss: 9.130998611450195
2025-12-09 12:05:11.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.00096 Training loss: 9.123586654663086
2025-12-09 12:05:11.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.00099 Training loss: 9.11230182647705
2025-12-09 12:05:11.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.00102 Training loss: 9.111957550048828
2025-12-09 12:05:11.622 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00105 Training loss: 9.107166290283203
2025-12-09 12:05:11.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00108 Training loss: 9.087074279785156
2025-12-09 12:05:11.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00111 Training loss: 9.08061695098877
2025-12-09 12:05:11.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00114 Training loss: 9.082479476928711
2025-12-09 12:05:11.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.00117 Training loss: 9.05104923248291
2025-12-09 12:05:11.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.0012000000000000001 Training loss: 9.05200481414795
2025-12-09 12:05:11.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00123 Training loss: 9.047411918640137
2025-12-09 12:05:11.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00126 Training loss: 9.031377792358398
2025-12-09 12:05:11.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.00129 Training loss: 9.022709846496582
2025-12-09 12:05:11.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00132 Training loss: 9.00292682647705
2025-12-09 12:05:11.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.00135 Training loss: 8.994657516479492
2025-12-09 12:05:11.816 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0013800000000000002 Training loss: 8.968361854553223
2025-12-09 12:05:11.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00141 Training loss: 8.953018188476562
2025-12-09 12:05:11.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0014399999999999999 Training loss: 8.938533782958984
2025-12-09 12:05:11.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.00147 Training loss: 8.911102294921875
2025-12-09 12:05:11.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.0015 Training loss: 8.876639366149902
2025-12-09 12:05:11.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0015300000000000001 Training loss: 8.856507301330566
2025-12-09 12:05:11.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.0015600000000000002 Training loss: 8.854846954345703
2025-12-09 12:05:11.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.00159 Training loss: 8.810342788696289
2025-12-09 12:05:11.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0016200000000000001 Training loss: 8.805413246154785
2025-12-09 12:05:11.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0016500000000000002 Training loss: 8.779342651367188
2025-12-09 12:05:11.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.0016800000000000003 Training loss: 8.763955116271973
2025-12-09 12:05:12.014 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00171 Training loss: 8.711836814880371
2025-12-09 12:05:12.032 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00174 Training loss: 8.65783977508545
2025-12-09 12:05:12.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0017699999999999999 Training loss: 8.65771198272705
2025-12-09 12:05:12.068 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.0018 Training loss: 8.623629570007324
2025-12-09 12:05:12.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00183 Training loss: 8.561601638793945
2025-12-09 12:05:12.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.00186 Training loss: 8.541531562805176
2025-12-09 12:05:12.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00189 Training loss: 8.508464813232422
2025-12-09 12:05:12.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00192 Training loss: 8.439845085144043
2025-12-09 12:05:12.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.0019500000000000001 Training loss: 8.430208206176758
2025-12-09 12:05:12.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.00198 Training loss: 8.386890411376953
2025-12-09 12:05:12.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.00201 Training loss: 8.360276222229004
2025-12-09 12:05:12.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.00204 Training loss: 8.29568862915039
2025-12-09 12:05:12.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00207 Training loss: 8.253575325012207
2025-12-09 12:05:12.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.0021 Training loss: 8.188616752624512
2025-12-09 12:05:12.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00213 Training loss: 8.177837371826172
2025-12-09 12:05:12.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00216 Training loss: 8.052769660949707
2025-12-09 12:05:12.303 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00219 Training loss: 8.004264831542969
2025-12-09 12:05:12.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00222 Training loss: 7.97055196762085
2025-12-09 12:05:12.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0022500000000000003 Training loss: 7.948243141174316
2025-12-09 12:05:12.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00228 Training loss: 7.84149169921875
2025-12-09 12:05:12.374 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00231 Training loss: 7.7923479080200195
2025-12-09 12:05:12.392 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.00234 Training loss: 7.663700580596924
2025-12-09 12:05:12.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.00237 Training loss: 7.689327239990234
2025-12-09 12:05:12.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.0024000000000000002 Training loss: 7.6546454429626465
2025-12-09 12:05:12.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.0024300000000000003 Training loss: 7.697422027587891
2025-12-09 12:05:12.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00246 Training loss: 7.617502689361572
2025-12-09 12:05:12.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.00249 Training loss: 7.58635139465332
2025-12-09 12:05:12.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00252 Training loss: 7.52396297454834
2025-12-09 12:05:12.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00255 Training loss: 7.371897220611572
2025-12-09 12:05:12.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.00258 Training loss: 7.4474873542785645
2025-12-09 12:05:12.551 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.00261 Training loss: 7.40054988861084
2025-12-09 12:05:12.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00264 Training loss: 7.380024433135986
2025-12-09 12:05:12.587 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.00267 Training loss: 7.222132682800293
2025-12-09 12:05:12.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.0027 Training loss: 7.3335957527160645
2025-12-09 12:05:12.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0027300000000000002 Training loss: 7.249615669250488
2025-12-09 12:05:12.641 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0027600000000000003 Training loss: 7.237180233001709
2025-12-09 12:05:12.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.0027900000000000004 Training loss: 7.201862335205078
2025-12-09 12:05:12.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00282 Training loss: 7.160663604736328
2025-12-09 12:05:12.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.00285 Training loss: 7.132973670959473
2025-12-09 12:05:12.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0028799999999999997 Training loss: 7.047317981719971
2025-12-09 12:05:12.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00291 Training loss: 6.9712371826171875
2025-12-09 12:05:12.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.00294 Training loss: 6.941079616546631
2025-12-09 12:05:12.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00297 Training loss: 6.873414993286133
2025-12-09 12:05:12.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.003 Training loss: 7.040337085723877
2025-12-09 12:05:12.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.002909538931178863 Training loss: 6.847962856292725
2025-12-09 12:05:12.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.002649066664678467 Training loss: 6.885359764099121
2025-12-09 12:05:12.837 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0022500000000000003 Training loss: 6.808192729949951
2025-12-09 12:05:12.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0017604722665003959 Training loss: 6.868605136871338
2025-12-09 12:05:12.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0012395277334996046 Training loss: 6.795557498931885
2025-12-09 12:05:12.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0007500000000000003 Training loss: 6.821191787719727
2025-12-09 12:05:12.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0003509333353215332 Training loss: 6.735558986663818
2025-12-09 12:05:12.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113752e-05 Training loss: 6.662150859832764
2025-12-09 12:05:12.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 6.690925598144531
