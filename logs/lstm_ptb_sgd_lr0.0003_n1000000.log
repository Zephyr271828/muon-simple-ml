2025-12-09 12:05:55.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 9.213406562805176
2025-12-09 12:05:55.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 9.212957382202148
2025-12-09 12:05:55.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 9.212878227233887
2025-12-09 12:05:55.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 9.21306324005127
2025-12-09 12:05:55.947 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 9.21302604675293
2025-12-09 12:05:55.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 9.213062286376953
2025-12-09 12:05:55.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 9.213298797607422
2025-12-09 12:05:56.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 9.213385581970215
2025-12-09 12:05:56.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 9.213308334350586
2025-12-09 12:05:56.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 9.213101387023926
2025-12-09 12:05:56.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 9.213150024414062
2025-12-09 12:05:56.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 9.212858200073242
2025-12-09 12:05:56.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 9.213953018188477
2025-12-09 12:05:56.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 9.212505340576172
2025-12-09 12:05:56.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 9.212604522705078
2025-12-09 12:05:56.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 9.213565826416016
2025-12-09 12:05:56.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 9.212772369384766
2025-12-09 12:05:56.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 9.21329116821289
2025-12-09 12:05:56.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 9.21231460571289
2025-12-09 12:05:56.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 9.213364601135254
2025-12-09 12:05:56.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 9.212437629699707
2025-12-09 12:05:56.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 9.213123321533203
2025-12-09 12:05:56.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 9.212934494018555
2025-12-09 12:05:56.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 9.213221549987793
2025-12-09 12:05:56.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 9.212377548217773
2025-12-09 12:05:56.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 9.212800979614258
2025-12-09 12:05:56.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 9.212904930114746
2025-12-09 12:05:56.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 9.212611198425293
2025-12-09 12:05:56.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 9.212214469909668
2025-12-09 12:05:56.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 9.213440895080566
2025-12-09 12:05:56.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 9.212778091430664
2025-12-09 12:05:56.401 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 9.21281623840332
2025-12-09 12:05:56.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 9.212443351745605
2025-12-09 12:05:56.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 9.213152885437012
2025-12-09 12:05:56.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 9.212494850158691
2025-12-09 12:05:56.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 9.213357925415039
2025-12-09 12:05:56.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 9.213594436645508
2025-12-09 12:05:56.494 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 9.212959289550781
2025-12-09 12:05:56.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 9.212284088134766
2025-12-09 12:05:56.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 9.212980270385742
2025-12-09 12:05:56.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 9.213155746459961
2025-12-09 12:05:56.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 9.213152885437012
2025-12-09 12:05:56.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 9.212691307067871
2025-12-09 12:05:56.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 9.213240623474121
2025-12-09 12:05:56.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 9.212889671325684
2025-12-09 12:05:56.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 9.212604522705078
2025-12-09 12:05:56.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 9.212992668151855
2025-12-09 12:05:56.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 9.212678909301758
2025-12-09 12:05:56.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 9.212873458862305
2025-12-09 12:05:56.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 9.21241283416748
2025-12-09 12:05:56.696 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 9.213296890258789
2025-12-09 12:05:56.711 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 9.212596893310547
2025-12-09 12:05:56.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 9.212512016296387
2025-12-09 12:05:56.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 9.212469100952148
2025-12-09 12:05:56.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 9.212516784667969
2025-12-09 12:05:56.773 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 9.212868690490723
2025-12-09 12:05:56.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 9.211922645568848
2025-12-09 12:05:56.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 9.21279239654541
2025-12-09 12:05:56.818 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 9.212549209594727
2025-12-09 12:05:56.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 9.212141036987305
2025-12-09 12:05:56.849 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 9.21339225769043
2025-12-09 12:05:56.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 9.212337493896484
2025-12-09 12:05:56.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 9.212580680847168
2025-12-09 12:05:56.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 9.211901664733887
2025-12-09 12:05:56.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 9.211590766906738
2025-12-09 12:05:56.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 9.2122802734375
2025-12-09 12:05:56.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 9.212647438049316
2025-12-09 12:05:56.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 9.211946487426758
2025-12-09 12:05:56.972 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 9.21134090423584
2025-12-09 12:05:56.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 9.212160110473633
2025-12-09 12:05:57.003 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 9.211423873901367
2025-12-09 12:05:57.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 9.211854934692383
2025-12-09 12:05:57.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 9.211485862731934
2025-12-09 12:05:57.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 9.212435722351074
2025-12-09 12:05:57.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 9.21203327178955
2025-12-09 12:05:57.080 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 9.212258338928223
2025-12-09 12:05:57.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 9.211906433105469
2025-12-09 12:05:57.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 9.211371421813965
2025-12-09 12:05:57.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 9.211746215820312
2025-12-09 12:05:57.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 9.211563110351562
2025-12-09 12:05:57.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 9.211729049682617
2025-12-09 12:05:57.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 9.2110595703125
2025-12-09 12:05:57.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 9.211434364318848
2025-12-09 12:05:57.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 9.212080001831055
2025-12-09 12:05:57.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 9.211519241333008
2025-12-09 12:05:57.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 9.211006164550781
2025-12-09 12:05:57.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 9.211880683898926
2025-12-09 12:05:57.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 9.21142292022705
2025-12-09 12:05:57.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 9.211287498474121
2025-12-09 12:05:57.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 9.210411071777344
2025-12-09 12:05:57.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 9.211216926574707
2025-12-09 12:05:57.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 9.210859298706055
2025-12-09 12:05:57.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 9.211082458496094
2025-12-09 12:05:57.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 9.210892677307129
2025-12-09 12:05:57.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 9.211174011230469
2025-12-09 12:05:57.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 9.211050033569336
2025-12-09 12:05:57.398 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 9.210896492004395
2025-12-09 12:05:57.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 9.210919380187988
2025-12-09 12:05:57.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 9.210366249084473
2025-12-09 12:05:57.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 9.210972785949707
2025-12-09 12:05:57.459 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.0002909538931178862 Training loss: 9.210627555847168
2025-12-09 12:05:57.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00026490666646784665 Training loss: 9.210921287536621
2025-12-09 12:05:57.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000225 Training loss: 9.210464477539062
2025-12-09 12:05:57.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00017604722665003956 Training loss: 9.211262702941895
2025-12-09 12:05:57.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00012395277334996044 Training loss: 9.210084915161133
2025-12-09 12:05:57.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 7.500000000000002e-05 Training loss: 9.210750579833984
2025-12-09 12:05:57.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 3.509333353215331e-05 Training loss: 9.210490226745605
2025-12-09 12:05:57.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 9.046106882113751e-06 Training loss: 9.210709571838379
2025-12-09 12:05:57.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0 Training loss: 9.209187507629395
