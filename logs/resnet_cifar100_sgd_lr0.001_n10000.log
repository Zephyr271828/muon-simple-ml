2025-12-09 10:06:52.729 | INFO     | __main__:train:24 - Epoch: 0 Step: 0 LR: 1e-05 Training loss: 4.874756336212158
2025-12-09 10:06:52.755 | INFO     | __main__:train:24 - Epoch: 0 Step: 1 LR: 2e-05 Training loss: 4.884998321533203
2025-12-09 10:06:52.765 | INFO     | __main__:train:24 - Epoch: 0 Step: 2 LR: 3e-05 Training loss: 4.889104843139648
2025-12-09 10:06:52.773 | INFO     | __main__:train:24 - Epoch: 0 Step: 3 LR: 4e-05 Training loss: 4.851559162139893
2025-12-09 10:06:52.782 | INFO     | __main__:train:24 - Epoch: 0 Step: 4 LR: 5e-05 Training loss: 4.8430891036987305
2025-12-09 10:06:52.792 | INFO     | __main__:train:24 - Epoch: 0 Step: 5 LR: 6e-05 Training loss: 4.8786187171936035
2025-12-09 10:06:52.800 | INFO     | __main__:train:24 - Epoch: 0 Step: 6 LR: 7.000000000000001e-05 Training loss: 4.915852069854736
2025-12-09 10:06:52.809 | INFO     | __main__:train:24 - Epoch: 0 Step: 7 LR: 8e-05 Training loss: 4.908506393432617
2025-12-09 10:06:52.817 | INFO     | __main__:train:24 - Epoch: 0 Step: 8 LR: 8.999999999999999e-05 Training loss: 4.870131969451904
2025-12-09 10:06:52.827 | INFO     | __main__:train:24 - Epoch: 0 Step: 9 LR: 0.0001 Training loss: 4.821465969085693
2025-12-09 10:06:52.835 | INFO     | __main__:train:24 - Epoch: 0 Step: 10 LR: 0.00011 Training loss: 4.859440803527832
2025-12-09 10:06:52.843 | INFO     | __main__:train:24 - Epoch: 0 Step: 11 LR: 0.00012 Training loss: 4.9232401847839355
2025-12-09 10:06:52.852 | INFO     | __main__:train:24 - Epoch: 0 Step: 12 LR: 0.00013000000000000002 Training loss: 4.866147518157959
2025-12-09 10:06:52.861 | INFO     | __main__:train:24 - Epoch: 0 Step: 13 LR: 0.00014000000000000001 Training loss: 4.819233417510986
2025-12-09 10:06:52.869 | INFO     | __main__:train:24 - Epoch: 0 Step: 14 LR: 0.00015 Training loss: 4.923543930053711
2025-12-09 10:06:52.878 | INFO     | __main__:train:24 - Epoch: 0 Step: 15 LR: 0.00016 Training loss: 4.872530460357666
2025-12-09 10:06:52.886 | INFO     | __main__:train:24 - Epoch: 0 Step: 16 LR: 0.00017 Training loss: 4.758083343505859
2025-12-09 10:06:52.895 | INFO     | __main__:train:24 - Epoch: 0 Step: 17 LR: 0.00017999999999999998 Training loss: 4.8693366050720215
2025-12-09 10:06:52.903 | INFO     | __main__:train:24 - Epoch: 0 Step: 18 LR: 0.00019 Training loss: 4.81231689453125
2025-12-09 10:06:52.911 | INFO     | __main__:train:24 - Epoch: 0 Step: 19 LR: 0.0002 Training loss: 4.873409271240234
2025-12-09 10:06:52.920 | INFO     | __main__:train:24 - Epoch: 0 Step: 20 LR: 0.00021 Training loss: 4.809484481811523
2025-12-09 10:06:52.929 | INFO     | __main__:train:24 - Epoch: 0 Step: 21 LR: 0.00022 Training loss: 4.857601642608643
2025-12-09 10:06:52.937 | INFO     | __main__:train:24 - Epoch: 0 Step: 22 LR: 0.00023 Training loss: 4.742677688598633
2025-12-09 10:06:52.945 | INFO     | __main__:train:24 - Epoch: 0 Step: 23 LR: 0.00024 Training loss: 4.748125076293945
2025-12-09 10:06:52.953 | INFO     | __main__:train:24 - Epoch: 0 Step: 24 LR: 0.00025 Training loss: 4.797171592712402
2025-12-09 10:06:52.962 | INFO     | __main__:train:24 - Epoch: 0 Step: 25 LR: 0.00026000000000000003 Training loss: 4.829459190368652
2025-12-09 10:06:52.971 | INFO     | __main__:train:24 - Epoch: 0 Step: 26 LR: 0.00027 Training loss: 4.8240065574646
2025-12-09 10:06:52.979 | INFO     | __main__:train:24 - Epoch: 0 Step: 27 LR: 0.00028000000000000003 Training loss: 4.810286045074463
2025-12-09 10:06:52.987 | INFO     | __main__:train:24 - Epoch: 0 Step: 28 LR: 0.00029 Training loss: 4.797153949737549
2025-12-09 10:06:52.998 | INFO     | __main__:train:24 - Epoch: 0 Step: 29 LR: 0.0003 Training loss: 4.74310302734375
2025-12-09 10:06:53.006 | INFO     | __main__:train:24 - Epoch: 0 Step: 30 LR: 0.00031 Training loss: 4.788296222686768
2025-12-09 10:06:53.014 | INFO     | __main__:train:24 - Epoch: 0 Step: 31 LR: 0.00032 Training loss: 4.81083869934082
2025-12-09 10:06:53.023 | INFO     | __main__:train:24 - Epoch: 0 Step: 32 LR: 0.00033 Training loss: 4.794476509094238
2025-12-09 10:06:53.031 | INFO     | __main__:train:24 - Epoch: 0 Step: 33 LR: 0.00034 Training loss: 4.733057022094727
2025-12-09 10:06:53.040 | INFO     | __main__:train:24 - Epoch: 0 Step: 34 LR: 0.00035 Training loss: 4.761483192443848
2025-12-09 10:06:53.048 | INFO     | __main__:train:24 - Epoch: 0 Step: 35 LR: 0.00035999999999999997 Training loss: 4.765005588531494
2025-12-09 10:06:53.056 | INFO     | __main__:train:24 - Epoch: 0 Step: 36 LR: 0.00037 Training loss: 4.742262363433838
2025-12-09 10:06:53.065 | INFO     | __main__:train:24 - Epoch: 0 Step: 37 LR: 0.00038 Training loss: 4.832600116729736
2025-12-09 10:06:53.073 | INFO     | __main__:train:24 - Epoch: 0 Step: 38 LR: 0.00039000000000000005 Training loss: 4.742458343505859
2025-12-09 10:06:53.081 | INFO     | __main__:train:24 - Epoch: 0 Step: 39 LR: 0.0004 Training loss: 4.759513854980469
2025-12-09 10:06:53.089 | INFO     | __main__:train:24 - Epoch: 0 Step: 40 LR: 0.00041 Training loss: 4.66024923324585
2025-12-09 10:06:53.098 | INFO     | __main__:train:24 - Epoch: 0 Step: 41 LR: 0.00042 Training loss: 4.784433364868164
2025-12-09 10:06:53.107 | INFO     | __main__:train:24 - Epoch: 0 Step: 42 LR: 0.00043 Training loss: 4.803483963012695
2025-12-09 10:06:53.115 | INFO     | __main__:train:24 - Epoch: 0 Step: 43 LR: 0.00044 Training loss: 4.733209609985352
2025-12-09 10:06:53.123 | INFO     | __main__:train:24 - Epoch: 0 Step: 44 LR: 0.00045000000000000004 Training loss: 4.723311901092529
2025-12-09 10:06:53.135 | INFO     | __main__:train:24 - Epoch: 0 Step: 45 LR: 0.00046 Training loss: 4.688187599182129
2025-12-09 10:06:53.144 | INFO     | __main__:train:24 - Epoch: 0 Step: 46 LR: 0.00047 Training loss: 4.759244441986084
2025-12-09 10:06:53.152 | INFO     | __main__:train:24 - Epoch: 0 Step: 47 LR: 0.00048 Training loss: 4.7305474281311035
2025-12-09 10:06:53.160 | INFO     | __main__:train:24 - Epoch: 0 Step: 48 LR: 0.00049 Training loss: 4.7437357902526855
2025-12-09 10:06:53.169 | INFO     | __main__:train:24 - Epoch: 0 Step: 49 LR: 0.0005 Training loss: 4.643575668334961
2025-12-09 10:06:53.179 | INFO     | __main__:train:24 - Epoch: 0 Step: 50 LR: 0.00051 Training loss: 4.7468485832214355
2025-12-09 10:06:53.187 | INFO     | __main__:train:24 - Epoch: 0 Step: 51 LR: 0.0005200000000000001 Training loss: 4.614004611968994
2025-12-09 10:06:53.195 | INFO     | __main__:train:24 - Epoch: 0 Step: 52 LR: 0.0005300000000000001 Training loss: 4.725849628448486
2025-12-09 10:06:53.203 | INFO     | __main__:train:24 - Epoch: 0 Step: 53 LR: 0.00054 Training loss: 4.643346309661865
2025-12-09 10:06:53.212 | INFO     | __main__:train:24 - Epoch: 0 Step: 54 LR: 0.00055 Training loss: 4.777357578277588
2025-12-09 10:06:53.220 | INFO     | __main__:train:24 - Epoch: 0 Step: 55 LR: 0.0005600000000000001 Training loss: 4.67272424697876
2025-12-09 10:06:53.228 | INFO     | __main__:train:24 - Epoch: 0 Step: 56 LR: 0.00057 Training loss: 4.6614484786987305
2025-12-09 10:06:53.237 | INFO     | __main__:train:24 - Epoch: 0 Step: 57 LR: 0.00058 Training loss: 4.6924357414245605
2025-12-09 10:06:53.245 | INFO     | __main__:train:24 - Epoch: 0 Step: 58 LR: 0.00059 Training loss: 4.653608322143555
2025-12-09 10:06:53.253 | INFO     | __main__:train:24 - Epoch: 0 Step: 59 LR: 0.0006 Training loss: 4.767950057983398
2025-12-09 10:06:53.262 | INFO     | __main__:train:24 - Epoch: 0 Step: 60 LR: 0.00061 Training loss: 4.642083168029785
2025-12-09 10:06:53.270 | INFO     | __main__:train:24 - Epoch: 0 Step: 61 LR: 0.00062 Training loss: 4.684841156005859
2025-12-09 10:06:53.278 | INFO     | __main__:train:24 - Epoch: 0 Step: 62 LR: 0.00063 Training loss: 4.6471052169799805
2025-12-09 10:06:53.287 | INFO     | __main__:train:24 - Epoch: 0 Step: 63 LR: 0.00064 Training loss: 4.656564712524414
2025-12-09 10:06:53.295 | INFO     | __main__:train:24 - Epoch: 0 Step: 64 LR: 0.0006500000000000001 Training loss: 4.740078449249268
2025-12-09 10:06:53.303 | INFO     | __main__:train:24 - Epoch: 0 Step: 65 LR: 0.00066 Training loss: 4.611047744750977
2025-12-09 10:06:53.312 | INFO     | __main__:train:24 - Epoch: 0 Step: 66 LR: 0.00067 Training loss: 4.638112545013428
2025-12-09 10:06:53.320 | INFO     | __main__:train:24 - Epoch: 0 Step: 67 LR: 0.00068 Training loss: 4.575746059417725
2025-12-09 10:06:53.328 | INFO     | __main__:train:24 - Epoch: 0 Step: 68 LR: 0.00069 Training loss: 4.540284633636475
2025-12-09 10:06:53.336 | INFO     | __main__:train:24 - Epoch: 0 Step: 69 LR: 0.0007 Training loss: 4.614887237548828
2025-12-09 10:06:53.345 | INFO     | __main__:train:24 - Epoch: 0 Step: 70 LR: 0.00071 Training loss: 4.666660308837891
2025-12-09 10:06:53.353 | INFO     | __main__:train:24 - Epoch: 0 Step: 71 LR: 0.0007199999999999999 Training loss: 4.629961967468262
2025-12-09 10:06:53.362 | INFO     | __main__:train:24 - Epoch: 0 Step: 72 LR: 0.00073 Training loss: 4.6244120597839355
2025-12-09 10:06:53.370 | INFO     | __main__:train:24 - Epoch: 0 Step: 73 LR: 0.00074 Training loss: 4.644190788269043
2025-12-09 10:06:53.378 | INFO     | __main__:train:24 - Epoch: 0 Step: 74 LR: 0.00075 Training loss: 4.629245281219482
2025-12-09 10:06:53.386 | INFO     | __main__:train:24 - Epoch: 0 Step: 75 LR: 0.00076 Training loss: 4.5541205406188965
2025-12-09 10:06:53.394 | INFO     | __main__:train:24 - Epoch: 0 Step: 76 LR: 0.0007700000000000001 Training loss: 4.645946979522705
2025-12-09 10:06:53.402 | INFO     | __main__:train:24 - Epoch: 0 Step: 77 LR: 0.0007800000000000001 Training loss: 4.638569355010986
2025-12-09 10:06:53.436 | INFO     | __main__:train:24 - Epoch: 0 Step: 78 LR: 0.00079 Training loss: 4.686175346374512
