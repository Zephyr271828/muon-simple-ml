2025-12-09 11:52:30.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 12.195647239685059
2025-12-09 11:52:30.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 12.210142135620117
2025-12-09 11:52:31.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 12.209911346435547
2025-12-09 11:52:31.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 12.047470092773438
2025-12-09 11:52:32.040 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 12.087320327758789
2025-12-09 11:52:32.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 12.045024871826172
2025-12-09 11:52:32.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 11.848505020141602
2025-12-09 11:52:33.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 11.786792755126953
2025-12-09 11:52:33.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 11.418610572814941
2025-12-09 11:52:33.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 11.632242202758789
2025-12-09 11:52:34.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 11.154977798461914
2025-12-09 11:52:34.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 11.309325218200684
2025-12-09 11:52:35.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 10.759820938110352
2025-12-09 11:52:35.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 10.69898509979248
2025-12-09 11:52:35.891 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 10.538938522338867
2025-12-09 11:52:36.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 10.37386417388916
2025-12-09 11:52:36.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 10.151605606079102
2025-12-09 11:52:37.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 10.043848991394043
2025-12-09 11:52:37.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 9.884604454040527
2025-12-09 11:52:37.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 9.698040962219238
2025-12-09 11:52:38.206 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 10.032525062561035
2025-12-09 11:52:38.591 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 9.76235580444336
2025-12-09 11:52:38.975 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 9.516702651977539
2025-12-09 11:52:39.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 9.517524719238281
2025-12-09 11:52:39.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 9.093731880187988
2025-12-09 11:52:40.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 9.345085144042969
2025-12-09 11:52:40.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 9.043966293334961
2025-12-09 11:52:40.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 8.905014038085938
2025-12-09 11:52:41.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 8.844878196716309
2025-12-09 11:52:41.675 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 8.68779468536377
2025-12-09 11:52:42.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 9.051750183105469
2025-12-09 11:52:42.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 8.652824401855469
2025-12-09 11:52:42.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 8.584383010864258
2025-12-09 11:52:43.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 8.822124481201172
2025-12-09 11:52:43.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 8.160841941833496
2025-12-09 11:52:43.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 8.335931777954102
2025-12-09 11:52:44.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 8.440910339355469
2025-12-09 11:52:44.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 8.458935737609863
2025-12-09 11:52:45.140 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 8.36422348022461
2025-12-09 11:52:45.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 8.24206829071045
2025-12-09 11:52:45.911 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 8.326736450195312
2025-12-09 11:52:46.299 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 8.578356742858887
2025-12-09 11:52:46.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 8.958684921264648
2025-12-09 11:52:47.069 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 8.57981014251709
2025-12-09 11:52:47.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 8.057558059692383
2025-12-09 11:52:47.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 8.08301067352295
2025-12-09 11:52:48.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 8.192325592041016
2025-12-09 11:52:48.611 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 8.264022827148438
2025-12-09 11:52:48.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 8.394608497619629
2025-12-09 11:52:49.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 8.031991004943848
2025-12-09 11:52:49.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 8.237576484680176
2025-12-09 11:52:50.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 7.996508598327637
2025-12-09 11:52:50.533 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 8.10703182220459
2025-12-09 11:52:50.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 7.972955226898193
2025-12-09 11:52:51.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 8.124898910522461
2025-12-09 11:52:51.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 8.069774627685547
2025-12-09 11:52:52.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 8.467214584350586
2025-12-09 11:52:52.462 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 8.084531784057617
2025-12-09 11:52:52.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 8.294401168823242
2025-12-09 11:52:53.231 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 8.344572067260742
2025-12-09 11:52:53.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 9.257359504699707
2025-12-09 11:52:54.002 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 8.463911056518555
2025-12-09 11:52:54.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 8.2970609664917
2025-12-09 11:52:54.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 7.9953460693359375
2025-12-09 11:52:55.157 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 9.164457321166992
2025-12-09 11:52:55.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 8.06363582611084
2025-12-09 11:52:55.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 8.085485458374023
2025-12-09 11:52:56.315 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 8.127724647521973
2025-12-09 11:52:56.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 7.9769697189331055
2025-12-09 11:52:57.086 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 8.089627265930176
2025-12-09 11:52:57.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 8.531134605407715
2025-12-09 11:52:57.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 8.155579566955566
2025-12-09 11:52:58.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 8.024641036987305
2025-12-09 11:52:58.627 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 8.28935718536377
2025-12-09 11:52:59.012 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 8.272858619689941
2025-12-09 11:52:59.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 7.763786315917969
2025-12-09 11:52:59.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 8.34685230255127
2025-12-09 11:53:00.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 8.44885540008545
2025-12-09 11:53:00.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 7.8034515380859375
2025-12-09 11:53:00.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 7.961642265319824
2025-12-09 11:53:01.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 7.820781230926514
2025-12-09 11:53:01.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 7.862807273864746
2025-12-09 11:53:02.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 8.350871086120605
2025-12-09 11:53:02.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 8.191899299621582
2025-12-09 11:53:02.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 8.106264114379883
2025-12-09 11:53:03.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 8.036953926086426
2025-12-09 11:53:03.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 7.682372093200684
2025-12-09 11:53:04.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 7.972251892089844
2025-12-09 11:53:04.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 8.544795989990234
2025-12-09 11:53:04.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 8.358997344970703
2025-12-09 11:53:05.196 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 8.467023849487305
2025-12-09 11:53:05.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 7.680288791656494
2025-12-09 11:53:05.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 8.244388580322266
2025-12-09 11:53:06.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 7.989962577819824
2025-12-09 11:53:06.740 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 7.862560272216797
2025-12-09 11:53:07.126 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 7.945010185241699
2025-12-09 11:53:07.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 7.59691047668457
2025-12-09 11:53:07.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 7.854055404663086
2025-12-09 11:53:08.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 8.045104026794434
2025-12-09 11:53:08.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 8.22482967376709
2025-12-09 11:53:09.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999997089396425 Training loss: 8.65170669555664
2025-12-09 11:53:09.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029999988357586825 Training loss: 8.318739891052246
2025-12-09 11:53:09.833 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.000299999738045746 Training loss: 7.779054641723633
2025-12-09 11:53:10.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.0002999995343036539 Training loss: 7.773654937744141
2025-12-09 11:53:10.607 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.00029999927234967104 Training loss: 8.369976997375488
2025-12-09 11:53:10.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.00029999895218389905 Training loss: 8.044071197509766
2025-12-09 11:53:11.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.0002999985738064622 Training loss: 8.034329414367676
2025-12-09 11:53:11.766 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.00029999813721750737 Training loss: 7.924981594085693
2025-12-09 11:53:12.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.00029999764241720394 Training loss: 8.248053550720215
2025-12-09 11:53:12.538 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.0002999970894057439 Training loss: 7.9607110023498535
2025-12-09 11:53:12.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029999647818334195 Training loss: 7.834194660186768
2025-12-09 11:53:13.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.0002999958087502352 Training loss: 7.589869499206543
2025-12-09 11:53:13.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.00029999508110668355 Training loss: 7.7927632331848145
2025-12-09 11:53:14.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.0002999942952529693 Training loss: 7.85597562789917
2025-12-09 11:53:14.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00029999345118939745 Training loss: 7.892955780029297
2025-12-09 11:53:14.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.0002999925489162956 Training loss: 8.02617359161377
2025-12-09 11:53:15.250 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029999158843401386 Training loss: 8.134603500366211
2025-12-09 11:53:15.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.000299990569742925 Training loss: 7.912607669830322
2025-12-09 11:53:16.023 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002999894928434243 Training loss: 7.783679485321045
2025-12-09 11:53:16.410 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.00029998835773592975 Training loss: 8.083732604980469
2025-12-09 11:53:16.797 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.00029998716442088184 Training loss: 7.949980735778809
2025-12-09 11:53:17.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.0002999859128987437 Training loss: 8.045610427856445
2025-12-09 11:53:17.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.00029998460317000097 Training loss: 7.695173263549805
2025-12-09 11:53:17.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00029998323523516195 Training loss: 7.75277853012085
2025-12-09 11:53:18.345 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002999818090947575 Training loss: 7.398220539093018
2025-12-09 11:53:18.734 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00029998032474934106 Training loss: 8.039436340332031
2025-12-09 11:53:19.121 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0002999787821994887 Training loss: 8.095203399658203
2025-12-09 11:53:19.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.00029997718144579913 Training loss: 7.930168151855469
2025-12-09 11:53:19.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0002999755224888935 Training loss: 7.917242527008057
2025-12-09 11:53:20.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029997380532941555 Training loss: 7.789340496063232
2025-12-09 11:53:20.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00029997202996803177 Training loss: 7.939821720123291
2025-12-09 11:53:21.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0002999701964054312 Training loss: 7.78075647354126
2025-12-09 11:53:21.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.0002999683046423252 Training loss: 7.913728713989258
2025-12-09 11:53:21.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.0002999663546794481 Training loss: 7.516773700714111
2025-12-09 11:53:22.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.00029996434651755657 Training loss: 8.30136489868164
2025-12-09 11:53:22.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.00029996228015743 Training loss: 7.126419544219971
2025-12-09 11:53:22.990 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002999601555998703 Training loss: 7.667633056640625
2025-12-09 11:53:23.381 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.0002999579728457019 Training loss: 7.4263715744018555
2025-12-09 11:53:23.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.0002999557318957719 Training loss: 7.385196208953857
2025-12-09 11:53:24.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.00029995343275095003 Training loss: 7.674373626708984
2025-12-09 11:53:24.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.00029995107541212843 Training loss: 7.678907871246338
2025-12-09 11:53:24.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00029994865988022205 Training loss: 7.8711724281311035
2025-12-09 11:53:25.314 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.0002999461861561683 Training loss: 8.061915397644043
2025-12-09 11:53:25.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.0002999436542409271 Training loss: 7.710690021514893
2025-12-09 11:53:26.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0002999410641354812 Training loss: 7.909791946411133
2025-12-09 11:53:26.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00029993841584083553 Training loss: 7.75919246673584
2025-12-09 11:53:26.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.00029993570935801805 Training loss: 7.4135355949401855
2025-12-09 11:53:27.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.000299932944688079 Training loss: 7.671759128570557
2025-12-09 11:53:27.636 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.00029993012183209135 Training loss: 7.536664009094238
2025-12-09 11:53:28.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002999272407911505 Training loss: 7.7433624267578125
2025-12-09 11:53:28.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.00029992430156637454 Training loss: 7.48207426071167
2025-12-09 11:53:28.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00029992130415890426 Training loss: 7.560174465179443
2025-12-09 11:53:29.188 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.00029991824856990276 Training loss: 8.239033699035645
2025-12-09 11:53:29.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.0002999151348005559 Training loss: 8.125737190246582
2025-12-09 11:53:29.961 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.0002999119628520721 Training loss: 7.751916408538818
2025-12-09 11:53:30.348 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.00029990873272568226 Training loss: 7.496130466461182
2025-12-09 11:53:30.735 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.00029990544442263996 Training loss: 7.509068965911865
2025-12-09 11:53:31.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002999020979442214 Training loss: 7.813720226287842
2025-12-09 11:53:31.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.0002998986932917252 Training loss: 7.479601860046387
2025-12-09 11:53:31.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.00029989523046647257 Training loss: 7.674168586730957
2025-12-09 11:53:32.286 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.00029989170946980755 Training loss: 7.649210453033447
2025-12-09 11:53:32.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.00029988813030309644 Training loss: 7.418205261230469
2025-12-09 11:53:33.064 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.0002998844929677283 Training loss: 7.441422462463379
2025-12-09 11:53:33.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00029988079746511465 Training loss: 7.348363876342773
2025-12-09 11:53:33.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.00029987704379668973 Training loss: 7.898510456085205
2025-12-09 11:53:34.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.0002998732319639102 Training loss: 7.562131404876709
2025-12-09 11:53:34.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.00029986936196825536 Training loss: 7.892760753631592
2025-12-09 11:53:34.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.0002998654338112271 Training loss: 7.546709060668945
2025-12-09 11:53:35.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00029986144749434985 Training loss: 7.525168418884277
2025-12-09 11:53:35.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.0002998574030191706 Training loss: 7.459802627563477
2025-12-09 11:53:36.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.000299853300387259 Training loss: 7.401913166046143
2025-12-09 11:53:36.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00029984913960020714 Training loss: 7.4280924797058105
2025-12-09 11:53:36.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.00029984492065962976 Training loss: 7.715527534484863
2025-12-09 11:53:37.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00029984064356716414 Training loss: 7.3319220542907715
2025-12-09 11:53:37.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.0002998363083244701 Training loss: 7.413334369659424
2025-12-09 11:53:38.103 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00029983191493323017 Training loss: 7.629110336303711
2025-12-09 11:53:38.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.0002998274633951493 Training loss: 8.234021186828613
2025-12-09 11:53:38.876 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00029982295371195494 Training loss: 8.259940147399902
2025-12-09 11:53:39.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00029981838588539735 Training loss: 7.390384197235107
2025-12-09 11:53:39.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.00029981375991724915 Training loss: 7.560504913330078
2025-12-09 11:53:40.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.0002998090758093056 Training loss: 7.733277320861816
2025-12-09 11:53:40.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00029980433356338447 Training loss: 7.515452861785889
2025-12-09 11:53:40.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.0002997995331813262 Training loss: 8.130315780639648
2025-12-09 11:53:41.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.00029979467466499367 Training loss: 7.275527000427246
2025-12-09 11:53:41.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00029978975801627243 Training loss: 7.114192008972168
2025-12-09 11:53:41.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0002997847832370704 Training loss: 7.586554050445557
2025-12-09 11:53:42.368 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.0002997797503293184 Training loss: 7.384702205657959
2025-12-09 11:53:42.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00029977465929496947 Training loss: 7.184194087982178
2025-12-09 11:53:43.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.0002997695101359994 Training loss: 7.781519412994385
2025-12-09 11:53:43.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.0002997643028544064 Training loss: 7.775705337524414
2025-12-09 11:53:43.919 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0002997590374522114 Training loss: 7.496984004974365
2025-12-09 11:53:44.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0002997537139314577 Training loss: 7.415854454040527
2025-12-09 11:53:44.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0002997483322942114 Training loss: 7.238037586212158
2025-12-09 11:53:45.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.0002997428925425609 Training loss: 7.7661027908325195
2025-12-09 11:53:45.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.0002997373946786173 Training loss: 7.47092342376709
2025-12-09 11:53:45.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.00029973183870451417 Training loss: 7.472369194030762
2025-12-09 11:53:46.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0002997262246224077 Training loss: 7.2180256843566895
2025-12-09 11:53:46.633 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00029972055243447665 Training loss: 7.600348472595215
2025-12-09 11:53:47.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.00029971482214292223 Training loss: 7.562334060668945
2025-12-09 11:53:47.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.00029970903374996826 Training loss: 7.181590557098389
2025-12-09 11:53:47.798 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.0002997031872578611 Training loss: 7.419060707092285
2025-12-09 11:53:48.185 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00029969728266886973 Training loss: 7.353801250457764
2025-12-09 11:53:48.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00029969131998528554 Training loss: 7.227292060852051
2025-12-09 11:53:48.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0002996852992094225 Training loss: 7.485999584197998
2025-12-09 11:53:49.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.00029967922034361723 Training loss: 7.368610382080078
2025-12-09 11:53:49.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.0002996730833902287 Training loss: 7.6003336906433105
2025-12-09 11:53:50.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00029966688835163875 Training loss: 7.408330917358398
2025-12-09 11:53:50.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00029966063523025136 Training loss: 7.876013278961182
2025-12-09 11:53:50.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00029965432402849333 Training loss: 7.452951908111572
2025-12-09 11:53:51.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0002996479547488139 Training loss: 7.333792209625244
2025-12-09 11:53:51.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.0002996415273936849 Training loss: 7.533891201019287
2025-12-09 11:53:52.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.00029963504196560056 Training loss: 7.620802879333496
2025-12-09 11:53:52.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.00029962849846707786 Training loss: 7.336310863494873
2025-12-09 11:53:52.839 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.0002996218969006561 Training loss: 7.351088523864746
2025-12-09 11:53:53.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00029961523726889733 Training loss: 7.250522136688232
2025-12-09 11:53:53.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.00029960851957438594 Training loss: 7.282327175140381
2025-12-09 11:53:54.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.000299601743819729 Training loss: 7.448734283447266
2025-12-09 11:53:54.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00029959491000755594 Training loss: 7.4393720626831055
2025-12-09 11:53:54.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00029958801814051897 Training loss: 7.1354594230651855
2025-12-09 11:53:55.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0002995810682212926 Training loss: 7.19436502456665
2025-12-09 11:53:55.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.0002995740602525739 Training loss: 7.450198173522949
2025-12-09 11:53:55.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.0002995669942370827 Training loss: 7.5184197425842285
2025-12-09 11:53:56.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.000299559870177561 Training loss: 7.31287956237793
2025-12-09 11:53:56.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.0002995526880767737 Training loss: 7.470338344573975
2025-12-09 11:53:57.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.00029954544793750785 Training loss: 7.125987529754639
2025-12-09 11:53:57.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00029953814976257335 Training loss: 7.126150608062744
2025-12-09 11:53:57.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.0002995307935548024 Training loss: 7.1201019287109375
2025-12-09 11:53:58.268 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.0002995233793170498 Training loss: 7.307815074920654
2025-12-09 11:53:58.656 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.00029951590705219283 Training loss: 7.288769721984863
2025-12-09 11:53:59.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0002995083767631314 Training loss: 7.018692493438721
2025-12-09 11:53:59.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.0002995007884527879 Training loss: 6.80881404876709
2025-12-09 11:53:59.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.00029949314212410715 Training loss: 7.283051013946533
2025-12-09 11:54:00.210 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.00029948543778005656 Training loss: 7.136895656585693
2025-12-09 11:54:00.605 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.00029947767542362597 Training loss: 7.234604358673096
2025-12-09 11:54:00.994 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.0002994698550578279 Training loss: 7.766472816467285
2025-12-09 11:54:01.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.0002994619766856972 Training loss: 7.3972859382629395
2025-12-09 11:54:01.772 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00029945404031029134 Training loss: 7.464795112609863
2025-12-09 11:54:02.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.00029944604593469033 Training loss: 7.16055154800415
2025-12-09 11:54:02.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00029943799356199656 Training loss: 7.044769763946533
2025-12-09 11:54:02.939 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.00029942988319533504 Training loss: 6.984248638153076
2025-12-09 11:54:03.328 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0002994217148378532 Training loss: 6.999874114990234
2025-12-09 11:54:03.716 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00029941348849272105 Training loss: 7.513742446899414
2025-12-09 11:54:04.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.0002994052041631311 Training loss: 7.274445533752441
2025-12-09 11:54:04.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.0002993968618522982 Training loss: 6.974969863891602
2025-12-09 11:54:04.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0002993884615634601 Training loss: 7.233747959136963
2025-12-09 11:54:05.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.00029938000329987645 Training loss: 7.3128743171691895
2025-12-09 11:54:05.666 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00029937148706483003 Training loss: 7.699583053588867
2025-12-09 11:54:06.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.00029936291286162577 Training loss: 7.516881465911865
2025-12-09 11:54:06.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.000299354280693591 Training loss: 7.096187591552734
2025-12-09 11:54:06.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0002993455905640758 Training loss: 7.286556720733643
2025-12-09 11:54:07.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0002993368424764526 Training loss: 7.0550360679626465
2025-12-09 11:54:07.613 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0002993280364341165 Training loss: 6.8728837966918945
2025-12-09 11:54:08.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.00029931917244048473 Training loss: 8.113113403320312
2025-12-09 11:54:08.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.0002993102504989974 Training loss: 7.236594200134277
2025-12-09 11:54:08.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00029930127061311685 Training loss: 7.375564098358154
2025-12-09 11:54:09.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0002992922327863281 Training loss: 7.372352123260498
2025-12-09 11:54:09.550 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00029928313702213844 Training loss: 7.289633750915527
2025-12-09 11:54:09.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.00029927398332407784 Training loss: 6.9264750480651855
2025-12-09 11:54:10.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00029926477169569865 Training loss: 7.135375499725342
2025-12-09 11:54:10.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00029925550214057565 Training loss: 7.485382556915283
2025-12-09 11:54:11.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.00029924617466230624 Training loss: 6.92376708984375
2025-12-09 11:54:11.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.00029923678926451034 Training loss: 7.09110164642334
2025-12-09 11:54:11.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00029922734595083005 Training loss: 7.117799758911133
2025-12-09 11:54:12.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.0002992178447249302 Training loss: 7.291856288909912
2025-12-09 11:54:12.657 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.00029920828559049805 Training loss: 7.019102573394775
2025-12-09 11:54:13.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.0002991986685512433 Training loss: 7.304093360900879
2025-12-09 11:54:13.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0002991889936108982 Training loss: 6.951170921325684
2025-12-09 11:54:13.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.0002991792607732173 Training loss: 7.742063999176025
2025-12-09 11:54:14.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.0002991694700419778 Training loss: 7.199301719665527
2025-12-09 11:54:14.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00029915962142097925 Training loss: 7.588949203491211
2025-12-09 11:54:14.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.00029914971491404373 Training loss: 7.131102561950684
2025-12-09 11:54:15.379 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00029913975052501575 Training loss: 7.146050453186035
2025-12-09 11:54:15.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0002991297282577623 Training loss: 7.319965839385986
2025-12-09 11:54:16.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00029911964811617285 Training loss: 7.095100402832031
2025-12-09 11:54:16.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.00029910951010415926 Training loss: 7.228006839752197
2025-12-09 11:54:16.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.0002990993142256559 Training loss: 7.242134094238281
2025-12-09 11:54:17.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.0002990890604846196 Training loss: 7.1615071296691895
2025-12-09 11:54:17.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00029907874888502966 Training loss: 6.974894046783447
2025-12-09 11:54:18.092 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.00029906837943088785 Training loss: 7.575570583343506
2025-12-09 11:54:18.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.00029905795212621823 Training loss: 6.9730658531188965
2025-12-09 11:54:18.869 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.00029904746697506754 Training loss: 6.8733649253845215
2025-12-09 11:54:19.260 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0002990369239815048 Training loss: 7.149638652801514
2025-12-09 11:54:19.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.00029902632314962157 Training loss: 7.344451427459717
2025-12-09 11:54:20.037 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002990156644835318 Training loss: 7.2489447593688965
2025-12-09 11:54:20.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.00029900494798737194 Training loss: 7.185527801513672
2025-12-09 11:54:20.812 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00029899417366530085 Training loss: 7.304479122161865
2025-12-09 11:54:21.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.00029898334152149984 Training loss: 7.226280212402344
2025-12-09 11:54:21.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0002989724515601726 Training loss: 7.696072101593018
2025-12-09 11:54:21.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.0002989615037855454 Training loss: 7.279362678527832
2025-12-09 11:54:22.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.0002989504982018668 Training loss: 7.50941276550293
2025-12-09 11:54:22.751 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.00029893943481340785 Training loss: 6.83786153793335
2025-12-09 11:54:23.139 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.000298928313624462 Training loss: 6.576047420501709
2025-12-09 11:54:23.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.0002989171346393453 Training loss: 7.252228736877441
2025-12-09 11:54:23.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00029890589786239595 Training loss: 7.305447101593018
2025-12-09 11:54:24.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0002988946032979748 Training loss: 7.00565767288208
2025-12-09 11:54:24.694 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.000298883250950465 Training loss: 7.329619407653809
2025-12-09 11:54:25.081 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.0002988718408242722 Training loss: 7.0783371925354
2025-12-09 11:54:25.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.00029886037292382455 Training loss: 7.148462295532227
2025-12-09 11:54:25.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.00029884884725357236 Training loss: 7.31771183013916
2025-12-09 11:54:26.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0002988372638179886 Training loss: 6.9070820808410645
2025-12-09 11:54:26.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002988256226215685 Training loss: 7.201871395111084
2025-12-09 11:54:27.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0002988139236688299 Training loss: 7.253376007080078
2025-12-09 11:54:27.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.00029880216696431285 Training loss: 7.003986835479736
2025-12-09 11:54:27.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.0002987903525125799 Training loss: 6.877485275268555
2025-12-09 11:54:28.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.000298778480318216 Training loss: 7.875543117523193
2025-12-09 11:54:28.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0002987665503858286 Training loss: 7.368495941162109
2025-12-09 11:54:28.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0002987545627200474 Training loss: 6.946967601776123
2025-12-09 11:54:29.352 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0002987425173255246 Training loss: 6.936091423034668
2025-12-09 11:54:29.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.0002987304142069348 Training loss: 6.9634904861450195
2025-12-09 11:54:30.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0002987182533689749 Training loss: 6.959895133972168
2025-12-09 11:54:30.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0002987060348163644 Training loss: 7.93217658996582
2025-12-09 11:54:30.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.000298693758553845 Training loss: 6.433786869049072
2025-12-09 11:54:31.291 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.00029868142458618096 Training loss: 7.170147895812988
2025-12-09 11:54:31.678 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0002986690329181587 Training loss: 7.205742359161377
2025-12-09 11:54:32.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00029865658355458736 Training loss: 6.899666786193848
2025-12-09 11:54:32.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0002986440765002982 Training loss: 8.135766983032227
2025-12-09 11:54:32.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.000298631511760145 Training loss: 7.174765110015869
2025-12-09 11:54:33.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.0002986188893390038 Training loss: 7.2752604484558105
2025-12-09 11:54:33.620 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.0002986062092417733 Training loss: 6.988500118255615
2025-12-09 11:54:34.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.00029859347147337417 Training loss: 7.48091459274292
2025-12-09 11:54:34.395 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.0002985806760387499 Training loss: 7.788593769073486
2025-12-09 11:54:34.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.00029856782294286594 Training loss: 7.074017524719238
2025-12-09 11:54:35.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00029855491219071053 Training loss: 7.728113174438477
2025-12-09 11:54:35.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.000298541943787294 Training loss: 6.880221843719482
2025-12-09 11:54:35.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00029852891773764906 Training loss: 7.5837483406066895
2025-12-09 11:54:36.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00029851583404683096 Training loss: 7.6195526123046875
2025-12-09 11:54:36.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.0002985026927199172 Training loss: 7.59470272064209
2025-12-09 11:54:37.108 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.00029848949376200766 Training loss: 7.275839805603027
2025-12-09 11:54:37.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.0002984762371782246 Training loss: 7.094937801361084
2025-12-09 11:54:37.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.00029846292297371264 Training loss: 6.799649715423584
2025-12-09 11:54:38.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.0002984495511536388 Training loss: 7.507339954376221
2025-12-09 11:54:38.665 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.0002984361217231923 Training loss: 7.298641204833984
2025-12-09 11:54:39.051 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.00029842263468758505 Training loss: 6.9161834716796875
2025-12-09 11:54:39.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.0002984090900520509 Training loss: 7.264341354370117
2025-12-09 11:54:39.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00029839548782184636 Training loss: 7.118619918823242
2025-12-09 11:54:40.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.00029838182800225017 Training loss: 7.042575836181641
2025-12-09 11:54:40.603 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00029836811059856354 Training loss: 7.821866035461426
2025-12-09 11:54:40.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.00029835433561610974 Training loss: 7.3389811515808105
2025-12-09 11:54:41.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0002983405030602346 Training loss: 7.238269805908203
2025-12-09 11:54:41.767 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.00029832661293630644 Training loss: 7.000759601593018
2025-12-09 11:54:42.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.00029831266524971557 Training loss: 7.077417850494385
2025-12-09 11:54:42.549 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0002982986600058749 Training loss: 7.063013076782227
2025-12-09 11:54:42.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.0002982845972102196 Training loss: 7.0888543128967285
2025-12-09 11:54:43.326 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.0002982704768682071 Training loss: 6.7171454429626465
2025-12-09 11:54:43.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00029825629898531724 Training loss: 7.399361610412598
2025-12-09 11:54:44.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0002982420635670523 Training loss: 7.119822025299072
2025-12-09 11:54:44.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.00029822777061893653 Training loss: 7.1462626457214355
2025-12-09 11:54:44.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.00029821342014651694 Training loss: 7.373182773590088
2025-12-09 11:54:45.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0002981990121553627 Training loss: 7.073023796081543
2025-12-09 11:54:45.650 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.0002981845466510651 Training loss: 6.896833419799805
2025-12-09 11:54:46.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00029817002363923803 Training loss: 6.845884799957275
2025-12-09 11:54:46.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00029815544312551754 Training loss: 7.687742233276367
2025-12-09 11:54:46.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00029814080511556207 Training loss: 7.585714817047119
2025-12-09 11:54:47.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00029812610961505234 Training loss: 6.997318267822266
2025-12-09 11:54:47.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.00029811135662969143 Training loss: 7.311058521270752
2025-12-09 11:54:47.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.00029809654616520456 Training loss: 6.88217306137085
2025-12-09 11:54:48.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00029808167822733953 Training loss: 6.837698936462402
2025-12-09 11:54:48.756 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0002980667528218662 Training loss: 6.6077799797058105
2025-12-09 11:54:49.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002980517699545769 Training loss: 6.683467864990234
2025-12-09 11:54:49.531 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.0002980367296312861 Training loss: 7.091337203979492
2025-12-09 11:54:49.918 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00029802163185783074 Training loss: 7.9117913246154785
2025-12-09 11:54:50.309 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.00029800647664006993 Training loss: 6.852082252502441
2025-12-09 11:54:50.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0002979912639838851 Training loss: 6.827653408050537
2025-12-09 11:54:51.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.00029797599389518 Training loss: 7.049091815948486
2025-12-09 11:54:51.470 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.0002979606663798807 Training loss: 7.242882251739502
2025-12-09 11:54:51.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0002979452814439354 Training loss: 6.870440483093262
2025-12-09 11:54:52.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.00029792983909331485 Training loss: 6.8034348487854
2025-12-09 11:54:52.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.0002979143393340117 Training loss: 7.15391731262207
2025-12-09 11:54:53.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.00029789878217204133 Training loss: 7.083178997039795
2025-12-09 11:54:53.418 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.00029788316761344106 Training loss: 6.989114761352539
2025-12-09 11:54:53.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00029786749566427064 Training loss: 6.842757701873779
2025-12-09 11:54:54.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.000297851766330612 Training loss: 6.896291255950928
2025-12-09 11:54:54.581 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.00029783597961856946 Training loss: 7.316189765930176
2025-12-09 11:54:54.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.00029782013553426937 Training loss: 7.263115406036377
2025-12-09 11:54:55.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.00029780423408386073 Training loss: 6.829526424407959
2025-12-09 11:54:55.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.00029778827527351443 Training loss: 7.199077606201172
2025-12-09 11:54:56.132 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.0002977722591094238 Training loss: 7.304202079772949
2025-12-09 11:54:56.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.00029775618559780447 Training loss: 7.60340690612793
2025-12-09 11:54:56.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.00029774005474489417 Training loss: 6.671960353851318
2025-12-09 11:54:57.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.00029772386655695305 Training loss: 6.793115615844727
2025-12-09 11:54:57.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.0002977076210402633 Training loss: 6.9647955894470215
2025-12-09 11:54:58.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.00029769131820112966 Training loss: 6.891860008239746
2025-12-09 11:54:58.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.00029767495804587885 Training loss: 6.86324405670166
2025-12-09 11:54:58.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.0002976585405808599 Training loss: 7.3610453605651855
2025-12-09 11:54:59.242 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0002976420658124441 Training loss: 7.125548362731934
2025-12-09 11:54:59.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.0002976255337470251 Training loss: 6.97001314163208
2025-12-09 11:55:00.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.00029760894439101855 Training loss: 7.375765800476074
2025-12-09 11:55:00.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.0002975922977508625 Training loss: 7.361415863037109
2025-12-09 11:55:00.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0002975755938330172 Training loss: 6.806918621063232
2025-12-09 11:55:01.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.00029755883264396513 Training loss: 7.1979265213012695
2025-12-09 11:55:01.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.00029754201419021094 Training loss: 7.105180740356445
2025-12-09 11:55:01.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.0002975251384782816 Training loss: 7.237445831298828
2025-12-09 11:55:02.351 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.00029750820551472615 Training loss: 6.6619391441345215
2025-12-09 11:55:02.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.00029749121530611597 Training loss: 7.684256553649902
2025-12-09 11:55:03.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.0002974741678590447 Training loss: 6.646976470947266
2025-12-09 11:55:03.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.00029745706318012806 Training loss: 7.024657726287842
2025-12-09 11:55:03.902 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.00029743990127600406 Training loss: 7.148715496063232
2025-12-09 11:55:04.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0002974226821533329 Training loss: 7.347435474395752
2025-12-09 11:55:04.677 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.000297405405818797 Training loss: 7.2233476638793945
2025-12-09 11:55:05.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0002973880722791009 Training loss: 7.044113636016846
2025-12-09 11:55:05.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.0002973706815409715 Training loss: 7.061619281768799
2025-12-09 11:55:05.846 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.0002973532336111577 Training loss: 6.73193883895874
2025-12-09 11:55:06.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00029733572849643085 Training loss: 7.574886798858643
2025-12-09 11:55:06.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.00029731816620358424 Training loss: 7.151193618774414
2025-12-09 11:55:07.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0002973005467394334 Training loss: 7.718868255615234
2025-12-09 11:55:07.400 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.00029728287011081625 Training loss: 7.595010757446289
2025-12-09 11:55:07.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0002972651363245927 Training loss: 6.591740131378174
2025-12-09 11:55:08.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00029724734538764475 Training loss: 6.6961822509765625
2025-12-09 11:55:08.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.0002972294973068768 Training loss: 6.9801154136657715
2025-12-09 11:55:08.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.00029721159208921546 Training loss: 7.065634250640869
2025-12-09 11:55:09.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.00029719362974160924 Training loss: 7.116903305053711
2025-12-09 11:55:09.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.000297175610271029 Training loss: 6.952546119689941
2025-12-09 11:55:10.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.00029715753368446786 Training loss: 7.134626865386963
2025-12-09 11:55:10.509 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00029713939998894087 Training loss: 7.083832263946533
2025-12-09 11:55:10.896 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0002971212091914854 Training loss: 7.420498371124268
2025-12-09 11:55:11.285 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0002971029612991609 Training loss: 7.248666286468506
2025-12-09 11:55:11.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.0002970846563190491 Training loss: 6.987319469451904
2025-12-09 11:55:12.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00029706629425825374 Training loss: 7.043112754821777
2025-12-09 11:55:12.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.00029704787512390085 Training loss: 6.980787754058838
2025-12-09 11:55:12.834 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0002970293989231385 Training loss: 6.539548397064209
2025-12-09 11:55:13.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.0002970108656631369 Training loss: 7.198231220245361
2025-12-09 11:55:13.610 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0002969922753510885 Training loss: 7.159551620483398
2025-12-09 11:55:13.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00029697362799420776 Training loss: 7.487451076507568
2025-12-09 11:55:14.387 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0002969549235997315 Training loss: 7.052190780639648
2025-12-09 11:55:14.774 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0002969361621749184 Training loss: 7.762582302093506
2025-12-09 11:55:15.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.00029691734372704943 Training loss: 6.976710796356201
2025-12-09 11:55:15.555 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0002968984682634277 Training loss: 6.559445858001709
2025-12-09 11:55:15.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0002968795357913784 Training loss: 6.614408016204834
2025-12-09 11:55:16.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0002968605463182488 Training loss: 7.37146520614624
2025-12-09 11:55:16.719 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0002968414998514085 Training loss: 7.363891124725342
2025-12-09 11:55:17.110 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0002968223963982488 Training loss: 7.134531021118164
2025-12-09 11:55:17.498 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.00029680323596618355 Training loss: 6.9343342781066895
2025-12-09 11:55:17.886 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00029678401856264857 Training loss: 6.618861675262451
2025-12-09 11:55:18.275 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.0002967647441951017 Training loss: 7.036452293395996
2025-12-09 11:55:18.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0002967454128710229 Training loss: 6.209503173828125
2025-12-09 11:55:19.050 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.00029672602459791434 Training loss: 6.746883869171143
2025-12-09 11:55:19.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0002967065793833002 Training loss: 7.075491428375244
2025-12-09 11:55:19.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.0002966870772347269 Training loss: 6.591805458068848
2025-12-09 11:55:20.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.0002966675181597627 Training loss: 7.0756402015686035
2025-12-09 11:55:20.609 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0002966479021659981 Training loss: 7.444648265838623
2025-12-09 11:55:20.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.00029662822926104576 Training loss: 6.824526309967041
2025-12-09 11:55:21.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.0002966084994525403 Training loss: 7.126953601837158
2025-12-09 11:55:21.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.00029658871274813853 Training loss: 6.74571418762207
2025-12-09 11:55:22.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.00029656886915551924 Training loss: 6.55606746673584
2025-12-09 11:55:22.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.0002965489686823833 Training loss: 6.85092830657959
2025-12-09 11:55:22.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.00029652901133645377 Training loss: 6.6263604164123535
2025-12-09 11:55:23.322 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.0002965089971254757 Training loss: 6.890448570251465
2025-12-09 11:55:23.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.0002964889260572162 Training loss: 6.878946781158447
2025-12-09 11:55:24.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0002964687981394644 Training loss: 7.194085121154785
2025-12-09 11:55:24.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.00029644861338003165 Training loss: 6.866166591644287
2025-12-09 11:55:24.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0002964283717867512 Training loss: 6.843255996704102
2025-12-09 11:55:25.267 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.0002964080733674784 Training loss: 6.96978759765625
2025-12-09 11:55:25.655 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.0002963877181300907 Training loss: 6.847822666168213
2025-12-09 11:55:26.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00029636730608248766 Training loss: 6.901224613189697
2025-12-09 11:55:26.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.0002963468372325906 Training loss: 6.81647253036499
2025-12-09 11:55:26.819 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.00029632631158834326 Training loss: 6.949038982391357
2025-12-09 11:55:27.207 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.00029630572915771117 Training loss: 7.763039588928223
2025-12-09 11:55:27.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.000296285089948682 Training loss: 6.977027893066406
2025-12-09 11:55:27.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.00029626439396926533 Training loss: 6.598813533782959
2025-12-09 11:55:28.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.00029624364122749294 Training loss: 6.763906002044678
2025-12-09 11:55:28.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.0002962228317314186 Training loss: 7.543667316436768
2025-12-09 11:55:29.153 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00029620196548911797 Training loss: 7.168282985687256
2025-12-09 11:55:29.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.0002961810425086889 Training loss: 7.107926368713379
2025-12-09 11:55:29.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00029616006279825126 Training loss: 7.28717041015625
2025-12-09 11:55:30.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0002961390263659467 Training loss: 7.271278381347656
2025-12-09 11:55:30.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.0002961179332199391 Training loss: 6.741591930389404
2025-12-09 11:55:31.090 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00029609678336841444 Training loss: 6.648401737213135
2025-12-09 11:55:31.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.00029607557681958035 Training loss: 8.256195068359375
2025-12-09 11:55:31.867 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.00029605431358166684 Training loss: 6.810608863830566
2025-12-09 11:55:32.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.00029603299366292565 Training loss: 6.966558933258057
2025-12-09 11:55:32.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00029601161707163077 Training loss: 8.152031898498535
2025-12-09 11:55:33.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.00029599018381607785 Training loss: 6.796609878540039
2025-12-09 11:55:33.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.0002959686939045848 Training loss: 7.142396450042725
2025-12-09 11:55:33.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.00029594714734549146 Training loss: 7.198575973510742
2025-12-09 11:55:34.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.0002959255441471597 Training loss: 7.215454578399658
2025-12-09 11:55:34.586 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.0002959038843179731 Training loss: 6.591285228729248
2025-12-09 11:55:34.974 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.0002958821678663376 Training loss: 7.332264423370361
2025-12-09 11:55:35.361 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.00029586039480068087 Training loss: 7.066429615020752
2025-12-09 11:55:35.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0002958385651294525 Training loss: 6.906401634216309
2025-12-09 11:55:36.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00029581667886112434 Training loss: 6.518440246582031
2025-12-09 11:55:36.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.00029579473600418993 Training loss: 6.7340264320373535
2025-12-09 11:55:36.915 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0002957727365671649 Training loss: 6.856858730316162
2025-12-09 11:55:37.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.0002957506805585867 Training loss: 6.723696231842041
2025-12-09 11:55:37.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00029572856798701504 Training loss: 6.975331783294678
2025-12-09 11:55:38.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0002957063988610312 Training loss: 7.777789115905762
2025-12-09 11:55:38.473 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0002956841731892386 Training loss: 7.276788711547852
2025-12-09 11:55:38.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0002956618909802627 Training loss: 7.2566633224487305
2025-12-09 11:55:39.248 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.00029563955224275065 Training loss: 7.044919013977051
2025-12-09 11:55:39.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.00029561715698537183 Training loss: 7.115651607513428
2025-12-09 11:55:40.025 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0002955947052168172 Training loss: 7.207121849060059
2025-12-09 11:55:40.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0002955721969458001 Training loss: 7.002445220947266
2025-12-09 11:55:40.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0002955496321810553 Training loss: 6.63572359085083
2025-12-09 11:55:41.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.00029552701093133994 Training loss: 7.31461763381958
2025-12-09 11:55:41.579 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.00029550433320543284 Training loss: 7.107491970062256
2025-12-09 11:55:41.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.0002954815990121347 Training loss: 6.898401260375977
2025-12-09 11:55:42.355 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00029545880836026833 Training loss: 6.694540500640869
2025-12-09 11:55:42.743 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0002954359612586782 Training loss: 6.934370040893555
2025-12-09 11:55:43.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.00029541305771623095 Training loss: 7.917392253875732
2025-12-09 11:55:43.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.00029539009774181494 Training loss: 7.324766635894775
2025-12-09 11:55:43.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.00029536708134434054 Training loss: 7.6818413734436035
2025-12-09 11:55:44.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.00029534400853273985 Training loss: 6.63301944732666
2025-12-09 11:55:44.691 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0002953208793159671 Training loss: 7.0661821365356445
2025-12-09 11:55:45.079 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.00029529769370299823 Training loss: 6.7913079261779785
2025-12-09 11:55:45.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0002952744517028311 Training loss: 6.790693759918213
2025-12-09 11:55:45.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00029525115332448555 Training loss: 6.852602481842041
2025-12-09 11:55:46.243 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.0002952277985770032 Training loss: 6.788434982299805
2025-12-09 11:55:46.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.0002952043874694475 Training loss: 7.528634548187256
2025-12-09 11:55:47.018 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.00029518092001090397 Training loss: 6.592212677001953
2025-12-09 11:55:47.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00029515739621047973 Training loss: 7.170669078826904
2025-12-09 11:55:47.800 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.000295133816077304 Training loss: 7.242835521697998
2025-12-09 11:55:48.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.0002951101796205278 Training loss: 6.829017162322998
2025-12-09 11:55:48.578 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.0002950864868493239 Training loss: 6.952441692352295
2025-12-09 11:55:48.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.00029506273777288696 Training loss: 6.785737037658691
2025-12-09 11:55:49.353 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.0002950389324004337 Training loss: 6.67531156539917
2025-12-09 11:55:49.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.00029501507074120237 Training loss: 6.839783668518066
2025-12-09 11:55:50.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.00029499115280445326 Training loss: 7.002652645111084
2025-12-09 11:55:50.517 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.0002949671785994685 Training loss: 7.393033504486084
2025-12-09 11:55:50.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.00029494314813555193 Training loss: 7.090306758880615
2025-12-09 11:55:51.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.00029491906142202934 Training loss: 6.92723274230957
2025-12-09 11:55:51.681 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.00029489491846824837 Training loss: 6.841902256011963
2025-12-09 11:55:52.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0002948707192835783 Training loss: 6.773946762084961
2025-12-09 11:55:52.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0002948464638774105 Training loss: 7.188416957855225
2025-12-09 11:55:52.851 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.00029482215225915795 Training loss: 6.714441776275635
2025-12-09 11:55:53.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.0002947977844382555 Training loss: 6.711036682128906
2025-12-09 11:55:53.628 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.0002947733604241599 Training loss: 6.9096856117248535
2025-12-09 11:55:54.016 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.00029474888022634955 Training loss: 6.783294677734375
2025-12-09 11:55:54.405 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.00029472434385432474 Training loss: 6.6444411277771
2025-12-09 11:55:54.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.0002946997513176076 Training loss: 6.908658027648926
2025-12-09 11:55:55.181 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.000294675102625742 Training loss: 7.159716606140137
2025-12-09 11:55:55.569 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.00029465039778829366 Training loss: 6.684567928314209
2025-12-09 11:55:55.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.0002946256368148499 Training loss: 6.767345905303955
2025-12-09 11:55:56.343 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.00029460081971502015 Training loss: 6.310988903045654
2025-12-09 11:55:56.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.00029457594649843534 Training loss: 7.036595344543457
2025-12-09 11:55:57.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0002945510171747483 Training loss: 6.94980525970459
2025-12-09 11:55:57.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.0002945260317536336 Training loss: 6.607010364532471
2025-12-09 11:55:57.899 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0002945009902447876 Training loss: 6.758669376373291
2025-12-09 11:55:58.288 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.00029447589265792847 Training loss: 7.242245197296143
2025-12-09 11:55:58.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.00029445073900279605 Training loss: 6.916139602661133
2025-12-09 11:55:59.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.000294425529289152 Training loss: 7.554798603057861
2025-12-09 11:55:59.452 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.00029440026352677966 Training loss: 6.71931266784668
2025-12-09 11:55:59.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.00029437494172548424 Training loss: 6.304664611816406
2025-12-09 11:56:00.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.00029434956389509263 Training loss: 6.476449489593506
2025-12-09 11:56:00.616 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.0002943241300454534 Training loss: 6.967843532562256
2025-12-09 11:56:01.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.0002942986401864371 Training loss: 6.211884498596191
2025-12-09 11:56:01.393 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0002942730943279357 Training loss: 7.030936241149902
2025-12-09 11:56:01.788 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.000294247492479863 Training loss: 6.6605143547058105
2025-12-09 11:56:02.175 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.00029422183465215474 Training loss: 7.864816188812256
2025-12-09 11:56:02.563 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00029419612085476813 Training loss: 6.859327793121338
2025-12-09 11:56:02.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.0002941703510976822 Training loss: 6.943105220794678
2025-12-09 11:56:03.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.00029414452539089776 Training loss: 6.740509986877441
2025-12-09 11:56:03.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.00029411864374443716 Training loss: 7.004462718963623
2025-12-09 11:56:04.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.0002940927061683446 Training loss: 6.916112899780273
2025-12-09 11:56:04.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.0002940667126726859 Training loss: 7.255512237548828
2025-12-09 11:56:04.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0002940406632675487 Training loss: 6.696594715118408
2025-12-09 11:56:05.283 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0002940145579630423 Training loss: 7.277486801147461
2025-12-09 11:56:05.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.00029398839676929756 Training loss: 7.448177337646484
2025-12-09 11:56:06.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.00029396217969646717 Training loss: 6.908821105957031
2025-12-09 11:56:06.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00029393590675472545 Training loss: 7.343203067779541
2025-12-09 11:56:06.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00029390957795426845 Training loss: 6.864994049072266
2025-12-09 11:56:07.230 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.0002938831933053138 Training loss: 7.097275257110596
2025-12-09 11:56:07.617 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.000293856752818101 Training loss: 6.9997453689575195
2025-12-09 11:56:08.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.00029383025650289095 Training loss: 6.802065372467041
2025-12-09 11:56:08.394 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0002938037043699664 Training loss: 6.697347640991211
2025-12-09 11:56:08.782 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.0002937770964296317 Training loss: 7.066537380218506
2025-12-09 11:56:09.170 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.0002937504326922129 Training loss: 7.726354122161865
2025-12-09 11:56:09.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.00029372371316805767 Training loss: 7.348162651062012
2025-12-09 11:56:09.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.00029369693786753534 Training loss: 7.218082427978516
2025-12-09 11:56:10.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.0002936701068010368 Training loss: 7.056727409362793
2025-12-09 11:56:10.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.0002936432199789748 Training loss: 6.955348014831543
2025-12-09 11:56:11.115 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.00029361627741178356 Training loss: 7.224085807800293
2025-12-09 11:56:11.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.00029358927910991885 Training loss: 6.798649787902832
2025-12-09 11:56:11.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.00029356222508385827 Training loss: 7.757386207580566
2025-12-09 11:56:12.280 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.000293535115344101 Training loss: 6.844369411468506
2025-12-09 11:56:12.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0002935079499011677 Training loss: 6.6421308517456055
2025-12-09 11:56:13.056 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.0002934807287656008 Training loss: 6.872237682342529
2025-12-09 11:56:13.445 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.00029345345194796435 Training loss: 6.136438369750977
2025-12-09 11:56:13.831 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.0002934261194588438 Training loss: 6.493834495544434
2025-12-09 11:56:14.220 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.00029339873130884654 Training loss: 6.425230026245117
2025-12-09 11:56:14.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.00029337128750860124 Training loss: 6.605944633483887
2025-12-09 11:56:14.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.00029334378806875836 Training loss: 6.592581272125244
2025-12-09 11:56:15.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.00029331623299998986 Training loss: 6.884833335876465
2025-12-09 11:56:15.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0002932886223129894 Training loss: 8.05169677734375
2025-12-09 11:56:16.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.000293260956018472 Training loss: 6.961767196655273
2025-12-09 11:56:16.554 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.0002932332341271746 Training loss: 6.772994518280029
2025-12-09 11:56:16.942 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00029320545664985535 Training loss: 6.947725772857666
2025-12-09 11:56:17.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.00029317762359729423 Training loss: 6.601851463317871
2025-12-09 11:56:17.718 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.00029314973498029275 Training loss: 6.551305770874023
2025-12-09 11:56:18.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0002931217908096739 Training loss: 6.838006496429443
2025-12-09 11:56:18.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0002930937910962822 Training loss: 6.8493828773498535
2025-12-09 11:56:18.883 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.00029306573585098384 Training loss: 6.894174575805664
2025-12-09 11:56:19.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.00029303762508466654 Training loss: 7.50305700302124
2025-12-09 11:56:19.660 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00029300945880823956 Training loss: 6.591477870941162
2025-12-09 11:56:20.048 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.0002929812370326336 Training loss: 7.590169906616211
2025-12-09 11:56:20.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.000292952959768801 Training loss: 6.9849629402160645
2025-12-09 11:56:20.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0002929246270277157 Training loss: 6.729162693023682
2025-12-09 11:56:21.218 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.000292896238820373 Training loss: 6.5081706047058105
2025-12-09 11:56:21.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.0002928677951577898 Training loss: 7.07421350479126
2025-12-09 11:56:21.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.00029283929605100455 Training loss: 7.203721046447754
2025-12-09 11:56:22.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0002928107415110772 Training loss: 6.953268527984619
2025-12-09 11:56:22.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0002927821315490893 Training loss: 6.819796085357666
2025-12-09 11:56:23.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0002927534661761436 Training loss: 7.048008918762207
2025-12-09 11:56:23.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.00029272474540336475 Training loss: 7.061766147613525
2025-12-09 11:56:23.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.00029269596924189875 Training loss: 6.967432022094727
2025-12-09 11:56:24.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0002926671377029129 Training loss: 6.731805801391602
2025-12-09 11:56:24.713 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.00029263825079759635 Training loss: 6.5012640953063965
2025-12-09 11:56:25.107 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.00029260930853715935 Training loss: 6.740126132965088
2025-12-09 11:56:25.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.0002925803109328339 Training loss: 6.172352313995361
2025-12-09 11:56:25.882 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0002925512579958735 Training loss: 6.933715343475342
2025-12-09 11:56:26.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.0002925221497375529 Training loss: 6.889273166656494
2025-12-09 11:56:26.659 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.00029249298616916856 Training loss: 6.673946857452393
2025-12-09 11:56:27.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.00029246376730203817 Training loss: 6.749152183532715
2025-12-09 11:56:27.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0002924344931475011 Training loss: 6.700533866882324
2025-12-09 11:56:27.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.000292405163716918 Training loss: 6.410143852233887
2025-12-09 11:56:28.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.0002923757790216711 Training loss: 6.712266445159912
2025-12-09 11:56:28.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.000292346339073164 Training loss: 6.796163082122803
2025-12-09 11:56:28.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.00029231684388282184 Training loss: 7.32967472076416
2025-12-09 11:56:29.376 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.000292287293462091 Training loss: 6.622994899749756
2025-12-09 11:56:29.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0002922576878224395 Training loss: 6.438577651977539
2025-12-09 11:56:30.158 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.00029222802697535674 Training loss: 7.111904621124268
2025-12-09 11:56:30.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.0002921983109323535 Training loss: 7.343465805053711
2025-12-09 11:56:30.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0002921685397049619 Training loss: 6.123937129974365
2025-12-09 11:56:31.324 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0002921387133047357 Training loss: 6.8111395835876465
2025-12-09 11:56:31.710 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0002921088317432499 Training loss: 7.123471736907959
2025-12-09 11:56:32.099 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.0002920788950321009 Training loss: 7.217993259429932
2025-12-09 11:56:32.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.00029204890318290666 Training loss: 6.682277202606201
2025-12-09 11:56:32.875 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0002920188562073063 Training loss: 7.332352638244629
2025-12-09 11:56:33.265 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0002919887541169605 Training loss: 6.393797397613525
2025-12-09 11:56:33.653 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.0002919585969235514 Training loss: 7.331409454345703
2025-12-09 11:56:34.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.00029192838463878236 Training loss: 7.065472602844238
2025-12-09 11:56:34.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0002918981172743781 Training loss: 7.198538303375244
2025-12-09 11:56:34.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.00029186779484208485 Training loss: 6.794921875
2025-12-09 11:56:35.211 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.0002918374173536702 Training loss: 7.326994895935059
2025-12-09 11:56:35.599 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.000291806984820923 Training loss: 6.60535192489624
2025-12-09 11:56:35.986 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.00029177649725565353 Training loss: 6.61992883682251
2025-12-09 11:56:36.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.00029174595466969344 Training loss: 6.6432785987854
2025-12-09 11:56:36.765 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.00029171535707489565 Training loss: 6.95419454574585
2025-12-09 11:56:37.152 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.0002916847044831346 Training loss: 7.160248279571533
2025-12-09 11:56:37.540 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.0002916539969063059 Training loss: 6.71013879776001
2025-12-09 11:56:37.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.0002916232343563265 Training loss: 6.905999660491943
2025-12-09 11:56:38.316 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0002915924168451349 Training loss: 6.98860502243042
2025-12-09 11:56:38.703 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0002915615443846906 Training loss: 6.276131629943848
2025-12-09 11:56:39.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0002915306169869747 Training loss: 7.065808296203613
2025-12-09 11:56:39.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0002914996346639895 Training loss: 6.955865859985352
2025-12-09 11:56:39.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.00029146859742775865 Training loss: 6.930798530578613
2025-12-09 11:56:40.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00029143750529032707 Training loss: 6.4257049560546875
2025-12-09 11:56:40.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0002914063582637611 Training loss: 7.156493663787842
2025-12-09 11:56:41.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0002913751563601481 Training loss: 7.296015739440918
2025-12-09 11:56:41.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.0002913438995915971 Training loss: 6.752323627471924
2025-12-09 11:56:41.813 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0002913125879702381 Training loss: 6.642221927642822
2025-12-09 11:56:42.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.00029128122150822263 Training loss: 6.650130271911621
2025-12-09 11:56:42.588 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.0002912498002177234 Training loss: 6.6374101638793945
2025-12-09 11:56:42.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0002912183241109344 Training loss: 6.9699387550354
2025-12-09 11:56:43.364 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.00029118679320007087 Training loss: 6.647225379943848
2025-12-09 11:56:43.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.0002911552074973693 Training loss: 7.062304973602295
2025-12-09 11:56:44.147 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.0002911235670150875 Training loss: 6.289029121398926
2025-12-09 11:56:44.535 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.0002910918717655046 Training loss: 6.671955585479736
2025-12-09 11:56:44.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00029106012176092084 Training loss: 7.213873863220215
2025-12-09 11:56:45.312 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0002910283170136578 Training loss: 6.799468994140625
2025-12-09 11:56:45.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00029099645753605827 Training loss: 6.955523490905762
2025-12-09 11:56:46.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.00029096454334048627 Training loss: 7.159491062164307
2025-12-09 11:56:46.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.0002909325744393271 Training loss: 6.622128009796143
2025-12-09 11:56:46.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.0002909005508449873 Training loss: 7.331874847412109
2025-12-09 11:56:47.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.0002908684725698946 Training loss: 6.441458702087402
2025-12-09 11:56:47.640 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0002908363396264978 Training loss: 7.276391506195068
2025-12-09 11:56:48.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.00029080415202726727 Training loss: 6.693237781524658
2025-12-09 11:56:48.420 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0002907719097846943 Training loss: 6.720526695251465
2025-12-09 11:56:48.808 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.0002907396129112915 Training loss: 6.830204486846924
2025-12-09 11:56:49.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.00029070726141959265 Training loss: 6.4827961921691895
2025-12-09 11:56:49.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.00029067485532215267 Training loss: 6.967102527618408
2025-12-09 11:56:49.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.0002906423946315478 Training loss: 6.832348823547363
2025-12-09 11:56:50.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.00029060987936037536 Training loss: 6.824953079223633
2025-12-09 11:56:50.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.00029057730952125393 Training loss: 6.756855487823486
2025-12-09 11:56:51.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0002905446851268233 Training loss: 6.802938461303711
2025-12-09 11:56:51.524 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.0002905120061897441 Training loss: 6.33267879486084
2025-12-09 11:56:51.913 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.0002904792727226987 Training loss: 6.657593727111816
2025-12-09 11:56:52.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.00029044648473839014 Training loss: 6.6889848709106445
2025-12-09 11:56:52.689 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0002904136422495429 Training loss: 6.630516052246094
2025-12-09 11:56:53.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0002903807452689024 Training loss: 6.390352249145508
2025-12-09 11:56:53.472 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00029034779380923535 Training loss: 7.108647346496582
2025-12-09 11:56:53.861 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.00029031478788332955 Training loss: 6.749696254730225
2025-12-09 11:56:54.249 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.0002902817275039941 Training loss: 6.6192426681518555
2025-12-09 11:56:54.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.00029024861268405887 Training loss: 6.893702507019043
2025-12-09 11:56:55.026 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.00029021544343637526 Training loss: 6.53472900390625
2025-12-09 11:56:55.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.00029018221977381546 Training loss: 6.485311985015869
2025-12-09 11:56:55.802 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.00029014894170927306 Training loss: 6.856256008148193
2025-12-09 11:56:56.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.0002901156092556625 Training loss: 7.32341194152832
2025-12-09 11:56:56.577 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.0002900822224259195 Training loss: 6.595483779907227
2025-12-09 11:56:56.966 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.0002900487812330009 Training loss: 6.684064865112305
