2025-12-09 11:49:25.872 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 2.9999999999999997e-06 Training loss: 12.022917747497559
2025-12-09 11:49:25.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 5.999999999999999e-06 Training loss: 12.03929328918457
2025-12-09 11:49:26.033 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 8.999999999999999e-06 Training loss: 12.031697273254395
2025-12-09 11:49:26.111 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 1.1999999999999999e-05 Training loss: 11.9603853225708
2025-12-09 11:49:26.189 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 1.4999999999999999e-05 Training loss: 12.022083282470703
2025-12-09 11:49:26.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 1.7999999999999997e-05 Training loss: 11.983308792114258
2025-12-09 11:49:26.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 2.1e-05 Training loss: 11.94221305847168
2025-12-09 11:49:26.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 2.3999999999999997e-05 Training loss: 11.926393508911133
2025-12-09 11:49:26.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 2.6999999999999996e-05 Training loss: 11.912534713745117
2025-12-09 11:49:26.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 2.9999999999999997e-05 Training loss: 11.789511680603027
2025-12-09 11:49:26.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 3.2999999999999996e-05 Training loss: 11.833733558654785
2025-12-09 11:49:26.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 3.5999999999999994e-05 Training loss: 11.715140342712402
2025-12-09 11:49:26.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 3.9e-05 Training loss: 11.670446395874023
2025-12-09 11:49:26.892 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 4.2e-05 Training loss: 11.641057968139648
2025-12-09 11:49:26.969 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 4.4999999999999996e-05 Training loss: 11.525322914123535
2025-12-09 11:49:27.047 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 4.7999999999999994e-05 Training loss: 11.559477806091309
2025-12-09 11:49:27.125 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 5.1e-05 Training loss: 11.37295150756836
2025-12-09 11:49:27.202 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 5.399999999999999e-05 Training loss: 11.32907772064209
2025-12-09 11:49:27.357 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 5.6999999999999996e-05 Training loss: 11.142629623413086
2025-12-09 11:49:27.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 5.9999999999999995e-05 Training loss: 11.212306022644043
2025-12-09 11:49:27.516 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 6.299999999999999e-05 Training loss: 11.152433395385742
2025-12-09 11:49:27.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 6.599999999999999e-05 Training loss: 11.029457092285156
2025-12-09 11:49:27.671 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 6.9e-05 Training loss: 11.05952262878418
2025-12-09 11:49:27.749 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 7.199999999999999e-05 Training loss: 11.004185676574707
2025-12-09 11:49:27.826 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 7.5e-05 Training loss: 11.053193092346191
2025-12-09 11:49:27.904 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 7.8e-05 Training loss: 11.303510665893555
2025-12-09 11:49:27.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 8.1e-05 Training loss: 10.967705726623535
2025-12-09 11:49:28.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 8.4e-05 Training loss: 10.916887283325195
2025-12-09 11:49:28.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 8.699999999999999e-05 Training loss: 10.690774917602539
2025-12-09 11:49:28.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 8.999999999999999e-05 Training loss: 10.790489196777344
2025-12-09 11:49:28.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 9.3e-05 Training loss: 10.883894920349121
2025-12-09 11:49:28.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 9.599999999999999e-05 Training loss: 10.636125564575195
2025-12-09 11:49:28.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 9.9e-05 Training loss: 10.789085388183594
2025-12-09 11:49:28.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.000102 Training loss: 10.508096694946289
2025-12-09 11:49:28.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.00010499999999999999 Training loss: 10.67058277130127
2025-12-09 11:49:28.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.00010799999999999998 Training loss: 10.597228050231934
2025-12-09 11:49:28.762 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.00011099999999999999 Training loss: 10.308250427246094
2025-12-09 11:49:28.840 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.00011399999999999999 Training loss: 10.39227294921875
2025-12-09 11:49:28.917 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.000117 Training loss: 10.377647399902344
2025-12-09 11:49:28.995 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.00011999999999999999 Training loss: 10.2415189743042
2025-12-09 11:49:29.073 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.00012299999999999998 Training loss: 10.13681697845459
2025-12-09 11:49:29.150 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.00012599999999999997 Training loss: 10.184930801391602
2025-12-09 11:49:29.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.000129 Training loss: 10.11464786529541
2025-12-09 11:49:29.308 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.00013199999999999998 Training loss: 10.063512802124023
2025-12-09 11:49:29.386 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.000135 Training loss: 9.947866439819336
2025-12-09 11:49:29.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.000138 Training loss: 9.973384857177734
2025-12-09 11:49:29.542 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.00014099999999999998 Training loss: 9.760729789733887
2025-12-09 11:49:29.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.00014399999999999998 Training loss: 9.80138111114502
2025-12-09 11:49:29.697 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.000147 Training loss: 9.841633796691895
2025-12-09 11:49:29.775 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.00015 Training loss: 9.597935676574707
2025-12-09 11:49:29.852 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.00015299999999999998 Training loss: 9.641388893127441
2025-12-09 11:49:29.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.000156 Training loss: 9.442887306213379
2025-12-09 11:49:30.008 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.000159 Training loss: 9.244016647338867
2025-12-09 11:49:30.085 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.000162 Training loss: 9.449249267578125
2025-12-09 11:49:30.163 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.000165 Training loss: 9.2712984085083
2025-12-09 11:49:30.244 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.000168 Training loss: 9.295279502868652
2025-12-09 11:49:30.321 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.00017099999999999998 Training loss: 9.213801383972168
2025-12-09 11:49:30.399 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.00017399999999999997 Training loss: 9.375447273254395
2025-12-09 11:49:30.477 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.00017699999999999997 Training loss: 8.85957145690918
2025-12-09 11:49:30.556 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.00017999999999999998 Training loss: 8.92824649810791
2025-12-09 11:49:30.634 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.00018299999999999998 Training loss: 8.835716247558594
2025-12-09 11:49:30.712 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.000186 Training loss: 8.8471040725708
2025-12-09 11:49:30.790 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.00018899999999999999 Training loss: 8.641042709350586
2025-12-09 11:49:30.868 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.00019199999999999998 Training loss: 8.723389625549316
2025-12-09 11:49:30.946 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.000195 Training loss: 8.779032707214355
2025-12-09 11:49:31.024 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.000198 Training loss: 8.61290168762207
2025-12-09 11:49:31.102 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.000201 Training loss: 8.319899559020996
2025-12-09 11:49:31.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.000204 Training loss: 8.687915802001953
2025-12-09 11:49:31.261 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.00020699999999999996 Training loss: 8.70229434967041
2025-12-09 11:49:31.339 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.00020999999999999998 Training loss: 8.46846866607666
2025-12-09 11:49:31.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.00021299999999999997 Training loss: 8.25837230682373
2025-12-09 11:49:31.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.00021599999999999996 Training loss: 8.289531707763672
2025-12-09 11:49:31.573 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.00021899999999999998 Training loss: 8.456315994262695
2025-12-09 11:49:31.651 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.00022199999999999998 Training loss: 8.081035614013672
2025-12-09 11:49:31.729 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.000225 Training loss: 8.089778900146484
2025-12-09 11:49:31.807 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.00022799999999999999 Training loss: 8.366904258728027
2025-12-09 11:49:31.885 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.00023099999999999998 Training loss: 8.236995697021484
2025-12-09 11:49:31.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.000234 Training loss: 8.283830642700195
2025-12-09 11:49:32.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.000237 Training loss: 8.00112533569336
2025-12-09 11:49:32.123 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.00023999999999999998 Training loss: 8.206353187561035
2025-12-09 11:49:32.201 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.000243 Training loss: 8.083961486816406
2025-12-09 11:49:32.279 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.00024599999999999996 Training loss: 8.037602424621582
2025-12-09 11:49:32.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.000249 Training loss: 7.384226322174072
2025-12-09 11:49:32.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.00025199999999999995 Training loss: 7.777473449707031
2025-12-09 11:49:32.512 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.00025499999999999996 Training loss: 8.31849479675293
2025-12-09 11:49:32.590 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.000258 Training loss: 7.641660690307617
2025-12-09 11:49:32.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.000261 Training loss: 7.975449085235596
2025-12-09 11:49:32.745 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.00026399999999999997 Training loss: 7.174251556396484
2025-12-09 11:49:32.823 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.000267 Training loss: 7.996501922607422
2025-12-09 11:49:32.900 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.00027 Training loss: 8.278509140014648
2025-12-09 11:49:32.978 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.00027299999999999997 Training loss: 8.074238777160645
2025-12-09 11:49:33.059 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.000276 Training loss: 7.717060089111328
2025-12-09 11:49:33.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.000279 Training loss: 8.062973976135254
2025-12-09 11:49:33.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.00028199999999999997 Training loss: 8.042778015136719
2025-12-09 11:49:33.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.000285 Training loss: 8.040776252746582
2025-12-09 11:49:33.371 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.00028799999999999995 Training loss: 7.623143672943115
2025-12-09 11:49:33.449 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.00029099999999999997 Training loss: 7.953939437866211
2025-12-09 11:49:33.527 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.000294 Training loss: 8.164855003356934
2025-12-09 11:49:33.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.00029699999999999996 Training loss: 7.768545150756836
2025-12-09 11:49:33.682 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.0003 Training loss: 7.9188008308410645
2025-12-09 11:49:33.760 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.00029999997217736103 Training loss: 8.20323657989502
2025-12-09 11:49:33.838 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00029999988870945456 Training loss: 8.005054473876953
2025-12-09 11:49:33.916 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.0002999997495963115 Training loss: 7.83929967880249
2025-12-09 11:49:33.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.00029999955483798346 Training loss: 8.53234577178955
2025-12-09 11:49:34.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.0002999993044345427 Training loss: 7.690252780914307
2025-12-09 11:49:34.154 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.0002999989983860821 Training loss: 7.840057373046875
2025-12-09 11:49:34.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.00029999863669271526 Training loss: 7.150750637054443
2025-12-09 11:49:34.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.0002999982193545762 Training loss: 8.490509986877441
2025-12-09 11:49:34.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.0002999977463718199 Training loss: 7.715825080871582
2025-12-09 11:49:34.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.00029999721774462174 Training loss: 7.819842338562012
2025-12-09 11:49:34.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.00029999663347317785 Training loss: 7.634561538696289
2025-12-09 11:49:34.623 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.00029999599355770497 Training loss: 7.623813152313232
2025-12-09 11:49:34.701 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.0002999952979984405 Training loss: 7.507184028625488
2025-12-09 11:49:34.779 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.00029999454679564244 Training loss: 7.826843738555908
2025-12-09 11:49:34.857 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.0002999937399495895 Training loss: 7.669567108154297
2025-12-09 11:49:34.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.00029999287746058093 Training loss: 7.615144729614258
2025-12-09 11:49:35.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00029999195932893676 Training loss: 7.569164276123047
2025-12-09 11:49:35.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.00029999098555499756 Training loss: 7.653378486633301
2025-12-09 11:49:35.173 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.0002999899561391246 Training loss: 7.626823902130127
2025-12-09 11:49:35.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.00029998887108169967 Training loss: 7.546725749969482
2025-12-09 11:49:35.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.0002999877303831254 Training loss: 7.783524513244629
2025-12-09 11:49:35.408 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.00029998653404382487 Training loss: 7.596975803375244
2025-12-09 11:49:35.486 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.000299985282064242 Training loss: 7.596611499786377
2025-12-09 11:49:35.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.00029998397444484104 Training loss: 7.735917568206787
2025-12-09 11:49:35.643 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.0002999826111861073 Training loss: 7.653336524963379
2025-12-09 11:49:35.721 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00029998119228854625 Training loss: 7.614952087402344
2025-12-09 11:49:35.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.0002999797177526845 Training loss: 7.855758190155029
2025-12-09 11:49:35.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.000299978187579069 Training loss: 7.687498569488525
2025-12-09 11:49:35.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.0002999766017682673 Training loss: 7.477484226226807
2025-12-09 11:49:36.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.00029997496032086775 Training loss: 7.758425235748291
2025-12-09 11:49:36.116 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00029997326323747927 Training loss: 7.753204345703125
2025-12-09 11:49:36.194 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.0002999715105187314 Training loss: 7.59666633605957
2025-12-09 11:49:36.272 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.00029996970216527436 Training loss: 7.760939121246338
2025-12-09 11:49:36.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.000299967838177779 Training loss: 7.668829441070557
2025-12-09 11:49:36.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.00029996591855693686 Training loss: 7.57655668258667
2025-12-09 11:49:36.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.00029996394330345996 Training loss: 7.664094924926758
2025-12-09 11:49:36.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.0002999619124180811 Training loss: 7.112219333648682
2025-12-09 11:49:36.662 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00029995982590155367 Training loss: 7.913653373718262
2025-12-09 11:49:36.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00029995768375465164 Training loss: 7.447478294372559
2025-12-09 11:49:36.822 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.0002999554859781698 Training loss: 7.90937614440918
2025-12-09 11:49:36.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.00029995323257292337 Training loss: 7.654564380645752
2025-12-09 11:49:36.979 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.0002999509235397483 Training loss: 7.392548561096191
2025-12-09 11:49:37.057 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.00029994855887950124 Training loss: 8.060029983520508
2025-12-09 11:49:37.136 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.00029994613859305933 Training loss: 7.40842866897583
2025-12-09 11:49:37.214 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.0002999436626813204 Training loss: 7.742898464202881
2025-12-09 11:49:37.292 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.000299941131145203 Training loss: 7.9634904861450195
2025-12-09 11:49:37.370 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.0002999385439856462 Training loss: 7.627131938934326
2025-12-09 11:49:37.448 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.0002999359012036099 Training loss: 7.68714714050293
2025-12-09 11:49:37.526 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.0002999332028000742 Training loss: 7.557322978973389
2025-12-09 11:49:37.604 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.0002999304487760404 Training loss: 7.566822052001953
2025-12-09 11:49:37.683 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.00029992763913253 Training loss: 7.989405632019043
2025-12-09 11:49:37.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.00029992477387058537 Training loss: 7.92080020904541
2025-12-09 11:49:37.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.0002999218529912694 Training loss: 7.893219470977783
2025-12-09 11:49:37.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00029991887649566564 Training loss: 8.26335334777832
2025-12-09 11:49:37.999 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.00029991584438487825 Training loss: 7.482118606567383
2025-12-09 11:49:38.077 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.0002999127566600321 Training loss: 7.431668281555176
2025-12-09 11:49:38.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.00029990961332227264 Training loss: 7.479715347290039
2025-12-09 11:49:38.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.0002999064143727659 Training loss: 8.091959953308105
2025-12-09 11:49:38.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.00029990315981269863 Training loss: 7.562514781951904
2025-12-09 11:49:38.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.0002998998496432781 Training loss: 7.498954772949219
2025-12-09 11:49:38.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.0002998964838657324 Training loss: 7.412390232086182
2025-12-09 11:49:38.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.0002998930624813101 Training loss: 7.695816516876221
2025-12-09 11:49:38.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.00029988958549128026 Training loss: 7.486027240753174
2025-12-09 11:49:38.706 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00029988605289693295 Training loss: 7.64178991317749
2025-12-09 11:49:38.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.0002998824646995785 Training loss: 7.528872966766357
2025-12-09 11:49:38.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.00029987882090054817 Training loss: 7.4131693840026855
2025-12-09 11:49:38.941 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.0002998751215011935 Training loss: 7.4649434089660645
2025-12-09 11:49:39.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.000299871366502887 Training loss: 7.277200698852539
2025-12-09 11:49:39.098 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.00029986755590702164 Training loss: 7.118022918701172
2025-12-09 11:49:39.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.000299863689715011 Training loss: 7.506657600402832
2025-12-09 11:49:39.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.0002998597679282893 Training loss: 6.9591898918151855
2025-12-09 11:49:39.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.00029985579054831146 Training loss: 7.888956546783447
2025-12-09 11:49:39.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.0002998517575765528 Training loss: 7.224789142608643
2025-12-09 11:49:39.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.00029984766901450965 Training loss: 7.393390655517578
2025-12-09 11:49:39.567 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.00029984352486369867 Training loss: 7.5859761238098145
2025-12-09 11:49:39.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.00029983932512565707 Training loss: 7.636523246765137
2025-12-09 11:49:39.727 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.00029983506980194296 Training loss: 7.344714164733887
2025-12-09 11:49:39.805 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.00029983075889413493 Training loss: 7.284005165100098
2025-12-09 11:49:39.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.00029982639240383214 Training loss: 7.697219371795654
2025-12-09 11:49:39.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.00029982197033265437 Training loss: 7.361255645751953
2025-12-09 11:49:40.041 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.00029981749268224225 Training loss: 7.5401506423950195
2025-12-09 11:49:40.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00029981295945425665 Training loss: 7.259552955627441
2025-12-09 11:49:40.197 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.00029980837065037935 Training loss: 6.888531684875488
2025-12-09 11:49:40.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.00029980372627231265 Training loss: 7.325112342834473
2025-12-09 11:49:40.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.00029979902632177945 Training loss: 7.1315131187438965
2025-12-09 11:49:40.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.0002997942708005233 Training loss: 7.490108489990234
2025-12-09 11:49:40.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.00029978945971030835 Training loss: 7.44459342956543
2025-12-09 11:49:40.593 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.0002997845930529194 Training loss: 8.354593276977539
2025-12-09 11:49:40.672 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.00029977967083016173 Training loss: 7.371376991271973
2025-12-09 11:49:40.750 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.00029977469304386133 Training loss: 7.486782073974609
2025-12-09 11:49:40.828 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.0002997696596958649 Training loss: 7.233062267303467
2025-12-09 11:49:40.906 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.0002997645707880396 Training loss: 7.0738091468811035
2025-12-09 11:49:40.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.0002997594263222733 Training loss: 7.612941265106201
2025-12-09 11:49:41.063 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.00029975422630047435 Training loss: 7.537703514099121
2025-12-09 11:49:41.141 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.00029974897072457187 Training loss: 7.292914867401123
2025-12-09 11:49:41.219 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.0002997436595965154 Training loss: 7.52146053314209
2025-12-09 11:49:41.297 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.0002997382929182754 Training loss: 7.222888469696045
2025-12-09 11:49:41.375 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.00029973287069184255 Training loss: 7.2586493492126465
2025-12-09 11:49:41.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.0002997273929192284 Training loss: 7.581826686859131
2025-12-09 11:49:41.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.0002997218596024651 Training loss: 7.263091564178467
2025-12-09 11:49:41.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.00029971627074360516 Training loss: 7.02011251449585
2025-12-09 11:49:41.692 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.00029971062634472203 Training loss: 7.642578125
2025-12-09 11:49:41.770 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.00029970492640790956 Training loss: 7.052178859710693
2025-12-09 11:49:41.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.0002996991709352822 Training loss: 7.293113708496094
2025-12-09 11:49:41.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.0002996933599289751 Training loss: 7.299419403076172
2025-12-09 11:49:42.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.000299687493391144 Training loss: 7.398262023925781
2025-12-09 11:49:42.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00029968157132396507 Training loss: 7.6060895919799805
2025-12-09 11:49:42.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00029967559372963534 Training loss: 7.498603343963623
2025-12-09 11:49:42.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.00029966956061037227 Training loss: 7.180474281311035
2025-12-09 11:49:42.318 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.00029966347196841393 Training loss: 7.59361457824707
2025-12-09 11:49:42.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.000299657327806019 Training loss: 7.484757423400879
2025-12-09 11:49:42.479 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.0002996511281254668 Training loss: 7.336860179901123
2025-12-09 11:49:42.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.0002996448729290572 Training loss: 7.502363681793213
2025-12-09 11:49:42.635 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.00029963856221911075 Training loss: 7.71257209777832
2025-12-09 11:49:42.714 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.00029963219599796843 Training loss: 7.242525577545166
2025-12-09 11:49:42.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.000299625774267992 Training loss: 7.2464399337768555
2025-12-09 11:49:42.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.0002996192970315636 Training loss: 7.1378655433654785
2025-12-09 11:49:42.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.00029961276429108625 Training loss: 6.31089448928833
2025-12-09 11:49:43.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.00029960617604898323 Training loss: 7.283717632293701
2025-12-09 11:49:43.105 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.0002995995323076986 Training loss: 7.2384796142578125
2025-12-09 11:49:43.183 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.00029959283306969705 Training loss: 6.701837539672852
2025-12-09 11:49:43.262 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.00029958607833746375 Training loss: 7.37832498550415
2025-12-09 11:49:43.340 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.0002995792681135045 Training loss: 7.7987895011901855
2025-12-09 11:49:43.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.00029957240240034564 Training loss: 7.271308898925781
2025-12-09 11:49:43.501 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.0002995654812005342 Training loss: 7.36983585357666
2025-12-09 11:49:43.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.0002995585045166376 Training loss: 7.008050441741943
2025-12-09 11:49:43.658 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00029955147235124417 Training loss: 7.186526298522949
2025-12-09 11:49:43.736 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00029954438470696247 Training loss: 7.276304721832275
2025-12-09 11:49:43.814 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.0002995372415864218 Training loss: 7.2980170249938965
2025-12-09 11:49:43.893 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.0002995300429922721 Training loss: 7.459540843963623
2025-12-09 11:49:43.971 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00029952278892718376 Training loss: 7.219966411590576
2025-12-09 11:49:44.049 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.0002995154793938479 Training loss: 7.008509635925293
2025-12-09 11:49:44.127 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.00029950811439497606 Training loss: 7.380312442779541
2025-12-09 11:49:44.205 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.0002995006939333004 Training loss: 7.123464107513428
2025-12-09 11:49:44.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.00029949321801157365 Training loss: 7.138940811157227
2025-12-09 11:49:44.366 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.00029948568663256927 Training loss: 7.8550190925598145
2025-12-09 11:49:44.444 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.0002994780997990811 Training loss: 7.453029155731201
2025-12-09 11:49:44.523 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.0002994704575139236 Training loss: 6.979371547698975
2025-12-09 11:49:44.602 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.00029946275977993175 Training loss: 7.439810276031494
2025-12-09 11:49:44.680 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.0002994550065999613 Training loss: 7.310916423797607
2025-12-09 11:49:44.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.0002994471979768884 Training loss: 7.368133068084717
2025-12-09 11:49:44.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.00029943933391360974 Training loss: 7.197159290313721
2025-12-09 11:49:44.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00029943141441304274 Training loss: 7.395552158355713
2025-12-09 11:49:44.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.00029942343947812517 Training loss: 6.909994125366211
2025-12-09 11:49:45.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.0002994154091118156 Training loss: 7.093952655792236
2025-12-09 11:49:45.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.0002994073233170929 Training loss: 7.159701347351074
2025-12-09 11:49:45.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00029939918209695676 Training loss: 6.964846134185791
2025-12-09 11:49:45.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.0002993909854544273 Training loss: 7.227149963378906
2025-12-09 11:49:45.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.00029938273339254515 Training loss: 7.2635416984558105
2025-12-09 11:49:45.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.0002993744259143716 Training loss: 7.118897438049316
2025-12-09 11:49:45.545 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.0002993660630229886 Training loss: 7.068315029144287
2025-12-09 11:49:45.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.0002993576447214983 Training loss: 7.1756720542907715
2025-12-09 11:49:45.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.0002993491710130237 Training loss: 7.512078285217285
2025-12-09 11:49:45.780 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00029934064190070836 Training loss: 7.155608177185059
2025-12-09 11:49:45.859 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.00029933205738771624 Training loss: 7.242990493774414
2025-12-09 11:49:45.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.0002993234174772319 Training loss: 7.165905952453613
2025-12-09 11:49:46.015 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.00029931472217246057 Training loss: 6.726588726043701
2025-12-09 11:49:46.094 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.0002993059714766278 Training loss: 7.088388919830322
2025-12-09 11:49:46.172 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.00029929716539297993 Training loss: 6.9714674949646
2025-12-09 11:49:46.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.00029928830392478376 Training loss: 6.74341344833374
2025-12-09 11:49:46.333 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.0002992793870753265 Training loss: 7.154893398284912
2025-12-09 11:49:46.411 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.0002992704148479161 Training loss: 6.987329959869385
2025-12-09 11:49:46.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.00029926138724588097 Training loss: 7.281476020812988
2025-12-09 11:49:46.568 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.00029925230427257004 Training loss: 6.912214756011963
2025-12-09 11:49:46.647 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.0002992431659313528 Training loss: 7.243417263031006
2025-12-09 11:49:46.725 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.00029923397222561933 Training loss: 7.840315818786621
2025-12-09 11:49:46.803 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.0002992247231587802 Training loss: 7.213523864746094
2025-12-09 11:49:46.881 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.00029921541873426647 Training loss: 7.886925220489502
2025-12-09 11:49:46.960 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.00029920605895552985 Training loss: 7.107638835906982
2025-12-09 11:49:47.038 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.0002991966438260425 Training loss: 7.42542839050293
2025-12-09 11:49:47.117 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.0002991871733492971 Training loss: 7.299022197723389
2025-12-09 11:49:47.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.00029917764752880697 Training loss: 7.3768768310546875
2025-12-09 11:49:47.278 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.0002991680663681059 Training loss: 7.52036714553833
2025-12-09 11:49:47.356 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.00029915842987074804 Training loss: 7.092085361480713
2025-12-09 11:49:47.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.0002991487380403084 Training loss: 7.474972248077393
2025-12-09 11:49:47.513 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.00029913899088038226 Training loss: 7.294279098510742
2025-12-09 11:49:47.592 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.00029912918839458555 Training loss: 7.094139099121094
2025-12-09 11:49:47.670 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.00029911933058655464 Training loss: 7.124512195587158
2025-12-09 11:49:47.748 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.00029910941745994653 Training loss: 7.50665283203125
2025-12-09 11:49:47.827 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.00029909944901843863 Training loss: 7.116771697998047
2025-12-09 11:49:47.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.0002990894252657289 Training loss: 6.9041900634765625
2025-12-09 11:49:47.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.0002990793462055359 Training loss: 6.95803165435791
2025-12-09 11:49:48.061 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.0002990692118415986 Training loss: 6.889223098754883
2025-12-09 11:49:48.144 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.0002990590221776765 Training loss: 7.020948886871338
2025-12-09 11:49:48.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.0002990487772175497 Training loss: 7.208645343780518
2025-12-09 11:49:48.301 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00029903847696501876 Training loss: 6.96891975402832
2025-12-09 11:49:48.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.00029902812142390474 Training loss: 6.951238632202148
2025-12-09 11:49:48.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.0002990177105980492 Training loss: 7.1861748695373535
2025-12-09 11:49:48.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.00029900724449131424 Training loss: 6.882845401763916
2025-12-09 11:49:48.615 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.00029899672310758243 Training loss: 6.836357116699219
2025-12-09 11:49:48.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.0002989861464507569 Training loss: 6.654816627502441
2025-12-09 11:49:48.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.0002989755145247613 Training loss: 7.014607906341553
2025-12-09 11:49:48.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.00029896482733353965 Training loss: 7.401691436767578
2025-12-09 11:49:48.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.00029895408488105665 Training loss: 7.27256965637207
2025-12-09 11:49:49.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.0002989432871712973 Training loss: 6.901540756225586
2025-12-09 11:49:49.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.0002989324342082673 Training loss: 7.48282527923584
2025-12-09 11:49:49.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00029892152599599275 Training loss: 7.106666564941406
2025-12-09 11:49:49.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.00029891056253852026 Training loss: 7.835544109344482
2025-12-09 11:49:49.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.0002988995438399169 Training loss: 7.236545562744141
2025-12-09 11:49:49.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.0002988884699042702 Training loss: 7.30889368057251
2025-12-09 11:49:49.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.0002988773407356884 Training loss: 7.317384719848633
2025-12-09 11:49:49.560 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.0002988661563382999 Training loss: 7.837449550628662
2025-12-09 11:49:49.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.0002988549167162539 Training loss: 7.297603130340576
2025-12-09 11:49:49.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.00029884362187371986 Training loss: 6.668960094451904
2025-12-09 11:49:49.796 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.0002988322718148878 Training loss: 7.190618515014648
2025-12-09 11:49:49.874 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.0002988208665439683 Training loss: 7.102064609527588
2025-12-09 11:49:49.953 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.0002988094060651923 Training loss: 6.860217571258545
2025-12-09 11:49:50.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.0002987978903828114 Training loss: 6.934686183929443
2025-12-09 11:49:50.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.00029878631950109734 Training loss: 7.077159881591797
2025-12-09 11:49:50.193 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.0002987746934243427 Training loss: 6.853156089782715
2025-12-09 11:49:50.271 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.0002987630121568604 Training loss: 7.040361404418945
2025-12-09 11:49:50.350 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.00029875127570298376 Training loss: 7.695837497711182
2025-12-09 11:49:50.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.0002987394840670666 Training loss: 6.96958589553833
2025-12-09 11:49:50.507 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.0002987276372534834 Training loss: 6.9690327644348145
2025-12-09 11:49:50.585 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.0002987157352666288 Training loss: 7.0918121337890625
2025-12-09 11:49:50.664 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.0002987037781109182 Training loss: 6.972352504730225
2025-12-09 11:49:50.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.00029869176579078714 Training loss: 7.3644609451293945
2025-12-09 11:49:50.821 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.000298679698310692 Training loss: 6.977267265319824
2025-12-09 11:49:50.905 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00029866757567510927 Training loss: 6.9114089012146
2025-12-09 11:49:50.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.0002986553978885362 Training loss: 6.998930931091309
2025-12-09 11:49:51.067 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.00029864316495549037 Training loss: 6.9258294105529785
2025-12-09 11:49:51.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0002986308768805097 Training loss: 7.066584587097168
2025-12-09 11:49:51.223 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00029861853366815275 Training loss: 7.1897101402282715
2025-12-09 11:49:51.302 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.00029860613532299845 Training loss: 6.6911773681640625
2025-12-09 11:49:51.380 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.00029859368184964624 Training loss: 7.052047252655029
2025-12-09 11:49:51.458 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.00029858117325271585 Training loss: 7.251319885253906
2025-12-09 11:49:51.536 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.00029856860953684773 Training loss: 6.826042652130127
2025-12-09 11:49:51.614 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.0002985559907067025 Training loss: 7.129032135009766
2025-12-09 11:49:51.693 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00029854331676696137 Training loss: 6.866272926330566
2025-12-09 11:49:51.771 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.000298530587722326 Training loss: 7.072092056274414
2025-12-09 11:49:51.850 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.00029851780357751853 Training loss: 6.937743186950684
2025-12-09 11:49:51.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.00029850496433728136 Training loss: 7.320376396179199
2025-12-09 11:49:52.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.0002984920700063775 Training loss: 7.310477256774902
2025-12-09 11:49:52.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.00029847912058959033 Training loss: 7.033052444458008
2025-12-09 11:49:52.168 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.00029846611609172363 Training loss: 6.689600467681885
2025-12-09 11:49:52.246 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.00029845305651760175 Training loss: 6.80612325668335
2025-12-09 11:49:52.325 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.00029843994187206933 Training loss: 7.201594829559326
2025-12-09 11:49:52.404 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.0002984267721599915 Training loss: 7.283949375152588
2025-12-09 11:49:52.482 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.0002984135473862538 Training loss: 7.17133092880249
2025-12-09 11:49:52.561 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.0002984002675557622 Training loss: 6.637888431549072
2025-12-09 11:49:52.639 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.0002983869326734432 Training loss: 7.073939800262451
2025-12-09 11:49:52.717 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.0002983735427442434 Training loss: 6.9979119300842285
2025-12-09 11:49:52.795 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.00029836009777313026 Training loss: 7.105987071990967
2025-12-09 11:49:52.878 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.00029834659776509134 Training loss: 6.672767162322998
2025-12-09 11:49:52.957 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.0002983330427251347 Training loss: 6.90500545501709
2025-12-09 11:49:53.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.0002983194326582889 Training loss: 7.249108791351318
2025-12-09 11:49:53.114 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.0002983057675696028 Training loss: 7.024021148681641
2025-12-09 11:49:53.192 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.0002982920474641457 Training loss: 6.874941349029541
2025-12-09 11:49:53.270 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.0002982782723470074 Training loss: 6.864948749542236
2025-12-09 11:49:53.349 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.000298264442223298 Training loss: 7.800896167755127
2025-12-09 11:49:53.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.00029825055709814795 Training loss: 6.908851623535156
2025-12-09 11:49:53.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.00029823661697670834 Training loss: 6.639284610748291
2025-12-09 11:49:53.584 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.00029822262186415046 Training loss: 6.547501564025879
2025-12-09 11:49:53.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.00029820857176566606 Training loss: 6.634566783905029
2025-12-09 11:49:53.741 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.0002981944666864672 Training loss: 7.455486297607422
2025-12-09 11:49:53.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.0002981803066317865 Training loss: 7.01628303527832
2025-12-09 11:49:53.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.00029816609160687697 Training loss: 7.07353401184082
2025-12-09 11:49:53.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.0002981518216170118 Training loss: 7.848316192626953
2025-12-09 11:49:54.060 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.0002981374966674848 Training loss: 6.669867038726807
2025-12-09 11:49:54.138 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.00029812311676361003 Training loss: 7.197988033294678
2025-12-09 11:49:54.216 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.00029810868191072195 Training loss: 7.114689826965332
2025-12-09 11:49:54.294 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.00029809419211417553 Training loss: 6.891899108886719
2025-12-09 11:49:54.373 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.0002980796473793459 Training loss: 7.423203468322754
2025-12-09 11:49:54.451 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.0002980650477116288 Training loss: 7.0113348960876465
2025-12-09 11:49:54.530 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00029805039311644023 Training loss: 6.705874919891357
2025-12-09 11:49:54.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.0002980356835992166 Training loss: 6.885561943054199
2025-12-09 11:49:54.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.0002980209191654146 Training loss: 6.778318881988525
2025-12-09 11:49:54.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.00029800609982051147 Training loss: 7.150258541107178
2025-12-09 11:49:54.848 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.0002979912255700046 Training loss: 6.888184547424316
2025-12-09 11:49:54.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.000297976296419412 Training loss: 6.918683052062988
2025-12-09 11:49:55.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.00029796131237427186 Training loss: 6.73431921005249
2025-12-09 11:49:55.083 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.00029794627344014276 Training loss: 7.07568359375
2025-12-09 11:49:55.162 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.00029793117962260366 Training loss: 6.878641605377197
2025-12-09 11:49:55.240 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.000297916030927254 Training loss: 6.944648742675781
2025-12-09 11:49:55.319 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.0002979008273597133 Training loss: 6.976009368896484
2025-12-09 11:49:55.397 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.0002978855689256218 Training loss: 6.81984806060791
2025-12-09 11:49:55.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.00029787025563063975 Training loss: 6.596701145172119
2025-12-09 11:49:55.553 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.0002978548874804479 Training loss: 7.0378098487854
2025-12-09 11:49:55.632 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.0002978394644807475 Training loss: 6.311950206756592
2025-12-09 11:49:55.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.0002978239866372598 Training loss: 6.731938362121582
2025-12-09 11:49:55.794 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.00029780845395572673 Training loss: 7.114897727966309
2025-12-09 11:49:55.873 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.0002977928664419104 Training loss: 7.123117446899414
2025-12-09 11:49:55.952 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.0002977772241015933 Training loss: 6.946656227111816
2025-12-09 11:49:56.030 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.00029776152694057815 Training loss: 7.067821979522705
2025-12-09 11:49:56.109 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.0002977457749646882 Training loss: 6.910171985626221
2025-12-09 11:49:56.187 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.00029772996817976693 Training loss: 6.5286688804626465
2025-12-09 11:49:56.266 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.00029771410659167806 Training loss: 7.132164478302002
2025-12-09 11:49:56.344 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.00029769819020630594 Training loss: 6.806880474090576
2025-12-09 11:49:56.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.0002976822190295548 Training loss: 6.871652603149414
2025-12-09 11:49:56.502 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.00029766619306734963 Training loss: 7.210798263549805
2025-12-09 11:49:56.580 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0002976501123256355 Training loss: 7.180484771728516
2025-12-09 11:49:56.663 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.00029763397681037787 Training loss: 6.804178714752197
2025-12-09 11:49:56.742 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.00029761778652756245 Training loss: 7.081483364105225
2025-12-09 11:49:56.820 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.00029760154148319534 Training loss: 6.968852519989014
2025-12-09 11:49:56.898 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.000297585241683303 Training loss: 7.386040210723877
2025-12-09 11:49:56.977 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.00029756888713393213 Training loss: 6.483817100524902
2025-12-09 11:49:57.055 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.00029755247784114976 Training loss: 6.551835060119629
2025-12-09 11:49:57.134 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.0002975360138110431 Training loss: 6.2974114418029785
2025-12-09 11:49:57.212 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.00029751949504972 Training loss: 6.825104713439941
2025-12-09 11:49:57.290 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.0002975029215633082 Training loss: 6.7991414070129395
2025-12-09 11:49:57.369 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.000297486293357956 Training loss: 6.379952430725098
2025-12-09 11:49:57.447 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.00029746961043983206 Training loss: 6.70382833480835
2025-12-09 11:49:57.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00029745287281512505 Training loss: 6.691915035247803
2025-12-09 11:49:57.608 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.0002974360804900442 Training loss: 7.042909145355225
2025-12-09 11:49:57.686 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.0002974192334708189 Training loss: 6.677278518676758
2025-12-09 11:49:57.764 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.00029740233176369887 Training loss: 6.87652587890625
2025-12-09 11:49:57.843 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.0002973853753749541 Training loss: 7.229581832885742
2025-12-09 11:49:57.921 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.00029736836431087493 Training loss: 6.565544605255127
2025-12-09 11:49:58.000 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.00029735129857777183 Training loss: 6.83445405960083
2025-12-09 11:49:58.078 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.00029733417818197575 Training loss: 6.96191930770874
2025-12-09 11:49:58.156 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.00029731700312983776 Training loss: 6.621639728546143
2025-12-09 11:49:58.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.0002972997734277293 Training loss: 6.774930953979492
2025-12-09 11:49:58.313 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.000297282489082042 Training loss: 6.855233669281006
2025-12-09 11:49:58.391 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00029726515009918786 Training loss: 6.657163619995117
2025-12-09 11:49:58.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.0002972477564855991 Training loss: 7.0287652015686035
2025-12-09 11:49:58.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.0002972303082477281 Training loss: 6.955630302429199
2025-12-09 11:49:58.631 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.00029721280539204774 Training loss: 6.642544746398926
2025-12-09 11:49:58.709 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.0002971952479250509 Training loss: 6.596696853637695
2025-12-09 11:49:58.787 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.000297177635853251 Training loss: 6.670385837554932
2025-12-09 11:49:58.865 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.0002971599691831815 Training loss: 6.996053218841553
2025-12-09 11:49:58.944 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00029714224792139605 Training loss: 6.119178295135498
2025-12-09 11:49:59.022 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.0002971244720744688 Training loss: 7.06197452545166
2025-12-09 11:49:59.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00029710664164899413 Training loss: 6.934283256530762
2025-12-09 11:49:59.179 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.0002970887566515864 Training loss: 6.941514015197754
2025-12-09 11:49:59.258 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.0002970708170888804 Training loss: 7.117506504058838
2025-12-09 11:49:59.336 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.0002970528229675312 Training loss: 7.174221038818359
2025-12-09 11:49:59.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.0002970347742942141 Training loss: 6.634354591369629
2025-12-09 11:49:59.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.0002970166710756244 Training loss: 6.900686740875244
2025-12-09 11:49:59.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.0002969985133184781 Training loss: 6.797189712524414
2025-12-09 11:49:59.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.0002969803010295109 Training loss: 8.17842960357666
2025-12-09 11:49:59.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.0002969620342154791 Training loss: 6.94358491897583
2025-12-09 11:49:59.811 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.0002969437128831591 Training loss: 6.982273578643799
2025-12-09 11:49:59.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.00029692533703934757 Training loss: 6.847466945648193
2025-12-09 11:49:59.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.00029690690669086127 Training loss: 6.719287395477295
2025-12-09 11:50:00.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.0002968884218445374 Training loss: 6.8390326499938965
2025-12-09 11:50:00.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.0002968698825072332 Training loss: 6.677685737609863
2025-12-09 11:50:00.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.0002968512886858262 Training loss: 7.026154518127441
2025-12-09 11:50:00.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.00029683264038721414 Training loss: 6.809884071350098
2025-12-09 11:50:00.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.00029681393761831485 Training loss: 7.0807952880859375
2025-12-09 11:50:00.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.0002967951803860665 Training loss: 6.743804454803467
2025-12-09 11:50:00.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.0002967763686974276 Training loss: 6.711183071136475
2025-12-09 11:50:00.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.00029675750255937647 Training loss: 6.981916904449463
2025-12-09 11:50:00.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.000296738581978912 Training loss: 6.4919328689575195
2025-12-09 11:50:00.758 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.00029671960696305304 Training loss: 6.96497917175293
2025-12-09 11:50:00.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.00029670057751883874 Training loss: 6.705583572387695
2025-12-09 11:50:00.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.0002966814936533285 Training loss: 7.028931617736816
2025-12-09 11:50:00.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.00029666235537360175 Training loss: 6.883530139923096
2025-12-09 11:50:01.071 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.00029664316268675824 Training loss: 6.623773574829102
2025-12-09 11:50:01.149 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.0002966239155999178 Training loss: 6.9324049949646
2025-12-09 11:50:01.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.0002966046141202205 Training loss: 6.657166004180908
2025-12-09 11:50:01.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.0002965852582548267 Training loss: 6.505704879760742
2025-12-09 11:50:01.390 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.00029656584801091663 Training loss: 6.881917953491211
2025-12-09 11:50:01.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.000296546383395691 Training loss: 6.634422779083252
2025-12-09 11:50:01.547 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.00029652686441637054 Training loss: 6.759552001953125
2025-12-09 11:50:01.626 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.00029650729108019624 Training loss: 7.0146002769470215
2025-12-09 11:50:01.704 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.0002964876633944291 Training loss: 6.7103705406188965
2025-12-09 11:50:01.784 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.00029646798136635034 Training loss: 7.206474781036377
2025-12-09 11:50:01.862 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.0002964482450032615 Training loss: 6.956394672393799
2025-12-09 11:50:01.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.00029642845431248406 Training loss: 6.673056602478027
2025-12-09 11:50:02.019 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.0002964086093013597 Training loss: 6.8764967918396
2025-12-09 11:50:02.097 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.00029638870997725046 Training loss: 7.1491875648498535
2025-12-09 11:50:02.176 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.00029636875634753824 Training loss: 6.782650470733643
2025-12-09 11:50:02.254 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.00029634874841962525 Training loss: 6.443638324737549
2025-12-09 11:50:02.338 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.00029632868620093375 Training loss: 7.563922882080078
2025-12-09 11:50:02.417 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.0002963085696989063 Training loss: 6.857316970825195
2025-12-09 11:50:02.495 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.00029628839892100535 Training loss: 7.049998760223389
2025-12-09 11:50:02.574 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.00029626817387471365 Training loss: 7.14515495300293
2025-12-09 11:50:02.652 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.00029624789456753417 Training loss: 6.77110481262207
2025-12-09 11:50:02.730 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.00029622756100698976 Training loss: 6.86090087890625
2025-12-09 11:50:02.809 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.0002962071732006237 Training loss: 6.503394603729248
2025-12-09 11:50:02.887 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.00029618673115599896 Training loss: 6.8166279792785645
2025-12-09 11:50:02.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.0002961662348806992 Training loss: 6.635363578796387
2025-12-09 11:50:03.044 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.00029614568438232766 Training loss: 6.664464473724365
2025-12-09 11:50:03.122 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.000296125079668508 Training loss: 6.486570358276367
2025-12-09 11:50:03.200 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.00029610442074688394 Training loss: 6.901461601257324
2025-12-09 11:50:03.284 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.00029608370762511935 Training loss: 6.554469108581543
2025-12-09 11:50:03.362 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.000296062940310898 Training loss: 6.614957809448242
2025-12-09 11:50:03.441 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.000296042118811924 Training loss: 6.725991725921631
2025-12-09 11:50:03.519 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.0002960212431359215 Training loss: 6.900108814239502
2025-12-09 11:50:03.597 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00029600031329063463 Training loss: 7.153680324554443
2025-12-09 11:50:03.676 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.0002959793292838277 Training loss: 6.660323619842529
2025-12-09 11:50:03.754 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.0002959582911232853 Training loss: 6.590625286102295
2025-12-09 11:50:03.832 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.00029593719881681167 Training loss: 7.173040866851807
2025-12-09 11:50:03.910 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.00029591605237223157 Training loss: 6.939190864562988
2025-12-09 11:50:03.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.0002958948517973896 Training loss: 7.402918815612793
2025-12-09 11:50:04.066 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.0002958735971001505 Training loss: 6.724474906921387
2025-12-09 11:50:04.145 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.0002958522882883991 Training loss: 6.594705104827881
2025-12-09 11:50:04.228 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.0002958309253700404 Training loss: 6.425965785980225
2025-12-09 11:50:04.307 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.00029580950835299914 Training loss: 6.970651149749756
2025-12-09 11:50:04.385 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.0002957880372452206 Training loss: 7.637017250061035
2025-12-09 11:50:04.464 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.0002957665120546697 Training loss: 7.455521106719971
2025-12-09 11:50:04.543 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.0002957449327893317 Training loss: 6.461907863616943
2025-12-09 11:50:04.621 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.00029572329945721186 Training loss: 6.714443683624268
2025-12-09 11:50:04.700 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.0002957016120663354 Training loss: 6.24020528793335
2025-12-09 11:50:04.778 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.00029567987062474767 Training loss: 6.6962151527404785
2025-12-09 11:50:04.856 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.00029565807514051406 Training loss: 6.379500865936279
2025-12-09 11:50:04.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.0002956362256217201 Training loss: 6.8755879402160645
2025-12-09 11:50:05.013 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.0002956143220764711 Training loss: 7.04494047164917
2025-12-09 11:50:05.091 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.0002955923645128927 Training loss: 6.600071907043457
2025-12-09 11:50:05.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.00029557035293913044 Training loss: 7.010098457336426
2025-12-09 11:50:05.253 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.00029554828736334994 Training loss: 6.692595958709717
2025-12-09 11:50:05.331 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.0002955261677937368 Training loss: 6.863259792327881
2025-12-09 11:50:05.409 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.00029550399423849673 Training loss: 6.352699279785156
2025-12-09 11:50:05.487 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.0002954817667058554 Training loss: 6.4724321365356445
2025-12-09 11:50:05.566 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.00029545948520405844 Training loss: 6.652831554412842
2025-12-09 11:50:05.644 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00029543714974137177 Training loss: 6.425192832946777
2025-12-09 11:50:05.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.000295414760326081 Training loss: 6.739644527435303
2025-12-09 11:50:05.801 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.0002953923169664919 Training loss: 6.768087387084961
2025-12-09 11:50:05.879 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00029536981967093033 Training loss: 6.982234001159668
2025-12-09 11:50:05.958 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.00029534726844774196 Training loss: 5.84339714050293
2025-12-09 11:50:06.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.00029532466330529277 Training loss: 6.096770286560059
2025-12-09 11:50:06.119 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.00029530200425196835 Training loss: 6.9405598640441895
2025-12-09 11:50:06.198 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.00029527929129617464 Training loss: 5.648384094238281
2025-12-09 11:50:06.276 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.00029525652444633736 Training loss: 6.780670166015625
2025-12-09 11:50:06.354 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.0002952337037109023 Training loss: 6.37116813659668
2025-12-09 11:50:06.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.0002952108290983353 Training loss: 6.858189105987549
2025-12-09 11:50:06.511 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.00029518790061712204 Training loss: 6.550508975982666
2025-12-09 11:50:06.589 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.0002951649182757683 Training loss: 6.638645172119141
2025-12-09 11:50:06.667 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.00029514188208279977 Training loss: 6.69959020614624
2025-12-09 11:50:06.746 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.0002951187920467622 Training loss: 6.8729166984558105
2025-12-09 11:50:06.824 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.0002950956481762213 Training loss: 6.840127944946289
2025-12-09 11:50:06.903 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.0002950724504797626 Training loss: 6.349193572998047
2025-12-09 11:50:06.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.0002950491989659918 Training loss: 6.79841947555542
2025-12-09 11:50:07.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.00029502589364353447 Training loss: 6.701127529144287
2025-12-09 11:50:07.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.00029500253452103615 Training loss: 7.182693004608154
2025-12-09 11:50:07.222 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.00029497912160716234 Training loss: 6.897195816040039
2025-12-09 11:50:07.300 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.0002949556549105985 Training loss: 6.567178249359131
2025-12-09 11:50:07.378 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.00029493213444005 Training loss: 7.269718647003174
2025-12-09 11:50:07.456 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.0002949085602042422 Training loss: 7.823538303375244
2025-12-09 11:50:07.534 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.00029488493221192043 Training loss: 6.525765895843506
2025-12-09 11:50:07.612 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.00029486125047184985 Training loss: 7.020506381988525
2025-12-09 11:50:07.690 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.0002948375149928158 Training loss: 6.776528835296631
2025-12-09 11:50:07.769 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.0002948137257836233 Training loss: 6.999644756317139
2025-12-09 11:50:07.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.0002947898828530974 Training loss: 6.735950469970703
2025-12-09 11:50:07.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.000294765986210083 Training loss: 6.5086588859558105
2025-12-09 11:50:08.009 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.0002947420358634451 Training loss: 7.060620307922363
2025-12-09 11:50:08.088 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.00029471803182206855 Training loss: 6.552618026733398
2025-12-09 11:50:08.166 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.000294693974094858 Training loss: 6.454341888427734
2025-12-09 11:50:08.245 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0002946698626907382 Training loss: 7.0145182609558105
2025-12-09 11:50:08.323 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.00029464569761865366 Training loss: 6.753801345825195
2025-12-09 11:50:08.402 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.0002946214788875689 Training loss: 6.971619129180908
2025-12-09 11:50:08.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.00029459720650646824 Training loss: 6.3994526863098145
2025-12-09 11:50:08.558 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.00029457288048435605 Training loss: 7.024234294891357
2025-12-09 11:50:08.637 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.00029454850083025644 Training loss: 6.470425128936768
2025-12-09 11:50:08.715 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.0002945240675532136 Training loss: 7.077261924743652
2025-12-09 11:50:08.793 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.0002944995806622914 Training loss: 6.905831336975098
2025-12-09 11:50:08.871 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.0002944750401665738 Training loss: 6.554381847381592
2025-12-09 11:50:08.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.00029445044607516447 Training loss: 6.639280796051025
2025-12-09 11:50:09.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.00029442579839718703 Training loss: 6.992579460144043
2025-12-09 11:50:09.112 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.0002944010971417851 Training loss: 6.784980773925781
2025-12-09 11:50:09.190 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.000294376342318122 Training loss: 6.531332015991211
2025-12-09 11:50:09.269 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.000294351533935381 Training loss: 6.609664440155029
2025-12-09 11:50:09.347 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.00029432667200276515 Training loss: 6.488472938537598
2025-12-09 11:50:09.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.0002943017565294976 Training loss: 7.112024784088135
2025-12-09 11:50:09.504 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.0002942767875248211 Training loss: 6.856496810913086
2025-12-09 11:50:09.582 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.0002942517649979984 Training loss: 6.899887561798096
2025-12-09 11:50:09.661 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.00029422668895831203 Training loss: 6.329341411590576
2025-12-09 11:50:09.739 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00029420155941506447 Training loss: 6.565956115722656
2025-12-09 11:50:09.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.00029417637637757797 Training loss: 6.3351569175720215
2025-12-09 11:50:09.901 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.00029415113985519463 Training loss: 6.880064010620117
2025-12-09 11:50:09.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.0002941258498572764 Training loss: 6.735240459442139
2025-12-09 11:50:10.058 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.0002941005063932051 Training loss: 6.653906345367432
2025-12-09 11:50:10.137 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.0002940751094723823 Training loss: 6.8442912101745605
2025-12-09 11:50:10.215 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.00029404965910422953 Training loss: 6.600853443145752
2025-12-09 11:50:10.293 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.00029402415529818804 Training loss: 7.222557544708252
2025-12-09 11:50:10.372 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.00029399859806371895 Training loss: 6.922097682952881
2025-12-09 11:50:10.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.0002939729874103032 Training loss: 7.018787860870361
2025-12-09 11:50:10.528 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.00029394732334744146 Training loss: 6.635349750518799
2025-12-09 11:50:10.606 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.00029392160588465434 Training loss: 6.588504314422607
2025-12-09 11:50:10.685 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.0002938958350314823 Training loss: 6.642216682434082
2025-12-09 11:50:10.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.00029387001079748536 Training loss: 7.201101303100586
2025-12-09 11:50:10.847 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.0002938441331922436 Training loss: 6.653697490692139
2025-12-09 11:50:10.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.0002938182022253568 Training loss: 6.75634765625
2025-12-09 11:50:11.004 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.0002937922179064445 Training loss: 6.732439994812012
2025-12-09 11:50:11.082 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.000293766180245146 Training loss: 6.878624439239502
2025-12-09 11:50:11.161 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.00029374008925112056 Training loss: 6.757541656494141
2025-12-09 11:50:11.239 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.00029371394493404705 Training loss: 6.581099510192871
2025-12-09 11:50:11.317 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.00029368774730362425 Training loss: 6.819985389709473
2025-12-09 11:50:11.396 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.0002936614963695706 Training loss: 6.368555068969727
2025-12-09 11:50:11.474 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.0002936351921416244 Training loss: 6.36779260635376
2025-12-09 11:50:11.552 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.0002936088346295436 Training loss: 6.76501989364624
2025-12-09 11:50:11.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.0002935824238431062 Training loss: 6.898863315582275
2025-12-09 11:50:11.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.0002935559597921096 Training loss: 6.657962322235107
2025-12-09 11:50:11.792 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.00029352944248637117 Training loss: 6.896695137023926
2025-12-09 11:50:11.870 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.00029350287193572806 Training loss: 6.728277683258057
2025-12-09 11:50:11.949 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.000293476248150037 Training loss: 7.098748683929443
2025-12-09 11:50:12.027 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.00029344957113917473 Training loss: 6.648132801055908
2025-12-09 11:50:12.106 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.0002934228409130374 Training loss: 6.438867568969727
2025-12-09 11:50:12.184 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.0002933960574815412 Training loss: 7.323878288269043
2025-12-09 11:50:12.263 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.0002933692208546219 Training loss: 6.703121185302734
2025-12-09 11:50:12.341 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.00029334233104223506 Training loss: 6.689023017883301
2025-12-09 11:50:12.419 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.00029331538805435595 Training loss: 7.045675754547119
2025-12-09 11:50:12.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.00029328839190097955 Training loss: 6.794045448303223
2025-12-09 11:50:12.576 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.00029326134259212064 Training loss: 6.451050281524658
2025-12-09 11:50:12.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.0002932342401378136 Training loss: 6.675292491912842
2025-12-09 11:50:12.738 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0002932070845481126 Training loss: 6.417590618133545
2025-12-09 11:50:12.817 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.00029317987583309156 Training loss: 7.3681721687316895
2025-12-09 11:50:12.895 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.00029315261400284404 Training loss: 6.810523509979248
2025-12-09 11:50:12.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.0002931252990674832 Training loss: 6.74442195892334
2025-12-09 11:50:13.052 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.0002930979310371422 Training loss: 6.8791890144348145
2025-12-09 11:50:13.131 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.0002930705099219736 Training loss: 6.514463901519775
2025-12-09 11:50:13.209 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.0002930430357321498 Training loss: 6.531470775604248
2025-12-09 11:50:13.287 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.0002930155084778629 Training loss: 6.357743263244629
2025-12-09 11:50:13.365 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.0002929879281693246 Training loss: 6.264065742492676
2025-12-09 11:50:13.443 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.0002929602948167663 Training loss: 5.8646464347839355
2025-12-09 11:50:13.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.0002929326084304392 Training loss: 6.782566547393799
2025-12-09 11:50:13.601 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.00029290486902061396 Training loss: 6.816980838775635
2025-12-09 11:50:13.684 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.0002928770765975811 Training loss: 6.637170314788818
2025-12-09 11:50:13.763 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.00029284923117165075 Training loss: 6.7876973152160645
2025-12-09 11:50:13.841 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.0002928213327531526 Training loss: 6.417848110198975
2025-12-09 11:50:13.920 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.00029279338135243624 Training loss: 7.452795028686523
2025-12-09 11:50:13.998 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.0002927653769798706 Training loss: 6.790030002593994
2025-12-09 11:50:14.076 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.00029273731964584446 Training loss: 6.493849754333496
2025-12-09 11:50:14.155 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.00029270920936076624 Training loss: 6.689105987548828
2025-12-09 11:50:14.233 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.00029268104613506396 Training loss: 6.50053596496582
2025-12-09 11:50:14.311 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.00029265282997918533 Training loss: 6.975635051727295
2025-12-09 11:50:14.389 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.00029262456090359756 Training loss: 6.36379337310791
2025-12-09 11:50:14.468 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.0002925962389187877 Training loss: 7.249479293823242
2025-12-09 11:50:14.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.0002925678640352622 Training loss: 6.629060745239258
2025-12-09 11:50:14.630 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.00029253943626354734 Training loss: 6.683732032775879
2025-12-09 11:50:14.708 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.0002925109556141889 Training loss: 7.426257133483887
2025-12-09 11:50:14.786 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.0002924824220977523 Training loss: 6.822257995605469
2025-12-09 11:50:14.864 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.0002924538357248226 Training loss: 6.664454460144043
2025-12-09 11:50:14.943 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.00029242519650600436 Training loss: 7.069363594055176
2025-12-09 11:50:15.021 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.0002923965044519219 Training loss: 6.289216995239258
2025-12-09 11:50:15.100 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.0002923677595732191 Training loss: 6.475266456604004
2025-12-09 11:50:15.178 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.0002923389618805593 Training loss: 6.549525737762451
2025-12-09 11:50:15.257 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.00029231011138462564 Training loss: 6.27994441986084
2025-12-09 11:50:15.335 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.0002922812080961207 Training loss: 6.704629898071289
2025-12-09 11:50:15.413 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.0002922522520257667 Training loss: 6.3486127853393555
2025-12-09 11:50:15.491 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.0002922232431843054 Training loss: 6.625386714935303
2025-12-09 11:50:15.575 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.00029219418158249824 Training loss: 6.681668281555176
2025-12-09 11:50:15.654 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.0002921650672311261 Training loss: 6.6633219718933105
2025-12-09 11:50:15.732 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.0002921359001409895 Training loss: 6.711310863494873
2025-12-09 11:50:15.810 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.0002921066803229085 Training loss: 6.673356533050537
2025-12-09 11:50:15.889 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.00029207740778772277 Training loss: 6.866294860839844
2025-12-09 11:50:15.968 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.00029204808254629146 Training loss: 6.758859634399414
2025-12-09 11:50:16.046 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.00029201870460949326 Training loss: 6.694269180297852
2025-12-09 11:50:16.124 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.00029198927398822657 Training loss: 6.560081481933594
2025-12-09 11:50:16.203 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.0002919597906934092 Training loss: 6.610280990600586
2025-12-09 11:50:16.281 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.0002919302547359785 Training loss: 7.726731777191162
2025-12-09 11:50:16.359 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.0002919006661268914 Training loss: 6.866664886474609
2025-12-09 11:50:16.438 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.0002918710248771243 Training loss: 6.6115193367004395
2025-12-09 11:50:16.522 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.0002918413309976732 Training loss: 6.626249313354492
2025-12-09 11:50:16.600 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00029181158449955363 Training loss: 6.644842147827148
2025-12-09 11:50:16.679 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.0002917817853938005 Training loss: 7.202512264251709
2025-12-09 11:50:16.757 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.0002917519336914684 Training loss: 6.417269706726074
2025-12-09 11:50:16.836 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.00029172202940363145 Training loss: 6.740836143493652
2025-12-09 11:50:16.914 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.0002916920725413831 Training loss: 6.077275276184082
2025-12-09 11:50:16.992 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.00029166206311583644 Training loss: 6.747816562652588
2025-12-09 11:50:17.070 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.000291632001138124 Training loss: 6.482982635498047
2025-12-09 11:50:17.148 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.0002916018866193978 Training loss: 5.952305793762207
2025-12-09 11:50:17.226 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.0002915717195708295 Training loss: 6.628898620605469
2025-12-09 11:50:17.305 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.00029154150000360995 Training loss: 6.3786163330078125
2025-12-09 11:50:17.383 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.00029151122792894985 Training loss: 6.881253242492676
2025-12-09 11:50:17.467 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.000291480903358079 Training loss: 6.617435932159424
2025-12-09 11:50:17.546 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.00029145052630224696 Training loss: 6.695003032684326
2025-12-09 11:50:17.624 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.0002914200967727227 Training loss: 7.191332817077637
2025-12-09 11:50:17.702 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00029138961478079455 Training loss: 6.811758995056152
2025-12-09 11:50:17.781 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.00029135908033777033 Training loss: 6.8850507736206055
2025-12-09 11:50:17.860 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.00029132849345497755 Training loss: 6.766876220703125
2025-12-09 11:50:17.938 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.00029129785414376276 Training loss: 6.493654727935791
2025-12-09 11:50:18.017 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.00029126716241549224 Training loss: 6.788607597351074
2025-12-09 11:50:18.095 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.0002912364182815517 Training loss: 6.7405571937561035
2025-12-09 11:50:18.174 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.00029120562175334624 Training loss: 6.699353218078613
2025-12-09 11:50:18.252 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.0002911747728423004 Training loss: 6.798404216766357
2025-12-09 11:50:18.330 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.00029114387155985814 Training loss: 6.732692241668701
2025-12-09 11:50:18.414 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.0002911129179174828 Training loss: 6.592498302459717
2025-12-09 11:50:18.492 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.00029108191192665734 Training loss: 6.4325642585754395
2025-12-09 11:50:18.571 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.00029105085359888396 Training loss: 7.313146114349365
2025-12-09 11:50:18.649 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.00029101974294568425 Training loss: 6.778499603271484
2025-12-09 11:50:18.728 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.0002909885799785993 Training loss: 5.925474643707275
2025-12-09 11:50:18.806 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.0002909573647091897 Training loss: 7.056021213531494
2025-12-09 11:50:18.884 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.00029092609714903523 Training loss: 7.646312236785889
2025-12-09 11:50:18.963 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.00029089477730973517 Training loss: 7.285665512084961
2025-12-09 11:50:19.042 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.0002908634052029083 Training loss: 6.762640476226807
2025-12-09 11:50:19.120 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.0002908319808401925 Training loss: 6.577089309692383
2025-12-09 11:50:19.199 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.0002908005042332454 Training loss: 6.624037742614746
2025-12-09 11:50:19.277 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00029076897539374375 Training loss: 6.868714332580566
2025-12-09 11:50:19.360 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.00029073739433338377 Training loss: 6.671572208404541
2025-12-09 11:50:19.439 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.000290705761063881 Training loss: 6.546563148498535
2025-12-09 11:50:19.518 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.00029067407559697046 Training loss: 7.165592670440674
2025-12-09 11:50:19.595 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.0002906423379444063 Training loss: 6.791481971740723
2025-12-09 11:50:19.674 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.00029061054811796243 Training loss: 6.666398048400879
2025-12-09 11:50:19.752 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.0002905787061294317 Training loss: 6.694204330444336
2025-12-09 11:50:19.830 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.00029054681199062657 Training loss: 6.774204730987549
2025-12-09 11:50:19.909 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.00029051486571337877 Training loss: 6.8090596199035645
2025-12-09 11:50:19.987 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.00029048286730953924 Training loss: 6.506462097167969
2025-12-09 11:50:20.065 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.0002904508167909785 Training loss: 6.654618740081787
2025-12-09 11:50:20.143 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.00029041871416958623 Training loss: 6.535301208496094
2025-12-09 11:50:20.221 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00029038655945727153 Training loss: 6.320193290710449
2025-12-09 11:50:20.306 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.0002903543526659628 Training loss: 6.574721336364746
2025-12-09 11:50:20.384 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00029032209380760765 Training loss: 6.695728778839111
2025-12-09 11:50:20.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.0002902897828941732 Training loss: 6.506008148193359
2025-12-09 11:50:20.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.0002902574199376457 Training loss: 6.803055286407471
2025-12-09 11:50:20.619 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.00029022500495003086 Training loss: 6.829071998596191
2025-12-09 11:50:20.698 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.0002901925379433536 Training loss: 6.476319789886475
2025-12-09 11:50:20.776 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.0002901600189296581 Training loss: 6.542169570922852
2025-12-09 11:50:20.854 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.000290127447921008 Training loss: 6.9863600730896
2025-12-09 11:50:20.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.00029009482492948607 Training loss: 7.095929145812988
2025-12-09 11:50:21.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.00029006214996719437 Training loss: 6.917407512664795
2025-12-09 11:50:21.089 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.0002900294230462543 Training loss: 6.540807723999023
2025-12-09 11:50:21.167 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.00028999664417880654 Training loss: 7.4326934814453125
2025-12-09 11:50:21.251 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.000289963813377011 Training loss: 6.614760398864746
2025-12-09 11:50:21.329 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.0002899309306530469 Training loss: 6.843942165374756
2025-12-09 11:50:21.407 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.0002898979960191127 Training loss: 6.404439449310303
2025-12-09 11:50:21.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.000289865009487426 Training loss: 6.751867771148682
2025-12-09 11:50:21.564 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00028983197107022396 Training loss: 7.012430667877197
2025-12-09 11:50:21.642 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.0002897988807797627 Training loss: 7.57473087310791
2025-12-09 11:50:21.720 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.00028976573862831757 Training loss: 6.704747676849365
2025-12-09 11:50:21.799 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.0002897325446281834 Training loss: 6.527198791503906
2025-12-09 11:50:21.877 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.0002896992987916741 Training loss: 6.707852363586426
2025-12-09 11:50:21.955 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.00028966600113112276 Training loss: 6.727788925170898
2025-12-09 11:50:22.034 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.00028963265165888187 Training loss: 7.038327217102051
