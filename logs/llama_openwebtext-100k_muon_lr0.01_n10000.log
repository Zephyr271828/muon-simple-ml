2025-12-09 12:41:21.234 | INFO     | __main__:train:25 - Epoch: 0 Step: 0 LR: 0.0001 Training loss: 12.170351028442383
2025-12-09 12:41:21.731 | INFO     | __main__:train:25 - Epoch: 0 Step: 1 LR: 0.0002 Training loss: 12.115022659301758
2025-12-09 12:41:22.227 | INFO     | __main__:train:25 - Epoch: 0 Step: 2 LR: 0.0003 Training loss: 12.136765480041504
2025-12-09 12:41:22.723 | INFO     | __main__:train:25 - Epoch: 0 Step: 3 LR: 0.0004 Training loss: 12.105313301086426
2025-12-09 12:41:23.557 | INFO     | __main__:train:25 - Epoch: 0 Step: 4 LR: 0.0005 Training loss: 12.0382719039917
2025-12-09 12:41:24.053 | INFO     | __main__:train:25 - Epoch: 0 Step: 5 LR: 0.0006 Training loss: 11.724254608154297
2025-12-09 12:41:24.548 | INFO     | __main__:train:25 - Epoch: 0 Step: 6 LR: 0.0007000000000000001 Training loss: 11.699348449707031
2025-12-09 12:41:25.045 | INFO     | __main__:train:25 - Epoch: 0 Step: 7 LR: 0.0008 Training loss: 11.32693862915039
2025-12-09 12:41:25.541 | INFO     | __main__:train:25 - Epoch: 0 Step: 8 LR: 0.0009 Training loss: 10.357242584228516
2025-12-09 12:41:26.036 | INFO     | __main__:train:25 - Epoch: 0 Step: 9 LR: 0.001 Training loss: 9.782731056213379
2025-12-09 12:41:26.532 | INFO     | __main__:train:25 - Epoch: 0 Step: 10 LR: 0.0011 Training loss: 8.96280288696289
2025-12-09 12:41:27.029 | INFO     | __main__:train:25 - Epoch: 0 Step: 11 LR: 0.0012 Training loss: 9.345754623413086
2025-12-09 12:41:27.525 | INFO     | __main__:train:25 - Epoch: 0 Step: 12 LR: 0.0013000000000000002 Training loss: 8.736298561096191
2025-12-09 12:41:28.020 | INFO     | __main__:train:25 - Epoch: 0 Step: 13 LR: 0.0014000000000000002 Training loss: 8.561903953552246
2025-12-09 12:41:28.515 | INFO     | __main__:train:25 - Epoch: 0 Step: 14 LR: 0.0015 Training loss: 8.478850364685059
2025-12-09 12:41:29.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 15 LR: 0.0016 Training loss: 9.146171569824219
2025-12-09 12:41:29.510 | INFO     | __main__:train:25 - Epoch: 0 Step: 16 LR: 0.0017000000000000001 Training loss: 8.515888214111328
2025-12-09 12:41:30.011 | INFO     | __main__:train:25 - Epoch: 0 Step: 17 LR: 0.0018 Training loss: 9.202595710754395
2025-12-09 12:41:30.506 | INFO     | __main__:train:25 - Epoch: 0 Step: 18 LR: 0.0019 Training loss: 8.505722045898438
2025-12-09 12:41:31.007 | INFO     | __main__:train:25 - Epoch: 0 Step: 19 LR: 0.002 Training loss: 9.404093742370605
2025-12-09 12:41:31.508 | INFO     | __main__:train:25 - Epoch: 0 Step: 20 LR: 0.0021 Training loss: 9.329771041870117
2025-12-09 12:41:32.005 | INFO     | __main__:train:25 - Epoch: 0 Step: 21 LR: 0.0022 Training loss: 9.33764934539795
2025-12-09 12:41:32.503 | INFO     | __main__:train:25 - Epoch: 0 Step: 22 LR: 0.0023 Training loss: 9.133565902709961
2025-12-09 12:41:33.001 | INFO     | __main__:train:25 - Epoch: 0 Step: 23 LR: 0.0024 Training loss: 9.874764442443848
2025-12-09 12:41:33.497 | INFO     | __main__:train:25 - Epoch: 0 Step: 24 LR: 0.0025 Training loss: 9.056174278259277
2025-12-09 12:41:33.996 | INFO     | __main__:train:25 - Epoch: 0 Step: 25 LR: 0.0026000000000000003 Training loss: 9.369487762451172
2025-12-09 12:41:34.493 | INFO     | __main__:train:25 - Epoch: 0 Step: 26 LR: 0.0027 Training loss: 9.244820594787598
2025-12-09 12:41:34.991 | INFO     | __main__:train:25 - Epoch: 0 Step: 27 LR: 0.0028000000000000004 Training loss: 9.88313102722168
2025-12-09 12:41:35.489 | INFO     | __main__:train:25 - Epoch: 0 Step: 28 LR: 0.0029 Training loss: 9.330126762390137
2025-12-09 12:41:35.993 | INFO     | __main__:train:25 - Epoch: 0 Step: 29 LR: 0.003 Training loss: 9.169276237487793
2025-12-09 12:41:36.490 | INFO     | __main__:train:25 - Epoch: 0 Step: 30 LR: 0.0031 Training loss: 9.677091598510742
2025-12-09 12:41:36.988 | INFO     | __main__:train:25 - Epoch: 0 Step: 31 LR: 0.0032 Training loss: 8.987390518188477
2025-12-09 12:41:37.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 32 LR: 0.0033000000000000004 Training loss: 9.408859252929688
2025-12-09 12:41:37.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 33 LR: 0.0034000000000000002 Training loss: 9.06749153137207
2025-12-09 12:41:38.484 | INFO     | __main__:train:25 - Epoch: 0 Step: 34 LR: 0.0034999999999999996 Training loss: 10.785601615905762
2025-12-09 12:41:38.983 | INFO     | __main__:train:25 - Epoch: 0 Step: 35 LR: 0.0036 Training loss: 8.660170555114746
2025-12-09 12:41:39.485 | INFO     | __main__:train:25 - Epoch: 0 Step: 36 LR: 0.0037 Training loss: 9.177667617797852
2025-12-09 12:41:39.984 | INFO     | __main__:train:25 - Epoch: 0 Step: 37 LR: 0.0038 Training loss: 8.496101379394531
2025-12-09 12:41:40.483 | INFO     | __main__:train:25 - Epoch: 0 Step: 38 LR: 0.0039000000000000003 Training loss: 9.14905834197998
2025-12-09 12:41:40.981 | INFO     | __main__:train:25 - Epoch: 0 Step: 39 LR: 0.004 Training loss: 8.667450904846191
2025-12-09 12:41:41.480 | INFO     | __main__:train:25 - Epoch: 0 Step: 40 LR: 0.0040999999999999995 Training loss: 8.640336990356445
2025-12-09 12:41:41.980 | INFO     | __main__:train:25 - Epoch: 0 Step: 41 LR: 0.0042 Training loss: 9.03247356414795
2025-12-09 12:41:42.478 | INFO     | __main__:train:25 - Epoch: 0 Step: 42 LR: 0.0043 Training loss: 9.406944274902344
2025-12-09 12:41:42.976 | INFO     | __main__:train:25 - Epoch: 0 Step: 43 LR: 0.0044 Training loss: 9.051220893859863
2025-12-09 12:41:43.475 | INFO     | __main__:train:25 - Epoch: 0 Step: 44 LR: 0.0045000000000000005 Training loss: 8.862008094787598
2025-12-09 12:41:43.973 | INFO     | __main__:train:25 - Epoch: 0 Step: 45 LR: 0.0046 Training loss: 8.104360580444336
2025-12-09 12:41:44.471 | INFO     | __main__:train:25 - Epoch: 0 Step: 46 LR: 0.0047 Training loss: 8.636046409606934
2025-12-09 12:41:44.970 | INFO     | __main__:train:25 - Epoch: 0 Step: 47 LR: 0.0048 Training loss: 8.648259162902832
2025-12-09 12:41:45.469 | INFO     | __main__:train:25 - Epoch: 0 Step: 48 LR: 0.0049 Training loss: 8.776143074035645
2025-12-09 12:41:45.967 | INFO     | __main__:train:25 - Epoch: 0 Step: 49 LR: 0.005 Training loss: 8.685002326965332
2025-12-09 12:41:46.466 | INFO     | __main__:train:25 - Epoch: 0 Step: 50 LR: 0.0051 Training loss: 8.731597900390625
2025-12-09 12:41:46.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 51 LR: 0.005200000000000001 Training loss: 8.18691349029541
2025-12-09 12:41:47.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 52 LR: 0.0053 Training loss: 8.423876762390137
2025-12-09 12:41:47.965 | INFO     | __main__:train:25 - Epoch: 0 Step: 53 LR: 0.0054 Training loss: 8.70719051361084
2025-12-09 12:41:48.465 | INFO     | __main__:train:25 - Epoch: 0 Step: 54 LR: 0.0055000000000000005 Training loss: 8.839229583740234
2025-12-09 12:41:48.964 | INFO     | __main__:train:25 - Epoch: 0 Step: 55 LR: 0.005600000000000001 Training loss: 8.728604316711426
2025-12-09 12:41:49.463 | INFO     | __main__:train:25 - Epoch: 0 Step: 56 LR: 0.005699999999999999 Training loss: 8.007814407348633
2025-12-09 12:41:49.962 | INFO     | __main__:train:25 - Epoch: 0 Step: 57 LR: 0.0058 Training loss: 8.653824806213379
2025-12-09 12:41:50.460 | INFO     | __main__:train:25 - Epoch: 0 Step: 58 LR: 0.0059 Training loss: 8.158750534057617
2025-12-09 12:41:50.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 59 LR: 0.006 Training loss: 8.165692329406738
2025-12-09 12:41:51.457 | INFO     | __main__:train:25 - Epoch: 0 Step: 60 LR: 0.0061 Training loss: 7.857446670532227
2025-12-09 12:41:51.956 | INFO     | __main__:train:25 - Epoch: 0 Step: 61 LR: 0.0062 Training loss: 7.980341911315918
2025-12-09 12:41:52.455 | INFO     | __main__:train:25 - Epoch: 0 Step: 62 LR: 0.0063 Training loss: 8.464825630187988
2025-12-09 12:41:52.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 63 LR: 0.0064 Training loss: 8.186655044555664
2025-12-09 12:41:53.453 | INFO     | __main__:train:25 - Epoch: 0 Step: 64 LR: 0.006500000000000001 Training loss: 8.214642524719238
2025-12-09 12:41:53.954 | INFO     | __main__:train:25 - Epoch: 0 Step: 65 LR: 0.006600000000000001 Training loss: 7.9993696212768555
2025-12-09 12:41:54.454 | INFO     | __main__:train:25 - Epoch: 0 Step: 66 LR: 0.0067 Training loss: 7.882334232330322
2025-12-09 12:41:54.951 | INFO     | __main__:train:25 - Epoch: 0 Step: 67 LR: 0.0068000000000000005 Training loss: 8.345382690429688
2025-12-09 12:41:55.450 | INFO     | __main__:train:25 - Epoch: 0 Step: 68 LR: 0.0069 Training loss: 8.35860538482666
2025-12-09 12:41:55.950 | INFO     | __main__:train:25 - Epoch: 0 Step: 69 LR: 0.006999999999999999 Training loss: 7.529832363128662
2025-12-09 12:41:56.446 | INFO     | __main__:train:25 - Epoch: 0 Step: 70 LR: 0.0070999999999999995 Training loss: 7.9425883293151855
2025-12-09 12:41:56.945 | INFO     | __main__:train:25 - Epoch: 0 Step: 71 LR: 0.0072 Training loss: 7.982936382293701
2025-12-09 12:41:57.442 | INFO     | __main__:train:25 - Epoch: 0 Step: 72 LR: 0.0073 Training loss: 8.67192554473877
2025-12-09 12:41:57.940 | INFO     | __main__:train:25 - Epoch: 0 Step: 73 LR: 0.0074 Training loss: 7.682762622833252
2025-12-09 12:41:58.437 | INFO     | __main__:train:25 - Epoch: 0 Step: 74 LR: 0.0075 Training loss: 8.303510665893555
2025-12-09 12:41:58.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 75 LR: 0.0076 Training loss: 8.275867462158203
2025-12-09 12:41:59.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 76 LR: 0.0077 Training loss: 7.6717400550842285
2025-12-09 12:41:59.937 | INFO     | __main__:train:25 - Epoch: 0 Step: 77 LR: 0.0078000000000000005 Training loss: 7.668149471282959
2025-12-09 12:42:00.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 78 LR: 0.0079 Training loss: 8.24330997467041
2025-12-09 12:42:00.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 79 LR: 0.008 Training loss: 7.734947681427002
2025-12-09 12:42:01.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 80 LR: 0.008100000000000001 Training loss: 7.738102436065674
2025-12-09 12:42:01.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 81 LR: 0.008199999999999999 Training loss: 7.754218578338623
2025-12-09 12:42:02.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 82 LR: 0.0083 Training loss: 8.431215286254883
2025-12-09 12:42:02.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 83 LR: 0.0084 Training loss: 8.32417106628418
2025-12-09 12:42:03.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 84 LR: 0.0085 Training loss: 8.476675033569336
2025-12-09 12:42:03.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 85 LR: 0.0086 Training loss: 8.495378494262695
2025-12-09 12:42:04.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 86 LR: 0.0087 Training loss: 8.242676734924316
2025-12-09 12:42:04.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 87 LR: 0.0088 Training loss: 7.7237091064453125
2025-12-09 12:42:05.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 88 LR: 0.0089 Training loss: 8.228574752807617
2025-12-09 12:42:05.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 89 LR: 0.009000000000000001 Training loss: 7.998266220092773
2025-12-09 12:42:06.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 90 LR: 0.0091 Training loss: 7.490607261657715
2025-12-09 12:42:06.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 91 LR: 0.0092 Training loss: 8.07024097442627
2025-12-09 12:42:07.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 92 LR: 0.009300000000000001 Training loss: 8.388936996459961
2025-12-09 12:42:07.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 93 LR: 0.0094 Training loss: 7.8942389488220215
2025-12-09 12:42:08.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 94 LR: 0.0095 Training loss: 8.027433395385742
2025-12-09 12:42:08.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 95 LR: 0.0096 Training loss: 7.442301273345947
2025-12-09 12:42:09.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 96 LR: 0.0097 Training loss: 7.90503454208374
2025-12-09 12:42:09.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 97 LR: 0.0098 Training loss: 7.430388927459717
2025-12-09 12:42:10.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 98 LR: 0.0099 Training loss: 7.697278022766113
2025-12-09 12:42:10.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 99 LR: 0.01 Training loss: 7.494693279266357
2025-12-09 12:42:11.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 100 LR: 0.009999999029798808 Training loss: 7.323974609375
2025-12-09 12:42:11.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 101 LR: 0.00999999611919561 Training loss: 7.566224575042725
2025-12-09 12:42:12.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 102 LR: 0.009999991268191535 Training loss: 7.832344055175781
2025-12-09 12:42:12.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 103 LR: 0.009999984476788465 Training loss: 7.841397285461426
2025-12-09 12:42:13.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 104 LR: 0.009999975744989035 Training loss: 7.418730735778809
2025-12-09 12:42:13.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 105 LR: 0.009999965072796636 Training loss: 8.402981758117676
2025-12-09 12:42:14.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 106 LR: 0.009999952460215409 Training loss: 7.79241943359375
2025-12-09 12:42:14.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 107 LR: 0.009999937907250246 Training loss: 7.763362407684326
2025-12-09 12:42:15.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 108 LR: 0.009999921413906798 Training loss: 8.298759460449219
2025-12-09 12:42:15.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 109 LR: 0.009999902980191464 Training loss: 7.489439010620117
2025-12-09 12:42:16.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 110 LR: 0.009999882606111399 Training loss: 8.022537231445312
2025-12-09 12:42:16.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 111 LR: 0.009999860291674507 Training loss: 7.592624187469482
2025-12-09 12:42:17.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 112 LR: 0.009999836036889453 Training loss: 7.181353569030762
2025-12-09 12:42:17.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 113 LR: 0.009999809841765645 Training loss: 9.04453182220459
2025-12-09 12:42:18.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 114 LR: 0.00999978170631325 Training loss: 8.321451187133789
2025-12-09 12:42:18.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 115 LR: 0.009999751630543188 Training loss: 7.604058742523193
2025-12-09 12:42:19.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 116 LR: 0.00999971961446713 Training loss: 7.5996503829956055
2025-12-09 12:42:19.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 117 LR: 0.009999685658097501 Training loss: 7.66883659362793
2025-12-09 12:42:20.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 118 LR: 0.009999649761447477 Training loss: 7.743205547332764
2025-12-09 12:42:20.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 119 LR: 0.009999611924530994 Training loss: 7.4949445724487305
2025-12-09 12:42:21.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 120 LR: 0.00999957214736273 Training loss: 7.221380710601807
2025-12-09 12:42:21.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 121 LR: 0.009999530429958124 Training loss: 7.519382953643799
2025-12-09 12:42:22.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 122 LR: 0.009999486772333366 Training loss: 7.547115325927734
2025-12-09 12:42:22.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 123 LR: 0.0099994411745054 Training loss: 8.369566917419434
2025-12-09 12:42:23.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 124 LR: 0.009999393636491919 Training loss: 7.47849178314209
2025-12-09 12:42:23.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 125 LR: 0.00999934415831137 Training loss: 7.774749279022217
2025-12-09 12:42:24.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 126 LR: 0.009999292739982958 Training loss: 7.326566219329834
2025-12-09 12:42:24.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 127 LR: 0.009999239381526638 Training loss: 8.2337064743042
2025-12-09 12:42:25.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 128 LR: 0.009999184082963117 Training loss: 7.713865756988525
2025-12-09 12:42:25.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 129 LR: 0.009999126844313852 Training loss: 7.628945350646973
2025-12-09 12:42:26.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 130 LR: 0.00999906766560106 Training loss: 7.804828643798828
2025-12-09 12:42:26.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 131 LR: 0.009999006546847707 Training loss: 7.23618745803833
2025-12-09 12:42:27.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 132 LR: 0.009998943488077507 Training loss: 7.083462238311768
2025-12-09 12:42:27.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 133 LR: 0.009998878489314937 Training loss: 7.491841793060303
2025-12-09 12:42:28.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 134 LR: 0.009998811550585221 Training loss: 7.698217868804932
2025-12-09 12:42:28.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 135 LR: 0.009998742671914335 Training loss: 7.309119701385498
2025-12-09 12:42:29.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 136 LR: 0.00999867185332901 Training loss: 6.936102390289307
2025-12-09 12:42:29.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 137 LR: 0.00999859909485673 Training loss: 7.9234771728515625
2025-12-09 12:42:30.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 138 LR: 0.00999852439652573 Training loss: 7.16434907913208
2025-12-09 12:42:30.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 139 LR: 0.009998447758365002 Training loss: 7.664010047912598
2025-12-09 12:42:31.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 140 LR: 0.009998369180404282 Training loss: 7.658667087554932
2025-12-09 12:42:31.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 141 LR: 0.00999828866267407 Training loss: 7.40318489074707
2025-12-09 12:42:32.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 142 LR: 0.00999820620520561 Training loss: 7.8492631912231445
2025-12-09 12:42:32.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 143 LR: 0.009998121808030905 Training loss: 7.456338405609131
2025-12-09 12:42:33.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 144 LR: 0.009998035471182706 Training loss: 7.5387959480285645
2025-12-09 12:42:33.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 145 LR: 0.00999794719469452 Training loss: 7.403537273406982
2025-12-09 12:42:34.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 146 LR: 0.009997856978600603 Training loss: 7.581790924072266
2025-12-09 12:42:34.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 147 LR: 0.009997764822935967 Training loss: 7.718843936920166
2025-12-09 12:42:35.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 148 LR: 0.009997670727736378 Training loss: 7.856426239013672
2025-12-09 12:42:35.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 149 LR: 0.009997574693038351 Training loss: 7.549262523651123
2025-12-09 12:42:36.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 150 LR: 0.009997476718879152 Training loss: 7.3941802978515625
2025-12-09 12:42:36.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 151 LR: 0.009997376805296809 Training loss: 7.518932342529297
2025-12-09 12:42:37.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 152 LR: 0.009997274952330094 Training loss: 7.105958938598633
2025-12-09 12:42:37.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 153 LR: 0.00999717116001853 Training loss: 7.096723556518555
2025-12-09 12:42:38.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 154 LR: 0.009997065428402403 Training loss: 7.444065093994141
2025-12-09 12:42:38.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 155 LR: 0.009996957757522741 Training loss: 7.703850269317627
2025-12-09 12:42:39.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 156 LR: 0.009996848147421333 Training loss: 7.699076175689697
2025-12-09 12:42:39.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 157 LR: 0.009996736598140715 Training loss: 7.450543403625488
2025-12-09 12:42:40.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 158 LR: 0.009996623109724174 Training loss: 7.008502960205078
2025-12-09 12:42:40.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 159 LR: 0.009996507682215754 Training loss: 7.684058666229248
2025-12-09 12:42:41.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 160 LR: 0.009996390315660254 Training loss: 7.467440128326416
2025-12-09 12:42:41.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 161 LR: 0.009996271010103216 Training loss: 7.805999279022217
2025-12-09 12:42:42.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 162 LR: 0.009996149765590946 Training loss: 7.523479461669922
2025-12-09 12:42:42.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 163 LR: 0.00999602658217049 Training loss: 7.330877780914307
2025-12-09 12:42:43.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 164 LR: 0.009995901459889657 Training loss: 8.091928482055664
2025-12-09 12:42:43.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 165 LR: 0.009995774398797007 Training loss: 7.746781826019287
2025-12-09 12:42:44.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 166 LR: 0.009995645398941846 Training loss: 6.613270282745361
2025-12-09 12:42:44.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 167 LR: 0.009995514460374237 Training loss: 7.839529514312744
2025-12-09 12:42:45.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 168 LR: 0.009995381583144995 Training loss: 7.408051013946533
2025-12-09 12:42:45.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 169 LR: 0.009995246767305689 Training loss: 7.421999931335449
2025-12-09 12:42:46.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 170 LR: 0.009995110012908634 Training loss: 7.431645393371582
2025-12-09 12:42:46.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 171 LR: 0.009994971320006905 Training loss: 7.3372344970703125
2025-12-09 12:42:47.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 172 LR: 0.009994830688654326 Training loss: 7.770325660705566
2025-12-09 12:42:47.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 173 LR: 0.009994688118905472 Training loss: 7.365876197814941
2025-12-09 12:42:48.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 174 LR: 0.009994543610815672 Training loss: 7.358321189880371
2025-12-09 12:42:48.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 175 LR: 0.009994397164441006 Training loss: 7.200776100158691
2025-12-09 12:42:49.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 176 LR: 0.009994248779838311 Training loss: 6.917827606201172
2025-12-09 12:42:49.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 177 LR: 0.009994098457065167 Training loss: 7.186360836029053
2025-12-09 12:42:50.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 178 LR: 0.009993946196179913 Training loss: 7.120929718017578
2025-12-09 12:42:50.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 179 LR: 0.009993791997241638 Training loss: 7.361905097961426
2025-12-09 12:42:51.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 180 LR: 0.009993635860310187 Training loss: 7.424858570098877
2025-12-09 12:42:51.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 181 LR: 0.00999347778544615 Training loss: 7.161836624145508
2025-12-09 12:42:52.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 182 LR: 0.009993317772710874 Training loss: 7.08610200881958
2025-12-09 12:42:52.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 183 LR: 0.009993155822166457 Training loss: 7.236615180969238
2025-12-09 12:42:53.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 184 LR: 0.009992991933875747 Training loss: 7.594260215759277
2025-12-09 12:42:53.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 185 LR: 0.009992826107902348 Training loss: 7.166801452636719
2025-12-09 12:42:54.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 186 LR: 0.009992658344310614 Training loss: 7.0483269691467285
2025-12-09 12:42:54.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 187 LR: 0.00999248864316565 Training loss: 6.895529747009277
2025-12-09 12:42:55.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 188 LR: 0.009992317004533314 Training loss: 7.575483322143555
2025-12-09 12:42:55.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 189 LR: 0.009992143428480213 Training loss: 7.165027618408203
2025-12-09 12:42:56.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 190 LR: 0.009991967915073714 Training loss: 7.417134761810303
2025-12-09 12:42:56.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 191 LR: 0.009991790464381926 Training loss: 7.196866989135742
2025-12-09 12:42:57.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 192 LR: 0.009991611076473714 Training loss: 6.968716144561768
2025-12-09 12:42:57.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 193 LR: 0.009991429751418698 Training loss: 7.555809497833252
2025-12-09 12:42:58.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 194 LR: 0.009991246489287245 Training loss: 7.0888190269470215
2025-12-09 12:42:58.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 195 LR: 0.009991061290150474 Training loss: 6.847196102142334
2025-12-09 12:42:59.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 196 LR: 0.009990874154080258 Training loss: 7.640418529510498
2025-12-09 12:42:59.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 197 LR: 0.009990685081149222 Training loss: 7.093740463256836
2025-12-09 12:43:00.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 198 LR: 0.009990494071430742 Training loss: 7.166308879852295
2025-12-09 12:43:00.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 199 LR: 0.009990301124998944 Training loss: 6.607621669769287
2025-12-09 12:43:01.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 200 LR: 0.009990106241928705 Training loss: 6.873034954071045
2025-12-09 12:43:01.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 201 LR: 0.009989909422295658 Training loss: 7.2411932945251465
2025-12-09 12:43:02.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 202 LR: 0.009989710666176184 Training loss: 6.900091171264648
2025-12-09 12:43:02.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 203 LR: 0.009989509973647417 Training loss: 6.674678802490234
2025-12-09 12:43:03.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 204 LR: 0.009989307344787242 Training loss: 7.245971202850342
2025-12-09 12:43:03.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 205 LR: 0.009989102779674294 Training loss: 7.068427562713623
2025-12-09 12:43:04.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 206 LR: 0.00998889627838796 Training loss: 6.8079142570495605
2025-12-09 12:43:04.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 207 LR: 0.00998868784100838 Training loss: 7.071592330932617
2025-12-09 12:43:05.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 208 LR: 0.009988477467616446 Training loss: 7.181904315948486
2025-12-09 12:43:05.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 209 LR: 0.0099882651582938 Training loss: 7.025269508361816
2025-12-09 12:43:06.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 210 LR: 0.009988050913122831 Training loss: 7.837571144104004
2025-12-09 12:43:06.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 211 LR: 0.009987834732186687 Training loss: 7.479762554168701
2025-12-09 12:43:07.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 212 LR: 0.009987616615569263 Training loss: 7.270496368408203
2025-12-09 12:43:07.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 213 LR: 0.009987396563355204 Training loss: 7.113997459411621
2025-12-09 12:43:08.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 214 LR: 0.009987174575629911 Training loss: 7.690854549407959
2025-12-09 12:43:08.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 215 LR: 0.009986950652479532 Training loss: 7.0354390144348145
2025-12-09 12:43:09.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 216 LR: 0.009986724793990967 Training loss: 6.854018211364746
2025-12-09 12:43:09.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 217 LR: 0.009986497000251867 Training loss: 6.821666240692139
2025-12-09 12:43:10.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 218 LR: 0.009986267271350633 Training loss: 7.497819900512695
2025-12-09 12:43:10.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 219 LR: 0.00998603560737642 Training loss: 6.860793113708496
2025-12-09 12:43:11.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 220 LR: 0.009985802008419132 Training loss: 6.903954982757568
2025-12-09 12:43:11.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 221 LR: 0.009985566474569425 Training loss: 6.870201587677002
2025-12-09 12:43:12.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 222 LR: 0.009985329005918702 Training loss: 7.027231693267822
2025-12-09 12:43:12.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 223 LR: 0.009985089602559125 Training loss: 7.270394802093506
2025-12-09 12:43:13.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 224 LR: 0.009984848264583597 Training loss: 7.13750696182251
2025-12-09 12:43:13.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 225 LR: 0.00998460499208578 Training loss: 7.153573989868164
2025-12-09 12:43:14.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 226 LR: 0.00998435978516008 Training loss: 7.6044020652771
2025-12-09 12:43:14.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 227 LR: 0.00998411264390166 Training loss: 7.5101189613342285
2025-12-09 12:43:15.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 228 LR: 0.009983863568406429 Training loss: 7.171511173248291
2025-12-09 12:43:15.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 229 LR: 0.009983612558771048 Training loss: 7.186981678009033
2025-12-09 12:43:16.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 230 LR: 0.00998335961509293 Training loss: 7.219714641571045
2025-12-09 12:43:16.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 231 LR: 0.009983104737470239 Training loss: 7.060098171234131
2025-12-09 12:43:17.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 232 LR: 0.009982847926001886 Training loss: 7.315389633178711
2025-12-09 12:43:17.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 233 LR: 0.009982589180787534 Training loss: 7.050279140472412
2025-12-09 12:43:18.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 234 LR: 0.009982328501927597 Training loss: 7.318639278411865
2025-12-09 12:43:18.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 235 LR: 0.009982065889523242 Training loss: 7.119284629821777
2025-12-09 12:43:19.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 236 LR: 0.00998180134367638 Training loss: 7.187157154083252
2025-12-09 12:43:19.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 237 LR: 0.009981534864489678 Training loss: 6.820873260498047
2025-12-09 12:43:20.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 238 LR: 0.009981266452066553 Training loss: 7.1109819412231445
2025-12-09 12:43:20.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 239 LR: 0.009980996106511169 Training loss: 7.395974159240723
2025-12-09 12:43:21.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 240 LR: 0.009980723827928441 Training loss: 7.428614139556885
2025-12-09 12:43:21.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 241 LR: 0.009980449616424037 Training loss: 6.603586196899414
2025-12-09 12:43:22.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 242 LR: 0.00998017347210437 Training loss: 7.666954517364502
2025-12-09 12:43:22.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 243 LR: 0.009979895395076608 Training loss: 7.033082485198975
2025-12-09 12:43:23.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 244 LR: 0.009979615385448668 Training loss: 7.113259792327881
2025-12-09 12:43:23.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 245 LR: 0.009979333443329217 Training loss: 7.45045280456543
2025-12-09 12:43:24.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 246 LR: 0.00997904956882767 Training loss: 7.145931243896484
2025-12-09 12:43:24.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 247 LR: 0.009978763762054192 Training loss: 6.697127819061279
2025-12-09 12:43:25.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 248 LR: 0.0099784760231197 Training loss: 7.678164005279541
2025-12-09 12:43:25.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 249 LR: 0.00997818635213586 Training loss: 7.484626293182373
2025-12-09 12:43:26.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 250 LR: 0.009977894749215089 Training loss: 7.4503912925720215
2025-12-09 12:43:26.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 251 LR: 0.00997760121447055 Training loss: 7.224595546722412
2025-12-09 12:43:27.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 252 LR: 0.009977305748016158 Training loss: 7.292801856994629
2025-12-09 12:43:27.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 253 LR: 0.00997700834996658 Training loss: 6.854926109313965
2025-12-09 12:43:28.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 254 LR: 0.009976709020437229 Training loss: 7.494583606719971
2025-12-09 12:43:28.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 255 LR: 0.00997640775954427 Training loss: 8.171175956726074
2025-12-09 12:43:29.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 256 LR: 0.009976104567404616 Training loss: 7.103387355804443
2025-12-09 12:43:29.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 257 LR: 0.009975799444135928 Training loss: 7.291759014129639
2025-12-09 12:43:30.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 258 LR: 0.009975492389856622 Training loss: 7.233290672302246
2025-12-09 12:43:30.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 259 LR: 0.009975183404685856 Training loss: 7.309473037719727
2025-12-09 12:43:31.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 260 LR: 0.009974872488743543 Training loss: 7.269880294799805
2025-12-09 12:43:31.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 261 LR: 0.009974559642150344 Training loss: 6.976792812347412
2025-12-09 12:43:32.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 262 LR: 0.009974244865027668 Training loss: 7.023744583129883
2025-12-09 12:43:32.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 263 LR: 0.009973928157497673 Training loss: 7.247967720031738
2025-12-09 12:43:33.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 264 LR: 0.009973609519683268 Training loss: 6.443594932556152
2025-12-09 12:43:33.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 265 LR: 0.009973288951708112 Training loss: 7.238213539123535
2025-12-09 12:43:34.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 266 LR: 0.009972966453696608 Training loss: 7.095010757446289
2025-12-09 12:43:34.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 267 LR: 0.009972642025773911 Training loss: 7.194639205932617
2025-12-09 12:43:35.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 268 LR: 0.009972315668065928 Training loss: 7.279710292816162
2025-12-09 12:43:35.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 269 LR: 0.00997198738069931 Training loss: 7.349696636199951
2025-12-09 12:43:36.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 270 LR: 0.009971657163801459 Training loss: 7.208883285522461
2025-12-09 12:43:36.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 271 LR: 0.009971325017500525 Training loss: 6.739727973937988
2025-12-09 12:43:37.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 272 LR: 0.00997099094192541 Training loss: 6.992547988891602
2025-12-09 12:43:37.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 273 LR: 0.009970654937205762 Training loss: 7.162473201751709
2025-12-09 12:43:38.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 274 LR: 0.009970317003471976 Training loss: 7.396735668182373
2025-12-09 12:43:38.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 275 LR: 0.009969977140855197 Training loss: 7.509301662445068
2025-12-09 12:43:39.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 276 LR: 0.009969635349487322 Training loss: 6.751310348510742
2025-12-09 12:43:39.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 277 LR: 0.009969291629500991 Training loss: 7.292081832885742
2025-12-09 12:43:40.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 278 LR: 0.009968945981029596 Training loss: 7.8093581199646
2025-12-09 12:43:40.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 279 LR: 0.009968598404207276 Training loss: 7.492965221405029
2025-12-09 12:43:41.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 280 LR: 0.009968248899168919 Training loss: 7.130009651184082
2025-12-09 12:43:41.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 281 LR: 0.00996789746605016 Training loss: 7.164113521575928
2025-12-09 12:43:42.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 282 LR: 0.009967544104987387 Training loss: 7.726715087890625
2025-12-09 12:43:42.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 283 LR: 0.009967188816117727 Training loss: 7.7236104011535645
2025-12-09 12:43:43.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 284 LR: 0.009966831599579066 Training loss: 7.127745628356934
2025-12-09 12:43:43.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 285 LR: 0.00996647245551003 Training loss: 7.321977615356445
2025-12-09 12:43:44.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 286 LR: 0.009966111384049996 Training loss: 7.045519828796387
2025-12-09 12:43:44.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 287 LR: 0.009965748385339089 Training loss: 7.179984092712402
2025-12-09 12:43:45.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 288 LR: 0.00996538345951818 Training loss: 7.23288106918335
2025-12-09 12:43:45.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 289 LR: 0.009965016606728895 Training loss: 6.952159404754639
2025-12-09 12:43:46.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 290 LR: 0.009964647827113595 Training loss: 7.306218147277832
2025-12-09 12:43:46.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 291 LR: 0.009964277120815402 Training loss: 7.350615501403809
2025-12-09 12:43:47.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 292 LR: 0.009963904487978178 Training loss: 7.298469066619873
2025-12-09 12:43:47.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 293 LR: 0.009963529928746533 Training loss: 7.178114891052246
2025-12-09 12:43:48.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 294 LR: 0.009963153443265827 Training loss: 6.801499843597412
2025-12-09 12:43:48.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 295 LR: 0.00996277503168217 Training loss: 7.510593891143799
2025-12-09 12:43:49.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 296 LR: 0.00996239469414241 Training loss: 7.242609977722168
2025-12-09 12:43:49.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 297 LR: 0.009962012430794153 Training loss: 7.951026439666748
2025-12-09 12:43:50.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 298 LR: 0.009961628241785746 Training loss: 7.192435264587402
2025-12-09 12:43:50.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 299 LR: 0.009961242127266288 Training loss: 7.265941619873047
2025-12-09 12:43:51.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 300 LR: 0.009960854087385618 Training loss: 7.19232702255249
2025-12-09 12:43:51.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 301 LR: 0.00996046412229433 Training loss: 7.035221576690674
2025-12-09 12:43:52.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 302 LR: 0.009960072232143761 Training loss: 7.27801513671875
2025-12-09 12:43:52.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 303 LR: 0.009959678417085997 Training loss: 6.87296724319458
2025-12-09 12:43:53.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 304 LR: 0.009959282677273869 Training loss: 7.468862056732178
2025-12-09 12:43:53.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 305 LR: 0.009958885012860954 Training loss: 6.946866035461426
2025-12-09 12:43:54.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 306 LR: 0.009958485424001582 Training loss: 7.359542369842529
2025-12-09 12:43:54.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 307 LR: 0.009958083910850821 Training loss: 7.1967973709106445
2025-12-09 12:43:55.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 308 LR: 0.009957680473564495 Training loss: 7.247198581695557
2025-12-09 12:43:55.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 309 LR: 0.009957275112299165 Training loss: 7.0409393310546875
2025-12-09 12:43:56.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 310 LR: 0.009956867827212149 Training loss: 7.179038047790527
2025-12-09 12:43:56.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 311 LR: 0.009956458618461502 Training loss: 7.09916877746582
2025-12-09 12:43:57.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 312 LR: 0.009956047486206033 Training loss: 7.0428619384765625
2025-12-09 12:43:57.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 313 LR: 0.00995563443060529 Training loss: 6.683989524841309
2025-12-09 12:43:58.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 314 LR: 0.00995521945181958 Training loss: 7.401625156402588
2025-12-09 12:43:58.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 315 LR: 0.00995480255000994 Training loss: 6.920444965362549
2025-12-09 12:43:59.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 316 LR: 0.009954383725338167 Training loss: 7.248817443847656
2025-12-09 12:43:59.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 317 LR: 0.009953962977966795 Training loss: 6.999892234802246
2025-12-09 12:44:00.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 318 LR: 0.00995354030805911 Training loss: 7.043951034545898
2025-12-09 12:44:00.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 319 LR: 0.009953115715779141 Training loss: 7.037802219390869
2025-12-09 12:44:01.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 320 LR: 0.009952689201291663 Training loss: 6.550197601318359
2025-12-09 12:44:01.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 321 LR: 0.0099522607647622 Training loss: 7.1680006980896
2025-12-09 12:44:02.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 322 LR: 0.00995183040635702 Training loss: 6.845522403717041
2025-12-09 12:44:02.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 323 LR: 0.009951398126243134 Training loss: 7.282987594604492
2025-12-09 12:44:03.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 324 LR: 0.009950963924588304 Training loss: 7.004817008972168
2025-12-09 12:44:03.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 325 LR: 0.009950527801561034 Training loss: 7.416269302368164
2025-12-09 12:44:04.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 326 LR: 0.009950089757330574 Training loss: 7.126711845397949
2025-12-09 12:44:04.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 327 LR: 0.009949649792066922 Training loss: 6.766808032989502
2025-12-09 12:44:05.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 328 LR: 0.00994920790594082 Training loss: 7.107395172119141
2025-12-09 12:44:05.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 329 LR: 0.009948764099123755 Training loss: 7.116005897521973
2025-12-09 12:44:06.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 330 LR: 0.00994831837178796 Training loss: 7.015739917755127
2025-12-09 12:44:06.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 331 LR: 0.009947870724106411 Training loss: 6.648236274719238
2025-12-09 12:44:07.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 332 LR: 0.009947421156252837 Training loss: 7.218929767608643
2025-12-09 12:44:07.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 333 LR: 0.009946969668401697 Training loss: 6.95290994644165
2025-12-09 12:44:08.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 334 LR: 0.009946516260728214 Training loss: 6.673097133636475
2025-12-09 12:44:08.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 335 LR: 0.009946060933408342 Training loss: 6.702050685882568
2025-12-09 12:44:09.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 336 LR: 0.009945603686618785 Training loss: 7.226283073425293
2025-12-09 12:44:09.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 337 LR: 0.009945144520536991 Training loss: 7.263240337371826
2025-12-09 12:44:10.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 338 LR: 0.009944683435341155 Training loss: 6.702829360961914
2025-12-09 12:44:10.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 339 LR: 0.009944220431210215 Training loss: 7.007819175720215
2025-12-09 12:44:11.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 340 LR: 0.009943755508323854 Training loss: 7.232687950134277
2025-12-09 12:44:11.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 341 LR: 0.009943288666862497 Training loss: 6.909900188446045
2025-12-09 12:44:12.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 342 LR: 0.00994281990700732 Training loss: 7.074356555938721
2025-12-09 12:44:12.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 343 LR: 0.009942349228940238 Training loss: 6.79145622253418
2025-12-09 12:44:13.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 344 LR: 0.00994187663284391 Training loss: 6.665388584136963
2025-12-09 12:44:13.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 345 LR: 0.009941402118901743 Training loss: 7.210897922515869
2025-12-09 12:44:14.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 346 LR: 0.009940925687297887 Training loss: 7.2491068840026855
2025-12-09 12:44:14.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 347 LR: 0.009940447338217234 Training loss: 7.480123043060303
2025-12-09 12:44:15.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 348 LR: 0.009939967071845425 Training loss: 6.401063442230225
2025-12-09 12:44:15.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 349 LR: 0.009939484888368837 Training loss: 7.444952011108398
2025-12-09 12:44:16.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 350 LR: 0.009939000787974602 Training loss: 7.498937129974365
2025-12-09 12:44:16.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 351 LR: 0.009938514770850585 Training loss: 7.168690204620361
2025-12-09 12:44:17.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 352 LR: 0.009938026837185403 Training loss: 7.241108417510986
2025-12-09 12:44:17.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 353 LR: 0.009937536987168413 Training loss: 7.098414421081543
2025-12-09 12:44:18.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 354 LR: 0.009937045220989716 Training loss: 7.327215194702148
2025-12-09 12:44:18.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 355 LR: 0.009936551538840153 Training loss: 6.772062301635742
2025-12-09 12:44:19.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 356 LR: 0.009936055940911319 Training loss: 6.914669513702393
2025-12-09 12:44:19.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 357 LR: 0.009935558427395541 Training loss: 6.795671463012695
2025-12-09 12:44:20.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 358 LR: 0.009935058998485898 Training loss: 6.982972145080566
2025-12-09 12:44:20.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 359 LR: 0.009934557654376204 Training loss: 7.222626686096191
2025-12-09 12:44:21.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 360 LR: 0.009934054395261025 Training loss: 7.170962333679199
2025-12-09 12:44:21.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 361 LR: 0.009933549221335665 Training loss: 6.899124622344971
2025-12-09 12:44:22.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 362 LR: 0.009933042132796171 Training loss: 6.897152900695801
2025-12-09 12:44:22.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 363 LR: 0.009932533129839334 Training loss: 7.114818572998047
2025-12-09 12:44:23.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 364 LR: 0.00993202221266269 Training loss: 6.715932846069336
2025-12-09 12:44:23.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 365 LR: 0.009931509381464514 Training loss: 7.302417755126953
2025-12-09 12:44:24.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 366 LR: 0.009930994636443828 Training loss: 6.840936660766602
2025-12-09 12:44:24.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 367 LR: 0.009930477977800391 Training loss: 6.763546943664551
2025-12-09 12:44:25.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 368 LR: 0.009929959405734712 Training loss: 7.349926948547363
2025-12-09 12:44:25.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 369 LR: 0.009929438920448038 Training loss: 7.017276287078857
2025-12-09 12:44:26.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 370 LR: 0.009928916522142357 Training loss: 6.900688171386719
2025-12-09 12:44:26.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 371 LR: 0.0099283922110204 Training loss: 7.132015705108643
2025-12-09 12:44:27.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 372 LR: 0.009927865987285648 Training loss: 7.50365686416626
2025-12-09 12:44:27.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 373 LR: 0.009927337851142314 Training loss: 7.870297908782959
2025-12-09 12:44:28.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 374 LR: 0.009926807802795359 Training loss: 6.882498264312744
2025-12-09 12:44:28.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 375 LR: 0.009926275842450481 Training loss: 6.968447685241699
2025-12-09 12:44:29.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 376 LR: 0.009925741970314128 Training loss: 6.895899772644043
2025-12-09 12:44:29.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 377 LR: 0.009925206186593483 Training loss: 7.0632643699646
2025-12-09 12:44:30.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 378 LR: 0.009924668491496473 Training loss: 7.025409698486328
2025-12-09 12:44:30.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 379 LR: 0.009924128885231769 Training loss: 7.281083583831787
2025-12-09 12:44:31.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 380 LR: 0.009923587368008779 Training loss: 6.960066318511963
2025-12-09 12:44:31.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 381 LR: 0.009923043940037657 Training loss: 7.231663227081299
2025-12-09 12:44:32.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 382 LR: 0.009922498601529295 Training loss: 6.651087284088135
2025-12-09 12:44:32.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 383 LR: 0.00992195135269533 Training loss: 6.757845401763916
2025-12-09 12:44:33.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 384 LR: 0.009921402193748138 Training loss: 6.989624500274658
2025-12-09 12:44:33.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 385 LR: 0.009920851124900838 Training loss: 6.702486038208008
2025-12-09 12:44:34.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 386 LR: 0.009920298146367286 Training loss: 6.913912296295166
2025-12-09 12:44:34.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 387 LR: 0.009919743258362085 Training loss: 8.019289016723633
2025-12-09 12:44:35.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 388 LR: 0.009919186461100576 Training loss: 7.129539966583252
2025-12-09 12:44:35.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 389 LR: 0.009918627754798839 Training loss: 7.040722370147705
2025-12-09 12:44:36.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 390 LR: 0.0099180671396737 Training loss: 6.7806572914123535
2025-12-09 12:44:36.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 391 LR: 0.009917504615942721 Training loss: 6.9883880615234375
2025-12-09 12:44:37.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 392 LR: 0.009916940183824205 Training loss: 6.780886173248291
2025-12-09 12:44:37.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 393 LR: 0.009916373843537201 Training loss: 7.033484935760498
2025-12-09 12:44:38.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 394 LR: 0.009915805595301492 Training loss: 6.858616828918457
2025-12-09 12:44:38.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 395 LR: 0.009915235439337602 Training loss: 7.096217155456543
2025-12-09 12:44:39.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 396 LR: 0.009914663375866804 Training loss: 7.0482306480407715
2025-12-09 12:44:39.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 397 LR: 0.009914089405111097 Training loss: 6.995991230010986
2025-12-09 12:44:40.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 398 LR: 0.009913513527293234 Training loss: 6.93223762512207
2025-12-09 12:44:40.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 399 LR: 0.009912935742636698 Training loss: 7.169092655181885
2025-12-09 12:44:41.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 400 LR: 0.009912356051365718 Training loss: 7.5624003410339355
2025-12-09 12:44:41.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 401 LR: 0.009911774453705257 Training loss: 6.992119312286377
2025-12-09 12:44:42.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 402 LR: 0.00991119094988103 Training loss: 6.204849720001221
2025-12-09 12:44:42.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 403 LR: 0.009910605540119475 Training loss: 6.128866672515869
2025-12-09 12:44:43.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 404 LR: 0.009910018224647781 Training loss: 8.966459274291992
2025-12-09 12:44:43.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 405 LR: 0.009909429003693876 Training loss: 7.048325538635254
2025-12-09 12:44:44.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 406 LR: 0.009908837877486422 Training loss: 7.598023891448975
2025-12-09 12:44:44.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 407 LR: 0.009908244846254825 Training loss: 7.130185604095459
2025-12-09 12:44:45.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 408 LR: 0.009907649910229228 Training loss: 7.201645374298096
2025-12-09 12:44:45.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 409 LR: 0.009907053069640516 Training loss: 6.812040328979492
2025-12-09 12:44:46.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 410 LR: 0.009906454324720308 Training loss: 6.911349296569824
2025-12-09 12:44:46.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 411 LR: 0.009905853675700968 Training loss: 7.606995582580566
2025-12-09 12:44:47.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 412 LR: 0.009905251122815597 Training loss: 7.1157307624816895
2025-12-09 12:44:47.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 413 LR: 0.00990464666629803 Training loss: 7.215779781341553
2025-12-09 12:44:48.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 414 LR: 0.009904040306382847 Training loss: 7.004698753356934
2025-12-09 12:44:48.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 415 LR: 0.009903432043305365 Training loss: 6.808079719543457
2025-12-09 12:44:49.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 416 LR: 0.009902821877301638 Training loss: 7.077596187591553
2025-12-09 12:44:49.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 417 LR: 0.00990220980860846 Training loss: 7.117050647735596
2025-12-09 12:44:50.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 418 LR: 0.009901595837463363 Training loss: 7.057714939117432
2025-12-09 12:44:50.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 419 LR: 0.009900979964104618 Training loss: 7.130357265472412
2025-12-09 12:44:51.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 420 LR: 0.00990036218877123 Training loss: 6.7648725509643555
2025-12-09 12:44:51.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 421 LR: 0.00989974251170295 Training loss: 7.730785846710205
2025-12-09 12:44:52.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 422 LR: 0.00989912093314026 Training loss: 7.043667793273926
2025-12-09 12:44:52.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 423 LR: 0.009898497453324384 Training loss: 6.979488372802734
2025-12-09 12:44:53.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 424 LR: 0.009897872072497281 Training loss: 7.548293113708496
2025-12-09 12:44:53.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 425 LR: 0.00989724479090165 Training loss: 6.485877513885498
2025-12-09 12:44:54.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 426 LR: 0.009896615608780924 Training loss: 7.247476577758789
2025-12-09 12:44:54.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 427 LR: 0.00989598452637928 Training loss: 7.548773288726807
2025-12-09 12:44:55.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 428 LR: 0.009895351543941628 Training loss: 7.258933067321777
2025-12-09 12:44:55.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 429 LR: 0.009894716661713616 Training loss: 7.020913600921631
2025-12-09 12:44:56.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 430 LR: 0.009894079879941628 Training loss: 6.778873920440674
2025-12-09 12:44:56.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 431 LR: 0.009893441198872787 Training loss: 6.958953857421875
2025-12-09 12:44:57.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 432 LR: 0.009892800618754954 Training loss: 6.836483001708984
2025-12-09 12:44:57.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 433 LR: 0.009892158139836724 Training loss: 6.834100723266602
2025-12-09 12:44:58.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 434 LR: 0.00989151376236743 Training loss: 7.409610271453857
2025-12-09 12:44:58.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 435 LR: 0.009890867486597146 Training loss: 7.246042251586914
2025-12-09 12:44:59.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 436 LR: 0.009890219312776677 Training loss: 7.029992580413818
2025-12-09 12:44:59.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 437 LR: 0.009889569241157564 Training loss: 6.817840099334717
2025-12-09 12:45:00.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 438 LR: 0.00988891727199209 Training loss: 7.0679240226745605
2025-12-09 12:45:00.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 439 LR: 0.009888263405533271 Training loss: 6.930448532104492
2025-12-09 12:45:01.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 440 LR: 0.009887607642034859 Training loss: 7.0561137199401855
2025-12-09 12:45:01.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 441 LR: 0.009886949981751346 Training loss: 6.794604778289795
2025-12-09 12:45:02.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 442 LR: 0.009886290424937952 Training loss: 6.894570827484131
2025-12-09 12:45:02.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 443 LR: 0.009885628971850642 Training loss: 8.138226509094238
2025-12-09 12:45:03.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 444 LR: 0.009884965622746112 Training loss: 7.922671794891357
2025-12-09 12:45:03.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 445 LR: 0.009884300377881794 Training loss: 7.7036614418029785
2025-12-09 12:45:04.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 446 LR: 0.009883633237515857 Training loss: 7.2094407081604
2025-12-09 12:45:04.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 447 LR: 0.009882964201907207 Training loss: 7.157731533050537
2025-12-09 12:45:05.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 448 LR: 0.00988229327131548 Training loss: 6.697330951690674
2025-12-09 12:45:05.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 449 LR: 0.009881620446001056 Training loss: 6.984570503234863
2025-12-09 12:45:06.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 450 LR: 0.00988094572622504 Training loss: 6.028395175933838
2025-12-09 12:45:06.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 451 LR: 0.00988026911224928 Training loss: 7.10154390335083
2025-12-09 12:45:07.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 452 LR: 0.00987959060433636 Training loss: 6.8543853759765625
2025-12-09 12:45:07.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 453 LR: 0.009878910202749589 Training loss: 6.771678924560547
2025-12-09 12:45:08.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 454 LR: 0.009878227907753022 Training loss: 7.785422325134277
2025-12-09 12:45:08.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 455 LR: 0.009877543719611444 Training loss: 7.689386367797852
2025-12-09 12:45:09.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 456 LR: 0.009876857638590373 Training loss: 7.7121148109436035
2025-12-09 12:45:09.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 457 LR: 0.009876169664956067 Training loss: 6.9222187995910645
2025-12-09 12:45:10.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 458 LR: 0.009875479798975512 Training loss: 6.841021537780762
2025-12-09 12:45:10.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 459 LR: 0.009874788040916432 Training loss: 6.775393486022949
2025-12-09 12:45:11.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 460 LR: 0.009874094391047288 Training loss: 8.094632148742676
2025-12-09 12:45:11.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 461 LR: 0.009873398849637267 Training loss: 6.784125804901123
2025-12-09 12:45:12.436 | INFO     | __main__:train:25 - Epoch: 0 Step: 462 LR: 0.009872701416956299 Training loss: 6.944911003112793
2025-12-09 12:45:12.936 | INFO     | __main__:train:25 - Epoch: 0 Step: 463 LR: 0.009872002093275042 Training loss: 7.2089762687683105
2025-12-09 12:45:13.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 464 LR: 0.00987130087886489 Training loss: 7.846539497375488
2025-12-09 12:45:13.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 465 LR: 0.009870597773997972 Training loss: 6.784156322479248
2025-12-09 12:45:14.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 466 LR: 0.009869892778947148 Training loss: 6.9719319343566895
2025-12-09 12:45:14.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 467 LR: 0.009869185893986013 Training loss: 7.23166561126709
2025-12-09 12:45:15.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 468 LR: 0.009868477119388895 Training loss: 6.915422439575195
2025-12-09 12:45:15.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 469 LR: 0.009867766455430856 Training loss: 7.049798965454102
2025-12-09 12:45:16.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 470 LR: 0.009867053902387693 Training loss: 7.383526802062988
2025-12-09 12:45:16.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 471 LR: 0.009866339460535929 Training loss: 7.522879600524902
2025-12-09 12:45:17.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 472 LR: 0.009865623130152828 Training loss: 6.76742696762085
2025-12-09 12:45:17.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 473 LR: 0.009864904911516384 Training loss: 6.900379180908203
2025-12-09 12:45:18.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 474 LR: 0.009864184804905323 Training loss: 7.574166297912598
2025-12-09 12:45:18.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 475 LR: 0.009863462810599103 Training loss: 6.989035606384277
2025-12-09 12:45:19.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 476 LR: 0.009862738928877922 Training loss: 6.883431911468506
2025-12-09 12:45:19.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 477 LR: 0.009862013160022696 Training loss: 6.795194149017334
2025-12-09 12:45:20.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 478 LR: 0.009861285504315085 Training loss: 7.415529251098633
2025-12-09 12:45:20.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 479 LR: 0.00986055596203748 Training loss: 7.345175266265869
2025-12-09 12:45:21.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 480 LR: 0.009859824533472998 Training loss: 6.899064540863037
2025-12-09 12:45:21.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 481 LR: 0.009859091218905498 Training loss: 6.876798152923584
2025-12-09 12:45:22.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 482 LR: 0.00985835601861956 Training loss: 6.860148906707764
2025-12-09 12:45:22.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 483 LR: 0.009857618932900504 Training loss: 6.679032325744629
2025-12-09 12:45:23.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 484 LR: 0.009856879962034375 Training loss: 6.904329776763916
2025-12-09 12:45:23.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 485 LR: 0.009856139106307955 Training loss: 7.067889213562012
2025-12-09 12:45:24.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 486 LR: 0.009855396366008757 Training loss: 7.481867790222168
2025-12-09 12:45:24.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 487 LR: 0.009854651741425023 Training loss: 7.074429512023926
2025-12-09 12:45:25.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 488 LR: 0.009853905232845728 Training loss: 7.068796157836914
2025-12-09 12:45:25.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 489 LR: 0.009853156840560576 Training loss: 7.398283004760742
2025-12-09 12:45:26.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 490 LR: 0.009852406564860004 Training loss: 6.976293563842773
2025-12-09 12:45:26.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 491 LR: 0.009851654406035179 Training loss: 7.004825592041016
2025-12-09 12:45:27.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 492 LR: 0.009850900364378 Training loss: 6.872819900512695
2025-12-09 12:45:27.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 493 LR: 0.009850144440181096 Training loss: 6.775216579437256
2025-12-09 12:45:28.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 494 LR: 0.009849386633737824 Training loss: 6.821646213531494
2025-12-09 12:45:28.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 495 LR: 0.009848626945342278 Training loss: 6.844836235046387
2025-12-09 12:45:29.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 496 LR: 0.009847865375289276 Training loss: 6.973506927490234
2025-12-09 12:45:29.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 497 LR: 0.009847101923874366 Training loss: 7.046770095825195
2025-12-09 12:45:30.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 498 LR: 0.009846336591393832 Training loss: 7.102224826812744
2025-12-09 12:45:30.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 499 LR: 0.009845569378144686 Training loss: 7.05117654800415
2025-12-09 12:45:31.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 500 LR: 0.009844800284424663 Training loss: 6.9765706062316895
2025-12-09 12:45:31.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 501 LR: 0.00984402931053224 Training loss: 7.59237003326416
2025-12-09 12:45:32.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 502 LR: 0.009843256456766609 Training loss: 6.805094242095947
2025-12-09 12:45:32.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 503 LR: 0.009842481723427705 Training loss: 7.033079624176025
2025-12-09 12:45:33.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 504 LR: 0.009841705110816185 Training loss: 6.671841621398926
2025-12-09 12:45:33.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 505 LR: 0.00984092661923344 Training loss: 7.167298316955566
2025-12-09 12:45:34.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 506 LR: 0.009840146248981585 Training loss: 6.961639881134033
2025-12-09 12:45:34.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 507 LR: 0.009839364000363466 Training loss: 6.824223518371582
2025-12-09 12:45:35.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 508 LR: 0.00983857987368266 Training loss: 7.233497619628906
2025-12-09 12:45:35.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 509 LR: 0.009837793869243468 Training loss: 6.828888893127441
2025-12-09 12:45:36.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 510 LR: 0.009837005987350926 Training loss: 6.855590343475342
2025-12-09 12:45:36.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 511 LR: 0.009836216228310797 Training loss: 6.886682033538818
2025-12-09 12:45:37.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 512 LR: 0.009835424592429568 Training loss: 6.7446441650390625
2025-12-09 12:45:37.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 513 LR: 0.009834631080014457 Training loss: 7.682192802429199
2025-12-09 12:45:38.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 514 LR: 0.009833835691373412 Training loss: 7.075412750244141
2025-12-09 12:45:38.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 515 LR: 0.00983303842681511 Training loss: 6.9431233406066895
2025-12-09 12:45:39.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 516 LR: 0.009832239286648949 Training loss: 6.69774055480957
2025-12-09 12:45:39.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 517 LR: 0.009831438271185065 Training loss: 7.452398777008057
2025-12-09 12:45:40.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 518 LR: 0.009830635380734313 Training loss: 7.036809921264648
2025-12-09 12:45:40.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 519 LR: 0.009829830615608279 Training loss: 7.163064479827881
2025-12-09 12:45:41.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 520 LR: 0.009829023976119278 Training loss: 7.057746410369873
2025-12-09 12:45:41.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 521 LR: 0.009828215462580352 Training loss: 7.280259132385254
2025-12-09 12:45:42.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 522 LR: 0.009827405075305266 Training loss: 7.139554977416992
2025-12-09 12:45:42.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 523 LR: 0.009826592814608518 Training loss: 7.105749607086182
2025-12-09 12:45:43.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 524 LR: 0.00982577868080533 Training loss: 6.689247131347656
2025-12-09 12:45:43.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 525 LR: 0.009824962674211653 Training loss: 7.355653762817383
2025-12-09 12:45:44.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 526 LR: 0.009824144795144159 Training loss: 6.792923450469971
2025-12-09 12:45:44.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 527 LR: 0.009823325043920255 Training loss: 7.354985237121582
2025-12-09 12:45:45.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 528 LR: 0.009822503420858067 Training loss: 7.209160327911377
2025-12-09 12:45:45.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 529 LR: 0.009821679926276456 Training loss: 6.657129764556885
2025-12-09 12:45:46.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 530 LR: 0.009820854560494998 Training loss: 7.073250770568848
2025-12-09 12:45:46.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 531 LR: 0.009820027323834007 Training loss: 7.09492301940918
2025-12-09 12:45:47.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 532 LR: 0.009819198216614512 Training loss: 6.9752020835876465
2025-12-09 12:45:47.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 533 LR: 0.009818367239158278 Training loss: 7.031611919403076
2025-12-09 12:45:48.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 534 LR: 0.009817534391787787 Training loss: 6.626362323760986
2025-12-09 12:45:48.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 535 LR: 0.009816699674826256 Training loss: 6.96261739730835
2025-12-09 12:45:49.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 536 LR: 0.009815863088597618 Training loss: 7.255606174468994
2025-12-09 12:45:49.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 537 LR: 0.009815024633426537 Training loss: 6.774003982543945
2025-12-09 12:45:50.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 538 LR: 0.0098141843096384 Training loss: 7.1764702796936035
2025-12-09 12:45:50.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 539 LR: 0.009813342117559323 Training loss: 6.889792442321777
2025-12-09 12:45:51.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 540 LR: 0.009812498057516142 Training loss: 6.769056797027588
2025-12-09 12:45:51.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 541 LR: 0.009811652129836422 Training loss: 6.893653392791748
2025-12-09 12:45:52.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 542 LR: 0.009810804334848449 Training loss: 6.624764442443848
2025-12-09 12:45:52.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 543 LR: 0.009809954672881238 Training loss: 7.18032169342041
2025-12-09 12:45:53.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 544 LR: 0.009809103144264524 Training loss: 7.044913291931152
2025-12-09 12:45:53.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 545 LR: 0.009808249749328769 Training loss: 6.815013408660889
2025-12-09 12:45:54.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 546 LR: 0.009807394488405159 Training loss: 6.902885437011719
2025-12-09 12:45:54.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 547 LR: 0.009806537361825607 Training loss: 6.694851398468018
2025-12-09 12:45:55.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 548 LR: 0.009805678369922742 Training loss: 7.03082275390625
2025-12-09 12:45:55.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 549 LR: 0.009804817513029926 Training loss: 6.874844551086426
2025-12-09 12:45:56.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 550 LR: 0.009803954791481238 Training loss: 6.866637706756592
2025-12-09 12:45:56.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 551 LR: 0.009803090205611487 Training loss: 6.9855170249938965
2025-12-09 12:45:57.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 552 LR: 0.009802223755756198 Training loss: 7.213197708129883
2025-12-09 12:45:57.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 553 LR: 0.009801355442251625 Training loss: 6.963052749633789
2025-12-09 12:45:58.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 554 LR: 0.009800485265434745 Training loss: 6.727146625518799
2025-12-09 12:45:58.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 555 LR: 0.009799613225643253 Training loss: 6.846096038818359
2025-12-09 12:45:59.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 556 LR: 0.009798739323215573 Training loss: 6.682992458343506
2025-12-09 12:45:59.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 557 LR: 0.00979786355849085 Training loss: 6.924779415130615
2025-12-09 12:46:00.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 558 LR: 0.009796985931808949 Training loss: 7.61607027053833
2025-12-09 12:46:00.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 559 LR: 0.009796106443510462 Training loss: 7.165246486663818
2025-12-09 12:46:01.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 560 LR: 0.009795225093936702 Training loss: 7.56447696685791
2025-12-09 12:46:01.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 561 LR: 0.009794341883429699 Training loss: 6.727937698364258
2025-12-09 12:46:02.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 562 LR: 0.009793456812332214 Training loss: 7.218570232391357
2025-12-09 12:46:02.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 563 LR: 0.009792569880987725 Training loss: 7.072213649749756
2025-12-09 12:46:03.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 564 LR: 0.009791681089740432 Training loss: 6.857078552246094
2025-12-09 12:46:03.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 565 LR: 0.009790790438935257 Training loss: 6.990890026092529
2025-12-09 12:46:04.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 566 LR: 0.009789897928917846 Training loss: 6.991334915161133
2025-12-09 12:46:04.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 567 LR: 0.009789003560034561 Training loss: 6.816929817199707
2025-12-09 12:46:05.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 568 LR: 0.009788107332632493 Training loss: 6.953880786895752
2025-12-09 12:46:05.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 569 LR: 0.009787209247059453 Training loss: 7.184940338134766
2025-12-09 12:46:06.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 570 LR: 0.009786309303663962 Training loss: 7.171700477600098
2025-12-09 12:46:06.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 571 LR: 0.009785407502795277 Training loss: 7.10505485534668
2025-12-09 12:46:07.434 | INFO     | __main__:train:25 - Epoch: 0 Step: 572 LR: 0.009784503844803368 Training loss: 6.988964080810547
2025-12-09 12:46:07.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 573 LR: 0.009783598330038924 Training loss: 6.481257915496826
2025-12-09 12:46:08.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 574 LR: 0.009782690958853361 Training loss: 7.128876686096191
2025-12-09 12:46:08.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 575 LR: 0.009781781731598813 Training loss: 7.120203495025635
2025-12-09 12:46:09.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 576 LR: 0.00978087064862813 Training loss: 6.952917575836182
2025-12-09 12:46:09.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 577 LR: 0.009779957710294886 Training loss: 7.383388519287109
2025-12-09 12:46:10.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 578 LR: 0.009779042916953376 Training loss: 6.605326175689697
2025-12-09 12:46:10.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 579 LR: 0.009778126268958612 Training loss: 7.111917972564697
2025-12-09 12:46:11.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 580 LR: 0.009777207766666329 Training loss: 6.864431858062744
2025-12-09 12:46:11.935 | INFO     | __main__:train:25 - Epoch: 0 Step: 581 LR: 0.00977628741043298 Training loss: 7.009601593017578
2025-12-09 12:46:12.435 | INFO     | __main__:train:25 - Epoch: 0 Step: 582 LR: 0.009775365200615735 Training loss: 6.771430492401123
2025-12-09 12:46:12.934 | INFO     | __main__:train:25 - Epoch: 0 Step: 583 LR: 0.009774441137572488 Training loss: 6.862941265106201
2025-12-09 12:46:13.433 | INFO     | __main__:train:25 - Epoch: 0 Step: 584 LR: 0.009773515221661847 Training loss: 7.211367130279541
2025-12-09 12:46:13.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 585 LR: 0.009772587453243142 Training loss: 7.087372779846191
2025-12-09 12:46:14.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 586 LR: 0.009771657832676426 Training loss: 7.1196489334106445
2025-12-09 12:46:14.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 587 LR: 0.009770726360322463 Training loss: 7.670738697052002
2025-12-09 12:46:15.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 588 LR: 0.009769793036542742 Training loss: 6.6284918785095215
2025-12-09 12:46:15.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 589 LR: 0.009768857861699462 Training loss: 7.503938674926758
2025-12-09 12:46:16.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 590 LR: 0.009767920836155552 Training loss: 6.690702438354492
2025-12-09 12:46:16.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 591 LR: 0.009766981960274652 Training loss: 6.508106231689453
2025-12-09 12:46:17.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 592 LR: 0.009766041234421121 Training loss: 6.947000980377197
2025-12-09 12:46:17.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 593 LR: 0.009765098658960036 Training loss: 6.8759355545043945
2025-12-09 12:46:18.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 594 LR: 0.00976415423425719 Training loss: 6.650432109832764
2025-12-09 12:46:18.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 595 LR: 0.0097632079606791 Training loss: 7.093774795532227
2025-12-09 12:46:19.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 596 LR: 0.009762259838592994 Training loss: 6.955671787261963
2025-12-09 12:46:19.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 597 LR: 0.009761309868366819 Training loss: 7.328284740447998
2025-12-09 12:46:20.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 598 LR: 0.009760358050369242 Training loss: 7.22489070892334
2025-12-09 12:46:20.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 599 LR: 0.009759404384969644 Training loss: 6.639915943145752
2025-12-09 12:46:21.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 600 LR: 0.009758448872538121 Training loss: 7.117063045501709
2025-12-09 12:46:21.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 601 LR: 0.009757491513445493 Training loss: 6.846997261047363
2025-12-09 12:46:22.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 602 LR: 0.009756532308063294 Training loss: 7.3877410888671875
2025-12-09 12:46:22.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 603 LR: 0.009755571256763764 Training loss: 6.873599529266357
2025-12-09 12:46:23.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 604 LR: 0.009754608359919878 Training loss: 6.59032678604126
2025-12-09 12:46:23.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 605 LR: 0.009753643617905313 Training loss: 6.715965270996094
2025-12-09 12:46:24.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 606 LR: 0.009752677031094465 Training loss: 7.014976501464844
2025-12-09 12:46:24.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 607 LR: 0.009751708599862451 Training loss: 7.382742881774902
2025-12-09 12:46:25.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 608 LR: 0.009750738324585098 Training loss: 7.03806209564209
2025-12-09 12:46:25.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 609 LR: 0.009749766205638952 Training loss: 7.343587398529053
2025-12-09 12:46:26.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 610 LR: 0.009748792243401274 Training loss: 6.814128398895264
2025-12-09 12:46:26.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 611 LR: 0.009747816438250036 Training loss: 6.9510650634765625
2025-12-09 12:46:27.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 612 LR: 0.009746838790563934 Training loss: 7.188531875610352
2025-12-09 12:46:27.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 613 LR: 0.009745859300722371 Training loss: 7.305503845214844
2025-12-09 12:46:28.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 614 LR: 0.009744877969105468 Training loss: 6.852449893951416
2025-12-09 12:46:28.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 615 LR: 0.009743894796094062 Training loss: 6.9766716957092285
2025-12-09 12:46:29.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 616 LR: 0.009742909782069702 Training loss: 7.937979698181152
2025-12-09 12:46:29.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 617 LR: 0.009741922927414652 Training loss: 7.1059393882751465
2025-12-09 12:46:30.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 618 LR: 0.009740934232511893 Training loss: 7.553287982940674
2025-12-09 12:46:30.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 619 LR: 0.009739943697745118 Training loss: 6.499965190887451
2025-12-09 12:46:31.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 620 LR: 0.009738951323498732 Training loss: 7.011884689331055
2025-12-09 12:46:31.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 621 LR: 0.009737957110157859 Training loss: 7.085821628570557
2025-12-09 12:46:32.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 622 LR: 0.009736961058108331 Training loss: 6.5639753341674805
2025-12-09 12:46:32.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 623 LR: 0.009735963167736698 Training loss: 6.6356682777404785
2025-12-09 12:46:33.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 624 LR: 0.009734963439430222 Training loss: 6.263533115386963
2025-12-09 12:46:33.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 625 LR: 0.009733961873576877 Training loss: 6.536466598510742
2025-12-09 12:46:34.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 626 LR: 0.009732958470565352 Training loss: 6.434812068939209
2025-12-09 12:46:34.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 627 LR: 0.009731953230785049 Training loss: 7.029122352600098
2025-12-09 12:46:35.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 628 LR: 0.009730946154626078 Training loss: 6.695685863494873
2025-12-09 12:46:35.933 | INFO     | __main__:train:25 - Epoch: 0 Step: 629 LR: 0.00972993724247927 Training loss: 6.680463790893555
2025-12-09 12:46:36.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 630 LR: 0.009728926494736164 Training loss: 6.623676300048828
2025-12-09 12:46:36.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 631 LR: 0.009727913911789008 Training loss: 6.5822224617004395
2025-12-09 12:46:37.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 632 LR: 0.009726899494030768 Training loss: 6.499149322509766
2025-12-09 12:46:37.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 633 LR: 0.009725883241855119 Training loss: 6.988124847412109
2025-12-09 12:46:38.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 634 LR: 0.009724865155656449 Training loss: 6.486441135406494
2025-12-09 12:46:38.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 635 LR: 0.009723845235829857 Training loss: 7.53091287612915
2025-12-09 12:46:39.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 636 LR: 0.009722823482771155 Training loss: 6.8653669357299805
2025-12-09 12:46:39.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 637 LR: 0.009721799896876864 Training loss: 6.918842792510986
2025-12-09 12:46:40.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 638 LR: 0.009720774478544218 Training loss: 7.134929180145264
2025-12-09 12:46:40.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 639 LR: 0.009719747228171163 Training loss: 6.6206583976745605
2025-12-09 12:46:41.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 640 LR: 0.009718718146156354 Training loss: 6.876285076141357
2025-12-09 12:46:41.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 641 LR: 0.00971768723289916 Training loss: 6.899353504180908
2025-12-09 12:46:42.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 642 LR: 0.009716654488799652 Training loss: 6.819641590118408
2025-12-09 12:46:42.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 643 LR: 0.009715619914258624 Training loss: 6.8068037033081055
2025-12-09 12:46:43.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 644 LR: 0.00971458350967757 Training loss: 6.964560031890869
2025-12-09 12:46:43.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 645 LR: 0.009713545275458703 Training loss: 7.100711345672607
2025-12-09 12:46:44.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 646 LR: 0.009712505212004938 Training loss: 6.286995887756348
2025-12-09 12:46:44.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 647 LR: 0.009711463319719903 Training loss: 6.906829357147217
2025-12-09 12:46:45.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 648 LR: 0.009710419599007938 Training loss: 6.850066661834717
2025-12-09 12:46:45.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 649 LR: 0.009709374050274088 Training loss: 6.973238468170166
2025-12-09 12:46:46.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 650 LR: 0.009708326673924114 Training loss: 7.4830522537231445
2025-12-09 12:46:46.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 651 LR: 0.009707277470364482 Training loss: 7.652136325836182
2025-12-09 12:46:47.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 652 LR: 0.009706226440002363 Training loss: 6.757518291473389
2025-12-09 12:46:47.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 653 LR: 0.009705173583245644 Training loss: 7.340140342712402
2025-12-09 12:46:48.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 654 LR: 0.009704118900502918 Training loss: 7.601779937744141
2025-12-09 12:46:48.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 655 LR: 0.009703062392183489 Training loss: 6.8100481033325195
2025-12-09 12:46:49.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 656 LR: 0.009702004058697363 Training loss: 6.849939823150635
2025-12-09 12:46:49.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 657 LR: 0.00970094390045526 Training loss: 6.528460502624512
2025-12-09 12:46:50.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 658 LR: 0.00969988191786861 Training loss: 6.7826080322265625
2025-12-09 12:46:50.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 659 LR: 0.009698818111349544 Training loss: 7.063129901885986
2025-12-09 12:46:51.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 660 LR: 0.009697752481310905 Training loss: 6.7569193840026855
2025-12-09 12:46:51.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 661 LR: 0.009696685028166244 Training loss: 6.804859638214111
2025-12-09 12:46:52.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 662 LR: 0.00969561575232982 Training loss: 7.349950313568115
2025-12-09 12:46:52.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 663 LR: 0.009694544654216595 Training loss: 7.048714637756348
2025-12-09 12:46:53.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 664 LR: 0.009693471734242244 Training loss: 7.397242069244385
2025-12-09 12:46:53.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 665 LR: 0.009692396992823146 Training loss: 6.779159069061279
2025-12-09 12:46:54.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 666 LR: 0.009691320430376385 Training loss: 6.604461669921875
2025-12-09 12:46:54.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 667 LR: 0.009690242047319756 Training loss: 7.05154275894165
2025-12-09 12:46:55.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 668 LR: 0.009689161844071757 Training loss: 7.159306049346924
2025-12-09 12:46:55.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 669 LR: 0.009688079821051594 Training loss: 6.9615559577941895
2025-12-09 12:46:56.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 670 LR: 0.009686995978679181 Training loss: 6.80505895614624
2025-12-09 12:46:56.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 671 LR: 0.009685910317375132 Training loss: 6.794353485107422
2025-12-09 12:46:57.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 672 LR: 0.009684822837560777 Training loss: 7.0748066902160645
2025-12-09 12:46:57.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 673 LR: 0.00968373353965814 Training loss: 6.898841381072998
2025-12-09 12:46:58.422 | INFO     | __main__:train:25 - Epoch: 0 Step: 674 LR: 0.009682642424089958 Training loss: 6.772862434387207
2025-12-09 12:46:58.922 | INFO     | __main__:train:25 - Epoch: 0 Step: 675 LR: 0.009681549491279673 Training loss: 7.249188423156738
2025-12-09 12:46:59.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 676 LR: 0.00968045474165143 Training loss: 6.85060453414917
2025-12-09 12:46:59.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 677 LR: 0.00967935817563008 Training loss: 6.699539661407471
2025-12-09 12:47:00.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 678 LR: 0.00967825979364118 Training loss: 6.208042144775391
2025-12-09 12:47:00.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 679 LR: 0.009677159596110986 Training loss: 6.649138927459717
2025-12-09 12:47:01.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 680 LR: 0.009676057583466471 Training loss: 7.252166748046875
2025-12-09 12:47:01.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 681 LR: 0.009674953756135297 Training loss: 7.106296539306641
2025-12-09 12:47:02.423 | INFO     | __main__:train:25 - Epoch: 0 Step: 682 LR: 0.009673848114545842 Training loss: 7.241261005401611
2025-12-09 12:47:02.923 | INFO     | __main__:train:25 - Epoch: 0 Step: 683 LR: 0.009672740659127184 Training loss: 7.276160717010498
2025-12-09 12:47:03.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 684 LR: 0.009671631390309103 Training loss: 6.9707350730896
2025-12-09 12:47:03.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 685 LR: 0.009670520308522083 Training loss: 7.080225467681885
2025-12-09 12:47:04.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 686 LR: 0.009669407414197318 Training loss: 6.103435039520264
2025-12-09 12:47:04.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 687 LR: 0.009668292707766698 Training loss: 7.082108497619629
2025-12-09 12:47:05.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 688 LR: 0.009667176189662818 Training loss: 6.2090044021606445
2025-12-09 12:47:05.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 689 LR: 0.009666057860318978 Training loss: 6.79522705078125
2025-12-09 12:47:06.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 690 LR: 0.00966493772016918 Training loss: 6.930951118469238
2025-12-09 12:47:06.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 691 LR: 0.009663815769648127 Training loss: 7.07249641418457
2025-12-09 12:47:07.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 692 LR: 0.00966269200919123 Training loss: 6.803319931030273
2025-12-09 12:47:07.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 693 LR: 0.009661566439234593 Training loss: 7.294331073760986
2025-12-09 12:47:08.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 694 LR: 0.00966043906021503 Training loss: 7.429880619049072
2025-12-09 12:47:08.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 695 LR: 0.009659309872570057 Training loss: 6.696549892425537
2025-12-09 12:47:09.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 696 LR: 0.009658178876737887 Training loss: 6.543484687805176
2025-12-09 12:47:09.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 697 LR: 0.009657046073157436 Training loss: 7.347438335418701
2025-12-09 12:47:10.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 698 LR: 0.009655911462268327 Training loss: 6.931416988372803
2025-12-09 12:47:10.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 699 LR: 0.00965477504451088 Training loss: 6.827164649963379
2025-12-09 12:47:11.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 700 LR: 0.009653636820326113 Training loss: 6.814478397369385
2025-12-09 12:47:11.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 701 LR: 0.009652496790155752 Training loss: 6.763557434082031
2025-12-09 12:47:12.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 702 LR: 0.009651354954442217 Training loss: 6.743243217468262
2025-12-09 12:47:12.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 703 LR: 0.009650211313628636 Training loss: 6.577255725860596
2025-12-09 12:47:13.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 704 LR: 0.009649065868158831 Training loss: 6.5556111335754395
2025-12-09 12:47:13.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 705 LR: 0.009647918618477328 Training loss: 6.799378395080566
2025-12-09 12:47:14.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 706 LR: 0.009646769565029354 Training loss: 6.710133075714111
2025-12-09 12:47:14.930 | INFO     | __main__:train:25 - Epoch: 0 Step: 707 LR: 0.00964561870826083 Training loss: 6.859903335571289
2025-12-09 12:47:15.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 708 LR: 0.009644466048618386 Training loss: 6.303062915802002
2025-12-09 12:47:15.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 709 LR: 0.009643311586549342 Training loss: 6.769557476043701
2025-12-09 12:47:16.427 | INFO     | __main__:train:25 - Epoch: 0 Step: 710 LR: 0.009642155322501724 Training loss: 6.9284868240356445
2025-12-09 12:47:16.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 711 LR: 0.009640997256924256 Training loss: 7.220927715301514
2025-12-09 12:47:17.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 712 LR: 0.009639837390266361 Training loss: 7.465236663818359
2025-12-09 12:47:17.932 | INFO     | __main__:train:25 - Epoch: 0 Step: 713 LR: 0.009638675722978161 Training loss: 6.917313098907471
2025-12-09 12:47:18.432 | INFO     | __main__:train:25 - Epoch: 0 Step: 714 LR: 0.009637512255510475 Training loss: 6.930831432342529
2025-12-09 12:47:18.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 715 LR: 0.009636346988314821 Training loss: 6.9777984619140625
2025-12-09 12:47:19.431 | INFO     | __main__:train:25 - Epoch: 0 Step: 716 LR: 0.009635179921843418 Training loss: 7.504559516906738
2025-12-09 12:47:19.931 | INFO     | __main__:train:25 - Epoch: 0 Step: 717 LR: 0.009634011056549182 Training loss: 6.980488300323486
2025-12-09 12:47:20.430 | INFO     | __main__:train:25 - Epoch: 0 Step: 718 LR: 0.009632840392885726 Training loss: 6.70923376083374
2025-12-09 12:47:20.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 719 LR: 0.009631667931307365 Training loss: 6.970551013946533
2025-12-09 12:47:21.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 720 LR: 0.009630493672269102 Training loss: 6.969278335571289
2025-12-09 12:47:21.926 | INFO     | __main__:train:25 - Epoch: 0 Step: 721 LR: 0.009629317616226648 Training loss: 6.9733357429504395
2025-12-09 12:47:22.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 722 LR: 0.009628139763636408 Training loss: 7.232212066650391
2025-12-09 12:47:22.924 | INFO     | __main__:train:25 - Epoch: 0 Step: 723 LR: 0.009626960114955483 Training loss: 7.351742744445801
2025-12-09 12:47:23.424 | INFO     | __main__:train:25 - Epoch: 0 Step: 724 LR: 0.009625778670641669 Training loss: 7.0244951248168945
2025-12-09 12:47:23.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 725 LR: 0.009624595431153467 Training loss: 7.823200225830078
2025-12-09 12:47:24.429 | INFO     | __main__:train:25 - Epoch: 0 Step: 726 LR: 0.009623410396950064 Training loss: 6.690030097961426
2025-12-09 12:47:24.929 | INFO     | __main__:train:25 - Epoch: 0 Step: 727 LR: 0.009622223568491349 Training loss: 7.730260848999023
2025-12-09 12:47:25.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 728 LR: 0.00962103494623791 Training loss: 6.795243740081787
2025-12-09 12:47:25.927 | INFO     | __main__:train:25 - Epoch: 0 Step: 729 LR: 0.009619844530651026 Training loss: 6.926873683929443
2025-12-09 12:47:26.426 | INFO     | __main__:train:25 - Epoch: 0 Step: 730 LR: 0.009618652322192675 Training loss: 6.904688358306885
2025-12-09 12:47:26.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 731 LR: 0.00961745832132553 Training loss: 6.86143684387207
2025-12-09 12:47:27.425 | INFO     | __main__:train:25 - Epoch: 0 Step: 732 LR: 0.009616262528512956 Training loss: 7.063655853271484
2025-12-09 12:47:27.925 | INFO     | __main__:train:25 - Epoch: 0 Step: 733 LR: 0.009615064944219022 Training loss: 7.114069938659668
2025-12-09 12:47:28.428 | INFO     | __main__:train:25 - Epoch: 0 Step: 734 LR: 0.009613865568908484 Training loss: 6.534045696258545
2025-12-09 12:47:28.928 | INFO     | __main__:train:25 - Epoch: 0 Step: 735 LR: 0.009612664403046797 Training loss: 7.032340049743652
